{
  "data": [
    {
      "_id": "628fa6b4256da2c054d06a66",
      "alm_examples": [
        {
          "api_version": "infoscale.veritas.com/v1",
          "kind": "InfoScaleCluster",
          "metadata": {
            "name": "infoscalecluster-dev"
          },
          "spec": {
            "clusterInfo": [
              {},
              {}
            ],
            "version": "8.0.30"
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/veritas-technologies/infoscale-operator-bundle@sha256:b4c0c14990302bce27404b1659575fd3c507fb514fc546d2d141e48337d39072",
      "bundle_path_digest": "sha256:b4c0c14990302bce27404b1659575fd3c507fb514fc546d2d141e48337d39072",
      "capabilities": [
        "Seamless Upgrades"
      ],
      "channel_name": "stable",
      "creation_date": "2022-05-26T16:11:32.751000+00:00",
      "csv_description": "# InfoScale\u2122 SDS Operator\n\nInfoScale\u2122 SDS Operator manages the lifecycle of the InfoScale\u2122 cluster\n\n## Overview\n\n- Veritas InfoScale\u2122 delivers Infrastructure resiliency and persistent storage for your critical containerized applications for OpenShift\u00ae and Kubernetes Native deployments\n- Engineered to support stateful workloads generated for mission-critical containerized applications.\n\n---\n\n## Data Services & Benefits\n\n**1. Software-Defined Persistent Storage Volumes:** Enables customers to support multiple container application requirements leveraging existing SAN or DAS storage\n\n**2. CSI API Driver:** Facilitates static and dynamic provisioning for applications with RWX, RWO and ROX access modes\n\n**3. Life Cycle Management:** Enables automated deployment, configuration and upgrades of InfoScale Software-defined container images. Certified and Integrated with Red Hat OpenShift for a single-click deployment\n\n**4. Availability:** Provides scaling, mounting and/or movement of InfoScale persistent storage volumes on cluster nodes with minimal disruption\n\n**5. Data Integrity:** Prevents data corruption by allowing only the active cluster nodes to write to the volume. The I/O fencing feature recovers from cluster disruptions quickly by ensuring that application pods are moved to another node to continue normal operations\n\n**6. Point-in-Time Data Copies:** Create snapshots of Persistent Volumes for backup products, data analytics or forensic discovery and analysis\n\n**7. Disaster Recovery (DR) Tech Preview:** This DR feature provides the ability to test and validate disaster recovery capabilities by migrating Kubernetes cluster metadata and application components to peer cluster in case of a local or remote disaster\n\n---\n\n## Pre-requisites\n\n- [Please refer to pre-requisite section from official documentation](https://www.veritas.com/support/en_US/doc/151215298-151215302-0)\n\n\n## InfoScale Cluster custom resource\n\n```\nkind: InfoScaleCluster\nmetadata:\n  name:  < Infoscale Cluster Name >\n\nspec:\n  version: \"8.0.30\"\n\n  clusterInfo:\n  - nodeName: <Name of the first node>\n  ip:\n  - <Optional - First IP address of the first node >\n  - <Optional - Second IP address of the first node>\n  excludeDevice:\n  - <Optional - Device path of the disk on the node that you want to exclude from Infoscale disk group.>\n\n  - nodeName: <Name of the second node>\n  ip:\n  - <Optional - First IP address of the second node >\n  - <Optional - Second IP address of the second node>\n  excludeDevice:\n  - <Optional - Device path of the disk on the node that you want to exclude from Infoscale disk group.>\n\n  # You can add up to 16 nodes.\n\n  licenseServer: < Optional - License server name/IP address >\n  licensePort: < Optional - License port number >\n\n  customImageRegistry: < Optional \u2013 Registry for Infoscale Container images>\n\n```\n\n#### Note\n\nYou can specify up to 16 worker nodes in CR. Although cluster configuration is allowed even with one Network Interface Card,\nVeritas recommends a minimum of two physical links for performance and High Availability (HA). Number of links for each network link must be same on all\nnodes. Optionally, you can enter node level IP addresses. If IP addresses are not provided, IP addresses of OpenShift cluster nodes are used.\n",
      "csv_display_name": "InfoScale\u2122 SDS Operator",
      "csv_metadata_description": "InfoScale\u2122 SDS Operator manages the lifecycle of the InfoScale\u2122 cluster",
      "csv_name": "infoscale-operator.v8.0.32",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": false,
          "type": "OwnNamespace"
        },
        {
          "supported": false,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-11-23T08:32:37.072000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.10",
      "organization": "certified-operators",
      "package": "infoscale-operator",
      "provided_apis": [
        {
          "group": "infoscale.veritas.com",
          "kind": "InfoScaleCluster",
          "plural": "infoscaleclusters",
          "version": "v1"
        }
      ],
      "provider": "Veritas Technologies LLC",
      "related_images": [
        {
          "digest": "sha256:5d2b00432d31159577cd3f423a40bc3cc79d033447032fe51905074ccf961c16",
          "image": "registry.connect.redhat.com/veritas-technologies/infoscale-operator@sha256:5d2b00432d31159577cd3f423a40bc3cc79d033447032fe51905074ccf961c16",
          "name": "manager"
        },
        {
          "digest": "sha256:5d2b00432d31159577cd3f423a40bc3cc79d033447032fe51905074ccf961c16",
          "image": "registry.connect.redhat.com/veritas-technologies/infoscale-operator@sha256:5d2b00432d31159577cd3f423a40bc3cc79d033447032fe51905074ccf961c16",
          "name": "pre-manager"
        },
        {
          "digest": "sha256:5d2b00432d31159577cd3f423a40bc3cc79d033447032fe51905074ccf961c16",
          "image": "registry.connect.redhat.com/veritas-technologies/infoscale-operator@sha256:5d2b00432d31159577cd3f423a40bc3cc79d033447032fe51905074ccf961c16",
          "name": "infoscale-operator-5d2b00432d31159577cd3f423a40bc3cc79d033447032fe51905074ccf961c16-annotation"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.10",
      "version": "8.0.32",
      "version_original": "8.0.32"
    },
    {
      "_id": "628fa6cb89e0a10bc8120089",
      "alm_examples": [
        {
          "api_version": "infoscale.veritas.com/v1",
          "kind": "InfoScaleCluster",
          "metadata": {
            "name": "infoscalecluster-dev"
          },
          "spec": {
            "clusterInfo": [
              {},
              {}
            ],
            "version": "8.0.30"
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/veritas-technologies/infoscale-operator-bundle@sha256:b4c0c14990302bce27404b1659575fd3c507fb514fc546d2d141e48337d39072",
      "bundle_path_digest": "sha256:b4c0c14990302bce27404b1659575fd3c507fb514fc546d2d141e48337d39072",
      "capabilities": [
        "Seamless Upgrades"
      ],
      "channel_name": "stable",
      "creation_date": "2022-05-26T16:11:55.509000+00:00",
      "csv_description": "# InfoScale\u2122 SDS Operator\n\nInfoScale\u2122 SDS Operator manages the lifecycle of the InfoScale\u2122 cluster\n\n## Overview\n\n- Veritas InfoScale\u2122 delivers Infrastructure resiliency and persistent storage for your critical containerized applications for OpenShift\u00ae and Kubernetes Native deployments\n- Engineered to support stateful workloads generated for mission-critical containerized applications.\n\n---\n\n## Data Services & Benefits\n\n**1. Software-Defined Persistent Storage Volumes:** Enables customers to support multiple container application requirements leveraging existing SAN or DAS storage\n\n**2. CSI API Driver:** Facilitates static and dynamic provisioning for applications with RWX, RWO and ROX access modes\n\n**3. Life Cycle Management:** Enables automated deployment, configuration and upgrades of InfoScale Software-defined container images. Certified and Integrated with Red Hat OpenShift for a single-click deployment\n\n**4. Availability:** Provides scaling, mounting and/or movement of InfoScale persistent storage volumes on cluster nodes with minimal disruption\n\n**5. Data Integrity:** Prevents data corruption by allowing only the active cluster nodes to write to the volume. The I/O fencing feature recovers from cluster disruptions quickly by ensuring that application pods are moved to another node to continue normal operations\n\n**6. Point-in-Time Data Copies:** Create snapshots of Persistent Volumes for backup products, data analytics or forensic discovery and analysis\n\n**7. Disaster Recovery (DR) Tech Preview:** This DR feature provides the ability to test and validate disaster recovery capabilities by migrating Kubernetes cluster metadata and application components to peer cluster in case of a local or remote disaster\n\n---\n\n## Pre-requisites\n\n- [Please refer to pre-requisite section from official documentation](https://www.veritas.com/support/en_US/doc/151215298-151215302-0)\n\n\n## InfoScale Cluster custom resource\n\n```\nkind: InfoScaleCluster\nmetadata:\n  name:  < Infoscale Cluster Name >\n\nspec:\n  version: \"8.0.30\"\n\n  clusterInfo:\n  - nodeName: <Name of the first node>\n  ip:\n  - <Optional - First IP address of the first node >\n  - <Optional - Second IP address of the first node>\n  excludeDevice:\n  - <Optional - Device path of the disk on the node that you want to exclude from Infoscale disk group.>\n\n  - nodeName: <Name of the second node>\n  ip:\n  - <Optional - First IP address of the second node >\n  - <Optional - Second IP address of the second node>\n  excludeDevice:\n  - <Optional - Device path of the disk on the node that you want to exclude from Infoscale disk group.>\n\n  # You can add up to 16 nodes.\n\n  licenseServer: < Optional - License server name/IP address >\n  licensePort: < Optional - License port number >\n\n  customImageRegistry: < Optional \u2013 Registry for Infoscale Container images>\n\n```\n\n#### Note\n\nYou can specify up to 16 worker nodes in CR. Although cluster configuration is allowed even with one Network Interface Card,\nVeritas recommends a minimum of two physical links for performance and High Availability (HA). Number of links for each network link must be same on all\nnodes. Optionally, you can enter node level IP addresses. If IP addresses are not provided, IP addresses of OpenShift cluster nodes are used.\n",
      "csv_display_name": "InfoScale\u2122 SDS Operator",
      "csv_metadata_description": "InfoScale\u2122 SDS Operator manages the lifecycle of the InfoScale\u2122 cluster",
      "csv_name": "infoscale-operator.v8.0.32",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": false,
          "type": "OwnNamespace"
        },
        {
          "supported": false,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-11-23T07:16:31.981000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.9",
      "organization": "certified-operators",
      "package": "infoscale-operator",
      "provided_apis": [
        {
          "group": "infoscale.veritas.com",
          "kind": "InfoScaleCluster",
          "plural": "infoscaleclusters",
          "version": "v1"
        }
      ],
      "provider": "Veritas Technologies LLC",
      "related_images": [
        {
          "digest": "sha256:5d2b00432d31159577cd3f423a40bc3cc79d033447032fe51905074ccf961c16",
          "image": "registry.connect.redhat.com/veritas-technologies/infoscale-operator@sha256:5d2b00432d31159577cd3f423a40bc3cc79d033447032fe51905074ccf961c16",
          "name": "manager"
        },
        {
          "digest": "sha256:5d2b00432d31159577cd3f423a40bc3cc79d033447032fe51905074ccf961c16",
          "image": "registry.connect.redhat.com/veritas-technologies/infoscale-operator@sha256:5d2b00432d31159577cd3f423a40bc3cc79d033447032fe51905074ccf961c16",
          "name": "pre-manager"
        },
        {
          "digest": "sha256:5d2b00432d31159577cd3f423a40bc3cc79d033447032fe51905074ccf961c16",
          "image": "registry.connect.redhat.com/veritas-technologies/infoscale-operator@sha256:5d2b00432d31159577cd3f423a40bc3cc79d033447032fe51905074ccf961c16",
          "name": "infoscale-operator-5d2b00432d31159577cd3f423a40bc3cc79d033447032fe51905074ccf961c16-annotation"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.9",
      "version": "8.0.32",
      "version_original": "8.0.32"
    },
    {
      "_id": "628faa4b89e0a10bc812018a",
      "alm_examples": [
        {
          "api_version": "infoscale.veritas.com/v1",
          "kind": "InfoScaleCluster",
          "metadata": {
            "name": "infoscalecluster-dev"
          },
          "spec": {
            "clusterInfo": [
              {},
              {}
            ],
            "version": "8.0.30"
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/veritas-technologies/infoscale-operator-bundle@sha256:b4c0c14990302bce27404b1659575fd3c507fb514fc546d2d141e48337d39072",
      "bundle_path_digest": "sha256:b4c0c14990302bce27404b1659575fd3c507fb514fc546d2d141e48337d39072",
      "capabilities": [
        "Seamless Upgrades"
      ],
      "channel_name": "stable",
      "creation_date": "2022-05-26T16:26:51.856000+00:00",
      "csv_description": "# InfoScale\u2122 SDS Operator\n\nInfoScale\u2122 SDS Operator manages the lifecycle of the InfoScale\u2122 cluster\n\n## Overview\n\n- Veritas InfoScale\u2122 delivers Infrastructure resiliency and persistent storage for your critical containerized applications for OpenShift\u00ae and Kubernetes Native deployments\n- Engineered to support stateful workloads generated for mission-critical containerized applications.\n\n---\n\n## Data Services & Benefits\n\n**1. Software-Defined Persistent Storage Volumes:** Enables customers to support multiple container application requirements leveraging existing SAN or DAS storage\n\n**2. CSI API Driver:** Facilitates static and dynamic provisioning for applications with RWX, RWO and ROX access modes\n\n**3. Life Cycle Management:** Enables automated deployment, configuration and upgrades of InfoScale Software-defined container images. Certified and Integrated with Red Hat OpenShift for a single-click deployment\n\n**4. Availability:** Provides scaling, mounting and/or movement of InfoScale persistent storage volumes on cluster nodes with minimal disruption\n\n**5. Data Integrity:** Prevents data corruption by allowing only the active cluster nodes to write to the volume. The I/O fencing feature recovers from cluster disruptions quickly by ensuring that application pods are moved to another node to continue normal operations\n\n**6. Point-in-Time Data Copies:** Create snapshots of Persistent Volumes for backup products, data analytics or forensic discovery and analysis\n\n**7. Disaster Recovery (DR) Tech Preview:** This DR feature provides the ability to test and validate disaster recovery capabilities by migrating Kubernetes cluster metadata and application components to peer cluster in case of a local or remote disaster\n\n---\n\n## Pre-requisites\n\n- [Please refer to pre-requisite section from official documentation](https://www.veritas.com/support/en_US/doc/151215298-151215302-0)\n\n\n## InfoScale Cluster custom resource\n\n```\nkind: InfoScaleCluster\nmetadata:\n  name:  < Infoscale Cluster Name >\n\nspec:\n  version: \"8.0.30\"\n\n  clusterInfo:\n  - nodeName: <Name of the first node>\n  ip:\n  - <Optional - First IP address of the first node >\n  - <Optional - Second IP address of the first node>\n  excludeDevice:\n  - <Optional - Device path of the disk on the node that you want to exclude from Infoscale disk group.>\n\n  - nodeName: <Name of the second node>\n  ip:\n  - <Optional - First IP address of the second node >\n  - <Optional - Second IP address of the second node>\n  excludeDevice:\n  - <Optional - Device path of the disk on the node that you want to exclude from Infoscale disk group.>\n\n  # You can add up to 16 nodes.\n\n  licenseServer: < Optional - License server name/IP address >\n  licensePort: < Optional - License port number >\n\n  customImageRegistry: < Optional \u2013 Registry for Infoscale Container images>\n\n```\n\n#### Note\n\nYou can specify up to 16 worker nodes in CR. Although cluster configuration is allowed even with one Network Interface Card,\nVeritas recommends a minimum of two physical links for performance and High Availability (HA). Number of links for each network link must be same on all\nnodes. Optionally, you can enter node level IP addresses. If IP addresses are not provided, IP addresses of OpenShift cluster nodes are used.\n",
      "csv_display_name": "InfoScale\u2122 SDS Operator",
      "csv_metadata_description": "InfoScale\u2122 SDS Operator manages the lifecycle of the InfoScale\u2122 cluster",
      "csv_name": "infoscale-operator.v8.0.32",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": false,
          "type": "OwnNamespace"
        },
        {
          "supported": false,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-11-23T07:39:19.145000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.8",
      "organization": "certified-operators",
      "package": "infoscale-operator",
      "provided_apis": [
        {
          "group": "infoscale.veritas.com",
          "kind": "InfoScaleCluster",
          "version": "v1"
        }
      ],
      "provider": "Veritas Technologies LLC",
      "related_images": [
        {
          "digest": "sha256:5d2b00432d31159577cd3f423a40bc3cc79d033447032fe51905074ccf961c16",
          "image": "registry.connect.redhat.com/veritas-technologies/infoscale-operator@sha256:5d2b00432d31159577cd3f423a40bc3cc79d033447032fe51905074ccf961c16",
          "name": "manager"
        },
        {
          "digest": "sha256:5d2b00432d31159577cd3f423a40bc3cc79d033447032fe51905074ccf961c16",
          "image": "registry.connect.redhat.com/veritas-technologies/infoscale-operator@sha256:5d2b00432d31159577cd3f423a40bc3cc79d033447032fe51905074ccf961c16",
          "name": "pre-manager"
        },
        {
          "digest": "sha256:5d2b00432d31159577cd3f423a40bc3cc79d033447032fe51905074ccf961c16",
          "image": "registry.connect.redhat.com/veritas-technologies/infoscale-operator@sha256:5d2b00432d31159577cd3f423a40bc3cc79d033447032fe51905074ccf961c16",
          "name": "infoscale-operator-5d2b00432d31159577cd3f423a40bc3cc79d033447032fe51905074ccf961c16-annotation"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.8",
      "version": "8.0.32",
      "version_original": "8.0.32"
    },
    {
      "_id": "6292f9e9301a731de19f1ee5",
      "alm_examples": [
        {
          "api_version": "apps.ibmz.com/v1alpha1",
          "kind": "MongoDB",
          "metadata": {
            "name": "mongodb-sample"
          },
          "spec": {
            "affinity": {},
            "autoscaling": {
              "enabled": false,
              "maxReplicas": 100,
              "minReplicas": 1,
              "targetCPUUtilizationPercentage": 80
            },
            "database": {
              "adminpassword": "snowball123",
              "adminuser": "adminuser",
              "name_database": "mydb"
            },
            "fullnameOverride": "",
            "global": {
              "license": true,
              "persistence": {
                "claims": {
                  "accessMode": "ReadWriteMany",
                  "capacity": 10,
                  "capacityUnit": "Gi",
                  "mountPath": "/data/db/",
                  "name": "mongo-pv-claim",
                  "storageClassName": "managed-nfs-storage"
                },
                "securityContext": {
                  "fsGroup": 0,
                  "supplementalGroup": 0
                }
              }
            },
            "image": {
              "pullPolicy": "Always",
              "repository": "quay.io/ibm/ibmz-mongodb-enterprise-database@sha256:6ddfec49c1682283b031168fa9a2e09c5746bb1288b6fc4b6f72feb49db35195"
            },
            "imagePullSecretRef": "registry-pull-secret",
            "imagePullSecrets": [
              {
                "name": "registry-pull-secret"
              }
            ],
            "ingress": {
              "annotations": {},
              "controller": "nginx",
              "tls": []
            },
            "name": "mongodb",
            "nameOverride": "",
            "namespaceList": [],
            "namespaced": true,
            "nodeSelector": {},
            "podSecurity": {
              "enabled": true,
              "securityContext": {
                "fsGroup": 1000650000,
                "runAsNonRoot": true,
                "runAsUser": 1000650000
              }
            },
            "replicaCount": 1,
            "resources": {
              "limits": {
                "cpu": "200m",
                "memory": "1Gi"
              },
              "requests": {
                "cpu": "200m",
                "memory": "1Gi"
              }
            },
            "service": {
              "port": 27017,
              "type": "ClusterIP"
            },
            "serviceAccount": {
              "annotations": {},
              "create": true,
              "name": "mongod"
            },
            "tolerations": []
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [
        "s390x"
      ],
      "bundle_path": "registry.connect.redhat.com/tonyfieit/ibmz-mongodb-operator-bundle@sha256:436d0ba532b631d78d9148ab2f2856beebae91a6ed605afcf8c3a69fe93948d0",
      "bundle_path_digest": "sha256:436d0ba532b631d78d9148ab2f2856beebae91a6ed605afcf8c3a69fe93948d0",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "alpha",
      "creation_date": "2022-05-29T04:43:21.035000+00:00",
      "csv_description": "IBM Operator to deploy MongoDB Enterprise on OpenShift Container",
      "csv_display_name": "ibmz-mongodb-enterprise-operator",
      "csv_metadata_description": "Run IBMz Mongodb Enterprise Edition on OpenShift on LinuxONE.",
      "csv_name": "ibmz-mongodb-enterprise-operator.v4.4.2",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": false,
          "type": "OwnNamespace"
        },
        {
          "supported": false,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-11-23T08:31:40.735000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.10",
      "organization": "certified-operators",
      "package": "ibmz-mongodb-enterprise-operator",
      "provided_apis": [
        {
          "group": "apps.ibmz.com",
          "kind": "MongoDB",
          "plural": "mongodbs",
          "version": "v1alpha1"
        }
      ],
      "provider": "IBMz",
      "related_images": [
        {
          "digest": "sha256:7b40fc146e6e7fc55cf3fe3e7637b2f6202ec124cbb2c9d875064eb2d4d4f220",
          "image": "quay.io/ibm/ibmz-mongodb-enterprise-operator@sha256:7b40fc146e6e7fc55cf3fe3e7637b2f6202ec124cbb2c9d875064eb2d4d4f220",
          "name": "mongodb-enterprise-operator"
        },
        {
          "digest": "sha256:7d51abdae2740d73a05cb3f60bbc047cbe332f29f559b6233b8d15d41f168966",
          "image": "quay.io/ibm/ibmz-mongodb-enterprise-database@sha256:7d51abdae2740d73a05cb3f60bbc047cbe332f29f559b6233b8d15d41f168966",
          "name": "mongo-enterprise-database"
        },
        {
          "digest": "sha256:db06cc4c084dd0253134f156dddaaf53ef1c3fb3cc809e5d81711baa4029ea4c",
          "image": "gcr.io/kubebuilder/kube-rbac-proxy@sha256:db06cc4c084dd0253134f156dddaaf53ef1c3fb3cc809e5d81711baa4029ea4c",
          "name": "kube-rbac-proxy"
        },
        {
          "digest": "sha256:331564d2061b566baf0d4d77008b08e7f7dfdeea6069f12a6df082988e5c3c01",
          "image": "quay.io/ibm/ibmz-mongodb-enterprise-operator@sha256:331564d2061b566baf0d4d77008b08e7f7dfdeea6069f12a6df082988e5c3c01",
          "name": "ibmz-mongodb-enterprise-operator-331564d2061b566baf0d4d77008b08e7f7dfdeea6069f12a6df082988e5c3c01-annotation"
        },
        {
          "digest": "sha256:331564d2061b566baf0d4d77008b08e7f7dfdeea6069f12a6df082988e5c3c01",
          "image": "quay.io/ibm/ibmz-mongodb-enterprise-operator@sha256:331564d2061b566baf0d4d77008b08e7f7dfdeea6069f12a6df082988e5c3c01",
          "name": "manager"
        },
        {
          "digest": "sha256:6ddfec49c1682283b031168fa9a2e09c5746bb1288b6fc4b6f72feb49db35195",
          "image": "quay.io/ibm/ibmz-mongodb-enterprise-database@sha256:6ddfec49c1682283b031168fa9a2e09c5746bb1288b6fc4b6f72feb49db35195",
          "name": "ibmz-mongodb-enterprise-database-6ddfec49c1682283b031168fa9a2e09c5746bb1288b6fc4b6f72feb49db35195-annotation"
        }
      ],
      "replaces": null,
      "skip_range": ">=4.4.0 <4.4.1",
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.10",
      "version": "4.4.2",
      "version_original": "4.4.2"
    },
    {
      "_id": "6292feb24ad695bac574fe6a",
      "alm_examples": [
        {
          "api_version": "apps.ibmz.com/v1alpha1",
          "kind": "MongoDB",
          "metadata": {
            "name": "mongodb-sample"
          },
          "spec": {
            "affinity": {},
            "autoscaling": {
              "enabled": false,
              "maxReplicas": 100,
              "minReplicas": 1,
              "targetCPUUtilizationPercentage": 80
            },
            "database": {
              "adminpassword": "snowball123",
              "adminuser": "adminuser",
              "name_database": "mydb"
            },
            "fullnameOverride": "",
            "global": {
              "license": true,
              "persistence": {
                "claims": {
                  "accessMode": "ReadWriteMany",
                  "capacity": 10,
                  "capacityUnit": "Gi",
                  "mountPath": "/data/db/",
                  "name": "mongo-pv-claim",
                  "storageClassName": "managed-nfs-storage"
                },
                "securityContext": {
                  "fsGroup": 0,
                  "supplementalGroup": 0
                }
              }
            },
            "image": {
              "pullPolicy": "Always",
              "repository": "quay.io/ibm/ibmz-mongodb-enterprise-database@sha256:6ddfec49c1682283b031168fa9a2e09c5746bb1288b6fc4b6f72feb49db35195"
            },
            "imagePullSecretRef": "registry-pull-secret",
            "imagePullSecrets": [
              {
                "name": "registry-pull-secret"
              }
            ],
            "ingress": {
              "annotations": {},
              "controller": "nginx",
              "tls": []
            },
            "name": "mongodb",
            "nameOverride": "",
            "namespaceList": [],
            "namespaced": true,
            "nodeSelector": {},
            "podSecurity": {
              "enabled": true,
              "securityContext": {
                "fsGroup": 1000650000,
                "runAsNonRoot": true,
                "runAsUser": 1000650000
              }
            },
            "replicaCount": 1,
            "resources": {
              "limits": {
                "cpu": "200m",
                "memory": "1Gi"
              },
              "requests": {
                "cpu": "200m",
                "memory": "1Gi"
              }
            },
            "service": {
              "port": 27017,
              "type": "ClusterIP"
            },
            "serviceAccount": {
              "annotations": {},
              "create": true,
              "name": "mongod"
            },
            "tolerations": []
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [
        "s390x"
      ],
      "bundle_path": "registry.connect.redhat.com/tonyfieit/ibmz-mongodb-operator-bundle@sha256:436d0ba532b631d78d9148ab2f2856beebae91a6ed605afcf8c3a69fe93948d0",
      "bundle_path_digest": "sha256:436d0ba532b631d78d9148ab2f2856beebae91a6ed605afcf8c3a69fe93948d0",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "alpha",
      "creation_date": "2022-05-29T05:03:46.060000+00:00",
      "csv_description": "IBM Operator to deploy MongoDB Enterprise on OpenShift Container",
      "csv_display_name": "ibmz-mongodb-enterprise-operator",
      "csv_metadata_description": "Run IBMz Mongodb Enterprise Edition on OpenShift on LinuxONE.",
      "csv_name": "ibmz-mongodb-enterprise-operator.v4.4.2",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": false,
          "type": "OwnNamespace"
        },
        {
          "supported": false,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-11-23T07:37:56.646000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.6",
      "organization": "certified-operators",
      "package": "ibmz-mongodb-enterprise-operator",
      "provided_apis": [
        {
          "group": "apps.ibmz.com",
          "kind": "MongoDB",
          "plural": "mongodbs",
          "version": "v1alpha1"
        }
      ],
      "provider": "IBMz",
      "related_images": [
        {
          "digest": "sha256:7b40fc146e6e7fc55cf3fe3e7637b2f6202ec124cbb2c9d875064eb2d4d4f220",
          "image": "quay.io/ibm/ibmz-mongodb-enterprise-operator@sha256:7b40fc146e6e7fc55cf3fe3e7637b2f6202ec124cbb2c9d875064eb2d4d4f220",
          "name": "mongodb-enterprise-operator"
        },
        {
          "digest": "sha256:7d51abdae2740d73a05cb3f60bbc047cbe332f29f559b6233b8d15d41f168966",
          "image": "quay.io/ibm/ibmz-mongodb-enterprise-database@sha256:7d51abdae2740d73a05cb3f60bbc047cbe332f29f559b6233b8d15d41f168966",
          "name": "mongo-enterprise-database"
        },
        {
          "digest": "sha256:db06cc4c084dd0253134f156dddaaf53ef1c3fb3cc809e5d81711baa4029ea4c",
          "image": "gcr.io/kubebuilder/kube-rbac-proxy@sha256:db06cc4c084dd0253134f156dddaaf53ef1c3fb3cc809e5d81711baa4029ea4c",
          "name": "kube-rbac-proxy"
        },
        {
          "digest": "sha256:331564d2061b566baf0d4d77008b08e7f7dfdeea6069f12a6df082988e5c3c01",
          "image": "quay.io/ibm/ibmz-mongodb-enterprise-operator@sha256:331564d2061b566baf0d4d77008b08e7f7dfdeea6069f12a6df082988e5c3c01",
          "name": "ibmz-mongodb-enterprise-operator-331564d2061b566baf0d4d77008b08e7f7dfdeea6069f12a6df082988e5c3c01-annotation"
        },
        {
          "digest": "sha256:331564d2061b566baf0d4d77008b08e7f7dfdeea6069f12a6df082988e5c3c01",
          "image": "quay.io/ibm/ibmz-mongodb-enterprise-operator@sha256:331564d2061b566baf0d4d77008b08e7f7dfdeea6069f12a6df082988e5c3c01",
          "name": "manager"
        },
        {
          "digest": "sha256:6ddfec49c1682283b031168fa9a2e09c5746bb1288b6fc4b6f72feb49db35195",
          "image": "quay.io/ibm/ibmz-mongodb-enterprise-database@sha256:6ddfec49c1682283b031168fa9a2e09c5746bb1288b6fc4b6f72feb49db35195",
          "name": "ibmz-mongodb-enterprise-database-6ddfec49c1682283b031168fa9a2e09c5746bb1288b6fc4b6f72feb49db35195-annotation"
        }
      ],
      "replaces": null,
      "skip_range": ">=4.4.0 <4.4.1",
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.6",
      "version": "4.4.2",
      "version_original": "4.4.2"
    },
    {
      "_id": "6292fec04ad695bac574fe6c",
      "alm_examples": [
        {
          "api_version": "apps.ibmz.com/v1alpha1",
          "kind": "MongoDB",
          "metadata": {
            "name": "mongodb-sample"
          },
          "spec": {
            "affinity": {},
            "autoscaling": {
              "enabled": false,
              "maxReplicas": 100,
              "minReplicas": 1,
              "targetCPUUtilizationPercentage": 80
            },
            "database": {
              "adminpassword": "snowball123",
              "adminuser": "adminuser",
              "name_database": "mydb"
            },
            "fullnameOverride": "",
            "global": {
              "license": true,
              "persistence": {
                "claims": {
                  "accessMode": "ReadWriteMany",
                  "capacity": 10,
                  "capacityUnit": "Gi",
                  "mountPath": "/data/db/",
                  "name": "mongo-pv-claim",
                  "storageClassName": "managed-nfs-storage"
                },
                "securityContext": {
                  "fsGroup": 0,
                  "supplementalGroup": 0
                }
              }
            },
            "image": {
              "pullPolicy": "Always",
              "repository": "quay.io/ibm/ibmz-mongodb-enterprise-database@sha256:6ddfec49c1682283b031168fa9a2e09c5746bb1288b6fc4b6f72feb49db35195"
            },
            "imagePullSecretRef": "registry-pull-secret",
            "imagePullSecrets": [
              {
                "name": "registry-pull-secret"
              }
            ],
            "ingress": {
              "annotations": {},
              "controller": "nginx",
              "tls": []
            },
            "name": "mongodb",
            "nameOverride": "",
            "namespaceList": [],
            "namespaced": true,
            "nodeSelector": {},
            "podSecurity": {
              "enabled": true,
              "securityContext": {
                "fsGroup": 1000650000,
                "runAsNonRoot": true,
                "runAsUser": 1000650000
              }
            },
            "replicaCount": 1,
            "resources": {
              "limits": {
                "cpu": "200m",
                "memory": "1Gi"
              },
              "requests": {
                "cpu": "200m",
                "memory": "1Gi"
              }
            },
            "service": {
              "port": 27017,
              "type": "ClusterIP"
            },
            "serviceAccount": {
              "annotations": {},
              "create": true,
              "name": "mongod"
            },
            "tolerations": []
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [
        "s390x"
      ],
      "bundle_path": "registry.connect.redhat.com/tonyfieit/ibmz-mongodb-operator-bundle@sha256:436d0ba532b631d78d9148ab2f2856beebae91a6ed605afcf8c3a69fe93948d0",
      "bundle_path_digest": "sha256:436d0ba532b631d78d9148ab2f2856beebae91a6ed605afcf8c3a69fe93948d0",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "alpha",
      "creation_date": "2022-05-29T05:04:00.750000+00:00",
      "csv_description": "IBM Operator to deploy MongoDB Enterprise on OpenShift Container",
      "csv_display_name": "ibmz-mongodb-enterprise-operator",
      "csv_metadata_description": "Run IBMz Mongodb Enterprise Edition on OpenShift on LinuxONE.",
      "csv_name": "ibmz-mongodb-enterprise-operator.v4.4.2",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": false,
          "type": "OwnNamespace"
        },
        {
          "supported": false,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-11-23T07:43:05.139000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.8",
      "organization": "certified-operators",
      "package": "ibmz-mongodb-enterprise-operator",
      "provided_apis": [
        {
          "group": "apps.ibmz.com",
          "kind": "MongoDB",
          "plural": "mongodbs",
          "version": "v1alpha1"
        }
      ],
      "provider": "IBMz",
      "related_images": [
        {
          "digest": "sha256:7b40fc146e6e7fc55cf3fe3e7637b2f6202ec124cbb2c9d875064eb2d4d4f220",
          "image": "quay.io/ibm/ibmz-mongodb-enterprise-operator@sha256:7b40fc146e6e7fc55cf3fe3e7637b2f6202ec124cbb2c9d875064eb2d4d4f220",
          "name": "mongodb-enterprise-operator"
        },
        {
          "digest": "sha256:7d51abdae2740d73a05cb3f60bbc047cbe332f29f559b6233b8d15d41f168966",
          "image": "quay.io/ibm/ibmz-mongodb-enterprise-database@sha256:7d51abdae2740d73a05cb3f60bbc047cbe332f29f559b6233b8d15d41f168966",
          "name": "mongo-enterprise-database"
        },
        {
          "digest": "sha256:db06cc4c084dd0253134f156dddaaf53ef1c3fb3cc809e5d81711baa4029ea4c",
          "image": "gcr.io/kubebuilder/kube-rbac-proxy@sha256:db06cc4c084dd0253134f156dddaaf53ef1c3fb3cc809e5d81711baa4029ea4c",
          "name": "kube-rbac-proxy"
        },
        {
          "digest": "sha256:331564d2061b566baf0d4d77008b08e7f7dfdeea6069f12a6df082988e5c3c01",
          "image": "quay.io/ibm/ibmz-mongodb-enterprise-operator@sha256:331564d2061b566baf0d4d77008b08e7f7dfdeea6069f12a6df082988e5c3c01",
          "name": "ibmz-mongodb-enterprise-operator-331564d2061b566baf0d4d77008b08e7f7dfdeea6069f12a6df082988e5c3c01-annotation"
        },
        {
          "digest": "sha256:331564d2061b566baf0d4d77008b08e7f7dfdeea6069f12a6df082988e5c3c01",
          "image": "quay.io/ibm/ibmz-mongodb-enterprise-operator@sha256:331564d2061b566baf0d4d77008b08e7f7dfdeea6069f12a6df082988e5c3c01",
          "name": "manager"
        },
        {
          "digest": "sha256:6ddfec49c1682283b031168fa9a2e09c5746bb1288b6fc4b6f72feb49db35195",
          "image": "quay.io/ibm/ibmz-mongodb-enterprise-database@sha256:6ddfec49c1682283b031168fa9a2e09c5746bb1288b6fc4b6f72feb49db35195",
          "name": "ibmz-mongodb-enterprise-database-6ddfec49c1682283b031168fa9a2e09c5746bb1288b6fc4b6f72feb49db35195-annotation"
        }
      ],
      "replaces": null,
      "skip_range": ">=4.4.0 <4.4.1",
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.8",
      "version": "4.4.2",
      "version_original": "4.4.2"
    },
    {
      "_id": "6292ff28bf1369b7ed5b07fe",
      "alm_examples": [
        {
          "api_version": "apps.ibmz.com/v1alpha1",
          "kind": "MongoDB",
          "metadata": {
            "name": "mongodb-sample"
          },
          "spec": {
            "affinity": {},
            "autoscaling": {
              "enabled": false,
              "maxReplicas": 100,
              "minReplicas": 1,
              "targetCPUUtilizationPercentage": 80
            },
            "database": {
              "adminpassword": "snowball123",
              "adminuser": "adminuser",
              "name_database": "mydb"
            },
            "fullnameOverride": "",
            "global": {
              "license": true,
              "persistence": {
                "claims": {
                  "accessMode": "ReadWriteMany",
                  "capacity": 10,
                  "capacityUnit": "Gi",
                  "mountPath": "/data/db/",
                  "name": "mongo-pv-claim",
                  "storageClassName": "managed-nfs-storage"
                },
                "securityContext": {
                  "fsGroup": 0,
                  "supplementalGroup": 0
                }
              }
            },
            "image": {
              "pullPolicy": "Always",
              "repository": "quay.io/ibm/ibmz-mongodb-enterprise-database@sha256:6ddfec49c1682283b031168fa9a2e09c5746bb1288b6fc4b6f72feb49db35195"
            },
            "imagePullSecretRef": "registry-pull-secret",
            "imagePullSecrets": [
              {
                "name": "registry-pull-secret"
              }
            ],
            "ingress": {
              "annotations": {},
              "controller": "nginx",
              "tls": []
            },
            "name": "mongodb",
            "nameOverride": "",
            "namespaceList": [],
            "namespaced": true,
            "nodeSelector": {},
            "podSecurity": {
              "enabled": true,
              "securityContext": {
                "fsGroup": 1000650000,
                "runAsNonRoot": true,
                "runAsUser": 1000650000
              }
            },
            "replicaCount": 1,
            "resources": {
              "limits": {
                "cpu": "200m",
                "memory": "1Gi"
              },
              "requests": {
                "cpu": "200m",
                "memory": "1Gi"
              }
            },
            "service": {
              "port": 27017,
              "type": "ClusterIP"
            },
            "serviceAccount": {
              "annotations": {},
              "create": true,
              "name": "mongod"
            },
            "tolerations": []
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [
        "s390x"
      ],
      "bundle_path": "registry.connect.redhat.com/tonyfieit/ibmz-mongodb-operator-bundle@sha256:436d0ba532b631d78d9148ab2f2856beebae91a6ed605afcf8c3a69fe93948d0",
      "bundle_path_digest": "sha256:436d0ba532b631d78d9148ab2f2856beebae91a6ed605afcf8c3a69fe93948d0",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "alpha",
      "creation_date": "2022-05-29T05:05:44.919000+00:00",
      "csv_description": "IBM Operator to deploy MongoDB Enterprise on OpenShift Container",
      "csv_display_name": "ibmz-mongodb-enterprise-operator",
      "csv_metadata_description": "Run IBMz Mongodb Enterprise Edition on OpenShift on LinuxONE.",
      "csv_name": "ibmz-mongodb-enterprise-operator.v4.4.2",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": false,
          "type": "OwnNamespace"
        },
        {
          "supported": false,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-11-23T07:15:59.288000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.9",
      "organization": "certified-operators",
      "package": "ibmz-mongodb-enterprise-operator",
      "provided_apis": [
        {
          "group": "apps.ibmz.com",
          "kind": "MongoDB",
          "plural": "mongodbs",
          "version": "v1alpha1"
        }
      ],
      "provider": "IBMz",
      "related_images": [
        {
          "digest": "sha256:7b40fc146e6e7fc55cf3fe3e7637b2f6202ec124cbb2c9d875064eb2d4d4f220",
          "image": "quay.io/ibm/ibmz-mongodb-enterprise-operator@sha256:7b40fc146e6e7fc55cf3fe3e7637b2f6202ec124cbb2c9d875064eb2d4d4f220",
          "name": "mongodb-enterprise-operator"
        },
        {
          "digest": "sha256:7d51abdae2740d73a05cb3f60bbc047cbe332f29f559b6233b8d15d41f168966",
          "image": "quay.io/ibm/ibmz-mongodb-enterprise-database@sha256:7d51abdae2740d73a05cb3f60bbc047cbe332f29f559b6233b8d15d41f168966",
          "name": "mongo-enterprise-database"
        },
        {
          "digest": "sha256:db06cc4c084dd0253134f156dddaaf53ef1c3fb3cc809e5d81711baa4029ea4c",
          "image": "gcr.io/kubebuilder/kube-rbac-proxy@sha256:db06cc4c084dd0253134f156dddaaf53ef1c3fb3cc809e5d81711baa4029ea4c",
          "name": "kube-rbac-proxy"
        },
        {
          "digest": "sha256:331564d2061b566baf0d4d77008b08e7f7dfdeea6069f12a6df082988e5c3c01",
          "image": "quay.io/ibm/ibmz-mongodb-enterprise-operator@sha256:331564d2061b566baf0d4d77008b08e7f7dfdeea6069f12a6df082988e5c3c01",
          "name": "ibmz-mongodb-enterprise-operator-331564d2061b566baf0d4d77008b08e7f7dfdeea6069f12a6df082988e5c3c01-annotation"
        },
        {
          "digest": "sha256:331564d2061b566baf0d4d77008b08e7f7dfdeea6069f12a6df082988e5c3c01",
          "image": "quay.io/ibm/ibmz-mongodb-enterprise-operator@sha256:331564d2061b566baf0d4d77008b08e7f7dfdeea6069f12a6df082988e5c3c01",
          "name": "manager"
        },
        {
          "digest": "sha256:6ddfec49c1682283b031168fa9a2e09c5746bb1288b6fc4b6f72feb49db35195",
          "image": "quay.io/ibm/ibmz-mongodb-enterprise-database@sha256:6ddfec49c1682283b031168fa9a2e09c5746bb1288b6fc4b6f72feb49db35195",
          "name": "ibmz-mongodb-enterprise-database-6ddfec49c1682283b031168fa9a2e09c5746bb1288b6fc4b6f72feb49db35195-annotation"
        }
      ],
      "replaces": null,
      "skip_range": ">=4.4.0 <4.4.1",
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.9",
      "version": "4.4.2",
      "version_original": "4.4.2"
    },
    {
      "_id": "6292ff9b301a731de19f1f24",
      "alm_examples": [
        {
          "api_version": "apps.ibmz.com/v1alpha1",
          "kind": "MongoDB",
          "metadata": {
            "name": "mongodb-sample"
          },
          "spec": {
            "affinity": {},
            "autoscaling": {
              "enabled": false,
              "maxReplicas": 100,
              "minReplicas": 1,
              "targetCPUUtilizationPercentage": 80
            },
            "database": {
              "adminpassword": "snowball123",
              "adminuser": "adminuser",
              "name_database": "mydb"
            },
            "fullnameOverride": "",
            "global": {
              "license": true,
              "persistence": {
                "claims": {
                  "accessMode": "ReadWriteMany",
                  "capacity": 10,
                  "capacityUnit": "Gi",
                  "mountPath": "/data/db/",
                  "name": "mongo-pv-claim",
                  "storageClassName": "managed-nfs-storage"
                },
                "securityContext": {
                  "fsGroup": 0,
                  "supplementalGroup": 0
                }
              }
            },
            "image": {
              "pullPolicy": "Always",
              "repository": "quay.io/ibm/ibmz-mongodb-enterprise-database@sha256:6ddfec49c1682283b031168fa9a2e09c5746bb1288b6fc4b6f72feb49db35195"
            },
            "imagePullSecretRef": "registry-pull-secret",
            "imagePullSecrets": [
              {
                "name": "registry-pull-secret"
              }
            ],
            "ingress": {
              "annotations": {},
              "controller": "nginx",
              "tls": []
            },
            "name": "mongodb",
            "nameOverride": "",
            "namespaceList": [],
            "namespaced": true,
            "nodeSelector": {},
            "podSecurity": {
              "enabled": true,
              "securityContext": {
                "fsGroup": 1000650000,
                "runAsNonRoot": true,
                "runAsUser": 1000650000
              }
            },
            "replicaCount": 1,
            "resources": {
              "limits": {
                "cpu": "200m",
                "memory": "1Gi"
              },
              "requests": {
                "cpu": "200m",
                "memory": "1Gi"
              }
            },
            "service": {
              "port": 27017,
              "type": "ClusterIP"
            },
            "serviceAccount": {
              "annotations": {},
              "create": true,
              "name": "mongod"
            },
            "tolerations": []
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [
        "s390x"
      ],
      "bundle_path": "registry.connect.redhat.com/tonyfieit/ibmz-mongodb-operator-bundle@sha256:436d0ba532b631d78d9148ab2f2856beebae91a6ed605afcf8c3a69fe93948d0",
      "bundle_path_digest": "sha256:436d0ba532b631d78d9148ab2f2856beebae91a6ed605afcf8c3a69fe93948d0",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "alpha",
      "creation_date": "2022-05-29T05:07:39.045000+00:00",
      "csv_description": "IBM Operator to deploy MongoDB Enterprise on OpenShift Container",
      "csv_display_name": "ibmz-mongodb-enterprise-operator",
      "csv_metadata_description": "Run IBMz Mongodb Enterprise Edition on OpenShift on LinuxONE.",
      "csv_name": "ibmz-mongodb-enterprise-operator.v4.4.2",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": false,
          "type": "OwnNamespace"
        },
        {
          "supported": false,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-11-23T06:46:17.281000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.7",
      "organization": "certified-operators",
      "package": "ibmz-mongodb-enterprise-operator",
      "provided_apis": [
        {
          "group": "apps.ibmz.com",
          "kind": "MongoDB",
          "plural": "mongodbs",
          "version": "v1alpha1"
        }
      ],
      "provider": "IBMz",
      "related_images": [
        {
          "digest": "sha256:7b40fc146e6e7fc55cf3fe3e7637b2f6202ec124cbb2c9d875064eb2d4d4f220",
          "image": "quay.io/ibm/ibmz-mongodb-enterprise-operator@sha256:7b40fc146e6e7fc55cf3fe3e7637b2f6202ec124cbb2c9d875064eb2d4d4f220",
          "name": "mongodb-enterprise-operator"
        },
        {
          "digest": "sha256:7d51abdae2740d73a05cb3f60bbc047cbe332f29f559b6233b8d15d41f168966",
          "image": "quay.io/ibm/ibmz-mongodb-enterprise-database@sha256:7d51abdae2740d73a05cb3f60bbc047cbe332f29f559b6233b8d15d41f168966",
          "name": "mongo-enterprise-database"
        },
        {
          "digest": "sha256:db06cc4c084dd0253134f156dddaaf53ef1c3fb3cc809e5d81711baa4029ea4c",
          "image": "gcr.io/kubebuilder/kube-rbac-proxy@sha256:db06cc4c084dd0253134f156dddaaf53ef1c3fb3cc809e5d81711baa4029ea4c",
          "name": "kube-rbac-proxy"
        },
        {
          "digest": "sha256:331564d2061b566baf0d4d77008b08e7f7dfdeea6069f12a6df082988e5c3c01",
          "image": "quay.io/ibm/ibmz-mongodb-enterprise-operator@sha256:331564d2061b566baf0d4d77008b08e7f7dfdeea6069f12a6df082988e5c3c01",
          "name": "ibmz-mongodb-enterprise-operator-331564d2061b566baf0d4d77008b08e7f7dfdeea6069f12a6df082988e5c3c01-annotation"
        },
        {
          "digest": "sha256:331564d2061b566baf0d4d77008b08e7f7dfdeea6069f12a6df082988e5c3c01",
          "image": "quay.io/ibm/ibmz-mongodb-enterprise-operator@sha256:331564d2061b566baf0d4d77008b08e7f7dfdeea6069f12a6df082988e5c3c01",
          "name": "manager"
        },
        {
          "digest": "sha256:6ddfec49c1682283b031168fa9a2e09c5746bb1288b6fc4b6f72feb49db35195",
          "image": "quay.io/ibm/ibmz-mongodb-enterprise-database@sha256:6ddfec49c1682283b031168fa9a2e09c5746bb1288b6fc4b6f72feb49db35195",
          "name": "ibmz-mongodb-enterprise-database-6ddfec49c1682283b031168fa9a2e09c5746bb1288b6fc4b6f72feb49db35195-annotation"
        }
      ],
      "replaces": null,
      "skip_range": ">=4.4.0 <4.4.1",
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.7",
      "version": "4.4.2",
      "version_original": "4.4.2"
    },
    {
      "_id": "6297b2af8c1a626fc9ae708d",
      "alm_examples": [
        {
          "api_version": "appdirector.randoli.ca/v1beta1",
          "kind": "ClusterWideGuvnor",
          "metadata": {
            "name": "guvnor"
          },
          "spec": {
            "guvnorCertsSecretName": "guvnor-certs",
            "guvnorSecretName": "guvnor-secret",
            "namespace": "app-director-example-app"
          }
        },
        {
          "api_version": "appdirector.randoli.ca/v1beta1",
          "kind": "Guvnor",
          "metadata": {
            "name": "guvnor",
            "namespace": "app-director-example-app"
          },
          "spec": {
            "guvnorCertsSecretName": "guvnor-certs",
            "guvnorSecretName": "guvnor-secret"
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/randoli/app-director-operator@sha256:e5d637b0d40a59dce96880c80d61bf8454c4b43ba83e22345da9b1cfeb526633",
      "bundle_path_digest": "sha256:e5d637b0d40a59dce96880c80d61bf8454c4b43ba83e22345da9b1cfeb526633",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "0.0.3",
      "creation_date": "2022-06-01T18:40:47.174000+00:00",
      "csv_description": "App Director\u00ae Operator manages the Guvnor agent that connects with the App Director\u00ae Platform to receive deployments and send back events. The agent can be scoped to a namespace or cluster-wide. More information can be found at https://www.randoli.ca/app-director-overview. Tekton Pipelines are a pre-requisite for this operator to work. If not already installed, the operator will install the correct version on initial start-up.",
      "csv_display_name": "App-Director",
      "csv_metadata_description": "",
      "csv_name": "app-director-operator.v0.0.3",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": false,
          "type": "OwnNamespace"
        },
        {
          "supported": false,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": false,
      "last_update_date": "2022-11-23T07:32:54.807000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.8",
      "organization": "certified-operators",
      "package": "app-director-operator",
      "provided_apis": [
        {
          "group": "appdirector.randoli.ca",
          "kind": "ClusterWideGuvnor",
          "plural": "clusterwideguvnors",
          "version": "v1beta1"
        },
        {
          "group": "appdirector.randoli.ca",
          "kind": "Guvnor",
          "plural": "guvnors",
          "version": "v1beta1"
        }
      ],
      "provider": "Randoli",
      "related_images": [
        {
          "digest": "sha256:cb9cb3bdea785e41b7b0da8a5357aaa0cb6c4e6b5d1e87db4a53798e25a56112",
          "image": "docker.io/randoli/app-director-operator:0.0.3@sha256:cb9cb3bdea785e41b7b0da8a5357aaa0cb6c4e6b5d1e87db4a53798e25a56112",
          "name": "app-director-operator:0.0.3-cb9cb3bdea785e41b7b0da8a5357aaa0cb6c4e6b5d1e87db4a53798e25a56112-annotation"
        },
        {
          "digest": "sha256:e10d1d982dd653db74ca87a1d1ad017bc5ef1aeb651bdea089debf16485b080b",
          "image": "gcr.io/kubebuilder/kube-rbac-proxy:v0.5.0@sha256:e10d1d982dd653db74ca87a1d1ad017bc5ef1aeb651bdea089debf16485b080b",
          "name": "kube-rbac-proxy"
        },
        {
          "digest": "sha256:cb9cb3bdea785e41b7b0da8a5357aaa0cb6c4e6b5d1e87db4a53798e25a56112",
          "image": "docker.io/randoli/app-director-operator:0.0.3@sha256:cb9cb3bdea785e41b7b0da8a5357aaa0cb6c4e6b5d1e87db4a53798e25a56112",
          "name": "manager"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.8",
      "version": "0.0.3",
      "version_original": "0.0.3"
    },
    {
      "_id": "6297b2eb8c1a626fc9ae70bd",
      "alm_examples": [
        {
          "api_version": "appdirector.randoli.ca/v1beta1",
          "kind": "ClusterWideGuvnor",
          "metadata": {
            "name": "guvnor"
          },
          "spec": {
            "guvnorCertsSecretName": "guvnor-certs",
            "guvnorSecretName": "guvnor-secret",
            "namespace": "app-director-example-app"
          }
        },
        {
          "api_version": "appdirector.randoli.ca/v1beta1",
          "kind": "Guvnor",
          "metadata": {
            "name": "guvnor",
            "namespace": "app-director-example-app"
          },
          "spec": {
            "guvnorCertsSecretName": "guvnor-certs",
            "guvnorSecretName": "guvnor-secret"
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/randoli/app-director-operator@sha256:e5d637b0d40a59dce96880c80d61bf8454c4b43ba83e22345da9b1cfeb526633",
      "bundle_path_digest": "sha256:e5d637b0d40a59dce96880c80d61bf8454c4b43ba83e22345da9b1cfeb526633",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "0.0.3",
      "creation_date": "2022-06-01T18:41:47.492000+00:00",
      "csv_description": "App Director\u00ae Operator manages the Guvnor agent that connects with the App Director\u00ae Platform to receive deployments and send back events. The agent can be scoped to a namespace or cluster-wide. More information can be found at https://www.randoli.ca/app-director-overview. Tekton Pipelines are a pre-requisite for this operator to work. If not already installed, the operator will install the correct version on initial start-up.",
      "csv_display_name": "App-Director",
      "csv_metadata_description": "",
      "csv_name": "app-director-operator.v0.0.3",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": false,
          "type": "OwnNamespace"
        },
        {
          "supported": false,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": false,
      "last_update_date": "2022-11-23T07:31:11.755000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.6",
      "organization": "certified-operators",
      "package": "app-director-operator",
      "provided_apis": [
        {
          "group": "appdirector.randoli.ca",
          "kind": "ClusterWideGuvnor",
          "plural": "clusterwideguvnors",
          "version": "v1beta1"
        },
        {
          "group": "appdirector.randoli.ca",
          "kind": "Guvnor",
          "plural": "guvnors",
          "version": "v1beta1"
        }
      ],
      "provider": "Randoli",
      "related_images": [
        {
          "digest": "sha256:cb9cb3bdea785e41b7b0da8a5357aaa0cb6c4e6b5d1e87db4a53798e25a56112",
          "image": "docker.io/randoli/app-director-operator:0.0.3@sha256:cb9cb3bdea785e41b7b0da8a5357aaa0cb6c4e6b5d1e87db4a53798e25a56112",
          "name": "app-director-operator:0.0.3-cb9cb3bdea785e41b7b0da8a5357aaa0cb6c4e6b5d1e87db4a53798e25a56112-annotation"
        },
        {
          "digest": "sha256:e10d1d982dd653db74ca87a1d1ad017bc5ef1aeb651bdea089debf16485b080b",
          "image": "gcr.io/kubebuilder/kube-rbac-proxy:v0.5.0@sha256:e10d1d982dd653db74ca87a1d1ad017bc5ef1aeb651bdea089debf16485b080b",
          "name": "kube-rbac-proxy"
        },
        {
          "digest": "sha256:cb9cb3bdea785e41b7b0da8a5357aaa0cb6c4e6b5d1e87db4a53798e25a56112",
          "image": "docker.io/randoli/app-director-operator:0.0.3@sha256:cb9cb3bdea785e41b7b0da8a5357aaa0cb6c4e6b5d1e87db4a53798e25a56112",
          "name": "manager"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.6",
      "version": "0.0.3",
      "version_original": "0.0.3"
    },
    {
      "_id": "6297b34eefe144fd5e92f2f1",
      "alm_examples": [
        {
          "api_version": "appdirector.randoli.ca/v1beta1",
          "kind": "ClusterWideGuvnor",
          "metadata": {
            "name": "guvnor"
          },
          "spec": {
            "guvnorCertsSecretName": "guvnor-certs",
            "guvnorSecretName": "guvnor-secret",
            "namespace": "app-director-example-app"
          }
        },
        {
          "api_version": "appdirector.randoli.ca/v1beta1",
          "kind": "Guvnor",
          "metadata": {
            "name": "guvnor",
            "namespace": "app-director-example-app"
          },
          "spec": {
            "guvnorCertsSecretName": "guvnor-certs",
            "guvnorSecretName": "guvnor-secret"
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/randoli/app-director-operator@sha256:e5d637b0d40a59dce96880c80d61bf8454c4b43ba83e22345da9b1cfeb526633",
      "bundle_path_digest": "sha256:e5d637b0d40a59dce96880c80d61bf8454c4b43ba83e22345da9b1cfeb526633",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "0.0.3",
      "creation_date": "2022-06-01T18:43:26.683000+00:00",
      "csv_description": "App Director\u00ae Operator manages the Guvnor agent that connects with the App Director\u00ae Platform to receive deployments and send back events. The agent can be scoped to a namespace or cluster-wide. More information can be found at https://www.randoli.ca/app-director-overview. Tekton Pipelines are a pre-requisite for this operator to work. If not already installed, the operator will install the correct version on initial start-up.",
      "csv_display_name": "App-Director",
      "csv_metadata_description": "",
      "csv_name": "app-director-operator.v0.0.3",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": false,
          "type": "OwnNamespace"
        },
        {
          "supported": false,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": false,
      "last_update_date": "2022-11-23T06:45:44.750000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.7",
      "organization": "certified-operators",
      "package": "app-director-operator",
      "provided_apis": [
        {
          "group": "appdirector.randoli.ca",
          "kind": "ClusterWideGuvnor",
          "plural": "clusterwideguvnors",
          "version": "v1beta1"
        },
        {
          "group": "appdirector.randoli.ca",
          "kind": "Guvnor",
          "plural": "guvnors",
          "version": "v1beta1"
        }
      ],
      "provider": "Randoli",
      "related_images": [
        {
          "digest": "sha256:cb9cb3bdea785e41b7b0da8a5357aaa0cb6c4e6b5d1e87db4a53798e25a56112",
          "image": "docker.io/randoli/app-director-operator:0.0.3@sha256:cb9cb3bdea785e41b7b0da8a5357aaa0cb6c4e6b5d1e87db4a53798e25a56112",
          "name": "app-director-operator:0.0.3-cb9cb3bdea785e41b7b0da8a5357aaa0cb6c4e6b5d1e87db4a53798e25a56112-annotation"
        },
        {
          "digest": "sha256:e10d1d982dd653db74ca87a1d1ad017bc5ef1aeb651bdea089debf16485b080b",
          "image": "gcr.io/kubebuilder/kube-rbac-proxy:v0.5.0@sha256:e10d1d982dd653db74ca87a1d1ad017bc5ef1aeb651bdea089debf16485b080b",
          "name": "kube-rbac-proxy"
        },
        {
          "digest": "sha256:cb9cb3bdea785e41b7b0da8a5357aaa0cb6c4e6b5d1e87db4a53798e25a56112",
          "image": "docker.io/randoli/app-director-operator:0.0.3@sha256:cb9cb3bdea785e41b7b0da8a5357aaa0cb6c4e6b5d1e87db4a53798e25a56112",
          "name": "manager"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.7",
      "version": "0.0.3",
      "version_original": "0.0.3"
    },
    {
      "_id": "6297bc41554afbde4c9b7000",
      "alm_examples": [
        {
          "api_version": "platform.confluent.io/v1beta1",
          "kind": "Zookeeper",
          "metadata": {
            "name": "zookeeper"
          },
          "spec": {
            "dataVolumeCapacity": "10Gi",
            "image": {
              "application": "confluentinc/cp-zookeeper@sha256:f80df50ad358773df08657bace7980dd89c97cfc518141ed1ebffe88ce610ef4",
              "init": "confluentinc/confluent-init-container@sha256:35c19bd2fd4300899a47bd06e66dc43569cba02e95286f9b9548a764de0e1001"
            },
            "logVolumeCapacity": "10Gi",
            "replicas": 3
          }
        },
        {
          "api_version": "platform.confluent.io/v1beta1",
          "kind": "Kafka",
          "metadata": {
            "name": "kafka"
          },
          "spec": {
            "dataVolumeCapacity": "10Gi",
            "image": {
              "application": "confluentinc/cp-server@sha256:bbc1493e52991094cde2c94ad334d6fbbf413e87fa5f203e942f6bcc0d488ffd",
              "init": "confluentinc/confluent-init-container@sha256:35c19bd2fd4300899a47bd06e66dc43569cba02e95286f9b9548a764de0e1001"
            },
            "metricReporter": {
              "enabled": true
            },
            "replicas": 3
          }
        },
        {
          "api_version": "platform.confluent.io/v1beta1",
          "kind": "KafkaRestProxy",
          "metadata": {
            "name": "kafkarestproxy"
          },
          "spec": {
            "image": {
              "application": "confluentinc/cp-kafka-rest@sha256:d380a9e0dbb456f95e70b7d6c56cca02d4734b1d0fde8fbe2a053c3e6d44e16d ",
              "init": "confluentinc/confluent-init-container@sha256:35c19bd2fd4300899a47bd06e66dc43569cba02e95286f9b9548a764de0e1001"
            },
            "metricReporter": {
              "enabled": true
            },
            "replicas": 3
          }
        },
        {
          "api_version": "platform.confluent.io/v1beta1",
          "kind": "Connect",
          "metadata": {
            "name": "connect"
          },
          "spec": {
            "dependencies": {
              "kafka": {
                "bootstrapEndpoint": "kafka:9071"
              }
            },
            "image": {
              "application": "confluentinc/cp-server-connect@sha256:7da14f9d34b48bed11cb307a4a080d2055a18d5acc95f5bcc31b7feac0e0395e",
              "init": "confluentinc/confluent-init-container@sha256:35c19bd2fd4300899a47bd06e66dc43569cba02e95286f9b9548a764de0e1001"
            },
            "replicas": 1
          }
        },
        {
          "api_version": "platform.confluent.io/v1beta1",
          "kind": "KsqlDB",
          "metadata": {
            "name": "ksqldb"
          },
          "spec": {
            "dataVolumeCapacity": "10Gi",
            "image": {
              "application": "confluentinc/cp-ksqldb-server@sha256:951af16f6249a3bc3909420bf0abe78a7d5d853f4aa4eaf66665e1a6b2676af2",
              "init": "confluentinc/confluent-init-container@sha256:35c19bd2fd4300899a47bd06e66dc43569cba02e95286f9b9548a764de0e1001"
            },
            "replicas": 1
          }
        },
        {
          "api_version": "platform.confluent.io/v1beta1",
          "kind": "ControlCenter",
          "metadata": {
            "name": "controlcenter"
          },
          "spec": {
            "dataVolumeCapacity": "10Gi",
            "dependencies": {
              "connect": [
                {
                  "name": "connect",
                  "url": "http://connect.confluent.svc.cluster.local:8083"
                }
              ],
              "ksqldb": [
                {
                  "name": "ksqldb",
                  "url": "http://ksqldb.confluent.svc.cluster.local:8088"
                }
              ],
              "schemaRegistry": {
                "url": "http://schemaregistry.confluent.svc.cluster.local:8081"
              }
            },
            "image": {
              "application": "confluentinc/cp-enterprise-control-center@sha256:ef89f3bc2656c215a80dd202958bbfd1f843297574dff5a537590d1733aaf38a",
              "init": "confluentinc/confluent-init-container@sha256:35c19bd2fd4300899a47bd06e66dc43569cba02e95286f9b9548a764de0e1001"
            },
            "replicas": 1
          }
        },
        {
          "api_version": "platform.confluent.io/v1beta1",
          "kind": "SchemaRegistry",
          "metadata": {
            "name": "schemaregistry"
          },
          "spec": {
            "image": {
              "application": "confluentinc/cp-schema-registry@sha256:dabf016b86abb9ca7ab7f800ccdc0c18fdd80975da2195d90672e3e488ba0797",
              "init": "confluentinc/confluent-init-container@sha256:35c19bd2fd4300899a47bd06e66dc43569cba02e95286f9b9548a764de0e1001"
            },
            "replicas": 1
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [
          "disconnected"
        ],
        "valid_subscription": [
          "Confluent Platform License"
        ]
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/confluentinc/confluent-for-kubernetes-v2@sha256:aaaa7c9077f0d2f844c7b2943b50e880ef7ed501dacae91d68cd14e30902eb02",
      "bundle_path_digest": "sha256:aaaa7c9077f0d2f844c7b2943b50e880ef7ed501dacae91d68cd14e30902eb02",
      "capabilities": [
        "Auto Pilot"
      ],
      "channel_name": "2.3",
      "creation_date": "2022-06-01T19:21:37.986000+00:00",
      "csv_description": "Confluent for Kubernetes (CFK) is a cloud-native control plane for deploying and managing Confluent in your private cloud environment. It provides standard and simple interface to customize, deploy, and manage Confluent Platform through declarative API.\nConfluent for Kubernetes runs on Kubernetes, the runtime for private cloud architectures.",
      "csv_display_name": "Confluent for Kubernetes",
      "csv_metadata_description": "",
      "csv_name": "confluent-for-kubernetes.v2.3.0",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": true,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-11-23T08:23:54.865000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.10",
      "organization": "certified-operators",
      "package": "confluent-for-kubernetes",
      "provided_apis": [
        {
          "group": "platform.confluent.io",
          "kind": "Kafka",
          "plural": "kafkas",
          "version": "v1beta1"
        },
        {
          "group": "platform.confluent.io",
          "kind": "Connect",
          "plural": "connects",
          "version": "v1beta1"
        },
        {
          "group": "platform.confluent.io",
          "kind": "KafkaRestClass",
          "plural": "kafkarestclasses",
          "version": "v1beta1"
        },
        {
          "group": "platform.confluent.io",
          "kind": "Zookeeper",
          "plural": "zookeepers",
          "version": "v1beta1"
        },
        {
          "group": "platform.confluent.io",
          "kind": "SchemaRegistry",
          "plural": "schemaregistries",
          "version": "v1beta1"
        },
        {
          "group": "platform.confluent.io",
          "kind": "KafkaRestProxy",
          "plural": "kafkarestproxies",
          "version": "v1beta1"
        },
        {
          "group": "platform.confluent.io",
          "kind": "ConfluentRolebinding",
          "plural": "confluentrolebindings",
          "version": "v1beta1"
        },
        {
          "group": "platform.confluent.io",
          "kind": "ControlCenter",
          "plural": "controlcenters",
          "version": "v1beta1"
        },
        {
          "group": "platform.confluent.io",
          "kind": "KafkaTopic",
          "plural": "kafkatopics",
          "version": "v1beta1"
        },
        {
          "group": "platform.confluent.io",
          "kind": "MigrationJob",
          "plural": "migrationjobs",
          "version": "v1beta1"
        },
        {
          "group": "platform.confluent.io",
          "kind": "KsqlDB",
          "plural": "ksqldbs",
          "version": "v1beta1"
        },
        {
          "group": "platform.confluent.io",
          "kind": "Connector",
          "plural": "connectors",
          "version": "v1beta1"
        },
        {
          "group": "platform.confluent.io",
          "kind": "Schema",
          "plural": "schemas",
          "version": "v1beta1"
        },
        {
          "group": "platform.confluent.io",
          "kind": "ClusterLink",
          "plural": "clusterlinks",
          "version": "v1beta1"
        },
        {
          "group": "platform.confluent.io",
          "kind": "SchemaExporter",
          "plural": "schemaexporters",
          "version": "v1beta1"
        }
      ],
      "provider": "Confluent, Inc.",
      "related_images": [
        {
          "digest": "sha256:236c01102003982ac70b8f7b101f73de1a049f1900c266fd2986d4f6d4c47ec5",
          "image": "docker.io/confluentinc/confluent-operator@sha256:236c01102003982ac70b8f7b101f73de1a049f1900c266fd2986d4f6d4c47ec5",
          "name": "confluent-operator"
        },
        {
          "digest": "sha256:35c19bd2fd4300899a47bd06e66dc43569cba02e95286f9b9548a764de0e1001",
          "image": "docker.io/confluentinc/confluent-init-container@sha256:35c19bd2fd4300899a47bd06e66dc43569cba02e95286f9b9548a764de0e1001",
          "name": "confluent-init-container"
        },
        {
          "digest": "sha256:f80df50ad358773df08657bace7980dd89c97cfc518141ed1ebffe88ce610ef4",
          "image": "docker.io/confluentinc/cp-zookeeper@sha256:f80df50ad358773df08657bace7980dd89c97cfc518141ed1ebffe88ce610ef4",
          "name": "cp-zookeeper"
        },
        {
          "digest": "sha256:bbc1493e52991094cde2c94ad334d6fbbf413e87fa5f203e942f6bcc0d488ffd",
          "image": "docker.io/confluentinc/cp-server@sha256:bbc1493e52991094cde2c94ad334d6fbbf413e87fa5f203e942f6bcc0d488ffd",
          "name": "cp-server"
        },
        {
          "digest": "sha256:d380a9e0dbb456f95e70b7d6c56cca02d4734b1d0fde8fbe2a053c3e6d44e16d",
          "image": "docker.io/confluentinc/cp-kafka-rest@sha256:d380a9e0dbb456f95e70b7d6c56cca02d4734b1d0fde8fbe2a053c3e6d44e16d",
          "name": "cp-kafka-rest"
        },
        {
          "digest": "sha256:951af16f6249a3bc3909420bf0abe78a7d5d853f4aa4eaf66665e1a6b2676af2",
          "image": "docker.io/confluentinc/cp-ksqldb-server@sha256:951af16f6249a3bc3909420bf0abe78a7d5d853f4aa4eaf66665e1a6b2676af2",
          "name": "cp-ksqldb-server"
        },
        {
          "digest": "sha256:dabf016b86abb9ca7ab7f800ccdc0c18fdd80975da2195d90672e3e488ba0797",
          "image": "docker.io/confluentinc/cp-schema-registry@sha256:dabf016b86abb9ca7ab7f800ccdc0c18fdd80975da2195d90672e3e488ba0797",
          "name": "cp-schema-registry"
        },
        {
          "digest": "sha256:7da14f9d34b48bed11cb307a4a080d2055a18d5acc95f5bcc31b7feac0e0395e",
          "image": "docker.io/confluentinc/cp-server-connect@sha256:7da14f9d34b48bed11cb307a4a080d2055a18d5acc95f5bcc31b7feac0e0395e",
          "name": "cp-server-connect"
        },
        {
          "digest": "sha256:ef89f3bc2656c215a80dd202958bbfd1f843297574dff5a537590d1733aaf38a",
          "image": "docker.io/confluentinc/cp-enterprise-control-center@sha256:ef89f3bc2656c215a80dd202958bbfd1f843297574dff5a537590d1733aaf38a",
          "name": "cp-enterprise-control-center"
        },
        {
          "digest": "sha256:236c01102003982ac70b8f7b101f73de1a049f1900c266fd2986d4f6d4c47ec5",
          "image": "docker.io/confluentinc/confluent-operator@sha256:236c01102003982ac70b8f7b101f73de1a049f1900c266fd2986d4f6d4c47ec5",
          "name": "confluent-operator-236c01102003982ac70b8f7b101f73de1a049f1900c266fd2986d4f6d4c47ec5-annotation"
        }
      ],
      "replaces": null,
      "skip_range": ">=2.2.0 <2.3.0",
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.10",
      "version": "2.3.0",
      "version_original": "2.3.0"
    },
    {
      "_id": "6297e3edefe144fd5e93017b",
      "alm_examples": [
        {
          "api_version": "charts.helm.k8s.io/v1",
          "kind": "Xl",
          "metadata": {
            "name": "xl-release"
          },
          "spec": {
            "global": {
              "customImageNames": false,
              "repository": "registry.connect.redhat.com/turbonomic",
              "tag": "8.5.3"
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/turbonomic/t8c-operator-bundle@sha256:de272015802808cfe5784f1d1326ccce008179d35f39793bb4a6b92d914bf72f",
      "bundle_path_digest": "sha256:de272015802808cfe5784f1d1326ccce008179d35f39793bb4a6b92d914bf72f",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "stable",
      "creation_date": "2022-06-01T22:10:53.695000+00:00",
      "csv_description": "### Realtime Decision Automation for Multicloud Applications\nTurbonomic Workload Automation for Multicloud simultaneously optimizes performance, compliance, and cost in real-time. Workloads are precisely resourced, automatically, to perform while satisfying business constraints:\n* Continuous placement of workload across multiple clouds both on-prem and public clouds providers.\n* Continuous scaling for applications and the underlying infrastructure.\n\nIt assures application performance by giving workloads the resources they need when they need them.\n\n### How does it work?\nTurbonomic uses a public APIs already exposed by application and infrastructure instrumentation to discover and monitor your environment.\nTurbonomic determines the right actions that drive continuous health, including continuous placement and continuous scaling for applications and the underlying cluster.\nTurbonomic leverages the built-on orchestration provided by the application and infrastructure deployment tools and automates the execution of these actions to continiously meet the respective service level objective of each application service.",
      "csv_display_name": "Turbonomic Platform Operator",
      "csv_metadata_description": "Turbonomic Workload Automation for Multicloud simultaneously optimizes performance, compliance, and cost in real-time. Workloads are precisely resourced, automatically, to perform while satisfying business constraints.",
      "csv_name": "t8c-operator.v42.10.0",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-11-23T07:29:21.555000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.9",
      "organization": "certified-operators",
      "package": "t8c-certified",
      "provided_apis": [
        {
          "group": "charts.helm.k8s.io",
          "kind": "Xl",
          "plural": "xls",
          "version": "v1alpha1"
        },
        {
          "group": "charts.helm.k8s.io",
          "kind": "Xl",
          "plural": "xls",
          "version": "v1"
        }
      ],
      "provider": "Turbonomic, Inc.",
      "related_images": [
        {
          "digest": "sha256:5721ddea33cb3cdb4562f9f8867fb1a2de8c7bc51db077459054557bd094dfa6",
          "image": "registry.connect.redhat.com/turbonomic/t8c-operator@sha256:5721ddea33cb3cdb4562f9f8867fb1a2de8c7bc51db077459054557bd094dfa6",
          "name": "t8c-operator-5721ddea33cb3cdb4562f9f8867fb1a2de8c7bc51db077459054557bd094dfa6-annotation"
        },
        {
          "digest": "sha256:5721ddea33cb3cdb4562f9f8867fb1a2de8c7bc51db077459054557bd094dfa6",
          "image": "registry.connect.redhat.com/turbonomic/t8c-operator@sha256:5721ddea33cb3cdb4562f9f8867fb1a2de8c7bc51db077459054557bd094dfa6",
          "name": "t8c-operator"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.9",
      "version": "42.10.0",
      "version_original": "42.10.0"
    },
    {
      "_id": "6297e468e0c0938179ef28a9",
      "alm_examples": [
        {
          "api_version": "charts.helm.k8s.io/v1",
          "kind": "Xl",
          "metadata": {
            "name": "xl-release"
          },
          "spec": {
            "global": {
              "customImageNames": false,
              "repository": "registry.connect.redhat.com/turbonomic",
              "tag": "8.5.3"
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/turbonomic/t8c-operator-bundle@sha256:de272015802808cfe5784f1d1326ccce008179d35f39793bb4a6b92d914bf72f",
      "bundle_path_digest": "sha256:de272015802808cfe5784f1d1326ccce008179d35f39793bb4a6b92d914bf72f",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "stable",
      "creation_date": "2022-06-01T22:12:56.533000+00:00",
      "csv_description": "### Realtime Decision Automation for Multicloud Applications\nTurbonomic Workload Automation for Multicloud simultaneously optimizes performance, compliance, and cost in real-time. Workloads are precisely resourced, automatically, to perform while satisfying business constraints:\n* Continuous placement of workload across multiple clouds both on-prem and public clouds providers.\n* Continuous scaling for applications and the underlying infrastructure.\n\nIt assures application performance by giving workloads the resources they need when they need them.\n\n### How does it work?\nTurbonomic uses a public APIs already exposed by application and infrastructure instrumentation to discover and monitor your environment.\nTurbonomic determines the right actions that drive continuous health, including continuous placement and continuous scaling for applications and the underlying cluster.\nTurbonomic leverages the built-on orchestration provided by the application and infrastructure deployment tools and automates the execution of these actions to continiously meet the respective service level objective of each application service.",
      "csv_display_name": "Turbonomic Platform Operator",
      "csv_metadata_description": "Turbonomic Workload Automation for Multicloud simultaneously optimizes performance, compliance, and cost in real-time. Workloads are precisely resourced, automatically, to perform while satisfying business constraints.",
      "csv_name": "t8c-operator.v42.10.0",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-11-23T06:35:00.653000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.10",
      "organization": "certified-operators",
      "package": "t8c-certified",
      "provided_apis": [
        {
          "group": "charts.helm.k8s.io",
          "kind": "Xl",
          "plural": "xls",
          "version": "v1"
        },
        {
          "group": "charts.helm.k8s.io",
          "kind": "Xl",
          "plural": "xls",
          "version": "v1alpha1"
        }
      ],
      "provider": "Turbonomic, Inc.",
      "related_images": [
        {
          "digest": "sha256:5721ddea33cb3cdb4562f9f8867fb1a2de8c7bc51db077459054557bd094dfa6",
          "image": "registry.connect.redhat.com/turbonomic/t8c-operator@sha256:5721ddea33cb3cdb4562f9f8867fb1a2de8c7bc51db077459054557bd094dfa6",
          "name": "t8c-operator-5721ddea33cb3cdb4562f9f8867fb1a2de8c7bc51db077459054557bd094dfa6-annotation"
        },
        {
          "digest": "sha256:5721ddea33cb3cdb4562f9f8867fb1a2de8c7bc51db077459054557bd094dfa6",
          "image": "registry.connect.redhat.com/turbonomic/t8c-operator@sha256:5721ddea33cb3cdb4562f9f8867fb1a2de8c7bc51db077459054557bd094dfa6",
          "name": "t8c-operator"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.10",
      "version": "42.10.0",
      "version_original": "42.10.0"
    },
    {
      "_id": "6297e70e554afbde4c9b7c23",
      "alm_examples": [
        {
          "api_version": "charts.helm.k8s.io/v1",
          "kind": "Xl",
          "metadata": {
            "name": "xl-release"
          },
          "spec": {
            "global": {
              "customImageNames": false,
              "repository": "registry.connect.redhat.com/turbonomic",
              "tag": "8.5.3"
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/turbonomic/t8c-operator-bundle@sha256:de272015802808cfe5784f1d1326ccce008179d35f39793bb4a6b92d914bf72f",
      "bundle_path_digest": "sha256:de272015802808cfe5784f1d1326ccce008179d35f39793bb4a6b92d914bf72f",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "stable",
      "creation_date": "2022-06-01T22:24:14.370000+00:00",
      "csv_description": "### Realtime Decision Automation for Multicloud Applications\nTurbonomic Workload Automation for Multicloud simultaneously optimizes performance, compliance, and cost in real-time. Workloads are precisely resourced, automatically, to perform while satisfying business constraints:\n* Continuous placement of workload across multiple clouds both on-prem and public clouds providers.\n* Continuous scaling for applications and the underlying infrastructure.\n\nIt assures application performance by giving workloads the resources they need when they need them.\n\n### How does it work?\nTurbonomic uses a public APIs already exposed by application and infrastructure instrumentation to discover and monitor your environment.\nTurbonomic determines the right actions that drive continuous health, including continuous placement and continuous scaling for applications and the underlying cluster.\nTurbonomic leverages the built-on orchestration provided by the application and infrastructure deployment tools and automates the execution of these actions to continiously meet the respective service level objective of each application service.",
      "csv_display_name": "Turbonomic Platform Operator",
      "csv_metadata_description": "Turbonomic Workload Automation for Multicloud simultaneously optimizes performance, compliance, and cost in real-time. Workloads are precisely resourced, automatically, to perform while satisfying business constraints.",
      "csv_name": "t8c-operator.v42.10.0",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-11-23T07:32:07.965000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.8",
      "organization": "certified-operators",
      "package": "t8c-certified",
      "provided_apis": [
        {
          "group": "charts.helm.k8s.io",
          "kind": "Xl",
          "plural": "xls",
          "version": "v1"
        },
        {
          "group": "charts.helm.k8s.io",
          "kind": "Xl",
          "plural": "xls",
          "version": "v1alpha1"
        }
      ],
      "provider": "Turbonomic, Inc.",
      "related_images": [
        {
          "digest": "sha256:5721ddea33cb3cdb4562f9f8867fb1a2de8c7bc51db077459054557bd094dfa6",
          "image": "registry.connect.redhat.com/turbonomic/t8c-operator@sha256:5721ddea33cb3cdb4562f9f8867fb1a2de8c7bc51db077459054557bd094dfa6",
          "name": "t8c-operator-5721ddea33cb3cdb4562f9f8867fb1a2de8c7bc51db077459054557bd094dfa6-annotation"
        },
        {
          "digest": "sha256:5721ddea33cb3cdb4562f9f8867fb1a2de8c7bc51db077459054557bd094dfa6",
          "image": "registry.connect.redhat.com/turbonomic/t8c-operator@sha256:5721ddea33cb3cdb4562f9f8867fb1a2de8c7bc51db077459054557bd094dfa6",
          "name": "t8c-operator"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.8",
      "version": "42.10.0",
      "version_original": "42.10.0"
    },
    {
      "_id": "6297e72f554afbde4c9b7c29",
      "alm_examples": [
        {
          "api_version": "charts.helm.k8s.io/v1",
          "kind": "Xl",
          "metadata": {
            "name": "xl-release"
          },
          "spec": {
            "global": {
              "customImageNames": false,
              "repository": "registry.connect.redhat.com/turbonomic",
              "tag": "8.5.3"
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/turbonomic/t8c-operator-bundle@sha256:de272015802808cfe5784f1d1326ccce008179d35f39793bb4a6b92d914bf72f",
      "bundle_path_digest": "sha256:de272015802808cfe5784f1d1326ccce008179d35f39793bb4a6b92d914bf72f",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "stable",
      "creation_date": "2022-06-01T22:24:47.082000+00:00",
      "csv_description": "### Realtime Decision Automation for Multicloud Applications\nTurbonomic Workload Automation for Multicloud simultaneously optimizes performance, compliance, and cost in real-time. Workloads are precisely resourced, automatically, to perform while satisfying business constraints:\n* Continuous placement of workload across multiple clouds both on-prem and public clouds providers.\n* Continuous scaling for applications and the underlying infrastructure.\n\nIt assures application performance by giving workloads the resources they need when they need them.\n\n### How does it work?\nTurbonomic uses a public APIs already exposed by application and infrastructure instrumentation to discover and monitor your environment.\nTurbonomic determines the right actions that drive continuous health, including continuous placement and continuous scaling for applications and the underlying cluster.\nTurbonomic leverages the built-on orchestration provided by the application and infrastructure deployment tools and automates the execution of these actions to continiously meet the respective service level objective of each application service.",
      "csv_display_name": "Turbonomic Platform Operator",
      "csv_metadata_description": "Turbonomic Workload Automation for Multicloud simultaneously optimizes performance, compliance, and cost in real-time. Workloads are precisely resourced, automatically, to perform while satisfying business constraints.",
      "csv_name": "t8c-operator.v42.10.0",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-11-23T07:38:24.277000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.6",
      "organization": "certified-operators",
      "package": "t8c-certified",
      "provided_apis": [
        {
          "group": "charts.helm.k8s.io",
          "kind": "Xl",
          "plural": "xls",
          "version": "v1alpha1"
        },
        {
          "group": "charts.helm.k8s.io",
          "kind": "Xl",
          "plural": "xls",
          "version": "v1"
        }
      ],
      "provider": "Turbonomic, Inc.",
      "related_images": [
        {
          "digest": "sha256:5721ddea33cb3cdb4562f9f8867fb1a2de8c7bc51db077459054557bd094dfa6",
          "image": "registry.connect.redhat.com/turbonomic/t8c-operator@sha256:5721ddea33cb3cdb4562f9f8867fb1a2de8c7bc51db077459054557bd094dfa6",
          "name": "t8c-operator-5721ddea33cb3cdb4562f9f8867fb1a2de8c7bc51db077459054557bd094dfa6-annotation"
        },
        {
          "digest": "sha256:5721ddea33cb3cdb4562f9f8867fb1a2de8c7bc51db077459054557bd094dfa6",
          "image": "registry.connect.redhat.com/turbonomic/t8c-operator@sha256:5721ddea33cb3cdb4562f9f8867fb1a2de8c7bc51db077459054557bd094dfa6",
          "name": "t8c-operator"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.6",
      "version": "42.10.0",
      "version_original": "42.10.0"
    },
    {
      "_id": "6297eb11554afbde4c9b7d77",
      "alm_examples": [
        {
          "api_version": "charts.helm.k8s.io/v1",
          "kind": "Xl",
          "metadata": {
            "name": "xl-release"
          },
          "spec": {
            "global": {
              "customImageNames": false,
              "repository": "registry.connect.redhat.com/turbonomic",
              "tag": "8.5.3"
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/turbonomic/t8c-operator-bundle@sha256:de272015802808cfe5784f1d1326ccce008179d35f39793bb4a6b92d914bf72f",
      "bundle_path_digest": "sha256:de272015802808cfe5784f1d1326ccce008179d35f39793bb4a6b92d914bf72f",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "stable",
      "creation_date": "2022-06-01T22:41:21.457000+00:00",
      "csv_description": "### Realtime Decision Automation for Multicloud Applications\nTurbonomic Workload Automation for Multicloud simultaneously optimizes performance, compliance, and cost in real-time. Workloads are precisely resourced, automatically, to perform while satisfying business constraints:\n* Continuous placement of workload across multiple clouds both on-prem and public clouds providers.\n* Continuous scaling for applications and the underlying infrastructure.\n\nIt assures application performance by giving workloads the resources they need when they need them.\n\n### How does it work?\nTurbonomic uses a public APIs already exposed by application and infrastructure instrumentation to discover and monitor your environment.\nTurbonomic determines the right actions that drive continuous health, including continuous placement and continuous scaling for applications and the underlying cluster.\nTurbonomic leverages the built-on orchestration provided by the application and infrastructure deployment tools and automates the execution of these actions to continiously meet the respective service level objective of each application service.",
      "csv_display_name": "Turbonomic Platform Operator",
      "csv_metadata_description": "Turbonomic Workload Automation for Multicloud simultaneously optimizes performance, compliance, and cost in real-time. Workloads are precisely resourced, automatically, to perform while satisfying business constraints.",
      "csv_name": "t8c-operator.v42.10.0",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-11-23T07:04:23.256000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.7",
      "organization": "certified-operators",
      "package": "t8c-certified",
      "provided_apis": [
        {
          "group": "charts.helm.k8s.io",
          "kind": "Xl",
          "plural": "xls",
          "version": "v1"
        },
        {
          "group": "charts.helm.k8s.io",
          "kind": "Xl",
          "plural": "xls",
          "version": "v1alpha1"
        }
      ],
      "provider": "Turbonomic, Inc.",
      "related_images": [
        {
          "digest": "sha256:5721ddea33cb3cdb4562f9f8867fb1a2de8c7bc51db077459054557bd094dfa6",
          "image": "registry.connect.redhat.com/turbonomic/t8c-operator@sha256:5721ddea33cb3cdb4562f9f8867fb1a2de8c7bc51db077459054557bd094dfa6",
          "name": "t8c-operator-5721ddea33cb3cdb4562f9f8867fb1a2de8c7bc51db077459054557bd094dfa6-annotation"
        },
        {
          "digest": "sha256:5721ddea33cb3cdb4562f9f8867fb1a2de8c7bc51db077459054557bd094dfa6",
          "image": "registry.connect.redhat.com/turbonomic/t8c-operator@sha256:5721ddea33cb3cdb4562f9f8867fb1a2de8c7bc51db077459054557bd094dfa6",
          "name": "t8c-operator"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.7",
      "version": "42.10.0",
      "version_original": "42.10.0"
    },
    {
      "_id": "62982cc9554afbde4c9b931f",
      "alm_examples": [
        {
          "api_version": "charts.helm.k8s.io/v1",
          "kind": "Kubeturbo",
          "metadata": {
            "name": "kubeturbo-release"
          },
          "spec": {
            "serverMeta": {
              "turboServer": "https://Turbo_server_URL"
            },
            "targetConfig": {
              "targetName": "Cluster_Name"
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/turbonomic/kubeturbo-operator-bundle@sha256:5884d075419f55d2df891c5efaf1613daf29a7998838deb916c5b599af4170c2",
      "bundle_path_digest": "sha256:5884d075419f55d2df891c5efaf1613daf29a7998838deb916c5b599af4170c2",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "stable",
      "creation_date": "2022-06-02T03:21:45.111000+00:00",
      "csv_description": "### Application Resource Management for Kubernetes\nTurbonomic AI-powered Application Resource Management simultaneously optimizes performance, compliance, and cost in real time.\nSoftware manages the complete application stack, automatically. Applications are continually resourced to perform while satisfying business constraints.\n\nTurbonomic makes workloads smart \u2014 enabling them to self-manage and determines the specific actions that will drive continuous health:\n\n* Continuous placement for Pods (rescheduling)\n* Continuous scaling for applications and  the underlying cluster.\n\nIt assures application performance by giving workloads the resources they need when they need them.\n\n### How does it work?\nTurbonomic uses a container \u2014 KubeTurbo \u2014 that runs in your Kubernetes or Red Hat OpenShift cluster to discover and monitor your environment.\nKubeTurbo runs together with the default scheduler and sends this data back to the Turbonomic analytics engine.\nTurbonomic determines the right actions that drive continuous health, including continuous placement for Pods and continuous scaling for applications and the underlying cluster.",
      "csv_display_name": "Kubeturbo Operator",
      "csv_metadata_description": "Turbonomic Workload Automation for Multicloud simultaneously optimizes performance, compliance, and cost in real-time. Workloads are precisely resourced, automatically, to perform while satisfying business constraints.",
      "csv_name": "kubeturbo-operator.v8.5.4",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-11-23T07:00:19.050000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.11",
      "organization": "certified-operators",
      "package": "kubeturbo-certified",
      "provided_apis": [
        {
          "group": "charts.helm.k8s.io",
          "kind": "Kubeturbo",
          "version": "v1"
        },
        {
          "group": "charts.helm.k8s.io",
          "kind": "Kubeturbo",
          "version": "v1alpha1"
        }
      ],
      "provider": "Turbonomic, Inc.",
      "related_images": [
        {
          "digest": "sha256:e5cdbdeeaf639afe9782fceb9bf32c5e06e2b6ed4ded41e06abe8f49bda7ac47",
          "image": "registry.connect.redhat.com/turbonomic/kubeturbo-operator@sha256:e5cdbdeeaf639afe9782fceb9bf32c5e06e2b6ed4ded41e06abe8f49bda7ac47",
          "name": "kubeturbo-operator-e5cdbdeeaf639afe9782fceb9bf32c5e06e2b6ed4ded41e06abe8f49bda7ac47-annotation"
        },
        {
          "digest": "sha256:e5cdbdeeaf639afe9782fceb9bf32c5e06e2b6ed4ded41e06abe8f49bda7ac47",
          "image": "registry.connect.redhat.com/turbonomic/kubeturbo-operator@sha256:e5cdbdeeaf639afe9782fceb9bf32c5e06e2b6ed4ded41e06abe8f49bda7ac47",
          "name": "kubeturbo-operator"
        },
        {
          "digest": "sha256:ef6e5f17c31eb5828b29751591778650c397d89b8de1960b215b34cd2e41810a",
          "image": "registry.connect.redhat.com/turbonomic/kubeturbo@sha256:ef6e5f17c31eb5828b29751591778650c397d89b8de1960b215b34cd2e41810a",
          "name": "kubeturbo"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.11",
      "version": "8.5.4",
      "version_original": "8.5.4"
    },
    {
      "_id": "62982cca8c1a626fc9ae9740",
      "alm_examples": [
        {
          "api_version": "charts.helm.k8s.io/v1",
          "kind": "Xl",
          "metadata": {
            "name": "xl-release"
          },
          "spec": {
            "global": {
              "customImageNames": false,
              "repository": "registry.connect.redhat.com/turbonomic",
              "tag": "8.5.3"
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/turbonomic/t8c-operator-bundle@sha256:de272015802808cfe5784f1d1326ccce008179d35f39793bb4a6b92d914bf72f",
      "bundle_path_digest": "sha256:de272015802808cfe5784f1d1326ccce008179d35f39793bb4a6b92d914bf72f",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "stable",
      "creation_date": "2022-06-02T03:21:46.005000+00:00",
      "csv_description": "### Realtime Decision Automation for Multicloud Applications\nTurbonomic Workload Automation for Multicloud simultaneously optimizes performance, compliance, and cost in real-time. Workloads are precisely resourced, automatically, to perform while satisfying business constraints:\n* Continuous placement of workload across multiple clouds both on-prem and public clouds providers.\n* Continuous scaling for applications and the underlying infrastructure.\n\nIt assures application performance by giving workloads the resources they need when they need them.\n\n### How does it work?\nTurbonomic uses a public APIs already exposed by application and infrastructure instrumentation to discover and monitor your environment.\nTurbonomic determines the right actions that drive continuous health, including continuous placement and continuous scaling for applications and the underlying cluster.\nTurbonomic leverages the built-on orchestration provided by the application and infrastructure deployment tools and automates the execution of these actions to continiously meet the respective service level objective of each application service.",
      "csv_display_name": "Turbonomic Platform Operator",
      "csv_metadata_description": "Turbonomic Workload Automation for Multicloud simultaneously optimizes performance, compliance, and cost in real-time. Workloads are precisely resourced, automatically, to perform while satisfying business constraints.",
      "csv_name": "t8c-operator.v42.10.0",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-11-23T07:07:23.909000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.11",
      "organization": "certified-operators",
      "package": "t8c-certified",
      "provided_apis": [
        {
          "group": "charts.helm.k8s.io",
          "kind": "Xl",
          "version": "v1"
        },
        {
          "group": "charts.helm.k8s.io",
          "kind": "Xl",
          "version": "v1alpha1"
        }
      ],
      "provider": "Turbonomic, Inc.",
      "related_images": [
        {
          "digest": "sha256:5721ddea33cb3cdb4562f9f8867fb1a2de8c7bc51db077459054557bd094dfa6",
          "image": "registry.connect.redhat.com/turbonomic/t8c-operator@sha256:5721ddea33cb3cdb4562f9f8867fb1a2de8c7bc51db077459054557bd094dfa6",
          "name": "t8c-operator-5721ddea33cb3cdb4562f9f8867fb1a2de8c7bc51db077459054557bd094dfa6-annotation"
        },
        {
          "digest": "sha256:5721ddea33cb3cdb4562f9f8867fb1a2de8c7bc51db077459054557bd094dfa6",
          "image": "registry.connect.redhat.com/turbonomic/t8c-operator@sha256:5721ddea33cb3cdb4562f9f8867fb1a2de8c7bc51db077459054557bd094dfa6",
          "name": "t8c-operator"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.11",
      "version": "42.10.0",
      "version_original": "42.10.0"
    },
    {
      "_id": "62982cd48c1a626fc9ae9749",
      "alm_examples": [
        {
          "api_version": "charts.helm.k8s.io/v1",
          "kind": "Kubeturbo",
          "metadata": {
            "name": "kubeturbo-release"
          },
          "spec": {
            "serverMeta": {
              "turboServer": "https://Turbo_server_URL"
            },
            "targetConfig": {
              "targetName": "Cluster_Name"
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/turbonomic/kubeturbo-operator-bundle@sha256:5884d075419f55d2df891c5efaf1613daf29a7998838deb916c5b599af4170c2",
      "bundle_path_digest": "sha256:5884d075419f55d2df891c5efaf1613daf29a7998838deb916c5b599af4170c2",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "stable",
      "creation_date": "2022-06-02T03:21:56.485000+00:00",
      "csv_description": "### Application Resource Management for Kubernetes\nTurbonomic AI-powered Application Resource Management simultaneously optimizes performance, compliance, and cost in real time.\nSoftware manages the complete application stack, automatically. Applications are continually resourced to perform while satisfying business constraints.\n\nTurbonomic makes workloads smart \u2014 enabling them to self-manage and determines the specific actions that will drive continuous health:\n\n* Continuous placement for Pods (rescheduling)\n* Continuous scaling for applications and  the underlying cluster.\n\nIt assures application performance by giving workloads the resources they need when they need them.\n\n### How does it work?\nTurbonomic uses a container \u2014 KubeTurbo \u2014 that runs in your Kubernetes or Red Hat OpenShift cluster to discover and monitor your environment.\nKubeTurbo runs together with the default scheduler and sends this data back to the Turbonomic analytics engine.\nTurbonomic determines the right actions that drive continuous health, including continuous placement for Pods and continuous scaling for applications and the underlying cluster.",
      "csv_display_name": "Kubeturbo Operator",
      "csv_metadata_description": "Turbonomic Workload Automation for Multicloud simultaneously optimizes performance, compliance, and cost in real-time. Workloads are precisely resourced, automatically, to perform while satisfying business constraints.",
      "csv_name": "kubeturbo-operator.v8.5.4",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-11-23T08:38:05.980000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.10",
      "organization": "certified-operators",
      "package": "kubeturbo-certified",
      "provided_apis": [
        {
          "group": "charts.helm.k8s.io",
          "kind": "Kubeturbo",
          "plural": "kubeturbos",
          "version": "v1alpha1"
        },
        {
          "group": "charts.helm.k8s.io",
          "kind": "Kubeturbo",
          "plural": "kubeturbos",
          "version": "v1"
        }
      ],
      "provider": "Turbonomic, Inc.",
      "related_images": [
        {
          "digest": "sha256:e5cdbdeeaf639afe9782fceb9bf32c5e06e2b6ed4ded41e06abe8f49bda7ac47",
          "image": "registry.connect.redhat.com/turbonomic/kubeturbo-operator@sha256:e5cdbdeeaf639afe9782fceb9bf32c5e06e2b6ed4ded41e06abe8f49bda7ac47",
          "name": "kubeturbo-operator-e5cdbdeeaf639afe9782fceb9bf32c5e06e2b6ed4ded41e06abe8f49bda7ac47-annotation"
        },
        {
          "digest": "sha256:e5cdbdeeaf639afe9782fceb9bf32c5e06e2b6ed4ded41e06abe8f49bda7ac47",
          "image": "registry.connect.redhat.com/turbonomic/kubeturbo-operator@sha256:e5cdbdeeaf639afe9782fceb9bf32c5e06e2b6ed4ded41e06abe8f49bda7ac47",
          "name": "kubeturbo-operator"
        },
        {
          "digest": "sha256:ef6e5f17c31eb5828b29751591778650c397d89b8de1960b215b34cd2e41810a",
          "image": "registry.connect.redhat.com/turbonomic/kubeturbo@sha256:ef6e5f17c31eb5828b29751591778650c397d89b8de1960b215b34cd2e41810a",
          "name": "kubeturbo"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.10",
      "version": "8.5.4",
      "version_original": "8.5.4"
    },
    {
      "_id": "62982d74554afbde4c9b9388",
      "alm_examples": [
        {
          "api_version": "charts.helm.k8s.io/v1",
          "kind": "Kubeturbo",
          "metadata": {
            "name": "kubeturbo-release"
          },
          "spec": {
            "serverMeta": {
              "turboServer": "https://Turbo_server_URL"
            },
            "targetConfig": {
              "targetName": "Cluster_Name"
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/turbonomic/kubeturbo-operator-bundle@sha256:5884d075419f55d2df891c5efaf1613daf29a7998838deb916c5b599af4170c2",
      "bundle_path_digest": "sha256:5884d075419f55d2df891c5efaf1613daf29a7998838deb916c5b599af4170c2",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "stable",
      "creation_date": "2022-06-02T03:24:36.584000+00:00",
      "csv_description": "### Application Resource Management for Kubernetes\nTurbonomic AI-powered Application Resource Management simultaneously optimizes performance, compliance, and cost in real time.\nSoftware manages the complete application stack, automatically. Applications are continually resourced to perform while satisfying business constraints.\n\nTurbonomic makes workloads smart \u2014 enabling them to self-manage and determines the specific actions that will drive continuous health:\n\n* Continuous placement for Pods (rescheduling)\n* Continuous scaling for applications and  the underlying cluster.\n\nIt assures application performance by giving workloads the resources they need when they need them.\n\n### How does it work?\nTurbonomic uses a container \u2014 KubeTurbo \u2014 that runs in your Kubernetes or Red Hat OpenShift cluster to discover and monitor your environment.\nKubeTurbo runs together with the default scheduler and sends this data back to the Turbonomic analytics engine.\nTurbonomic determines the right actions that drive continuous health, including continuous placement for Pods and continuous scaling for applications and the underlying cluster.",
      "csv_display_name": "Kubeturbo Operator",
      "csv_metadata_description": "Turbonomic Workload Automation for Multicloud simultaneously optimizes performance, compliance, and cost in real-time. Workloads are precisely resourced, automatically, to perform while satisfying business constraints.",
      "csv_name": "kubeturbo-operator.v8.5.4",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-11-23T07:20:29.698000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.9",
      "organization": "certified-operators",
      "package": "kubeturbo-certified",
      "provided_apis": [
        {
          "group": "charts.helm.k8s.io",
          "kind": "Kubeturbo",
          "plural": "kubeturbos",
          "version": "v1alpha1"
        },
        {
          "group": "charts.helm.k8s.io",
          "kind": "Kubeturbo",
          "plural": "kubeturbos",
          "version": "v1"
        }
      ],
      "provider": "Turbonomic, Inc.",
      "related_images": [
        {
          "digest": "sha256:e5cdbdeeaf639afe9782fceb9bf32c5e06e2b6ed4ded41e06abe8f49bda7ac47",
          "image": "registry.connect.redhat.com/turbonomic/kubeturbo-operator@sha256:e5cdbdeeaf639afe9782fceb9bf32c5e06e2b6ed4ded41e06abe8f49bda7ac47",
          "name": "kubeturbo-operator-e5cdbdeeaf639afe9782fceb9bf32c5e06e2b6ed4ded41e06abe8f49bda7ac47-annotation"
        },
        {
          "digest": "sha256:e5cdbdeeaf639afe9782fceb9bf32c5e06e2b6ed4ded41e06abe8f49bda7ac47",
          "image": "registry.connect.redhat.com/turbonomic/kubeturbo-operator@sha256:e5cdbdeeaf639afe9782fceb9bf32c5e06e2b6ed4ded41e06abe8f49bda7ac47",
          "name": "kubeturbo-operator"
        },
        {
          "digest": "sha256:ef6e5f17c31eb5828b29751591778650c397d89b8de1960b215b34cd2e41810a",
          "image": "registry.connect.redhat.com/turbonomic/kubeturbo@sha256:ef6e5f17c31eb5828b29751591778650c397d89b8de1960b215b34cd2e41810a",
          "name": "kubeturbo"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.9",
      "version": "8.5.4",
      "version_original": "8.5.4"
    },
    {
      "_id": "62982d9d8c1a626fc9ae97b9",
      "alm_examples": [
        {
          "api_version": "charts.helm.k8s.io/v1",
          "kind": "Kubeturbo",
          "metadata": {
            "name": "kubeturbo-release"
          },
          "spec": {
            "serverMeta": {
              "turboServer": "https://Turbo_server_URL"
            },
            "targetConfig": {
              "targetName": "Cluster_Name"
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/turbonomic/kubeturbo-operator-bundle@sha256:5884d075419f55d2df891c5efaf1613daf29a7998838deb916c5b599af4170c2",
      "bundle_path_digest": "sha256:5884d075419f55d2df891c5efaf1613daf29a7998838deb916c5b599af4170c2",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "stable",
      "creation_date": "2022-06-02T03:25:17.647000+00:00",
      "csv_description": "### Application Resource Management for Kubernetes\nTurbonomic AI-powered Application Resource Management simultaneously optimizes performance, compliance, and cost in real time.\nSoftware manages the complete application stack, automatically. Applications are continually resourced to perform while satisfying business constraints.\n\nTurbonomic makes workloads smart \u2014 enabling them to self-manage and determines the specific actions that will drive continuous health:\n\n* Continuous placement for Pods (rescheduling)\n* Continuous scaling for applications and  the underlying cluster.\n\nIt assures application performance by giving workloads the resources they need when they need them.\n\n### How does it work?\nTurbonomic uses a container \u2014 KubeTurbo \u2014 that runs in your Kubernetes or Red Hat OpenShift cluster to discover and monitor your environment.\nKubeTurbo runs together with the default scheduler and sends this data back to the Turbonomic analytics engine.\nTurbonomic determines the right actions that drive continuous health, including continuous placement for Pods and continuous scaling for applications and the underlying cluster.",
      "csv_display_name": "Kubeturbo Operator",
      "csv_metadata_description": "Turbonomic Workload Automation for Multicloud simultaneously optimizes performance, compliance, and cost in real-time. Workloads are precisely resourced, automatically, to perform while satisfying business constraints.",
      "csv_name": "kubeturbo-operator.v8.5.4",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-11-23T07:37:09.479000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.8",
      "organization": "certified-operators",
      "package": "kubeturbo-certified",
      "provided_apis": [
        {
          "group": "charts.helm.k8s.io",
          "kind": "Kubeturbo",
          "plural": "kubeturbos",
          "version": "v1alpha1"
        },
        {
          "group": "charts.helm.k8s.io",
          "kind": "Kubeturbo",
          "plural": "kubeturbos",
          "version": "v1"
        }
      ],
      "provider": "Turbonomic, Inc.",
      "related_images": [
        {
          "digest": "sha256:e5cdbdeeaf639afe9782fceb9bf32c5e06e2b6ed4ded41e06abe8f49bda7ac47",
          "image": "registry.connect.redhat.com/turbonomic/kubeturbo-operator@sha256:e5cdbdeeaf639afe9782fceb9bf32c5e06e2b6ed4ded41e06abe8f49bda7ac47",
          "name": "kubeturbo-operator-e5cdbdeeaf639afe9782fceb9bf32c5e06e2b6ed4ded41e06abe8f49bda7ac47-annotation"
        },
        {
          "digest": "sha256:e5cdbdeeaf639afe9782fceb9bf32c5e06e2b6ed4ded41e06abe8f49bda7ac47",
          "image": "registry.connect.redhat.com/turbonomic/kubeturbo-operator@sha256:e5cdbdeeaf639afe9782fceb9bf32c5e06e2b6ed4ded41e06abe8f49bda7ac47",
          "name": "kubeturbo-operator"
        },
        {
          "digest": "sha256:ef6e5f17c31eb5828b29751591778650c397d89b8de1960b215b34cd2e41810a",
          "image": "registry.connect.redhat.com/turbonomic/kubeturbo@sha256:ef6e5f17c31eb5828b29751591778650c397d89b8de1960b215b34cd2e41810a",
          "name": "kubeturbo"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.8",
      "version": "8.5.4",
      "version_original": "8.5.4"
    },
    {
      "_id": "62982db58c1a626fc9ae97c6",
      "alm_examples": [
        {
          "api_version": "charts.helm.k8s.io/v1",
          "kind": "Kubeturbo",
          "metadata": {
            "name": "kubeturbo-release"
          },
          "spec": {
            "serverMeta": {
              "turboServer": "https://Turbo_server_URL"
            },
            "targetConfig": {
              "targetName": "Cluster_Name"
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/turbonomic/kubeturbo-operator-bundle@sha256:5884d075419f55d2df891c5efaf1613daf29a7998838deb916c5b599af4170c2",
      "bundle_path_digest": "sha256:5884d075419f55d2df891c5efaf1613daf29a7998838deb916c5b599af4170c2",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "stable",
      "creation_date": "2022-06-02T03:25:41.377000+00:00",
      "csv_description": "### Application Resource Management for Kubernetes\nTurbonomic AI-powered Application Resource Management simultaneously optimizes performance, compliance, and cost in real time.\nSoftware manages the complete application stack, automatically. Applications are continually resourced to perform while satisfying business constraints.\n\nTurbonomic makes workloads smart \u2014 enabling them to self-manage and determines the specific actions that will drive continuous health:\n\n* Continuous placement for Pods (rescheduling)\n* Continuous scaling for applications and  the underlying cluster.\n\nIt assures application performance by giving workloads the resources they need when they need them.\n\n### How does it work?\nTurbonomic uses a container \u2014 KubeTurbo \u2014 that runs in your Kubernetes or Red Hat OpenShift cluster to discover and monitor your environment.\nKubeTurbo runs together with the default scheduler and sends this data back to the Turbonomic analytics engine.\nTurbonomic determines the right actions that drive continuous health, including continuous placement for Pods and continuous scaling for applications and the underlying cluster.",
      "csv_display_name": "Kubeturbo Operator",
      "csv_metadata_description": "Turbonomic Workload Automation for Multicloud simultaneously optimizes performance, compliance, and cost in real-time. Workloads are precisely resourced, automatically, to perform while satisfying business constraints.",
      "csv_name": "kubeturbo-operator.v8.5.4",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-11-23T07:20:38.451000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.7",
      "organization": "certified-operators",
      "package": "kubeturbo-certified",
      "provided_apis": [
        {
          "group": "charts.helm.k8s.io",
          "kind": "Kubeturbo",
          "plural": "kubeturbos",
          "version": "v1alpha1"
        },
        {
          "group": "charts.helm.k8s.io",
          "kind": "Kubeturbo",
          "plural": "kubeturbos",
          "version": "v1"
        }
      ],
      "provider": "Turbonomic, Inc.",
      "related_images": [
        {
          "digest": "sha256:e5cdbdeeaf639afe9782fceb9bf32c5e06e2b6ed4ded41e06abe8f49bda7ac47",
          "image": "registry.connect.redhat.com/turbonomic/kubeturbo-operator@sha256:e5cdbdeeaf639afe9782fceb9bf32c5e06e2b6ed4ded41e06abe8f49bda7ac47",
          "name": "kubeturbo-operator-e5cdbdeeaf639afe9782fceb9bf32c5e06e2b6ed4ded41e06abe8f49bda7ac47-annotation"
        },
        {
          "digest": "sha256:e5cdbdeeaf639afe9782fceb9bf32c5e06e2b6ed4ded41e06abe8f49bda7ac47",
          "image": "registry.connect.redhat.com/turbonomic/kubeturbo-operator@sha256:e5cdbdeeaf639afe9782fceb9bf32c5e06e2b6ed4ded41e06abe8f49bda7ac47",
          "name": "kubeturbo-operator"
        },
        {
          "digest": "sha256:ef6e5f17c31eb5828b29751591778650c397d89b8de1960b215b34cd2e41810a",
          "image": "registry.connect.redhat.com/turbonomic/kubeturbo@sha256:ef6e5f17c31eb5828b29751591778650c397d89b8de1960b215b34cd2e41810a",
          "name": "kubeturbo"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.7",
      "version": "8.5.4",
      "version_original": "8.5.4"
    },
    {
      "_id": "629830e0e0c0938179ef42a9",
      "alm_examples": [
        {
          "api_version": "charts.helm.k8s.io/v1",
          "kind": "Kubeturbo",
          "metadata": {
            "name": "kubeturbo-release"
          },
          "spec": {
            "serverMeta": {
              "turboServer": "https://Turbo_server_URL"
            },
            "targetConfig": {
              "targetName": "Cluster_Name"
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/turbonomic/kubeturbo-operator-bundle@sha256:5884d075419f55d2df891c5efaf1613daf29a7998838deb916c5b599af4170c2",
      "bundle_path_digest": "sha256:5884d075419f55d2df891c5efaf1613daf29a7998838deb916c5b599af4170c2",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "stable",
      "creation_date": "2022-06-02T03:39:12.098000+00:00",
      "csv_description": "### Application Resource Management for Kubernetes\nTurbonomic AI-powered Application Resource Management simultaneously optimizes performance, compliance, and cost in real time.\nSoftware manages the complete application stack, automatically. Applications are continually resourced to perform while satisfying business constraints.\n\nTurbonomic makes workloads smart \u2014 enabling them to self-manage and determines the specific actions that will drive continuous health:\n\n* Continuous placement for Pods (rescheduling)\n* Continuous scaling for applications and  the underlying cluster.\n\nIt assures application performance by giving workloads the resources they need when they need them.\n\n### How does it work?\nTurbonomic uses a container \u2014 KubeTurbo \u2014 that runs in your Kubernetes or Red Hat OpenShift cluster to discover and monitor your environment.\nKubeTurbo runs together with the default scheduler and sends this data back to the Turbonomic analytics engine.\nTurbonomic determines the right actions that drive continuous health, including continuous placement for Pods and continuous scaling for applications and the underlying cluster.",
      "csv_display_name": "Kubeturbo Operator",
      "csv_metadata_description": "Turbonomic Workload Automation for Multicloud simultaneously optimizes performance, compliance, and cost in real-time. Workloads are precisely resourced, automatically, to perform while satisfying business constraints.",
      "csv_name": "kubeturbo-operator.v8.5.4",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-11-23T07:35:39.713000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.6",
      "organization": "certified-operators",
      "package": "kubeturbo-certified",
      "provided_apis": [
        {
          "group": "charts.helm.k8s.io",
          "kind": "Kubeturbo",
          "plural": "kubeturbos",
          "version": "v1alpha1"
        },
        {
          "group": "charts.helm.k8s.io",
          "kind": "Kubeturbo",
          "plural": "kubeturbos",
          "version": "v1"
        }
      ],
      "provider": "Turbonomic, Inc.",
      "related_images": [
        {
          "digest": "sha256:e5cdbdeeaf639afe9782fceb9bf32c5e06e2b6ed4ded41e06abe8f49bda7ac47",
          "image": "registry.connect.redhat.com/turbonomic/kubeturbo-operator@sha256:e5cdbdeeaf639afe9782fceb9bf32c5e06e2b6ed4ded41e06abe8f49bda7ac47",
          "name": "kubeturbo-operator-e5cdbdeeaf639afe9782fceb9bf32c5e06e2b6ed4ded41e06abe8f49bda7ac47-annotation"
        },
        {
          "digest": "sha256:e5cdbdeeaf639afe9782fceb9bf32c5e06e2b6ed4ded41e06abe8f49bda7ac47",
          "image": "registry.connect.redhat.com/turbonomic/kubeturbo-operator@sha256:e5cdbdeeaf639afe9782fceb9bf32c5e06e2b6ed4ded41e06abe8f49bda7ac47",
          "name": "kubeturbo-operator"
        },
        {
          "digest": "sha256:ef6e5f17c31eb5828b29751591778650c397d89b8de1960b215b34cd2e41810a",
          "image": "registry.connect.redhat.com/turbonomic/kubeturbo@sha256:ef6e5f17c31eb5828b29751591778650c397d89b8de1960b215b34cd2e41810a",
          "name": "kubeturbo"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.6",
      "version": "8.5.4",
      "version_original": "8.5.4"
    },
    {
      "_id": "629852008c1a626fc9aea927",
      "alm_examples": [
        {
          "api_version": "ibm.com/v1",
          "kind": "IBMSecurityVerify",
          "metadata": {
            "name": "ibmsecurityverify-sample"
          },
          "spec": {
            "clientSecret": "--secret--",
            "sessionLifetime": 3600,
            "ssoPath": "/verify-sso"
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/ibm/verify-operator-bundle@sha256:0deef311d65c60bedc0af39fb6dfde09b0b1a60825eaa37188b21b5a692397e9",
      "bundle_path_digest": "sha256:0deef311d65c60bedc0af39fb6dfde09b0b1a60825eaa37188b21b5a692397e9",
      "capabilities": [
        "Seamless Upgrades"
      ],
      "channel_name": "stable",
      "creation_date": "2022-06-02T06:00:32.638000+00:00",
      "csv_description": "[IBM Security Verify](https://www.ibm.com/products/verify-saas) is an Identity-as-a-Service platform that allows IT, security and business leaders to protect their digital users, assets and data in a hybrid, multi-cloud world by enabling technical agility and operational efficiency. IBM Security Verify SaaS provides single sign-on (SSO), multi-factor authentication (MFA), AI-powered context for risk-based authentication for adaptive access decisions, user management, access recertification campaigns and identity analytics.\n\nAs part of using Red Hat OpenShift, you are entitled to Verify SaaS SSO for unlimited apps and users.  If you do not already have a Verify SaaS SSO tenant a new one can be [created](https://www.ibm.com/account/reg/us-en/signup?formid=urx-51255).\n\nFor a detailed description of IBM Security Verify refer to the [Official documentation](https://www.ibm.com/docs/en/security-verify).\n\nThe IBM Security Verify operator can consistently enforce policy-driven security by using the Ingress networking capability of OpenShift in conjunction with the [Nginx Ingress operator](https://www.nginx.com/blog/getting-started-nginx-ingress-operator-red-hat-openshift/). With this approach, you can enforce authentication and authorization policies for all of the applications in your cluster at the same time, without ever changing your application code. You can also dynamically register your application to start protecting them centrally from the cloud via Verify SaaS. \n\nSee the project [Readme](https://github.com/IBM-Security/verify-operator/blob/master/README.md) for further information and details.\n\n",
      "csv_display_name": "IBM Security Verify Operator",
      "csv_metadata_description": "The IBM Security Verify operator can consistently enforce policy-driven security, including authentication and authorization, using the Ingress networking capability of OpenShift.",
      "csv_name": "ibm-security-verify-operator.v22.6.0",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": false,
          "type": "OwnNamespace"
        },
        {
          "supported": false,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-11-23T06:44:11.538000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.11",
      "organization": "certified-operators",
      "package": "ibm-security-verify-operator",
      "provided_apis": [
        {
          "group": "ibm.com",
          "kind": "IBMSecurityVerify",
          "version": "v1"
        }
      ],
      "provider": "IBM",
      "related_images": [
        {
          "digest": "sha256:3c8add266bdabd7e19ec60c097e9048e5837d7b81c225dcaeec33df9f515b21a",
          "image": "ibmcom/verify-operator@sha256:3c8add266bdabd7e19ec60c097e9048e5837d7b81c225dcaeec33df9f515b21a",
          "name": "verify-operator-3c8add266bdabd7e19ec60c097e9048e5837d7b81c225dcaeec33df9f515b21a-annotation"
        },
        {
          "digest": "sha256:db06cc4c084dd0253134f156dddaaf53ef1c3fb3cc809e5d81711baa4029ea4c",
          "image": "gcr.io/kubebuilder/kube-rbac-proxy@sha256:db06cc4c084dd0253134f156dddaaf53ef1c3fb3cc809e5d81711baa4029ea4c",
          "name": "kube-rbac-proxy"
        },
        {
          "digest": "sha256:3c8add266bdabd7e19ec60c097e9048e5837d7b81c225dcaeec33df9f515b21a",
          "image": "ibmcom/verify-operator@sha256:3c8add266bdabd7e19ec60c097e9048e5837d7b81c225dcaeec33df9f515b21a",
          "name": "manager"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.11",
      "version": "22.6.0",
      "version_original": "22.6.0"
    },
    {
      "_id": "62985302e0c0938179ef5250",
      "alm_examples": [
        {
          "api_version": "ibm.com/v1",
          "kind": "IBMSecurityVerify",
          "metadata": {
            "name": "ibmsecurityverify-sample"
          },
          "spec": {
            "clientSecret": "--secret--",
            "sessionLifetime": 3600,
            "ssoPath": "/verify-sso"
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/ibm/verify-operator-bundle@sha256:0deef311d65c60bedc0af39fb6dfde09b0b1a60825eaa37188b21b5a692397e9",
      "bundle_path_digest": "sha256:0deef311d65c60bedc0af39fb6dfde09b0b1a60825eaa37188b21b5a692397e9",
      "capabilities": [
        "Seamless Upgrades"
      ],
      "channel_name": "stable",
      "creation_date": "2022-06-02T06:04:50.609000+00:00",
      "csv_description": "[IBM Security Verify](https://www.ibm.com/products/verify-saas) is an Identity-as-a-Service platform that allows IT, security and business leaders to protect their digital users, assets and data in a hybrid, multi-cloud world by enabling technical agility and operational efficiency. IBM Security Verify SaaS provides single sign-on (SSO), multi-factor authentication (MFA), AI-powered context for risk-based authentication for adaptive access decisions, user management, access recertification campaigns and identity analytics.\n\nAs part of using Red Hat OpenShift, you are entitled to Verify SaaS SSO for unlimited apps and users.  If you do not already have a Verify SaaS SSO tenant a new one can be [created](https://www.ibm.com/account/reg/us-en/signup?formid=urx-51255).\n\nFor a detailed description of IBM Security Verify refer to the [Official documentation](https://www.ibm.com/docs/en/security-verify).\n\nThe IBM Security Verify operator can consistently enforce policy-driven security by using the Ingress networking capability of OpenShift in conjunction with the [Nginx Ingress operator](https://www.nginx.com/blog/getting-started-nginx-ingress-operator-red-hat-openshift/). With this approach, you can enforce authentication and authorization policies for all of the applications in your cluster at the same time, without ever changing your application code. You can also dynamically register your application to start protecting them centrally from the cloud via Verify SaaS. \n\nSee the project [Readme](https://github.com/IBM-Security/verify-operator/blob/master/README.md) for further information and details.\n\n",
      "csv_display_name": "IBM Security Verify Operator",
      "csv_metadata_description": "The IBM Security Verify operator can consistently enforce policy-driven security, including authentication and authorization, using the Ingress networking capability of OpenShift.",
      "csv_name": "ibm-security-verify-operator.v22.6.0",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": false,
          "type": "OwnNamespace"
        },
        {
          "supported": false,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-11-23T07:43:31.278000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.8",
      "organization": "certified-operators",
      "package": "ibm-security-verify-operator",
      "provided_apis": [
        {
          "group": "ibm.com",
          "kind": "IBMSecurityVerify",
          "plural": "ibmsecurityverifies",
          "version": "v1"
        }
      ],
      "provider": "IBM",
      "related_images": [
        {
          "digest": "sha256:3c8add266bdabd7e19ec60c097e9048e5837d7b81c225dcaeec33df9f515b21a",
          "image": "ibmcom/verify-operator@sha256:3c8add266bdabd7e19ec60c097e9048e5837d7b81c225dcaeec33df9f515b21a",
          "name": "verify-operator-3c8add266bdabd7e19ec60c097e9048e5837d7b81c225dcaeec33df9f515b21a-annotation"
        },
        {
          "digest": "sha256:db06cc4c084dd0253134f156dddaaf53ef1c3fb3cc809e5d81711baa4029ea4c",
          "image": "gcr.io/kubebuilder/kube-rbac-proxy@sha256:db06cc4c084dd0253134f156dddaaf53ef1c3fb3cc809e5d81711baa4029ea4c",
          "name": "kube-rbac-proxy"
        },
        {
          "digest": "sha256:3c8add266bdabd7e19ec60c097e9048e5837d7b81c225dcaeec33df9f515b21a",
          "image": "ibmcom/verify-operator@sha256:3c8add266bdabd7e19ec60c097e9048e5837d7b81c225dcaeec33df9f515b21a",
          "name": "manager"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.8",
      "version": "22.6.0",
      "version_original": "22.6.0"
    },
    {
      "_id": "629853798c1a626fc9aea9ca",
      "alm_examples": [
        {
          "api_version": "ibm.com/v1",
          "kind": "IBMSecurityVerify",
          "metadata": {
            "name": "ibmsecurityverify-sample"
          },
          "spec": {
            "clientSecret": "--secret--",
            "sessionLifetime": 3600,
            "ssoPath": "/verify-sso"
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/ibm/verify-operator-bundle@sha256:0deef311d65c60bedc0af39fb6dfde09b0b1a60825eaa37188b21b5a692397e9",
      "bundle_path_digest": "sha256:0deef311d65c60bedc0af39fb6dfde09b0b1a60825eaa37188b21b5a692397e9",
      "capabilities": [
        "Seamless Upgrades"
      ],
      "channel_name": "stable",
      "creation_date": "2022-06-02T06:06:49.849000+00:00",
      "csv_description": "[IBM Security Verify](https://www.ibm.com/products/verify-saas) is an Identity-as-a-Service platform that allows IT, security and business leaders to protect their digital users, assets and data in a hybrid, multi-cloud world by enabling technical agility and operational efficiency. IBM Security Verify SaaS provides single sign-on (SSO), multi-factor authentication (MFA), AI-powered context for risk-based authentication for adaptive access decisions, user management, access recertification campaigns and identity analytics.\n\nAs part of using Red Hat OpenShift, you are entitled to Verify SaaS SSO for unlimited apps and users.  If you do not already have a Verify SaaS SSO tenant a new one can be [created](https://www.ibm.com/account/reg/us-en/signup?formid=urx-51255).\n\nFor a detailed description of IBM Security Verify refer to the [Official documentation](https://www.ibm.com/docs/en/security-verify).\n\nThe IBM Security Verify operator can consistently enforce policy-driven security by using the Ingress networking capability of OpenShift in conjunction with the [Nginx Ingress operator](https://www.nginx.com/blog/getting-started-nginx-ingress-operator-red-hat-openshift/). With this approach, you can enforce authentication and authorization policies for all of the applications in your cluster at the same time, without ever changing your application code. You can also dynamically register your application to start protecting them centrally from the cloud via Verify SaaS. \n\nSee the project [Readme](https://github.com/IBM-Security/verify-operator/blob/master/README.md) for further information and details.\n\n",
      "csv_display_name": "IBM Security Verify Operator",
      "csv_metadata_description": "The IBM Security Verify operator can consistently enforce policy-driven security, including authentication and authorization, using the Ingress networking capability of OpenShift.",
      "csv_name": "ibm-security-verify-operator.v22.6.0",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": false,
          "type": "OwnNamespace"
        },
        {
          "supported": false,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-11-23T08:31:17.006000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.10",
      "organization": "certified-operators",
      "package": "ibm-security-verify-operator",
      "provided_apis": [
        {
          "group": "ibm.com",
          "kind": "IBMSecurityVerify",
          "plural": "ibmsecurityverifies",
          "version": "v1"
        }
      ],
      "provider": "IBM",
      "related_images": [
        {
          "digest": "sha256:3c8add266bdabd7e19ec60c097e9048e5837d7b81c225dcaeec33df9f515b21a",
          "image": "ibmcom/verify-operator@sha256:3c8add266bdabd7e19ec60c097e9048e5837d7b81c225dcaeec33df9f515b21a",
          "name": "verify-operator-3c8add266bdabd7e19ec60c097e9048e5837d7b81c225dcaeec33df9f515b21a-annotation"
        },
        {
          "digest": "sha256:db06cc4c084dd0253134f156dddaaf53ef1c3fb3cc809e5d81711baa4029ea4c",
          "image": "gcr.io/kubebuilder/kube-rbac-proxy@sha256:db06cc4c084dd0253134f156dddaaf53ef1c3fb3cc809e5d81711baa4029ea4c",
          "name": "kube-rbac-proxy"
        },
        {
          "digest": "sha256:3c8add266bdabd7e19ec60c097e9048e5837d7b81c225dcaeec33df9f515b21a",
          "image": "ibmcom/verify-operator@sha256:3c8add266bdabd7e19ec60c097e9048e5837d7b81c225dcaeec33df9f515b21a",
          "name": "manager"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.10",
      "version": "22.6.0",
      "version_original": "22.6.0"
    },
    {
      "_id": "629853978c1a626fc9aea9e3",
      "alm_examples": [
        {
          "api_version": "ibm.com/v1",
          "kind": "IBMSecurityVerify",
          "metadata": {
            "name": "ibmsecurityverify-sample"
          },
          "spec": {
            "clientSecret": "--secret--",
            "sessionLifetime": 3600,
            "ssoPath": "/verify-sso"
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/ibm/verify-operator-bundle@sha256:0deef311d65c60bedc0af39fb6dfde09b0b1a60825eaa37188b21b5a692397e9",
      "bundle_path_digest": "sha256:0deef311d65c60bedc0af39fb6dfde09b0b1a60825eaa37188b21b5a692397e9",
      "capabilities": [
        "Seamless Upgrades"
      ],
      "channel_name": "stable",
      "creation_date": "2022-06-02T06:07:19.578000+00:00",
      "csv_description": "[IBM Security Verify](https://www.ibm.com/products/verify-saas) is an Identity-as-a-Service platform that allows IT, security and business leaders to protect their digital users, assets and data in a hybrid, multi-cloud world by enabling technical agility and operational efficiency. IBM Security Verify SaaS provides single sign-on (SSO), multi-factor authentication (MFA), AI-powered context for risk-based authentication for adaptive access decisions, user management, access recertification campaigns and identity analytics.\n\nAs part of using Red Hat OpenShift, you are entitled to Verify SaaS SSO for unlimited apps and users.  If you do not already have a Verify SaaS SSO tenant a new one can be [created](https://www.ibm.com/account/reg/us-en/signup?formid=urx-51255).\n\nFor a detailed description of IBM Security Verify refer to the [Official documentation](https://www.ibm.com/docs/en/security-verify).\n\nThe IBM Security Verify operator can consistently enforce policy-driven security by using the Ingress networking capability of OpenShift in conjunction with the [Nginx Ingress operator](https://www.nginx.com/blog/getting-started-nginx-ingress-operator-red-hat-openshift/). With this approach, you can enforce authentication and authorization policies for all of the applications in your cluster at the same time, without ever changing your application code. You can also dynamically register your application to start protecting them centrally from the cloud via Verify SaaS. \n\nSee the project [Readme](https://github.com/IBM-Security/verify-operator/blob/master/README.md) for further information and details.\n\n",
      "csv_display_name": "IBM Security Verify Operator",
      "csv_metadata_description": "The IBM Security Verify operator can consistently enforce policy-driven security, including authentication and authorization, using the Ingress networking capability of OpenShift.",
      "csv_name": "ibm-security-verify-operator.v22.6.0",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": false,
          "type": "OwnNamespace"
        },
        {
          "supported": false,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-11-23T07:15:39.303000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.9",
      "organization": "certified-operators",
      "package": "ibm-security-verify-operator",
      "provided_apis": [
        {
          "group": "ibm.com",
          "kind": "IBMSecurityVerify",
          "plural": "ibmsecurityverifies",
          "version": "v1"
        }
      ],
      "provider": "IBM",
      "related_images": [
        {
          "digest": "sha256:3c8add266bdabd7e19ec60c097e9048e5837d7b81c225dcaeec33df9f515b21a",
          "image": "ibmcom/verify-operator@sha256:3c8add266bdabd7e19ec60c097e9048e5837d7b81c225dcaeec33df9f515b21a",
          "name": "verify-operator-3c8add266bdabd7e19ec60c097e9048e5837d7b81c225dcaeec33df9f515b21a-annotation"
        },
        {
          "digest": "sha256:db06cc4c084dd0253134f156dddaaf53ef1c3fb3cc809e5d81711baa4029ea4c",
          "image": "gcr.io/kubebuilder/kube-rbac-proxy@sha256:db06cc4c084dd0253134f156dddaaf53ef1c3fb3cc809e5d81711baa4029ea4c",
          "name": "kube-rbac-proxy"
        },
        {
          "digest": "sha256:3c8add266bdabd7e19ec60c097e9048e5837d7b81c225dcaeec33df9f515b21a",
          "image": "ibmcom/verify-operator@sha256:3c8add266bdabd7e19ec60c097e9048e5837d7b81c225dcaeec33df9f515b21a",
          "name": "manager"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.9",
      "version": "22.6.0",
      "version_original": "22.6.0"
    },
    {
      "_id": "62987f20554afbde4c9bb236",
      "alm_examples": [
        {
          "api_version": "atlas.mongodb.com/v1",
          "kind": "AtlasBackupPolicy",
          "metadata": {
            "name": "atlasbackuppolicy-sample"
          },
          "spec": {
            "id": "1",
            "items": [
              {
                "frequencyInterval": 6,
                "frequencyType": "WEEKLY",
                "id": "2",
                "retentionUnit": "DAYS",
                "retentionValue": 6
              }
            ]
          }
        },
        {
          "api_version": "atlas.mongodb.com/v1",
          "kind": "AtlasBackupSchedule",
          "metadata": {
            "name": "atlasbackupschedule-sample"
          },
          "spec": {
            "autoExportEnabled": true,
            "policies": [
              {
                "name": "atlas-default-backuppolicy",
                "namespace": "mongodb-atlas-system"
              },
              {
                "name": "atlas-default-backuppolicy2",
                "namespace": "mongodb-atlas-system"
              }
            ],
            "referenceHourOfDay": 10,
            "referenceMinuteOfHour": 10,
            "restoreWindowDays": 2
          }
        },
        {
          "api_version": "atlas.mongodb.com/v1",
          "kind": "AtlasDatabaseUser",
          "metadata": {
            "name": "my-database-user"
          },
          "spec": {
            "databaseName": "admin",
            "passwordSecretRef": {
              "name": "my-database-user-password"
            },
            "projectRef": {
              "name": "my-project"
            },
            "roles": [
              {
                "databaseName": "admin",
                "roleName": "readWriteAnyDatabase"
              }
            ],
            "username": "david"
          }
        },
        {
          "api_version": "atlas.mongodb.com/v1",
          "kind": "AtlasDeployment",
          "metadata": {
            "name": "my-atlas-cluster"
          },
          "spec": {
            "deploymentSpec": {
              "name": "test-cluster",
              "providerSettings": {
                "instanceSizeName": "M10",
                "providerName": "AWS",
                "regionName": "US_EAST_1"
              }
            },
            "projectRef": {
              "name": "my-project"
            }
          }
        },
        {
          "api_version": "atlas.mongodb.com/v1",
          "kind": "AtlasProject",
          "metadata": {
            "name": "my-project"
          },
          "spec": {
            "name": "Test Atlas Operator Project",
            "projectIpAccessList": [
              {
                "comment": "IP address for Application Server A",
                "ipAddress": "192.0.2.15"
              }
            ]
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/mongodb/mongodb-atlas-kubernetes-operator-bundle@sha256:2338eaab863304250b3b1d267e9f2261c48eef1f97c280b7fa0af0ebc3ba208b",
      "bundle_path_digest": "sha256:2338eaab863304250b3b1d267e9f2261c48eef1f97c280b7fa0af0ebc3ba208b",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "beta",
      "creation_date": "2022-06-02T09:13:04.328000+00:00",
      "csv_description": "The MongoDB Atlas Operator provides a native integration between the Kubernetes orchestration platform and MongoDB Atlas \u2014\nthe only multi-cloud document database service that gives you the versatility you need to build sophisticated and resilient applications that can adapt to changing customer demands and market trends.\n\n> Current Status: *Beta*. The Operator gives users the ability to provision\n> Atlas projects, clusters and database users using Kubernetes Specifications and bind connection information\n> into applications deployed to Kubernetes.\n\n## Quick Start guide\n### Step 1. Deploy Kubernetes operator by clicking Install button.\n\n### Step 2. Create Atlas Cluster\n\n**1.** Create an Atlas API Key Secret\nIn order to work with the Atlas Operator you need to provide [authentication information](https://docs.atlas.mongodb.com/configure-api-access)\n to allow the Atlas Operator to communicate with Atlas API. Once you have generated a Public and Private key in Atlas, you can create a Kuberentes Secret with:\n```\nkubectl create secret generic mongodb-atlas-operator-api-key \\\n         --from-literal=\"orgId=<the_atlas_organization_id>\" \\\n         --from-literal=\"publicApiKey=<the_atlas_api_public_key>\" \\\n         --from-literal=\"privateApiKey=<the_atlas_api_private_key>\" \\\n         -n openshift-operators\n```\n(Note, that you should use the namespace where the Operator was installed - it's `openshift-operators` by default)\n\n**2.** Create an `AtlasProject` Custom Resource\n\nThe `AtlasProject` CustomResource represents Atlas Projects in our Kubernetes cluster. You need to specify\n`projectIpAccessList` with the IP addresses or CIDR blocks of any hosts that will connect to the Atlas Cluster.\n```\napiVersion: atlas.mongodb.com/v1\nkind: AtlasProject\nmetadata:\n  name: my-project\nspec:\n  name: Test Atlas Operator Project\n  projectIpAccessList:\n    - ipAddress: \"192.0.2.15\"\n      comment: \"IP address for Application Server A\"\n    - ipAddress: \"203.0.113.0/24\"\n      comment: \"CIDR block for Application Server B - D\"\n```\n**3.** Create an `AtlasDeployment` Custom Resource.\nThe example below is a minimal configuration to create an M10 Atlas cluster in the AWS US East region. For a full list of properties, check\n`atlasclusters.atlas.mongodb.com` [CRD specification](config/crd/bases/atlas.mongodb.com_atlasclusters.yaml)):\n```\napiVersion: atlas.mongodb.com/v1\nkind: AtlasDeployment\nmetadata:\n  name: my-atlas-cluster\nspec:\n  name: \"Test-cluster\"\n  projectRef:\n    name: my-project\n  providerSettings:\n    instanceSizeName: M10\n    providerName: AWS\n    regionName: US_EAST_1\n```\n\n**4.** Create a database user password Kubernetes Secret\nThe Secret must be created in the same namespace as the `AtlasDeployment` and `AtlasProject` were created.\n```\nkubectl create secret generic the-user-password --from-literal=\"password=P@@sword%\"\n```\n\n**5.** Create an `AtlasDatabaseUser` Custom Resource\n\nIn order to connect to an Atlas Cluster the database user needs to be created. `AtlasDatabaseUser` resource should reference\nthe password Kubernetes Secret created in the previous step.\n```\napiVersion: atlas.mongodb.com/v1\nkind: AtlasDatabaseUser\nmetadata:\n  name: my-database-user\nspec:\n  roles:\n    - roleName: \"readWriteAnyDatabase\"\n      databaseName: \"admin\"\n  projectRef:\n    name: my-project\n  username: theuser\n  passwordSecretRef:\n    name: the-user-password\n```\n**6.** Wait for the `AtlasDatabaseUser` Custom Resource to be ready\n\nWait until the AtlasDatabaseUser resource gets to \"ready\" status (it will wait until the cluster is created that may take around 10 minutes):\n```\nkubectl get atlasdatabaseusers my-database-user -o=jsonpath='{.status.conditions[?(@.type==\"Ready\")].status}'\nTrue\n```\n### Step 3. Connect your application to the Atlas Cluster\n\nThe Atlas Operator will create a Kubernetes Secret with the information necessary to connect to the Atlas Cluster created\nin the previous step. An application in the same Kubernetes Cluster can mount and use the Secret:\n\n```\n...\ncontainers:\n      - name: test-app\n        env:\n         - name: \"CONNECTION_STRING\"\n           valueFrom:\n             secretKeyRef:\n               name: test-atlas-operator-project-test-cluster-theuser\n               key: connectionStringStandardSrv\n\n```\n",
      "csv_display_name": "MongoDB Atlas Operator",
      "csv_metadata_description": "The MongoDB Atlas Kubernetes Operator enables easy management of Clusters in MongoDB Atlas",
      "csv_name": "mongodb-atlas-kubernetes.v1.0.0",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": false,
      "last_update_date": "2022-11-23T06:52:31.341000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.11",
      "organization": "certified-operators",
      "package": "mongodb-atlas-kubernetes",
      "provided_apis": [
        {
          "group": "atlas.mongodb.com",
          "kind": "AtlasBackupPolicy",
          "version": "v1"
        },
        {
          "group": "atlas.mongodb.com",
          "kind": "AtlasBackupSchedule",
          "version": "v1"
        },
        {
          "group": "atlas.mongodb.com",
          "kind": "AtlasDatabaseUser",
          "version": "v1"
        },
        {
          "group": "atlas.mongodb.com",
          "kind": "AtlasDeployment",
          "version": "v1"
        },
        {
          "group": "atlas.mongodb.com",
          "kind": "AtlasProject",
          "version": "v1"
        }
      ],
      "provider": "MongoDB, Inc",
      "related_images": [
        {
          "digest": "sha256:3cfbd700e4771c2c7e75c2c41cf6de57c9ef71ecd286e96a21d2567e43424bc3",
          "image": "registry.connect.redhat.com/mongodb/mongodb-atlas-kubernetes-operator@sha256:3cfbd700e4771c2c7e75c2c41cf6de57c9ef71ecd286e96a21d2567e43424bc3",
          "name": "mongodb-atlas-kubernetes-operator-3cfbd700e4771c2c7e75c2c41cf6de57c9ef71ecd286e96a21d2567e43424bc3-annotation"
        },
        {
          "digest": "sha256:3cfbd700e4771c2c7e75c2c41cf6de57c9ef71ecd286e96a21d2567e43424bc3",
          "image": "registry.connect.redhat.com/mongodb/mongodb-atlas-kubernetes-operator@sha256:3cfbd700e4771c2c7e75c2c41cf6de57c9ef71ecd286e96a21d2567e43424bc3",
          "name": "manager"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.11",
      "version": "1.0.0",
      "version_original": "1.0.0"
    },
    {
      "_id": "629881448c1a626fc9aeb790",
      "alm_examples": [
        {
          "api_version": "atlas.mongodb.com/v1",
          "kind": "AtlasBackupPolicy",
          "metadata": {
            "name": "atlasbackuppolicy-sample"
          },
          "spec": {
            "id": "1",
            "items": [
              {
                "frequencyInterval": 6,
                "frequencyType": "WEEKLY",
                "id": "2",
                "retentionUnit": "DAYS",
                "retentionValue": 6
              }
            ]
          }
        },
        {
          "api_version": "atlas.mongodb.com/v1",
          "kind": "AtlasBackupSchedule",
          "metadata": {
            "name": "atlasbackupschedule-sample"
          },
          "spec": {
            "autoExportEnabled": true,
            "policies": [
              {
                "name": "atlas-default-backuppolicy",
                "namespace": "mongodb-atlas-system"
              },
              {
                "name": "atlas-default-backuppolicy2",
                "namespace": "mongodb-atlas-system"
              }
            ],
            "referenceHourOfDay": 10,
            "referenceMinuteOfHour": 10,
            "restoreWindowDays": 2
          }
        },
        {
          "api_version": "atlas.mongodb.com/v1",
          "kind": "AtlasDatabaseUser",
          "metadata": {
            "name": "my-database-user"
          },
          "spec": {
            "databaseName": "admin",
            "passwordSecretRef": {
              "name": "my-database-user-password"
            },
            "projectRef": {
              "name": "my-project"
            },
            "roles": [
              {
                "databaseName": "admin",
                "roleName": "readWriteAnyDatabase"
              }
            ],
            "username": "david"
          }
        },
        {
          "api_version": "atlas.mongodb.com/v1",
          "kind": "AtlasDeployment",
          "metadata": {
            "name": "my-atlas-cluster"
          },
          "spec": {
            "deploymentSpec": {
              "name": "test-cluster",
              "providerSettings": {
                "instanceSizeName": "M10",
                "providerName": "AWS",
                "regionName": "US_EAST_1"
              }
            },
            "projectRef": {
              "name": "my-project"
            }
          }
        },
        {
          "api_version": "atlas.mongodb.com/v1",
          "kind": "AtlasProject",
          "metadata": {
            "name": "my-project"
          },
          "spec": {
            "name": "Test Atlas Operator Project",
            "projectIpAccessList": [
              {
                "comment": "IP address for Application Server A",
                "ipAddress": "192.0.2.15"
              }
            ]
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/mongodb/mongodb-atlas-kubernetes-operator-bundle@sha256:2338eaab863304250b3b1d267e9f2261c48eef1f97c280b7fa0af0ebc3ba208b",
      "bundle_path_digest": "sha256:2338eaab863304250b3b1d267e9f2261c48eef1f97c280b7fa0af0ebc3ba208b",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "beta",
      "creation_date": "2022-06-02T09:22:12.298000+00:00",
      "csv_description": "The MongoDB Atlas Operator provides a native integration between the Kubernetes orchestration platform and MongoDB Atlas \u2014\nthe only multi-cloud document database service that gives you the versatility you need to build sophisticated and resilient applications that can adapt to changing customer demands and market trends.\n\n> Current Status: *Beta*. The Operator gives users the ability to provision\n> Atlas projects, clusters and database users using Kubernetes Specifications and bind connection information\n> into applications deployed to Kubernetes.\n\n## Quick Start guide\n### Step 1. Deploy Kubernetes operator by clicking Install button.\n\n### Step 2. Create Atlas Cluster\n\n**1.** Create an Atlas API Key Secret\nIn order to work with the Atlas Operator you need to provide [authentication information](https://docs.atlas.mongodb.com/configure-api-access)\n to allow the Atlas Operator to communicate with Atlas API. Once you have generated a Public and Private key in Atlas, you can create a Kuberentes Secret with:\n```\nkubectl create secret generic mongodb-atlas-operator-api-key \\\n         --from-literal=\"orgId=<the_atlas_organization_id>\" \\\n         --from-literal=\"publicApiKey=<the_atlas_api_public_key>\" \\\n         --from-literal=\"privateApiKey=<the_atlas_api_private_key>\" \\\n         -n openshift-operators\n```\n(Note, that you should use the namespace where the Operator was installed - it's `openshift-operators` by default)\n\n**2.** Create an `AtlasProject` Custom Resource\n\nThe `AtlasProject` CustomResource represents Atlas Projects in our Kubernetes cluster. You need to specify\n`projectIpAccessList` with the IP addresses or CIDR blocks of any hosts that will connect to the Atlas Cluster.\n```\napiVersion: atlas.mongodb.com/v1\nkind: AtlasProject\nmetadata:\n  name: my-project\nspec:\n  name: Test Atlas Operator Project\n  projectIpAccessList:\n    - ipAddress: \"192.0.2.15\"\n      comment: \"IP address for Application Server A\"\n    - ipAddress: \"203.0.113.0/24\"\n      comment: \"CIDR block for Application Server B - D\"\n```\n**3.** Create an `AtlasDeployment` Custom Resource.\nThe example below is a minimal configuration to create an M10 Atlas cluster in the AWS US East region. For a full list of properties, check\n`atlasclusters.atlas.mongodb.com` [CRD specification](config/crd/bases/atlas.mongodb.com_atlasclusters.yaml)):\n```\napiVersion: atlas.mongodb.com/v1\nkind: AtlasDeployment\nmetadata:\n  name: my-atlas-cluster\nspec:\n  name: \"Test-cluster\"\n  projectRef:\n    name: my-project\n  providerSettings:\n    instanceSizeName: M10\n    providerName: AWS\n    regionName: US_EAST_1\n```\n\n**4.** Create a database user password Kubernetes Secret\nThe Secret must be created in the same namespace as the `AtlasDeployment` and `AtlasProject` were created.\n```\nkubectl create secret generic the-user-password --from-literal=\"password=P@@sword%\"\n```\n\n**5.** Create an `AtlasDatabaseUser` Custom Resource\n\nIn order to connect to an Atlas Cluster the database user needs to be created. `AtlasDatabaseUser` resource should reference\nthe password Kubernetes Secret created in the previous step.\n```\napiVersion: atlas.mongodb.com/v1\nkind: AtlasDatabaseUser\nmetadata:\n  name: my-database-user\nspec:\n  roles:\n    - roleName: \"readWriteAnyDatabase\"\n      databaseName: \"admin\"\n  projectRef:\n    name: my-project\n  username: theuser\n  passwordSecretRef:\n    name: the-user-password\n```\n**6.** Wait for the `AtlasDatabaseUser` Custom Resource to be ready\n\nWait until the AtlasDatabaseUser resource gets to \"ready\" status (it will wait until the cluster is created that may take around 10 minutes):\n```\nkubectl get atlasdatabaseusers my-database-user -o=jsonpath='{.status.conditions[?(@.type==\"Ready\")].status}'\nTrue\n```\n### Step 3. Connect your application to the Atlas Cluster\n\nThe Atlas Operator will create a Kubernetes Secret with the information necessary to connect to the Atlas Cluster created\nin the previous step. An application in the same Kubernetes Cluster can mount and use the Secret:\n\n```\n...\ncontainers:\n      - name: test-app\n        env:\n         - name: \"CONNECTION_STRING\"\n           valueFrom:\n             secretKeyRef:\n               name: test-atlas-operator-project-test-cluster-theuser\n               key: connectionStringStandardSrv\n\n```\n",
      "csv_display_name": "MongoDB Atlas Operator",
      "csv_metadata_description": "The MongoDB Atlas Kubernetes Operator enables easy management of Clusters in MongoDB Atlas",
      "csv_name": "mongodb-atlas-kubernetes.v1.0.0",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": false,
      "last_update_date": "2022-11-23T07:22:47.962000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.9",
      "organization": "certified-operators",
      "package": "mongodb-atlas-kubernetes",
      "provided_apis": [
        {
          "group": "atlas.mongodb.com",
          "kind": "AtlasDatabaseUser",
          "plural": "atlasdatabaseusers",
          "version": "v1"
        },
        {
          "group": "atlas.mongodb.com",
          "kind": "AtlasDeployment",
          "plural": "atlasdeployments",
          "version": "v1"
        },
        {
          "group": "atlas.mongodb.com",
          "kind": "AtlasProject",
          "plural": "atlasprojects",
          "version": "v1"
        },
        {
          "group": "atlas.mongodb.com",
          "kind": "AtlasBackupPolicy",
          "plural": "atlasbackuppolicies",
          "version": "v1"
        },
        {
          "group": "atlas.mongodb.com",
          "kind": "AtlasBackupSchedule",
          "plural": "atlasbackupschedules",
          "version": "v1"
        }
      ],
      "provider": "MongoDB, Inc",
      "related_images": [
        {
          "digest": "sha256:3cfbd700e4771c2c7e75c2c41cf6de57c9ef71ecd286e96a21d2567e43424bc3",
          "image": "registry.connect.redhat.com/mongodb/mongodb-atlas-kubernetes-operator@sha256:3cfbd700e4771c2c7e75c2c41cf6de57c9ef71ecd286e96a21d2567e43424bc3",
          "name": "mongodb-atlas-kubernetes-operator-3cfbd700e4771c2c7e75c2c41cf6de57c9ef71ecd286e96a21d2567e43424bc3-annotation"
        },
        {
          "digest": "sha256:3cfbd700e4771c2c7e75c2c41cf6de57c9ef71ecd286e96a21d2567e43424bc3",
          "image": "registry.connect.redhat.com/mongodb/mongodb-atlas-kubernetes-operator@sha256:3cfbd700e4771c2c7e75c2c41cf6de57c9ef71ecd286e96a21d2567e43424bc3",
          "name": "manager"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.9",
      "version": "1.0.0",
      "version_original": "1.0.0"
    },
    {
      "_id": "6298820e554afbde4c9bb2f5",
      "alm_examples": [
        {
          "api_version": "atlas.mongodb.com/v1",
          "kind": "AtlasBackupPolicy",
          "metadata": {
            "name": "atlasbackuppolicy-sample"
          },
          "spec": {
            "id": "1",
            "items": [
              {
                "frequencyInterval": 6,
                "frequencyType": "WEEKLY",
                "id": "2",
                "retentionUnit": "DAYS",
                "retentionValue": 6
              }
            ]
          }
        },
        {
          "api_version": "atlas.mongodb.com/v1",
          "kind": "AtlasBackupSchedule",
          "metadata": {
            "name": "atlasbackupschedule-sample"
          },
          "spec": {
            "autoExportEnabled": true,
            "policies": [
              {
                "name": "atlas-default-backuppolicy",
                "namespace": "mongodb-atlas-system"
              },
              {
                "name": "atlas-default-backuppolicy2",
                "namespace": "mongodb-atlas-system"
              }
            ],
            "referenceHourOfDay": 10,
            "referenceMinuteOfHour": 10,
            "restoreWindowDays": 2
          }
        },
        {
          "api_version": "atlas.mongodb.com/v1",
          "kind": "AtlasDatabaseUser",
          "metadata": {
            "name": "my-database-user"
          },
          "spec": {
            "databaseName": "admin",
            "passwordSecretRef": {
              "name": "my-database-user-password"
            },
            "projectRef": {
              "name": "my-project"
            },
            "roles": [
              {
                "databaseName": "admin",
                "roleName": "readWriteAnyDatabase"
              }
            ],
            "username": "david"
          }
        },
        {
          "api_version": "atlas.mongodb.com/v1",
          "kind": "AtlasDeployment",
          "metadata": {
            "name": "my-atlas-cluster"
          },
          "spec": {
            "deploymentSpec": {
              "name": "test-cluster",
              "providerSettings": {
                "instanceSizeName": "M10",
                "providerName": "AWS",
                "regionName": "US_EAST_1"
              }
            },
            "projectRef": {
              "name": "my-project"
            }
          }
        },
        {
          "api_version": "atlas.mongodb.com/v1",
          "kind": "AtlasProject",
          "metadata": {
            "name": "my-project"
          },
          "spec": {
            "name": "Test Atlas Operator Project",
            "projectIpAccessList": [
              {
                "comment": "IP address for Application Server A",
                "ipAddress": "192.0.2.15"
              }
            ]
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/mongodb/mongodb-atlas-kubernetes-operator-bundle@sha256:2338eaab863304250b3b1d267e9f2261c48eef1f97c280b7fa0af0ebc3ba208b",
      "bundle_path_digest": "sha256:2338eaab863304250b3b1d267e9f2261c48eef1f97c280b7fa0af0ebc3ba208b",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "beta",
      "creation_date": "2022-06-02T09:25:34.292000+00:00",
      "csv_description": "The MongoDB Atlas Operator provides a native integration between the Kubernetes orchestration platform and MongoDB Atlas \u2014\nthe only multi-cloud document database service that gives you the versatility you need to build sophisticated and resilient applications that can adapt to changing customer demands and market trends.\n\n> Current Status: *Beta*. The Operator gives users the ability to provision\n> Atlas projects, clusters and database users using Kubernetes Specifications and bind connection information\n> into applications deployed to Kubernetes.\n\n## Quick Start guide\n### Step 1. Deploy Kubernetes operator by clicking Install button.\n\n### Step 2. Create Atlas Cluster\n\n**1.** Create an Atlas API Key Secret\nIn order to work with the Atlas Operator you need to provide [authentication information](https://docs.atlas.mongodb.com/configure-api-access)\n to allow the Atlas Operator to communicate with Atlas API. Once you have generated a Public and Private key in Atlas, you can create a Kuberentes Secret with:\n```\nkubectl create secret generic mongodb-atlas-operator-api-key \\\n         --from-literal=\"orgId=<the_atlas_organization_id>\" \\\n         --from-literal=\"publicApiKey=<the_atlas_api_public_key>\" \\\n         --from-literal=\"privateApiKey=<the_atlas_api_private_key>\" \\\n         -n openshift-operators\n```\n(Note, that you should use the namespace where the Operator was installed - it's `openshift-operators` by default)\n\n**2.** Create an `AtlasProject` Custom Resource\n\nThe `AtlasProject` CustomResource represents Atlas Projects in our Kubernetes cluster. You need to specify\n`projectIpAccessList` with the IP addresses or CIDR blocks of any hosts that will connect to the Atlas Cluster.\n```\napiVersion: atlas.mongodb.com/v1\nkind: AtlasProject\nmetadata:\n  name: my-project\nspec:\n  name: Test Atlas Operator Project\n  projectIpAccessList:\n    - ipAddress: \"192.0.2.15\"\n      comment: \"IP address for Application Server A\"\n    - ipAddress: \"203.0.113.0/24\"\n      comment: \"CIDR block for Application Server B - D\"\n```\n**3.** Create an `AtlasDeployment` Custom Resource.\nThe example below is a minimal configuration to create an M10 Atlas cluster in the AWS US East region. For a full list of properties, check\n`atlasclusters.atlas.mongodb.com` [CRD specification](config/crd/bases/atlas.mongodb.com_atlasclusters.yaml)):\n```\napiVersion: atlas.mongodb.com/v1\nkind: AtlasDeployment\nmetadata:\n  name: my-atlas-cluster\nspec:\n  name: \"Test-cluster\"\n  projectRef:\n    name: my-project\n  providerSettings:\n    instanceSizeName: M10\n    providerName: AWS\n    regionName: US_EAST_1\n```\n\n**4.** Create a database user password Kubernetes Secret\nThe Secret must be created in the same namespace as the `AtlasDeployment` and `AtlasProject` were created.\n```\nkubectl create secret generic the-user-password --from-literal=\"password=P@@sword%\"\n```\n\n**5.** Create an `AtlasDatabaseUser` Custom Resource\n\nIn order to connect to an Atlas Cluster the database user needs to be created. `AtlasDatabaseUser` resource should reference\nthe password Kubernetes Secret created in the previous step.\n```\napiVersion: atlas.mongodb.com/v1\nkind: AtlasDatabaseUser\nmetadata:\n  name: my-database-user\nspec:\n  roles:\n    - roleName: \"readWriteAnyDatabase\"\n      databaseName: \"admin\"\n  projectRef:\n    name: my-project\n  username: theuser\n  passwordSecretRef:\n    name: the-user-password\n```\n**6.** Wait for the `AtlasDatabaseUser` Custom Resource to be ready\n\nWait until the AtlasDatabaseUser resource gets to \"ready\" status (it will wait until the cluster is created that may take around 10 minutes):\n```\nkubectl get atlasdatabaseusers my-database-user -o=jsonpath='{.status.conditions[?(@.type==\"Ready\")].status}'\nTrue\n```\n### Step 3. Connect your application to the Atlas Cluster\n\nThe Atlas Operator will create a Kubernetes Secret with the information necessary to connect to the Atlas Cluster created\nin the previous step. An application in the same Kubernetes Cluster can mount and use the Secret:\n\n```\n...\ncontainers:\n      - name: test-app\n        env:\n         - name: \"CONNECTION_STRING\"\n           valueFrom:\n             secretKeyRef:\n               name: test-atlas-operator-project-test-cluster-theuser\n               key: connectionStringStandardSrv\n\n```\n",
      "csv_display_name": "MongoDB Atlas Operator",
      "csv_metadata_description": "The MongoDB Atlas Kubernetes Operator enables easy management of Clusters in MongoDB Atlas",
      "csv_name": "mongodb-atlas-kubernetes.v1.0.0",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": false,
      "last_update_date": "2022-11-23T08:41:47.473000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.10",
      "organization": "certified-operators",
      "package": "mongodb-atlas-kubernetes",
      "provided_apis": [
        {
          "group": "atlas.mongodb.com",
          "kind": "AtlasBackupPolicy",
          "plural": "atlasbackuppolicies",
          "version": "v1"
        },
        {
          "group": "atlas.mongodb.com",
          "kind": "AtlasBackupSchedule",
          "plural": "atlasbackupschedules",
          "version": "v1"
        },
        {
          "group": "atlas.mongodb.com",
          "kind": "AtlasDatabaseUser",
          "plural": "atlasdatabaseusers",
          "version": "v1"
        },
        {
          "group": "atlas.mongodb.com",
          "kind": "AtlasDeployment",
          "plural": "atlasdeployments",
          "version": "v1"
        },
        {
          "group": "atlas.mongodb.com",
          "kind": "AtlasProject",
          "plural": "atlasprojects",
          "version": "v1"
        }
      ],
      "provider": "MongoDB, Inc",
      "related_images": [
        {
          "digest": "sha256:3cfbd700e4771c2c7e75c2c41cf6de57c9ef71ecd286e96a21d2567e43424bc3",
          "image": "registry.connect.redhat.com/mongodb/mongodb-atlas-kubernetes-operator@sha256:3cfbd700e4771c2c7e75c2c41cf6de57c9ef71ecd286e96a21d2567e43424bc3",
          "name": "mongodb-atlas-kubernetes-operator-3cfbd700e4771c2c7e75c2c41cf6de57c9ef71ecd286e96a21d2567e43424bc3-annotation"
        },
        {
          "digest": "sha256:3cfbd700e4771c2c7e75c2c41cf6de57c9ef71ecd286e96a21d2567e43424bc3",
          "image": "registry.connect.redhat.com/mongodb/mongodb-atlas-kubernetes-operator@sha256:3cfbd700e4771c2c7e75c2c41cf6de57c9ef71ecd286e96a21d2567e43424bc3",
          "name": "manager"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.10",
      "version": "1.0.0",
      "version_original": "1.0.0"
    },
    {
      "_id": "62988266efe144fd5e93396e",
      "alm_examples": [
        {
          "api_version": "ibm.com/v1",
          "kind": "IBMSecurityVerify",
          "metadata": {
            "name": "ibmsecurityverify-sample"
          },
          "spec": {
            "clientSecret": "--secret--",
            "sessionLifetime": 3600,
            "ssoPath": "/verify-sso"
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/ibm/verify-operator-bundle@sha256:0deef311d65c60bedc0af39fb6dfde09b0b1a60825eaa37188b21b5a692397e9",
      "bundle_path_digest": "sha256:0deef311d65c60bedc0af39fb6dfde09b0b1a60825eaa37188b21b5a692397e9",
      "capabilities": [
        "Seamless Upgrades"
      ],
      "channel_name": "stable",
      "creation_date": "2022-06-02T09:27:02.754000+00:00",
      "csv_description": "[IBM Security Verify](https://www.ibm.com/products/verify-saas) is an Identity-as-a-Service platform that allows IT, security and business leaders to protect their digital users, assets and data in a hybrid, multi-cloud world by enabling technical agility and operational efficiency. IBM Security Verify SaaS provides single sign-on (SSO), multi-factor authentication (MFA), AI-powered context for risk-based authentication for adaptive access decisions, user management, access recertification campaigns and identity analytics.\n\nAs part of using Red Hat OpenShift, you are entitled to Verify SaaS SSO for unlimited apps and users.  If you do not already have a Verify SaaS SSO tenant a new one can be [created](https://www.ibm.com/account/reg/us-en/signup?formid=urx-51255).\n\nFor a detailed description of IBM Security Verify refer to the [Official documentation](https://www.ibm.com/docs/en/security-verify).\n\nThe IBM Security Verify operator can consistently enforce policy-driven security by using the Ingress networking capability of OpenShift in conjunction with the [Nginx Ingress operator](https://www.nginx.com/blog/getting-started-nginx-ingress-operator-red-hat-openshift/). With this approach, you can enforce authentication and authorization policies for all of the applications in your cluster at the same time, without ever changing your application code. You can also dynamically register your application to start protecting them centrally from the cloud via Verify SaaS. \n\nSee the project [Readme](https://github.com/IBM-Security/verify-operator/blob/master/README.md) for further information and details.\n\n",
      "csv_display_name": "IBM Security Verify Operator",
      "csv_metadata_description": "The IBM Security Verify operator can consistently enforce policy-driven security, including authentication and authorization, using the Ingress networking capability of OpenShift.",
      "csv_name": "ibm-security-verify-operator.v22.6.0",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": false,
          "type": "OwnNamespace"
        },
        {
          "supported": false,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-11-23T06:53:25.876000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.7",
      "organization": "certified-operators",
      "package": "ibm-security-verify-operator",
      "provided_apis": [
        {
          "group": "ibm.com",
          "kind": "IBMSecurityVerify",
          "plural": "ibmsecurityverifies",
          "version": "v1"
        }
      ],
      "provider": "IBM",
      "related_images": [
        {
          "digest": "sha256:3c8add266bdabd7e19ec60c097e9048e5837d7b81c225dcaeec33df9f515b21a",
          "image": "ibmcom/verify-operator@sha256:3c8add266bdabd7e19ec60c097e9048e5837d7b81c225dcaeec33df9f515b21a",
          "name": "verify-operator-3c8add266bdabd7e19ec60c097e9048e5837d7b81c225dcaeec33df9f515b21a-annotation"
        },
        {
          "digest": "sha256:db06cc4c084dd0253134f156dddaaf53ef1c3fb3cc809e5d81711baa4029ea4c",
          "image": "gcr.io/kubebuilder/kube-rbac-proxy@sha256:db06cc4c084dd0253134f156dddaaf53ef1c3fb3cc809e5d81711baa4029ea4c",
          "name": "kube-rbac-proxy"
        },
        {
          "digest": "sha256:3c8add266bdabd7e19ec60c097e9048e5837d7b81c225dcaeec33df9f515b21a",
          "image": "ibmcom/verify-operator@sha256:3c8add266bdabd7e19ec60c097e9048e5837d7b81c225dcaeec33df9f515b21a",
          "name": "manager"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.7",
      "version": "22.6.0",
      "version_original": "22.6.0"
    },
    {
      "_id": "62988267e0c0938179ef6058",
      "alm_examples": [
        {
          "api_version": "atlas.mongodb.com/v1",
          "kind": "AtlasBackupPolicy",
          "metadata": {
            "name": "atlasbackuppolicy-sample"
          },
          "spec": {
            "id": "1",
            "items": [
              {
                "frequencyInterval": 6,
                "frequencyType": "WEEKLY",
                "id": "2",
                "retentionUnit": "DAYS",
                "retentionValue": 6
              }
            ]
          }
        },
        {
          "api_version": "atlas.mongodb.com/v1",
          "kind": "AtlasBackupSchedule",
          "metadata": {
            "name": "atlasbackupschedule-sample"
          },
          "spec": {
            "autoExportEnabled": true,
            "policies": [
              {
                "name": "atlas-default-backuppolicy",
                "namespace": "mongodb-atlas-system"
              },
              {
                "name": "atlas-default-backuppolicy2",
                "namespace": "mongodb-atlas-system"
              }
            ],
            "referenceHourOfDay": 10,
            "referenceMinuteOfHour": 10,
            "restoreWindowDays": 2
          }
        },
        {
          "api_version": "atlas.mongodb.com/v1",
          "kind": "AtlasDatabaseUser",
          "metadata": {
            "name": "my-database-user"
          },
          "spec": {
            "databaseName": "admin",
            "passwordSecretRef": {
              "name": "my-database-user-password"
            },
            "projectRef": {
              "name": "my-project"
            },
            "roles": [
              {
                "databaseName": "admin",
                "roleName": "readWriteAnyDatabase"
              }
            ],
            "username": "david"
          }
        },
        {
          "api_version": "atlas.mongodb.com/v1",
          "kind": "AtlasDeployment",
          "metadata": {
            "name": "my-atlas-cluster"
          },
          "spec": {
            "deploymentSpec": {
              "name": "test-cluster",
              "providerSettings": {
                "instanceSizeName": "M10",
                "providerName": "AWS",
                "regionName": "US_EAST_1"
              }
            },
            "projectRef": {
              "name": "my-project"
            }
          }
        },
        {
          "api_version": "atlas.mongodb.com/v1",
          "kind": "AtlasProject",
          "metadata": {
            "name": "my-project"
          },
          "spec": {
            "name": "Test Atlas Operator Project",
            "projectIpAccessList": [
              {
                "comment": "IP address for Application Server A",
                "ipAddress": "192.0.2.15"
              }
            ]
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/mongodb/mongodb-atlas-kubernetes-operator-bundle@sha256:2338eaab863304250b3b1d267e9f2261c48eef1f97c280b7fa0af0ebc3ba208b",
      "bundle_path_digest": "sha256:2338eaab863304250b3b1d267e9f2261c48eef1f97c280b7fa0af0ebc3ba208b",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "beta",
      "creation_date": "2022-06-02T09:27:03.274000+00:00",
      "csv_description": "The MongoDB Atlas Operator provides a native integration between the Kubernetes orchestration platform and MongoDB Atlas \u2014\nthe only multi-cloud document database service that gives you the versatility you need to build sophisticated and resilient applications that can adapt to changing customer demands and market trends.\n\n> Current Status: *Beta*. The Operator gives users the ability to provision\n> Atlas projects, clusters and database users using Kubernetes Specifications and bind connection information\n> into applications deployed to Kubernetes.\n\n## Quick Start guide\n### Step 1. Deploy Kubernetes operator by clicking Install button.\n\n### Step 2. Create Atlas Cluster\n\n**1.** Create an Atlas API Key Secret\nIn order to work with the Atlas Operator you need to provide [authentication information](https://docs.atlas.mongodb.com/configure-api-access)\n to allow the Atlas Operator to communicate with Atlas API. Once you have generated a Public and Private key in Atlas, you can create a Kuberentes Secret with:\n```\nkubectl create secret generic mongodb-atlas-operator-api-key \\\n         --from-literal=\"orgId=<the_atlas_organization_id>\" \\\n         --from-literal=\"publicApiKey=<the_atlas_api_public_key>\" \\\n         --from-literal=\"privateApiKey=<the_atlas_api_private_key>\" \\\n         -n openshift-operators\n```\n(Note, that you should use the namespace where the Operator was installed - it's `openshift-operators` by default)\n\n**2.** Create an `AtlasProject` Custom Resource\n\nThe `AtlasProject` CustomResource represents Atlas Projects in our Kubernetes cluster. You need to specify\n`projectIpAccessList` with the IP addresses or CIDR blocks of any hosts that will connect to the Atlas Cluster.\n```\napiVersion: atlas.mongodb.com/v1\nkind: AtlasProject\nmetadata:\n  name: my-project\nspec:\n  name: Test Atlas Operator Project\n  projectIpAccessList:\n    - ipAddress: \"192.0.2.15\"\n      comment: \"IP address for Application Server A\"\n    - ipAddress: \"203.0.113.0/24\"\n      comment: \"CIDR block for Application Server B - D\"\n```\n**3.** Create an `AtlasDeployment` Custom Resource.\nThe example below is a minimal configuration to create an M10 Atlas cluster in the AWS US East region. For a full list of properties, check\n`atlasclusters.atlas.mongodb.com` [CRD specification](config/crd/bases/atlas.mongodb.com_atlasclusters.yaml)):\n```\napiVersion: atlas.mongodb.com/v1\nkind: AtlasDeployment\nmetadata:\n  name: my-atlas-cluster\nspec:\n  name: \"Test-cluster\"\n  projectRef:\n    name: my-project\n  providerSettings:\n    instanceSizeName: M10\n    providerName: AWS\n    regionName: US_EAST_1\n```\n\n**4.** Create a database user password Kubernetes Secret\nThe Secret must be created in the same namespace as the `AtlasDeployment` and `AtlasProject` were created.\n```\nkubectl create secret generic the-user-password --from-literal=\"password=P@@sword%\"\n```\n\n**5.** Create an `AtlasDatabaseUser` Custom Resource\n\nIn order to connect to an Atlas Cluster the database user needs to be created. `AtlasDatabaseUser` resource should reference\nthe password Kubernetes Secret created in the previous step.\n```\napiVersion: atlas.mongodb.com/v1\nkind: AtlasDatabaseUser\nmetadata:\n  name: my-database-user\nspec:\n  roles:\n    - roleName: \"readWriteAnyDatabase\"\n      databaseName: \"admin\"\n  projectRef:\n    name: my-project\n  username: theuser\n  passwordSecretRef:\n    name: the-user-password\n```\n**6.** Wait for the `AtlasDatabaseUser` Custom Resource to be ready\n\nWait until the AtlasDatabaseUser resource gets to \"ready\" status (it will wait until the cluster is created that may take around 10 minutes):\n```\nkubectl get atlasdatabaseusers my-database-user -o=jsonpath='{.status.conditions[?(@.type==\"Ready\")].status}'\nTrue\n```\n### Step 3. Connect your application to the Atlas Cluster\n\nThe Atlas Operator will create a Kubernetes Secret with the information necessary to connect to the Atlas Cluster created\nin the previous step. An application in the same Kubernetes Cluster can mount and use the Secret:\n\n```\n...\ncontainers:\n      - name: test-app\n        env:\n         - name: \"CONNECTION_STRING\"\n           valueFrom:\n             secretKeyRef:\n               name: test-atlas-operator-project-test-cluster-theuser\n               key: connectionStringStandardSrv\n\n```\n",
      "csv_display_name": "MongoDB Atlas Operator",
      "csv_metadata_description": "The MongoDB Atlas Kubernetes Operator enables easy management of Clusters in MongoDB Atlas",
      "csv_name": "mongodb-atlas-kubernetes.v1.0.0",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": false,
      "last_update_date": "2022-11-23T08:55:14.746000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.7",
      "organization": "certified-operators",
      "package": "mongodb-atlas-kubernetes",
      "provided_apis": [
        {
          "group": "atlas.mongodb.com",
          "kind": "AtlasBackupPolicy",
          "plural": "atlasbackuppolicies",
          "version": "v1"
        },
        {
          "group": "atlas.mongodb.com",
          "kind": "AtlasBackupSchedule",
          "plural": "atlasbackupschedules",
          "version": "v1"
        },
        {
          "group": "atlas.mongodb.com",
          "kind": "AtlasDatabaseUser",
          "plural": "atlasdatabaseusers",
          "version": "v1"
        },
        {
          "group": "atlas.mongodb.com",
          "kind": "AtlasDeployment",
          "plural": "atlasdeployments",
          "version": "v1"
        },
        {
          "group": "atlas.mongodb.com",
          "kind": "AtlasProject",
          "plural": "atlasprojects",
          "version": "v1"
        }
      ],
      "provider": "MongoDB, Inc",
      "related_images": [
        {
          "digest": "sha256:3cfbd700e4771c2c7e75c2c41cf6de57c9ef71ecd286e96a21d2567e43424bc3",
          "image": "registry.connect.redhat.com/mongodb/mongodb-atlas-kubernetes-operator@sha256:3cfbd700e4771c2c7e75c2c41cf6de57c9ef71ecd286e96a21d2567e43424bc3",
          "name": "mongodb-atlas-kubernetes-operator-3cfbd700e4771c2c7e75c2c41cf6de57c9ef71ecd286e96a21d2567e43424bc3-annotation"
        },
        {
          "digest": "sha256:3cfbd700e4771c2c7e75c2c41cf6de57c9ef71ecd286e96a21d2567e43424bc3",
          "image": "registry.connect.redhat.com/mongodb/mongodb-atlas-kubernetes-operator@sha256:3cfbd700e4771c2c7e75c2c41cf6de57c9ef71ecd286e96a21d2567e43424bc3",
          "name": "manager"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.7",
      "version": "1.0.0",
      "version_original": "1.0.0"
    },
    {
      "_id": "62988486554afbde4c9bb401",
      "alm_examples": [
        {
          "api_version": "atlas.mongodb.com/v1",
          "kind": "AtlasBackupPolicy",
          "metadata": {
            "name": "atlasbackuppolicy-sample"
          },
          "spec": {
            "id": "1",
            "items": [
              {
                "frequencyInterval": 6,
                "frequencyType": "WEEKLY",
                "id": "2",
                "retentionUnit": "DAYS",
                "retentionValue": 6
              }
            ]
          }
        },
        {
          "api_version": "atlas.mongodb.com/v1",
          "kind": "AtlasBackupSchedule",
          "metadata": {
            "name": "atlasbackupschedule-sample"
          },
          "spec": {
            "autoExportEnabled": true,
            "policies": [
              {
                "name": "atlas-default-backuppolicy",
                "namespace": "mongodb-atlas-system"
              },
              {
                "name": "atlas-default-backuppolicy2",
                "namespace": "mongodb-atlas-system"
              }
            ],
            "referenceHourOfDay": 10,
            "referenceMinuteOfHour": 10,
            "restoreWindowDays": 2
          }
        },
        {
          "api_version": "atlas.mongodb.com/v1",
          "kind": "AtlasDatabaseUser",
          "metadata": {
            "name": "my-database-user"
          },
          "spec": {
            "databaseName": "admin",
            "passwordSecretRef": {
              "name": "my-database-user-password"
            },
            "projectRef": {
              "name": "my-project"
            },
            "roles": [
              {
                "databaseName": "admin",
                "roleName": "readWriteAnyDatabase"
              }
            ],
            "username": "david"
          }
        },
        {
          "api_version": "atlas.mongodb.com/v1",
          "kind": "AtlasDeployment",
          "metadata": {
            "name": "my-atlas-cluster"
          },
          "spec": {
            "deploymentSpec": {
              "name": "test-cluster",
              "providerSettings": {
                "instanceSizeName": "M10",
                "providerName": "AWS",
                "regionName": "US_EAST_1"
              }
            },
            "projectRef": {
              "name": "my-project"
            }
          }
        },
        {
          "api_version": "atlas.mongodb.com/v1",
          "kind": "AtlasProject",
          "metadata": {
            "name": "my-project"
          },
          "spec": {
            "name": "Test Atlas Operator Project",
            "projectIpAccessList": [
              {
                "comment": "IP address for Application Server A",
                "ipAddress": "192.0.2.15"
              }
            ]
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/mongodb/mongodb-atlas-kubernetes-operator-bundle@sha256:2338eaab863304250b3b1d267e9f2261c48eef1f97c280b7fa0af0ebc3ba208b",
      "bundle_path_digest": "sha256:2338eaab863304250b3b1d267e9f2261c48eef1f97c280b7fa0af0ebc3ba208b",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "beta",
      "creation_date": "2022-06-02T09:36:06.357000+00:00",
      "csv_description": "The MongoDB Atlas Operator provides a native integration between the Kubernetes orchestration platform and MongoDB Atlas \u2014\nthe only multi-cloud document database service that gives you the versatility you need to build sophisticated and resilient applications that can adapt to changing customer demands and market trends.\n\n> Current Status: *Beta*. The Operator gives users the ability to provision\n> Atlas projects, clusters and database users using Kubernetes Specifications and bind connection information\n> into applications deployed to Kubernetes.\n\n## Quick Start guide\n### Step 1. Deploy Kubernetes operator by clicking Install button.\n\n### Step 2. Create Atlas Cluster\n\n**1.** Create an Atlas API Key Secret\nIn order to work with the Atlas Operator you need to provide [authentication information](https://docs.atlas.mongodb.com/configure-api-access)\n to allow the Atlas Operator to communicate with Atlas API. Once you have generated a Public and Private key in Atlas, you can create a Kuberentes Secret with:\n```\nkubectl create secret generic mongodb-atlas-operator-api-key \\\n         --from-literal=\"orgId=<the_atlas_organization_id>\" \\\n         --from-literal=\"publicApiKey=<the_atlas_api_public_key>\" \\\n         --from-literal=\"privateApiKey=<the_atlas_api_private_key>\" \\\n         -n openshift-operators\n```\n(Note, that you should use the namespace where the Operator was installed - it's `openshift-operators` by default)\n\n**2.** Create an `AtlasProject` Custom Resource\n\nThe `AtlasProject` CustomResource represents Atlas Projects in our Kubernetes cluster. You need to specify\n`projectIpAccessList` with the IP addresses or CIDR blocks of any hosts that will connect to the Atlas Cluster.\n```\napiVersion: atlas.mongodb.com/v1\nkind: AtlasProject\nmetadata:\n  name: my-project\nspec:\n  name: Test Atlas Operator Project\n  projectIpAccessList:\n    - ipAddress: \"192.0.2.15\"\n      comment: \"IP address for Application Server A\"\n    - ipAddress: \"203.0.113.0/24\"\n      comment: \"CIDR block for Application Server B - D\"\n```\n**3.** Create an `AtlasDeployment` Custom Resource.\nThe example below is a minimal configuration to create an M10 Atlas cluster in the AWS US East region. For a full list of properties, check\n`atlasclusters.atlas.mongodb.com` [CRD specification](config/crd/bases/atlas.mongodb.com_atlasclusters.yaml)):\n```\napiVersion: atlas.mongodb.com/v1\nkind: AtlasDeployment\nmetadata:\n  name: my-atlas-cluster\nspec:\n  name: \"Test-cluster\"\n  projectRef:\n    name: my-project\n  providerSettings:\n    instanceSizeName: M10\n    providerName: AWS\n    regionName: US_EAST_1\n```\n\n**4.** Create a database user password Kubernetes Secret\nThe Secret must be created in the same namespace as the `AtlasDeployment` and `AtlasProject` were created.\n```\nkubectl create secret generic the-user-password --from-literal=\"password=P@@sword%\"\n```\n\n**5.** Create an `AtlasDatabaseUser` Custom Resource\n\nIn order to connect to an Atlas Cluster the database user needs to be created. `AtlasDatabaseUser` resource should reference\nthe password Kubernetes Secret created in the previous step.\n```\napiVersion: atlas.mongodb.com/v1\nkind: AtlasDatabaseUser\nmetadata:\n  name: my-database-user\nspec:\n  roles:\n    - roleName: \"readWriteAnyDatabase\"\n      databaseName: \"admin\"\n  projectRef:\n    name: my-project\n  username: theuser\n  passwordSecretRef:\n    name: the-user-password\n```\n**6.** Wait for the `AtlasDatabaseUser` Custom Resource to be ready\n\nWait until the AtlasDatabaseUser resource gets to \"ready\" status (it will wait until the cluster is created that may take around 10 minutes):\n```\nkubectl get atlasdatabaseusers my-database-user -o=jsonpath='{.status.conditions[?(@.type==\"Ready\")].status}'\nTrue\n```\n### Step 3. Connect your application to the Atlas Cluster\n\nThe Atlas Operator will create a Kubernetes Secret with the information necessary to connect to the Atlas Cluster created\nin the previous step. An application in the same Kubernetes Cluster can mount and use the Secret:\n\n```\n...\ncontainers:\n      - name: test-app\n        env:\n         - name: \"CONNECTION_STRING\"\n           valueFrom:\n             secretKeyRef:\n               name: test-atlas-operator-project-test-cluster-theuser\n               key: connectionStringStandardSrv\n\n```\n",
      "csv_display_name": "MongoDB Atlas Operator",
      "csv_metadata_description": "The MongoDB Atlas Kubernetes Operator enables easy management of Clusters in MongoDB Atlas",
      "csv_name": "mongodb-atlas-kubernetes.v1.0.0",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": false,
      "last_update_date": "2022-11-23T07:33:45.610000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.8",
      "organization": "certified-operators",
      "package": "mongodb-atlas-kubernetes",
      "provided_apis": [
        {
          "group": "atlas.mongodb.com",
          "kind": "AtlasProject",
          "plural": "atlasprojects",
          "version": "v1"
        },
        {
          "group": "atlas.mongodb.com",
          "kind": "AtlasBackupPolicy",
          "plural": "atlasbackuppolicies",
          "version": "v1"
        },
        {
          "group": "atlas.mongodb.com",
          "kind": "AtlasBackupSchedule",
          "plural": "atlasbackupschedules",
          "version": "v1"
        },
        {
          "group": "atlas.mongodb.com",
          "kind": "AtlasDatabaseUser",
          "plural": "atlasdatabaseusers",
          "version": "v1"
        },
        {
          "group": "atlas.mongodb.com",
          "kind": "AtlasDeployment",
          "plural": "atlasdeployments",
          "version": "v1"
        }
      ],
      "provider": "MongoDB, Inc",
      "related_images": [
        {
          "digest": "sha256:3cfbd700e4771c2c7e75c2c41cf6de57c9ef71ecd286e96a21d2567e43424bc3",
          "image": "registry.connect.redhat.com/mongodb/mongodb-atlas-kubernetes-operator@sha256:3cfbd700e4771c2c7e75c2c41cf6de57c9ef71ecd286e96a21d2567e43424bc3",
          "name": "mongodb-atlas-kubernetes-operator-3cfbd700e4771c2c7e75c2c41cf6de57c9ef71ecd286e96a21d2567e43424bc3-annotation"
        },
        {
          "digest": "sha256:3cfbd700e4771c2c7e75c2c41cf6de57c9ef71ecd286e96a21d2567e43424bc3",
          "image": "registry.connect.redhat.com/mongodb/mongodb-atlas-kubernetes-operator@sha256:3cfbd700e4771c2c7e75c2c41cf6de57c9ef71ecd286e96a21d2567e43424bc3",
          "name": "manager"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.8",
      "version": "1.0.0",
      "version_original": "1.0.0"
    },
    {
      "_id": "6298856be0c0938179ef6195",
      "alm_examples": [
        {
          "api_version": "atlas.mongodb.com/v1",
          "kind": "AtlasBackupPolicy",
          "metadata": {
            "name": "atlasbackuppolicy-sample"
          },
          "spec": {
            "id": "1",
            "items": [
              {
                "frequencyInterval": 6,
                "frequencyType": "WEEKLY",
                "id": "2",
                "retentionUnit": "DAYS",
                "retentionValue": 6
              }
            ]
          }
        },
        {
          "api_version": "atlas.mongodb.com/v1",
          "kind": "AtlasBackupSchedule",
          "metadata": {
            "name": "atlasbackupschedule-sample"
          },
          "spec": {
            "autoExportEnabled": true,
            "policies": [
              {
                "name": "atlas-default-backuppolicy",
                "namespace": "mongodb-atlas-system"
              },
              {
                "name": "atlas-default-backuppolicy2",
                "namespace": "mongodb-atlas-system"
              }
            ],
            "referenceHourOfDay": 10,
            "referenceMinuteOfHour": 10,
            "restoreWindowDays": 2
          }
        },
        {
          "api_version": "atlas.mongodb.com/v1",
          "kind": "AtlasDatabaseUser",
          "metadata": {
            "name": "my-database-user"
          },
          "spec": {
            "databaseName": "admin",
            "passwordSecretRef": {
              "name": "my-database-user-password"
            },
            "projectRef": {
              "name": "my-project"
            },
            "roles": [
              {
                "databaseName": "admin",
                "roleName": "readWriteAnyDatabase"
              }
            ],
            "username": "david"
          }
        },
        {
          "api_version": "atlas.mongodb.com/v1",
          "kind": "AtlasDeployment",
          "metadata": {
            "name": "my-atlas-cluster"
          },
          "spec": {
            "deploymentSpec": {
              "name": "test-cluster",
              "providerSettings": {
                "instanceSizeName": "M10",
                "providerName": "AWS",
                "regionName": "US_EAST_1"
              }
            },
            "projectRef": {
              "name": "my-project"
            }
          }
        },
        {
          "api_version": "atlas.mongodb.com/v1",
          "kind": "AtlasProject",
          "metadata": {
            "name": "my-project"
          },
          "spec": {
            "name": "Test Atlas Operator Project",
            "projectIpAccessList": [
              {
                "comment": "IP address for Application Server A",
                "ipAddress": "192.0.2.15"
              }
            ]
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/mongodb/mongodb-atlas-kubernetes-operator-bundle@sha256:2338eaab863304250b3b1d267e9f2261c48eef1f97c280b7fa0af0ebc3ba208b",
      "bundle_path_digest": "sha256:2338eaab863304250b3b1d267e9f2261c48eef1f97c280b7fa0af0ebc3ba208b",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "beta",
      "creation_date": "2022-06-02T09:39:55.202000+00:00",
      "csv_description": "The MongoDB Atlas Operator provides a native integration between the Kubernetes orchestration platform and MongoDB Atlas \u2014\nthe only multi-cloud document database service that gives you the versatility you need to build sophisticated and resilient applications that can adapt to changing customer demands and market trends.\n\n> Current Status: *Beta*. The Operator gives users the ability to provision\n> Atlas projects, clusters and database users using Kubernetes Specifications and bind connection information\n> into applications deployed to Kubernetes.\n\n## Quick Start guide\n### Step 1. Deploy Kubernetes operator by clicking Install button.\n\n### Step 2. Create Atlas Cluster\n\n**1.** Create an Atlas API Key Secret\nIn order to work with the Atlas Operator you need to provide [authentication information](https://docs.atlas.mongodb.com/configure-api-access)\n to allow the Atlas Operator to communicate with Atlas API. Once you have generated a Public and Private key in Atlas, you can create a Kuberentes Secret with:\n```\nkubectl create secret generic mongodb-atlas-operator-api-key \\\n         --from-literal=\"orgId=<the_atlas_organization_id>\" \\\n         --from-literal=\"publicApiKey=<the_atlas_api_public_key>\" \\\n         --from-literal=\"privateApiKey=<the_atlas_api_private_key>\" \\\n         -n openshift-operators\n```\n(Note, that you should use the namespace where the Operator was installed - it's `openshift-operators` by default)\n\n**2.** Create an `AtlasProject` Custom Resource\n\nThe `AtlasProject` CustomResource represents Atlas Projects in our Kubernetes cluster. You need to specify\n`projectIpAccessList` with the IP addresses or CIDR blocks of any hosts that will connect to the Atlas Cluster.\n```\napiVersion: atlas.mongodb.com/v1\nkind: AtlasProject\nmetadata:\n  name: my-project\nspec:\n  name: Test Atlas Operator Project\n  projectIpAccessList:\n    - ipAddress: \"192.0.2.15\"\n      comment: \"IP address for Application Server A\"\n    - ipAddress: \"203.0.113.0/24\"\n      comment: \"CIDR block for Application Server B - D\"\n```\n**3.** Create an `AtlasDeployment` Custom Resource.\nThe example below is a minimal configuration to create an M10 Atlas cluster in the AWS US East region. For a full list of properties, check\n`atlasclusters.atlas.mongodb.com` [CRD specification](config/crd/bases/atlas.mongodb.com_atlasclusters.yaml)):\n```\napiVersion: atlas.mongodb.com/v1\nkind: AtlasDeployment\nmetadata:\n  name: my-atlas-cluster\nspec:\n  name: \"Test-cluster\"\n  projectRef:\n    name: my-project\n  providerSettings:\n    instanceSizeName: M10\n    providerName: AWS\n    regionName: US_EAST_1\n```\n\n**4.** Create a database user password Kubernetes Secret\nThe Secret must be created in the same namespace as the `AtlasDeployment` and `AtlasProject` were created.\n```\nkubectl create secret generic the-user-password --from-literal=\"password=P@@sword%\"\n```\n\n**5.** Create an `AtlasDatabaseUser` Custom Resource\n\nIn order to connect to an Atlas Cluster the database user needs to be created. `AtlasDatabaseUser` resource should reference\nthe password Kubernetes Secret created in the previous step.\n```\napiVersion: atlas.mongodb.com/v1\nkind: AtlasDatabaseUser\nmetadata:\n  name: my-database-user\nspec:\n  roles:\n    - roleName: \"readWriteAnyDatabase\"\n      databaseName: \"admin\"\n  projectRef:\n    name: my-project\n  username: theuser\n  passwordSecretRef:\n    name: the-user-password\n```\n**6.** Wait for the `AtlasDatabaseUser` Custom Resource to be ready\n\nWait until the AtlasDatabaseUser resource gets to \"ready\" status (it will wait until the cluster is created that may take around 10 minutes):\n```\nkubectl get atlasdatabaseusers my-database-user -o=jsonpath='{.status.conditions[?(@.type==\"Ready\")].status}'\nTrue\n```\n### Step 3. Connect your application to the Atlas Cluster\n\nThe Atlas Operator will create a Kubernetes Secret with the information necessary to connect to the Atlas Cluster created\nin the previous step. An application in the same Kubernetes Cluster can mount and use the Secret:\n\n```\n...\ncontainers:\n      - name: test-app\n        env:\n         - name: \"CONNECTION_STRING\"\n           valueFrom:\n             secretKeyRef:\n               name: test-atlas-operator-project-test-cluster-theuser\n               key: connectionStringStandardSrv\n\n```\n",
      "csv_display_name": "MongoDB Atlas Operator",
      "csv_metadata_description": "The MongoDB Atlas Kubernetes Operator enables easy management of Clusters in MongoDB Atlas",
      "csv_name": "mongodb-atlas-kubernetes.v1.0.0",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": false,
      "last_update_date": "2022-11-23T07:39:49.513000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.6",
      "organization": "certified-operators",
      "package": "mongodb-atlas-kubernetes",
      "provided_apis": [
        {
          "group": "atlas.mongodb.com",
          "kind": "AtlasBackupSchedule",
          "plural": "atlasbackupschedules",
          "version": "v1"
        },
        {
          "group": "atlas.mongodb.com",
          "kind": "AtlasDatabaseUser",
          "plural": "atlasdatabaseusers",
          "version": "v1"
        },
        {
          "group": "atlas.mongodb.com",
          "kind": "AtlasDeployment",
          "plural": "atlasdeployments",
          "version": "v1"
        },
        {
          "group": "atlas.mongodb.com",
          "kind": "AtlasProject",
          "plural": "atlasprojects",
          "version": "v1"
        },
        {
          "group": "atlas.mongodb.com",
          "kind": "AtlasBackupPolicy",
          "plural": "atlasbackuppolicies",
          "version": "v1"
        }
      ],
      "provider": "MongoDB, Inc",
      "related_images": [
        {
          "digest": "sha256:3cfbd700e4771c2c7e75c2c41cf6de57c9ef71ecd286e96a21d2567e43424bc3",
          "image": "registry.connect.redhat.com/mongodb/mongodb-atlas-kubernetes-operator@sha256:3cfbd700e4771c2c7e75c2c41cf6de57c9ef71ecd286e96a21d2567e43424bc3",
          "name": "mongodb-atlas-kubernetes-operator-3cfbd700e4771c2c7e75c2c41cf6de57c9ef71ecd286e96a21d2567e43424bc3-annotation"
        },
        {
          "digest": "sha256:3cfbd700e4771c2c7e75c2c41cf6de57c9ef71ecd286e96a21d2567e43424bc3",
          "image": "registry.connect.redhat.com/mongodb/mongodb-atlas-kubernetes-operator@sha256:3cfbd700e4771c2c7e75c2c41cf6de57c9ef71ecd286e96a21d2567e43424bc3",
          "name": "manager"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.6",
      "version": "1.0.0",
      "version_original": "1.0.0"
    },
    {
      "_id": "6299f3a1665978f21d29a116",
      "alm_examples": [
        {
          "api_version": "csi.nec.com/v1",
          "kind": "NSPC",
          "metadata": {
            "name": "nspc",
            "namespace": "kube-system"
          },
          "spec": {}
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/nec/nspc-operator-bundle@sha256:ae54f0df026b854a1d67b1e0b866a7ea4241d2b2330ad474e7ffccf511264030",
      "bundle_path_digest": "sha256:ae54f0df026b854a1d67b1e0b866a7ea4241d2b2330ad474e7ffccf511264030",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "stable",
      "creation_date": "2022-06-03T11:42:25.326000+00:00",
      "csv_description": "## About\nNEC Storage Plug-in for Containers (NSPC) is a plugin that integrates NEC Storage V Series into Kubernetes based clusters.\nNSPC provides dynamic persistent volume provisioning capabilities from NEC Storage V Series arrays.\n\nFor full documentation, refer to the reference guide.\n\n## Requirements\n\n### Supported Driver Version\n\n* NSPC v3.9.0\n\n### Supported Platforms\n\n* OpenShift v4.7, v4.8, v4.9\n\n### Supported Operating Systems\n\n* RHEL 7.x\n* RHEL 8.x",
      "csv_display_name": "NEC Storage Plug-in for Containers",
      "csv_metadata_description": "An operator for managing NEC Storage Plug-in for Containers CSI driver",
      "csv_name": "nspc-operator.v1.9.0",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-11-23T06:56:43.120000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.7",
      "organization": "certified-operators",
      "package": "nspc-operator",
      "provided_apis": [
        {
          "group": "csi.nec.com",
          "kind": "NSPC",
          "version": "v1"
        }
      ],
      "provider": "NEC",
      "related_images": [
        {
          "digest": "sha256:4a4330567d3e540bcfe144df8aec069315f51661e47357e858593283eaf66d0b",
          "image": "registry.connect.redhat.com/nec/nspc-csi-driver@sha256:4a4330567d3e540bcfe144df8aec069315f51661e47357e858593283eaf66d0b",
          "name": "nspc-csi-driver"
        },
        {
          "digest": "sha256:1ccfd3a8b7e730e2fb83480399385f4f68d6f9cbb8e55caf9c4a6307f916fbd2",
          "image": "registry.connect.redhat.com/nec/nspc-operator@sha256:1ccfd3a8b7e730e2fb83480399385f4f68d6f9cbb8e55caf9c4a6307f916fbd2",
          "name": "nspc-operator"
        },
        {
          "digest": "sha256:e4ce5adf0abe5add640b166bc9f658d069ca4783ed37fcd8774f20da89a7cad1",
          "image": "registry.redhat.io/openshift4/ose-csi-external-attacher@sha256:e4ce5adf0abe5add640b166bc9f658d069ca4783ed37fcd8774f20da89a7cad1",
          "name": "ose-csi-external-attacher-v4.7"
        },
        {
          "digest": "sha256:d99597158ef4baf204e0fe7ed24c1dbc7b0218305077cdaf22c3899a1b3988c7",
          "image": "registry.redhat.io/openshift4/ose-csi-external-provisioner@sha256:d99597158ef4baf204e0fe7ed24c1dbc7b0218305077cdaf22c3899a1b3988c7",
          "name": "ose-csi-external-provisioner-v4.7"
        },
        {
          "digest": "sha256:2c7bb13d648a5048584530fddc62deb4b2e47bb7d389b77992e1077b920867e8",
          "image": "registry.redhat.io/openshift4/ose-csi-node-driver-registrar-rhel8@sha256:2c7bb13d648a5048584530fddc62deb4b2e47bb7d389b77992e1077b920867e8",
          "name": "ose-csi-node-driver-registrar-v4.7"
        },
        {
          "digest": "sha256:221cda173d5ad2be44655dd8ab697a80bd94149358183b3375b09280ba025f92",
          "image": "registry.redhat.io/openshift4/ose-csi-livenessprobe@sha256:221cda173d5ad2be44655dd8ab697a80bd94149358183b3375b09280ba025f92",
          "name": "ose-csi-livenessprobe-v4.7"
        },
        {
          "digest": "sha256:890729e261e87ad12bf7c523f9c2c39a20fb07e18aa269cd015f86ed23ac22b7",
          "image": "registry.redhat.io/openshift4/ose-csi-external-resizer-rhel8@sha256:890729e261e87ad12bf7c523f9c2c39a20fb07e18aa269cd015f86ed23ac22b7",
          "name": "ose-csi-external-resizer-rhel8-v4.7"
        },
        {
          "digest": "sha256:727b6317ef2575c905cd87c9d6d14b7c45886c4f20f0d75904641c55bb1e1814",
          "image": "registry.redhat.io/openshift4/ose-csi-external-snapshotter-rhel8@sha256:727b6317ef2575c905cd87c9d6d14b7c45886c4f20f0d75904641c55bb1e1814",
          "name": "ose-csi-external-snapshotter-rhel8-v4.7"
        },
        {
          "digest": "sha256:89e05898626e7689113d46ffb98c98e95a2a49b71c6a8b32409699eab8f32ea4",
          "image": "registry.redhat.io/openshift4/ose-csi-external-attacher@sha256:89e05898626e7689113d46ffb98c98e95a2a49b71c6a8b32409699eab8f32ea4",
          "name": "ose-csi-external-attacher-v4.8"
        },
        {
          "digest": "sha256:f91ba2c92c97f9b4c4afe789040250c9a6cb675fbe24cbae3ad381e58bc3b83a",
          "image": "registry.redhat.io/openshift4/ose-csi-external-provisioner@sha256:f91ba2c92c97f9b4c4afe789040250c9a6cb675fbe24cbae3ad381e58bc3b83a",
          "name": "ose-csi-external-provisioner-v4.8"
        },
        {
          "digest": "sha256:2e01a77235d1b692b87ec7e3e842dab52762032fbf3988f258960e2cece04225",
          "image": "registry.redhat.io/openshift4/ose-csi-node-driver-registrar-rhel8@sha256:2e01a77235d1b692b87ec7e3e842dab52762032fbf3988f258960e2cece04225",
          "name": "ose-csi-node-driver-registrar-v4.8"
        },
        {
          "digest": "sha256:2f0c069bd085951f864c268ebbb28d5044e912327cf1de13cb40e082a2aaf4db",
          "image": "registry.redhat.io/openshift4/ose-csi-livenessprobe@sha256:2f0c069bd085951f864c268ebbb28d5044e912327cf1de13cb40e082a2aaf4db",
          "name": "ose-csi-livenessprobe-v4.8"
        },
        {
          "digest": "sha256:e55efd04b15565bfcfcb8bd64b4da3a58461d8a6e2cfbbf1b1ac4bc537cf307b",
          "image": "registry.redhat.io/openshift4/ose-csi-external-resizer-rhel8@sha256:e55efd04b15565bfcfcb8bd64b4da3a58461d8a6e2cfbbf1b1ac4bc537cf307b",
          "name": "ose-csi-external-resizer-rhel8-v4.8"
        },
        {
          "digest": "sha256:bca38e3c695cf2bbb338b4933982f6892782b7fec14a1c835cb5a08c1cc7a14c",
          "image": "registry.redhat.io/openshift4/ose-csi-external-snapshotter-rhel8@sha256:bca38e3c695cf2bbb338b4933982f6892782b7fec14a1c835cb5a08c1cc7a14c",
          "name": "ose-csi-external-snapshotter-rhel8-v4.8"
        },
        {
          "digest": "sha256:9ffed943cdc9597803faedfac8891b963e08ec86f4cb21a59ae7c37adc5ce349",
          "image": "registry.redhat.io/openshift4/ose-csi-external-attacher@sha256:9ffed943cdc9597803faedfac8891b963e08ec86f4cb21a59ae7c37adc5ce349",
          "name": "ose-csi-external-attacher-v4.9"
        },
        {
          "digest": "sha256:ea0797b04595d1bf46ca5d3b205cead25c70e8c252e844551944079000a3fbe1",
          "image": "registry.redhat.io/openshift4/ose-csi-external-provisioner@sha256:ea0797b04595d1bf46ca5d3b205cead25c70e8c252e844551944079000a3fbe1",
          "name": "ose-csi-external-provisioner-v4.9"
        },
        {
          "digest": "sha256:4ca6d4d2f88eb0435084e1e0703330181e69e9c5c205ce21071d30fce864efde",
          "image": "registry.redhat.io/openshift4/ose-csi-node-driver-registrar-rhel8@sha256:4ca6d4d2f88eb0435084e1e0703330181e69e9c5c205ce21071d30fce864efde",
          "name": "ose-csi-node-driver-registrar-v4.9"
        },
        {
          "digest": "sha256:7c9816e9265dcc83af534a61175814e174ec10c1b411e71baf5d379ceeca1ecd",
          "image": "registry.redhat.io/openshift4/ose-csi-livenessprobe@sha256:7c9816e9265dcc83af534a61175814e174ec10c1b411e71baf5d379ceeca1ecd",
          "name": "ose-csi-livenessprobe-v4.9"
        },
        {
          "digest": "sha256:49ed3001a156015c3acbe5ef058f9acbb471b456535e7d56f6baec742527de74",
          "image": "registry.redhat.io/openshift4/ose-csi-external-resizer-rhel8@sha256:49ed3001a156015c3acbe5ef058f9acbb471b456535e7d56f6baec742527de74",
          "name": "ose-csi-external-resizer-rhel8-v4.9"
        },
        {
          "digest": "sha256:3ce747b7e19c5d0fa095fdeb29db3c9133575d24d941bad8ef6e71130c3f3883",
          "image": "registry.redhat.io/openshift4/ose-csi-external-snapshotter-rhel8@sha256:3ce747b7e19c5d0fa095fdeb29db3c9133575d24d941bad8ef6e71130c3f3883",
          "name": "ose-csi-external-snapshotter-rhel8-v4.9"
        },
        {
          "digest": "sha256:1ccfd3a8b7e730e2fb83480399385f4f68d6f9cbb8e55caf9c4a6307f916fbd2",
          "image": "registry.connect.redhat.com/nec/nspc-operator@sha256:1ccfd3a8b7e730e2fb83480399385f4f68d6f9cbb8e55caf9c4a6307f916fbd2",
          "name": "nspc-operator-1ccfd3a8b7e730e2fb83480399385f4f68d6f9cbb8e55caf9c4a6307f916fbd2-annotation"
        },
        {
          "digest": "sha256:1ccfd3a8b7e730e2fb83480399385f4f68d6f9cbb8e55caf9c4a6307f916fbd2",
          "image": "registry.connect.redhat.com/nec/nspc-operator@sha256:1ccfd3a8b7e730e2fb83480399385f4f68d6f9cbb8e55caf9c4a6307f916fbd2",
          "name": "manager"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.7",
      "version": "1.9.0",
      "version_original": "1.9.0"
    },
    {
      "_id": "629e455c706b4bf5cee71131",
      "alm_examples": [
        {
          "api_version": "crunchydata.com/v1",
          "kind": "Pgcluster",
          "metadata": {
            "annotations": {
              "current-primary": "hippo"
            },
            "labels": {
              "crunchy-pgha-scope": "hippo",
              "deployment-name": "hippo",
              "name": "hippo",
              "namespace": "pgo",
              "pg-cluster": "hippo",
              "pgo-version": "4.7.5"
            },
            "name": "hippo"
          },
          "spec": {
            "BackrestStorage": {
              "accessmode": "ReadWriteMany",
              "matchLabels": "",
              "name": "",
              "size": "5Gi",
              "storageclass": "",
              "storagetype": "dynamic",
              "supplementalgroups": ""
            },
            "PrimaryStorage": {
              "accessmode": "ReadWriteMany",
              "matchLabels": "",
              "name": "hippo",
              "size": "5Gi",
              "storageclass": "",
              "storagetype": "dynamic",
              "supplementalgroups": ""
            },
            "ReplicaStorage": {
              "accessmode": "ReadWriteMany",
              "matchLabels": "",
              "name": "",
              "size": "5Gi",
              "storageclass": "",
              "storagetype": "dynamic",
              "supplementalgroups": ""
            },
            "ccpimage": "crunchy-postgres-ha",
            "ccpimagetag": "ubi8-13.6-4.7.5",
            "clustername": "hippo",
            "database": "hippo",
            "exporterport": "9187",
            "name": "hippo",
            "namespace": "pgo",
            "pgbadgerport": "10000",
            "podAntiAffinity": {
              "default": "preferred"
            },
            "port": "5432",
            "user": "hippo",
            "userlabels": {
              "pgo-version": "4.7.5"
            }
          }
        },
        {
          "api_version": "crunchydata.com/v1",
          "kind": "Pgreplica",
          "metadata": {
            "name": "example"
          },
          "spec": {}
        },
        {
          "api_version": "crunchydata.com/v1",
          "kind": "Pgpolicy",
          "metadata": {
            "name": "example"
          },
          "spec": {}
        },
        {
          "api_version": "crunchydata.com/v1",
          "kind": "Pgtask",
          "metadata": {
            "name": "example"
          },
          "spec": {}
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/crunchydata/postgres-operator-bundle@sha256:8ae562b0c37aeffd811263713d8577df0ce88dfc57aef83fb081cf957fbe2d1b",
      "bundle_path_digest": "sha256:8ae562b0c37aeffd811263713d8577df0ce88dfc57aef83fb081cf957fbe2d1b",
      "capabilities": [
        "Auto Pilot"
      ],
      "channel_name": "stable",
      "creation_date": "2022-06-06T18:20:12.633000+00:00",
      "csv_description": "Crunchy PostgreSQL for OpenShift lets you run your own production-grade PostgreSQL-as-a-Service on OpenShift!\n\nPowered by the Crunchy [PostgreSQL Operator](https://github.com/CrunchyData/postgres-operator), Crunchy PostgreSQL\nfor OpenShift automates and simplifies deploying and managing open source PostgreSQL clusters on OpenShift by\nproviding the essential features you need to keep your PostgreSQL clusters up and running, including:\n\n- **PostgreSQL Cluster Provisioning**: [Create, Scale, & Delete PostgreSQL clusters with ease][provisioning],\n  while fully customizing your Pods and PostgreSQL configuration!\n- **High-Availability**: Safe, automated failover backed by a [distributed consensus based high-availability solution][high-availability].\n  Uses [Pod Anti-Affinity][k8s-anti-affinity] to help resiliency; you can configure how aggressive this can be!\n  Failed primaries automatically heal, allowing for faster recovery time. You can even create regularly scheduled\n  backups as well and set your backup retention policy\n- **Disaster Recovery**: Backups and restores leverage the open source [pgBackRest][] utility\n  and [includes support for full, incremental, and differential backups as well as efficient delta restores][disaster-recovery].\n  Set how long you want your backups retained for. Works great with very large databases!\n- **Monitoring**: Track the health of your PostgreSQL clusters using the open source [pgMonitor][] library.\n- **Clone**: Create new clusters from your existing clusters or backups with a single [`pgo create cluster --restore-from`][pgo-create-cluster] command.\n- **TLS**: Secure communication between your applications and data servers by [enabling TLS for your PostgreSQL servers][pgo-task-tls], including the ability to enforce that all of your connections to use TLS.\n- **Connection Pooling**: Use [pgBouncer][] for connection pooling\n- **Affinity and Tolerations**: Have your PostgreSQL clusters deployed to [Kubernetes Nodes][k8s-nodes] of your preference with [node affinity][high-availability-node-affinity], or designate which nodes Kubernetes can schedule PostgreSQL instances to with Kubernetes [tolerations][high-availability-tolerations].\n- **Full Customizability**: Crunchy PostgreSQL for OpenShift makes it easy to get your own PostgreSQL-as-a-Service up and running on\n  and lets make further enhancements to customize your deployments, including:\n    - Selecting different storage classes for your primary, replica, and backup storage\n    - Select your own container resources class for each PostgreSQL cluster deployment; differentiate between resources applied for primary and replica clusters!\n    - Use your own container image repository, including support `imagePullSecrets` and private repositories\n    - Bring your own trusted certificate authority (CA) for use with the Operator API server\n    - Override your PostgreSQL configuration for each cluster\n\nand much more!\n\n[disaster-recovery]: https://access.crunchydata.com/documentation/postgres-operator/latest/architecture/disaster-recovery/\n[high-availability]: https://access.crunchydata.com/documentation/postgres-operator/latest/architecture/high-availability/\n[high-availability-node-affinity]: https://access.crunchydata.com/documentation/postgres-operator/latest/architecture/high-availability/#node-affinity\n[high-availability-tolerations]: https://access.crunchydata.com/documentation/postgres-operator/latest/architecture/high-availability/#tolerations\n[pgo-create-cluster]: https://access.crunchydata.com/documentation/postgres-operator/latest/pgo-client/reference/pgo_create_cluster/\n[pgo-task-tls]: https://access.crunchydata.com/documentation/postgres-operator/latest/tutorial/tls/\n[provisioning]: https://access.crunchydata.com/documentation/postgres-operator/latest/architecture/provisioning/\n\n[k8s-anti-affinity]: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#inter-pod-affinity-and-anti-affinity\n[k8s-nodes]: https://kubernetes.io/docs/concepts/architecture/nodes/\n\n[pgBackRest]: https://www.pgbackrest.org\n[pgBouncer]: https://access.crunchydata.com/documentation/postgres-operator/latest/tutorial/pgbouncer/\n[pgMonitor]: https://github.com/CrunchyData/pgmonitor\n\n## Pre-Installation\n\nThere are a few manual steps that the cluster administrator must perform prior to installing the PostgreSQL Operator.\nAt the very least, it must be provided with an initial configuration.\n\nFirst, select a namespace in which to install the PostgreSQL Operator. PostgreSQL clusters will also be deployed here.\nIf it does not exist, create it now.\n\n```\nexport PGO_OPERATOR_NAMESPACE=pgo\noc create namespace \"$PGO_OPERATOR_NAMESPACE\"\n```\n\n### Security\n\nFor the PostgreSQL Operator and PostgreSQL clusters to run in the recommended `restricted` [Security Context Constraint][],\nedit `conf/postgres-operator/pgo.yaml` and set `DisableFSGroup` to `true`.\n\n[Security Context Constraint]: https://docs.openshift.com/container-platform/latest/authentication/managing-security-context-constraints.html\n\n### Secrets (optional)\n\nIf you plan to use AWS S3 to store backups, you can configure your environment to automatically provide your AWS S3 credentials to all newly created PostgreSQL clusters:\n\n```\noc -n \"$PGO_OPERATOR_NAMESPACE\" create secret generic pgo-backrest-repo-config \\\n  --from-literal=aws-s3-key=\"<your-aws-s3-key>\" \\\n  --from-literal=aws-s3-key-secret=\"<your-aws-s3-key-secret>\"\noc -n \"$PGO_OPERATOR_NAMESPACE\" label secret pgo-backrest-repo-config vendor=crunchydata\n```\n\n### Certificates (optional)\n\nThe PostgreSQL Operator has an API that uses TLS to communicate securely with clients. If one is not provided, the API will automatically generated one for you.\n\nIf you have a certificate bundle validated by your organization, you can install it now.\n\n```\noc -n \"$PGO_OPERATOR_NAMESPACE\" create secret tls pgo.tls \\\n  --cert=/path/to/server.crt \\\n  --key=/path/to/server.key\n```\n\nOnce these resources are in place, the PostgreSQL Operator can be installed into the cluster.\n\n## Installation\n\nYou can now go ahead and install the PostgreSQL Operator from OperatorHub.\n\n### Security\n\nFor the PostgreSQL Operator and PostgreSQL clusters to run in the recommended `restricted` [Security Context Constraint][],\nedit the ConfigMap `pgo-config`, find the `pgo.yaml` entry, and set `DisableFSGroup` to `true`.\n\n[Security Context Constraint]: https://docs.openshift.com/container-platform/latest/authentication/managing-security-context-constraints.html\n\nYou will have to scale the `postgres-operator` Deployment down and up for the above change to take effect:\n\n```\noc -n pgo scale --replicas 0 deployment/postgres-operator\noc -n pgo scale --replicas 1 deployment/postgres-operator\n```\n\n## Post-Installation\n\n### Tutorial\n\nFor a guide on how to perform many of the daily functions of the PostgreSQL Operator, we recommend that you read the [Postgres Operator tutorial][pgo-tutorial]\n\n[pgo-tutorial]: https://access.crunchydata.com/documentation/postgres-operator/latest/tutorial/create-cluster/\n\nHowever, the below guide will show you how to create a Postgres cluster from a custom resource or from using the `pgo-client`.\n\n### Create a PostgreSQL Cluster from a Custom Resource\n\nThe fundamental workflow for interfacing with a PostgreSQL Operator Custom\nResource Definition is for creating a PostgreSQL cluster. There are several\nthat a PostgreSQL cluster requires to be deployed, including:\n\n- Secrets\n  - Information for setting up a pgBackRest repository\n  - PostgreSQL superuser bootstrap credentials\n  - PostgreSQL replication user bootstrap credentials\n  - PostgresQL standard user bootstrap credentials\n\nAdditionally, if you want to add some of the other sidecars, you may need to\ncreate additional secrets.\n\nThe good news is that if you do not provide these objects, the PostgreSQL\nOperator will create them for you to get your Postgres cluster up and running!\n\nThe following goes through how to create a PostgreSQL cluster called\n`hippo` by creating a new custom resource.\n\n```\n# this variable is the name of the cluster being created\nexport pgo_cluster_name=hippo\n# this variable is the namespace the cluster is being deployed into\nexport cluster_namespace=pgo\n# this variable is set to the location of your image repository\nexport cluster_image_prefix=registry.developers.crunchydata.com/crunchydata\n\ncat <<-EOF > \"${pgo_cluster_name}-pgcluster.yaml\"\napiVersion: crunchydata.com/v1\nkind: Pgcluster\nmetadata:\n  annotations:\n    current-primary: ${pgo_cluster_name}\n  labels:\n    crunchy-pgha-scope: ${pgo_cluster_name}\n    deployment-name: ${pgo_cluster_name}\n    name: ${pgo_cluster_name}\n    pg-cluster: ${pgo_cluster_name}\n    pgo-version: 4.7.5\n    pgouser: admin\n  name: ${pgo_cluster_name}\n  namespace: ${cluster_namespace}\nspec:\n  BackrestStorage:\n    accessmode: ReadWriteMany\n    matchLabels: \"\"\n    name: \"\"\n    size: 1G\n    storageclass: \"\"\n    storagetype: create\n    supplementalgroups: \"\"\n  PrimaryStorage:\n    accessmode: ReadWriteMany\n    matchLabels: \"\"\n    name: ${pgo_cluster_name}\n    size: 1G\n    storageclass: \"\"\n    storagetype: create\n    supplementalgroups: \"\"\n  ReplicaStorage:\n    accessmode: ReadWriteMany\n    matchLabels: \"\"\n    name: \"\"\n    size: 1G\n    storageclass: \"\"\n    storagetype: create\n    supplementalgroups: \"\"\n  annotations: {}\n  ccpimage: crunchy-postgres-ha\n  ccpimageprefix: ${cluster_image_prefix}\n  ccpimagetag: centos8-13.6-4.7.5\n  clustername: ${pgo_cluster_name}\n  database: ${pgo_cluster_name}\n  exporterport: \"9187\"\n  limits: {}\n  name: ${pgo_cluster_name}\n  pgDataSource:\n    restoreFrom: \"\"\n    restoreOpts: \"\"\n  pgbadgerport: \"10000\"\n  pgoimageprefix: ${cluster_image_prefix}\n  podAntiAffinity:\n    default: preferred\n    pgBackRest: preferred\n    pgBouncer: preferred\n  port: \"5432\"\n  tolerations: []\n  user: hippo\n  userlabels:\n    pgo-version: 4.7.5\nEOF\n\noc apply -f \"${pgo_cluster_name}-pgcluster.yaml\"\n```\n\nAnd that's all! The PostgreSQL Operator will go ahead and create the cluster.\n\nIf you have the PostgreSQL client `psql` installed on your host machine, you can\ntest connection to the PostgreSQL cluster using the following command:\n\n```\n# namespace that the cluster is running in\nexport PGO_OPERATOR_NAMESPACE=pgo\n# name of the cluster\nexport pgo_cluster_name=hippo\n# name of the user whose password we want to get\nexport pgo_cluster_username=hippo\n\n# get the password of the user and set it to a recognized psql environmental variable\nexport PGPASSWORD=$(oc -n \"${PGO_OPERATOR_NAMESPACE}\" get secrets \\\n  \"${pgo_cluster_name}-${pgo_cluster_username}-secret\" -o \"jsonpath={.data['password']}\" | base64 -d)\n\n# set up a port-forward either in a new terminal, or in the same terminal in the background:\noc -n pgo port-forward svc/hippo 5432:5432 &\n\npsql -h localhost -U \"${pgo_cluster_username}\" \"${pgo_cluster_name}\"\n```\n\n### Create a PostgreSQL Cluster the `pgo` Client\n\nOnce the PostgreSQL Operator is installed in your OpenShift cluster, you will need to do a few things\nto use the [PostgreSQL Operator Client][pgo-client].\n\n[pgo-client]: https://access.crunchydata.com/documentation/postgres-operator/latest/pgo-client/\n\nInstall the first set of client credentials and download the `pgo` binary and client certificates.\n\n```\ncurl https://raw.githubusercontent.com/CrunchyData/postgres-operator/v4.7.5/deploy/install-bootstrap-creds.sh > install-bootstrap-creds.sh\ncurl https://raw.githubusercontent.com/CrunchyData/postgres-operator/v4.7.5/installers/kubectl/client-setup.sh > client-setup.sh\n\nchmod +x install-bootstrap-creds.sh client-setup.sh\n\nPGO_CMD=oc ./install-bootstrap-creds.sh\nPGO_CMD=oc ./client-setup.sh\n```\n\nThe client needs to be able to reach the PostgreSQL Operator API from outside the OpenShift cluster.\nCreate an external service or forward a port locally.\n\n```\noc -n \"$PGO_OPERATOR_NAMESPACE\" expose deployment postgres-operator\noc -n \"$PGO_OPERATOR_NAMESPACE\" create route passthrough postgres-operator --service=postgres-operator\n\nexport PGO_APISERVER_URL=\"https://$(oc -n \"$PGO_OPERATOR_NAMESPACE\" get route postgres-operator -o jsonpath=\"{.spec.host}\")\"\n```\n_or_\n```\noc -n \"$PGO_OPERATOR_NAMESPACE\" port-forward deployment/postgres-operator 8443\n\nexport PGO_APISERVER_URL=\"https://127.0.0.1:8443\"\n```\n\nVerify connectivity using the `pgo` command.\n\n```\npgo version\n# pgo client version 4.7.5\n# pgo-apiserver version 4.7.5\n```\n\n\nYou can then create a cluster with the `pgo` client as simply as this:\n\n```\npgo create cluster -n pgo hippo\n```\n\nThe cluster may take a few moments to provision. You can verify that the cluster is up and running by using the `pgo test` command:\n\n```\npgo test cluster -n pgo hippo\n```\n\nIf you have the PostgreSQL client `psql` installed on your host machine, you can\ntest connection to the PostgreSQL cluster using the following command:\n\n```\n# namespace that the cluster is running in\nexport PGO_OPERATOR_NAMESPACE=pgo\n# name of the cluster\nexport pgo_cluster_name=hippo\n# name of the user whose password we want to get\nexport pgo_cluster_username=hippo\n\n# get the password of the user and set it to a recognized psql environmental variable\nexport PGPASSWORD=$(kubectl -n \"${PGO_OPERATOR_NAMESPACE}\" get secrets \\\n  \"${pgo_cluster_name}-${pgo_cluster_username}-secret\" -o \"jsonpath={.data['password']}\" | base64 -d)\n\n# set up a port-forward either in a new terminal, or in the same terminal in the background:\nkubectl -n pgo port-forward svc/hippo 5432:5432 &\n\npsql -h localhost -U \"${pgo_cluster_username}\" \"${pgo_cluster_name}\"\n```",
      "csv_display_name": "Crunchy PostgreSQL for OpenShift",
      "csv_metadata_description": "Enterprise open source PostgreSQL-as-a-Service",
      "csv_name": "postgresoperator.v4.7.5",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": true,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": false,
      "last_update_date": "2022-11-23T07:26:03.889000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.9",
      "organization": "certified-operators",
      "package": "crunchy-postgres-operator",
      "provided_apis": [
        {
          "group": "crunchydata.com",
          "kind": "Pgcluster",
          "plural": "pgclusters",
          "version": "v1"
        },
        {
          "group": "crunchydata.com",
          "kind": "Pgpolicy",
          "plural": "pgpolicies",
          "version": "v1"
        },
        {
          "group": "crunchydata.com",
          "kind": "Pgreplica",
          "plural": "pgreplicas",
          "version": "v1"
        },
        {
          "group": "crunchydata.com",
          "kind": "Pgtask",
          "plural": "pgtasks",
          "version": "v1"
        }
      ],
      "provider": "Crunchy Data",
      "related_images": [
        {
          "digest": "sha256:d9f91cb300615bcf39e610abd4741ff6be3d13418db4260f9409395a2a5c1ecb",
          "image": "registry.connect.redhat.com/crunchydata/postgres-operator@sha256:d9f91cb300615bcf39e610abd4741ff6be3d13418db4260f9409395a2a5c1ecb",
          "name": "postgres-operator-d9f91cb300615bcf39e610abd4741ff6be3d13418db4260f9409395a2a5c1ecb-annotation"
        },
        {
          "digest": "sha256:86b5b293c263667dfc5016fc587643261438bd8868cc1c523ccb77a7c4e1b85a",
          "image": "registry.connect.redhat.com/crunchydata/pgo-apiserver@sha256:86b5b293c263667dfc5016fc587643261438bd8868cc1c523ccb77a7c4e1b85a",
          "name": "apiserver"
        },
        {
          "digest": "sha256:d9f91cb300615bcf39e610abd4741ff6be3d13418db4260f9409395a2a5c1ecb",
          "image": "registry.connect.redhat.com/crunchydata/postgres-operator@sha256:d9f91cb300615bcf39e610abd4741ff6be3d13418db4260f9409395a2a5c1ecb",
          "name": "operator"
        },
        {
          "digest": "sha256:481c48699d7ba6aefc7ad27b43e4df54b8f8686f3445cf303a45bd43ceec4244",
          "image": "registry.connect.redhat.com/crunchydata/pgo-scheduler@sha256:481c48699d7ba6aefc7ad27b43e4df54b8f8686f3445cf303a45bd43ceec4244",
          "name": "scheduler"
        },
        {
          "digest": "sha256:bcc9f68cfce901352408e4645f7c6aa5d2157af93952b9124c3a20923e4f1288",
          "image": "registry.connect.redhat.com/crunchydata/pgo-event@sha256:bcc9f68cfce901352408e4645f7c6aa5d2157af93952b9124c3a20923e4f1288",
          "name": "event"
        },
        {
          "digest": "sha256:0016bf993af9ed5fdfdbbd56b1e743d1991ed390a4800cf1a04cdf55cafe119a",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-pgbackrest@sha256:0016bf993af9ed5fdfdbbd56b1e743d1991ed390a4800cf1a04cdf55cafe119a",
          "name": "pgo_backrest"
        },
        {
          "digest": "sha256:2991db135d2158fbb496afaabee221c432e13d11767e90130f4b2b5c2912a6b5",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-pgbackrest-repo@sha256:2991db135d2158fbb496afaabee221c432e13d11767e90130f4b2b5c2912a6b5",
          "name": "pgo_backrest_repo"
        },
        {
          "digest": "sha256:dabd59ff8e97a06e5e823c33805da6f66300c43c8b315cfd7ab0315bd92803e1",
          "image": "registry.connect.redhat.com/crunchydata/pgo-client@sha256:dabd59ff8e97a06e5e823c33805da6f66300c43c8b315cfd7ab0315bd92803e1",
          "name": "pgo_client"
        },
        {
          "digest": "sha256:c44507bfc3286017afef0697133595043256b8b595e1e2e470b6ff7598cff575",
          "image": "registry.connect.redhat.com/crunchydata/pgo-rmdata@sha256:c44507bfc3286017afef0697133595043256b8b595e1e2e470b6ff7598cff575",
          "name": "pgo_rmdata"
        },
        {
          "digest": "sha256:79d658ec93a10951cd06950e753f92332ea07ce191197213e14f4dc6042cc9f1",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres-exporter@sha256:79d658ec93a10951cd06950e753f92332ea07ce191197213e14f4dc6042cc9f1",
          "name": "crunchy_postgres_exporter"
        },
        {
          "digest": "sha256:77e508924d2358af0f3a8f597f7f59c38f7aecda2af9014ab9438d35f503de9c",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-pgadmin4@sha256:77e508924d2358af0f3a8f597f7f59c38f7aecda2af9014ab9438d35f503de9c",
          "name": "crunchy_pgadmin"
        },
        {
          "digest": "sha256:8850ac1be18d01b7dee9bb0f61d453c787961985bc84df3bc8c1595962a00fab",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-pgbadger@sha256:8850ac1be18d01b7dee9bb0f61d453c787961985bc84df3bc8c1595962a00fab",
          "name": "crunchy_pgbadger"
        },
        {
          "digest": "sha256:a339798a1712a3a3bcf303e6df8186460edfb47aca0e2ae4cadf9db2207d72d6",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-pgbouncer@sha256:a339798a1712a3a3bcf303e6df8186460edfb47aca0e2ae4cadf9db2207d72d6",
          "name": "crunchy_pgbouncer"
        },
        {
          "digest": "sha256:ea787e002ce3aee1b8c54127816885b07c1f063515fa5b80ce957676f4e0a21b",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres-ha@sha256:ea787e002ce3aee1b8c54127816885b07c1f063515fa5b80ce957676f4e0a21b",
          "name": "crunchy_postgres_ha"
        },
        {
          "digest": "sha256:f0d2c8df4adb038af6be43089d96af02afd88b82568a2da342d8d97e1fa4ba8f",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres-gis-ha@sha256:f0d2c8df4adb038af6be43089d96af02afd88b82568a2da342d8d97e1fa4ba8f",
          "name": "crunchy_postgres_gis_ha"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.9",
      "version": "4.7.5",
      "version_original": "4.7.5"
    },
    {
      "_id": "629e48b75bfb3ed63084b31d",
      "alm_examples": [
        {
          "api_version": "crunchydata.com/v1",
          "kind": "Pgcluster",
          "metadata": {
            "annotations": {
              "current-primary": "hippo"
            },
            "labels": {
              "crunchy-pgha-scope": "hippo",
              "deployment-name": "hippo",
              "name": "hippo",
              "namespace": "pgo",
              "pg-cluster": "hippo",
              "pgo-version": "4.7.5"
            },
            "name": "hippo"
          },
          "spec": {
            "BackrestStorage": {
              "accessmode": "ReadWriteMany",
              "matchLabels": "",
              "name": "",
              "size": "5Gi",
              "storageclass": "",
              "storagetype": "dynamic",
              "supplementalgroups": ""
            },
            "PrimaryStorage": {
              "accessmode": "ReadWriteMany",
              "matchLabels": "",
              "name": "hippo",
              "size": "5Gi",
              "storageclass": "",
              "storagetype": "dynamic",
              "supplementalgroups": ""
            },
            "ReplicaStorage": {
              "accessmode": "ReadWriteMany",
              "matchLabels": "",
              "name": "",
              "size": "5Gi",
              "storageclass": "",
              "storagetype": "dynamic",
              "supplementalgroups": ""
            },
            "ccpimage": "crunchy-postgres-ha",
            "ccpimagetag": "ubi8-13.6-4.7.5",
            "clustername": "hippo",
            "database": "hippo",
            "exporterport": "9187",
            "name": "hippo",
            "namespace": "pgo",
            "pgbadgerport": "10000",
            "podAntiAffinity": {
              "default": "preferred"
            },
            "port": "5432",
            "user": "hippo",
            "userlabels": {
              "pgo-version": "4.7.5"
            }
          }
        },
        {
          "api_version": "crunchydata.com/v1",
          "kind": "Pgreplica",
          "metadata": {
            "name": "example"
          },
          "spec": {}
        },
        {
          "api_version": "crunchydata.com/v1",
          "kind": "Pgpolicy",
          "metadata": {
            "name": "example"
          },
          "spec": {}
        },
        {
          "api_version": "crunchydata.com/v1",
          "kind": "Pgtask",
          "metadata": {
            "name": "example"
          },
          "spec": {}
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/crunchydata/postgres-operator-bundle@sha256:8ae562b0c37aeffd811263713d8577df0ce88dfc57aef83fb081cf957fbe2d1b",
      "bundle_path_digest": "sha256:8ae562b0c37aeffd811263713d8577df0ce88dfc57aef83fb081cf957fbe2d1b",
      "capabilities": [
        "Auto Pilot"
      ],
      "channel_name": "stable",
      "creation_date": "2022-06-06T18:34:31.755000+00:00",
      "csv_description": "Crunchy PostgreSQL for OpenShift lets you run your own production-grade PostgreSQL-as-a-Service on OpenShift!\n\nPowered by the Crunchy [PostgreSQL Operator](https://github.com/CrunchyData/postgres-operator), Crunchy PostgreSQL\nfor OpenShift automates and simplifies deploying and managing open source PostgreSQL clusters on OpenShift by\nproviding the essential features you need to keep your PostgreSQL clusters up and running, including:\n\n- **PostgreSQL Cluster Provisioning**: [Create, Scale, & Delete PostgreSQL clusters with ease][provisioning],\n  while fully customizing your Pods and PostgreSQL configuration!\n- **High-Availability**: Safe, automated failover backed by a [distributed consensus based high-availability solution][high-availability].\n  Uses [Pod Anti-Affinity][k8s-anti-affinity] to help resiliency; you can configure how aggressive this can be!\n  Failed primaries automatically heal, allowing for faster recovery time. You can even create regularly scheduled\n  backups as well and set your backup retention policy\n- **Disaster Recovery**: Backups and restores leverage the open source [pgBackRest][] utility\n  and [includes support for full, incremental, and differential backups as well as efficient delta restores][disaster-recovery].\n  Set how long you want your backups retained for. Works great with very large databases!\n- **Monitoring**: Track the health of your PostgreSQL clusters using the open source [pgMonitor][] library.\n- **Clone**: Create new clusters from your existing clusters or backups with a single [`pgo create cluster --restore-from`][pgo-create-cluster] command.\n- **TLS**: Secure communication between your applications and data servers by [enabling TLS for your PostgreSQL servers][pgo-task-tls], including the ability to enforce that all of your connections to use TLS.\n- **Connection Pooling**: Use [pgBouncer][] for connection pooling\n- **Affinity and Tolerations**: Have your PostgreSQL clusters deployed to [Kubernetes Nodes][k8s-nodes] of your preference with [node affinity][high-availability-node-affinity], or designate which nodes Kubernetes can schedule PostgreSQL instances to with Kubernetes [tolerations][high-availability-tolerations].\n- **Full Customizability**: Crunchy PostgreSQL for OpenShift makes it easy to get your own PostgreSQL-as-a-Service up and running on\n  and lets make further enhancements to customize your deployments, including:\n    - Selecting different storage classes for your primary, replica, and backup storage\n    - Select your own container resources class for each PostgreSQL cluster deployment; differentiate between resources applied for primary and replica clusters!\n    - Use your own container image repository, including support `imagePullSecrets` and private repositories\n    - Bring your own trusted certificate authority (CA) for use with the Operator API server\n    - Override your PostgreSQL configuration for each cluster\n\nand much more!\n\n[disaster-recovery]: https://access.crunchydata.com/documentation/postgres-operator/latest/architecture/disaster-recovery/\n[high-availability]: https://access.crunchydata.com/documentation/postgres-operator/latest/architecture/high-availability/\n[high-availability-node-affinity]: https://access.crunchydata.com/documentation/postgres-operator/latest/architecture/high-availability/#node-affinity\n[high-availability-tolerations]: https://access.crunchydata.com/documentation/postgres-operator/latest/architecture/high-availability/#tolerations\n[pgo-create-cluster]: https://access.crunchydata.com/documentation/postgres-operator/latest/pgo-client/reference/pgo_create_cluster/\n[pgo-task-tls]: https://access.crunchydata.com/documentation/postgres-operator/latest/tutorial/tls/\n[provisioning]: https://access.crunchydata.com/documentation/postgres-operator/latest/architecture/provisioning/\n\n[k8s-anti-affinity]: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#inter-pod-affinity-and-anti-affinity\n[k8s-nodes]: https://kubernetes.io/docs/concepts/architecture/nodes/\n\n[pgBackRest]: https://www.pgbackrest.org\n[pgBouncer]: https://access.crunchydata.com/documentation/postgres-operator/latest/tutorial/pgbouncer/\n[pgMonitor]: https://github.com/CrunchyData/pgmonitor\n\n## Pre-Installation\n\nThere are a few manual steps that the cluster administrator must perform prior to installing the PostgreSQL Operator.\nAt the very least, it must be provided with an initial configuration.\n\nFirst, select a namespace in which to install the PostgreSQL Operator. PostgreSQL clusters will also be deployed here.\nIf it does not exist, create it now.\n\n```\nexport PGO_OPERATOR_NAMESPACE=pgo\noc create namespace \"$PGO_OPERATOR_NAMESPACE\"\n```\n\n### Security\n\nFor the PostgreSQL Operator and PostgreSQL clusters to run in the recommended `restricted` [Security Context Constraint][],\nedit `conf/postgres-operator/pgo.yaml` and set `DisableFSGroup` to `true`.\n\n[Security Context Constraint]: https://docs.openshift.com/container-platform/latest/authentication/managing-security-context-constraints.html\n\n### Secrets (optional)\n\nIf you plan to use AWS S3 to store backups, you can configure your environment to automatically provide your AWS S3 credentials to all newly created PostgreSQL clusters:\n\n```\noc -n \"$PGO_OPERATOR_NAMESPACE\" create secret generic pgo-backrest-repo-config \\\n  --from-literal=aws-s3-key=\"<your-aws-s3-key>\" \\\n  --from-literal=aws-s3-key-secret=\"<your-aws-s3-key-secret>\"\noc -n \"$PGO_OPERATOR_NAMESPACE\" label secret pgo-backrest-repo-config vendor=crunchydata\n```\n\n### Certificates (optional)\n\nThe PostgreSQL Operator has an API that uses TLS to communicate securely with clients. If one is not provided, the API will automatically generated one for you.\n\nIf you have a certificate bundle validated by your organization, you can install it now.\n\n```\noc -n \"$PGO_OPERATOR_NAMESPACE\" create secret tls pgo.tls \\\n  --cert=/path/to/server.crt \\\n  --key=/path/to/server.key\n```\n\nOnce these resources are in place, the PostgreSQL Operator can be installed into the cluster.\n\n## Installation\n\nYou can now go ahead and install the PostgreSQL Operator from OperatorHub.\n\n### Security\n\nFor the PostgreSQL Operator and PostgreSQL clusters to run in the recommended `restricted` [Security Context Constraint][],\nedit the ConfigMap `pgo-config`, find the `pgo.yaml` entry, and set `DisableFSGroup` to `true`.\n\n[Security Context Constraint]: https://docs.openshift.com/container-platform/latest/authentication/managing-security-context-constraints.html\n\nYou will have to scale the `postgres-operator` Deployment down and up for the above change to take effect:\n\n```\noc -n pgo scale --replicas 0 deployment/postgres-operator\noc -n pgo scale --replicas 1 deployment/postgres-operator\n```\n\n## Post-Installation\n\n### Tutorial\n\nFor a guide on how to perform many of the daily functions of the PostgreSQL Operator, we recommend that you read the [Postgres Operator tutorial][pgo-tutorial]\n\n[pgo-tutorial]: https://access.crunchydata.com/documentation/postgres-operator/latest/tutorial/create-cluster/\n\nHowever, the below guide will show you how to create a Postgres cluster from a custom resource or from using the `pgo-client`.\n\n### Create a PostgreSQL Cluster from a Custom Resource\n\nThe fundamental workflow for interfacing with a PostgreSQL Operator Custom\nResource Definition is for creating a PostgreSQL cluster. There are several\nthat a PostgreSQL cluster requires to be deployed, including:\n\n- Secrets\n  - Information for setting up a pgBackRest repository\n  - PostgreSQL superuser bootstrap credentials\n  - PostgreSQL replication user bootstrap credentials\n  - PostgresQL standard user bootstrap credentials\n\nAdditionally, if you want to add some of the other sidecars, you may need to\ncreate additional secrets.\n\nThe good news is that if you do not provide these objects, the PostgreSQL\nOperator will create them for you to get your Postgres cluster up and running!\n\nThe following goes through how to create a PostgreSQL cluster called\n`hippo` by creating a new custom resource.\n\n```\n# this variable is the name of the cluster being created\nexport pgo_cluster_name=hippo\n# this variable is the namespace the cluster is being deployed into\nexport cluster_namespace=pgo\n# this variable is set to the location of your image repository\nexport cluster_image_prefix=registry.developers.crunchydata.com/crunchydata\n\ncat <<-EOF > \"${pgo_cluster_name}-pgcluster.yaml\"\napiVersion: crunchydata.com/v1\nkind: Pgcluster\nmetadata:\n  annotations:\n    current-primary: ${pgo_cluster_name}\n  labels:\n    crunchy-pgha-scope: ${pgo_cluster_name}\n    deployment-name: ${pgo_cluster_name}\n    name: ${pgo_cluster_name}\n    pg-cluster: ${pgo_cluster_name}\n    pgo-version: 4.7.5\n    pgouser: admin\n  name: ${pgo_cluster_name}\n  namespace: ${cluster_namespace}\nspec:\n  BackrestStorage:\n    accessmode: ReadWriteMany\n    matchLabels: \"\"\n    name: \"\"\n    size: 1G\n    storageclass: \"\"\n    storagetype: create\n    supplementalgroups: \"\"\n  PrimaryStorage:\n    accessmode: ReadWriteMany\n    matchLabels: \"\"\n    name: ${pgo_cluster_name}\n    size: 1G\n    storageclass: \"\"\n    storagetype: create\n    supplementalgroups: \"\"\n  ReplicaStorage:\n    accessmode: ReadWriteMany\n    matchLabels: \"\"\n    name: \"\"\n    size: 1G\n    storageclass: \"\"\n    storagetype: create\n    supplementalgroups: \"\"\n  annotations: {}\n  ccpimage: crunchy-postgres-ha\n  ccpimageprefix: ${cluster_image_prefix}\n  ccpimagetag: centos8-13.6-4.7.5\n  clustername: ${pgo_cluster_name}\n  database: ${pgo_cluster_name}\n  exporterport: \"9187\"\n  limits: {}\n  name: ${pgo_cluster_name}\n  pgDataSource:\n    restoreFrom: \"\"\n    restoreOpts: \"\"\n  pgbadgerport: \"10000\"\n  pgoimageprefix: ${cluster_image_prefix}\n  podAntiAffinity:\n    default: preferred\n    pgBackRest: preferred\n    pgBouncer: preferred\n  port: \"5432\"\n  tolerations: []\n  user: hippo\n  userlabels:\n    pgo-version: 4.7.5\nEOF\n\noc apply -f \"${pgo_cluster_name}-pgcluster.yaml\"\n```\n\nAnd that's all! The PostgreSQL Operator will go ahead and create the cluster.\n\nIf you have the PostgreSQL client `psql` installed on your host machine, you can\ntest connection to the PostgreSQL cluster using the following command:\n\n```\n# namespace that the cluster is running in\nexport PGO_OPERATOR_NAMESPACE=pgo\n# name of the cluster\nexport pgo_cluster_name=hippo\n# name of the user whose password we want to get\nexport pgo_cluster_username=hippo\n\n# get the password of the user and set it to a recognized psql environmental variable\nexport PGPASSWORD=$(oc -n \"${PGO_OPERATOR_NAMESPACE}\" get secrets \\\n  \"${pgo_cluster_name}-${pgo_cluster_username}-secret\" -o \"jsonpath={.data['password']}\" | base64 -d)\n\n# set up a port-forward either in a new terminal, or in the same terminal in the background:\noc -n pgo port-forward svc/hippo 5432:5432 &\n\npsql -h localhost -U \"${pgo_cluster_username}\" \"${pgo_cluster_name}\"\n```\n\n### Create a PostgreSQL Cluster the `pgo` Client\n\nOnce the PostgreSQL Operator is installed in your OpenShift cluster, you will need to do a few things\nto use the [PostgreSQL Operator Client][pgo-client].\n\n[pgo-client]: https://access.crunchydata.com/documentation/postgres-operator/latest/pgo-client/\n\nInstall the first set of client credentials and download the `pgo` binary and client certificates.\n\n```\ncurl https://raw.githubusercontent.com/CrunchyData/postgres-operator/v4.7.5/deploy/install-bootstrap-creds.sh > install-bootstrap-creds.sh\ncurl https://raw.githubusercontent.com/CrunchyData/postgres-operator/v4.7.5/installers/kubectl/client-setup.sh > client-setup.sh\n\nchmod +x install-bootstrap-creds.sh client-setup.sh\n\nPGO_CMD=oc ./install-bootstrap-creds.sh\nPGO_CMD=oc ./client-setup.sh\n```\n\nThe client needs to be able to reach the PostgreSQL Operator API from outside the OpenShift cluster.\nCreate an external service or forward a port locally.\n\n```\noc -n \"$PGO_OPERATOR_NAMESPACE\" expose deployment postgres-operator\noc -n \"$PGO_OPERATOR_NAMESPACE\" create route passthrough postgres-operator --service=postgres-operator\n\nexport PGO_APISERVER_URL=\"https://$(oc -n \"$PGO_OPERATOR_NAMESPACE\" get route postgres-operator -o jsonpath=\"{.spec.host}\")\"\n```\n_or_\n```\noc -n \"$PGO_OPERATOR_NAMESPACE\" port-forward deployment/postgres-operator 8443\n\nexport PGO_APISERVER_URL=\"https://127.0.0.1:8443\"\n```\n\nVerify connectivity using the `pgo` command.\n\n```\npgo version\n# pgo client version 4.7.5\n# pgo-apiserver version 4.7.5\n```\n\n\nYou can then create a cluster with the `pgo` client as simply as this:\n\n```\npgo create cluster -n pgo hippo\n```\n\nThe cluster may take a few moments to provision. You can verify that the cluster is up and running by using the `pgo test` command:\n\n```\npgo test cluster -n pgo hippo\n```\n\nIf you have the PostgreSQL client `psql` installed on your host machine, you can\ntest connection to the PostgreSQL cluster using the following command:\n\n```\n# namespace that the cluster is running in\nexport PGO_OPERATOR_NAMESPACE=pgo\n# name of the cluster\nexport pgo_cluster_name=hippo\n# name of the user whose password we want to get\nexport pgo_cluster_username=hippo\n\n# get the password of the user and set it to a recognized psql environmental variable\nexport PGPASSWORD=$(kubectl -n \"${PGO_OPERATOR_NAMESPACE}\" get secrets \\\n  \"${pgo_cluster_name}-${pgo_cluster_username}-secret\" -o \"jsonpath={.data['password']}\" | base64 -d)\n\n# set up a port-forward either in a new terminal, or in the same terminal in the background:\nkubectl -n pgo port-forward svc/hippo 5432:5432 &\n\npsql -h localhost -U \"${pgo_cluster_username}\" \"${pgo_cluster_name}\"\n```",
      "csv_display_name": "Crunchy PostgreSQL for OpenShift",
      "csv_metadata_description": "Enterprise open source PostgreSQL-as-a-Service",
      "csv_name": "postgresoperator.v4.7.5",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": true,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": false,
      "last_update_date": "2022-11-23T08:50:15.183000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.10",
      "organization": "certified-operators",
      "package": "crunchy-postgres-operator",
      "provided_apis": [
        {
          "group": "crunchydata.com",
          "kind": "Pgpolicy",
          "plural": "pgpolicies",
          "version": "v1"
        },
        {
          "group": "crunchydata.com",
          "kind": "Pgreplica",
          "plural": "pgreplicas",
          "version": "v1"
        },
        {
          "group": "crunchydata.com",
          "kind": "Pgtask",
          "plural": "pgtasks",
          "version": "v1"
        },
        {
          "group": "crunchydata.com",
          "kind": "Pgcluster",
          "plural": "pgclusters",
          "version": "v1"
        }
      ],
      "provider": "Crunchy Data",
      "related_images": [
        {
          "digest": "sha256:d9f91cb300615bcf39e610abd4741ff6be3d13418db4260f9409395a2a5c1ecb",
          "image": "registry.connect.redhat.com/crunchydata/postgres-operator@sha256:d9f91cb300615bcf39e610abd4741ff6be3d13418db4260f9409395a2a5c1ecb",
          "name": "postgres-operator-d9f91cb300615bcf39e610abd4741ff6be3d13418db4260f9409395a2a5c1ecb-annotation"
        },
        {
          "digest": "sha256:86b5b293c263667dfc5016fc587643261438bd8868cc1c523ccb77a7c4e1b85a",
          "image": "registry.connect.redhat.com/crunchydata/pgo-apiserver@sha256:86b5b293c263667dfc5016fc587643261438bd8868cc1c523ccb77a7c4e1b85a",
          "name": "apiserver"
        },
        {
          "digest": "sha256:d9f91cb300615bcf39e610abd4741ff6be3d13418db4260f9409395a2a5c1ecb",
          "image": "registry.connect.redhat.com/crunchydata/postgres-operator@sha256:d9f91cb300615bcf39e610abd4741ff6be3d13418db4260f9409395a2a5c1ecb",
          "name": "operator"
        },
        {
          "digest": "sha256:481c48699d7ba6aefc7ad27b43e4df54b8f8686f3445cf303a45bd43ceec4244",
          "image": "registry.connect.redhat.com/crunchydata/pgo-scheduler@sha256:481c48699d7ba6aefc7ad27b43e4df54b8f8686f3445cf303a45bd43ceec4244",
          "name": "scheduler"
        },
        {
          "digest": "sha256:bcc9f68cfce901352408e4645f7c6aa5d2157af93952b9124c3a20923e4f1288",
          "image": "registry.connect.redhat.com/crunchydata/pgo-event@sha256:bcc9f68cfce901352408e4645f7c6aa5d2157af93952b9124c3a20923e4f1288",
          "name": "event"
        },
        {
          "digest": "sha256:0016bf993af9ed5fdfdbbd56b1e743d1991ed390a4800cf1a04cdf55cafe119a",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-pgbackrest@sha256:0016bf993af9ed5fdfdbbd56b1e743d1991ed390a4800cf1a04cdf55cafe119a",
          "name": "pgo_backrest"
        },
        {
          "digest": "sha256:2991db135d2158fbb496afaabee221c432e13d11767e90130f4b2b5c2912a6b5",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-pgbackrest-repo@sha256:2991db135d2158fbb496afaabee221c432e13d11767e90130f4b2b5c2912a6b5",
          "name": "pgo_backrest_repo"
        },
        {
          "digest": "sha256:dabd59ff8e97a06e5e823c33805da6f66300c43c8b315cfd7ab0315bd92803e1",
          "image": "registry.connect.redhat.com/crunchydata/pgo-client@sha256:dabd59ff8e97a06e5e823c33805da6f66300c43c8b315cfd7ab0315bd92803e1",
          "name": "pgo_client"
        },
        {
          "digest": "sha256:c44507bfc3286017afef0697133595043256b8b595e1e2e470b6ff7598cff575",
          "image": "registry.connect.redhat.com/crunchydata/pgo-rmdata@sha256:c44507bfc3286017afef0697133595043256b8b595e1e2e470b6ff7598cff575",
          "name": "pgo_rmdata"
        },
        {
          "digest": "sha256:79d658ec93a10951cd06950e753f92332ea07ce191197213e14f4dc6042cc9f1",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres-exporter@sha256:79d658ec93a10951cd06950e753f92332ea07ce191197213e14f4dc6042cc9f1",
          "name": "crunchy_postgres_exporter"
        },
        {
          "digest": "sha256:77e508924d2358af0f3a8f597f7f59c38f7aecda2af9014ab9438d35f503de9c",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-pgadmin4@sha256:77e508924d2358af0f3a8f597f7f59c38f7aecda2af9014ab9438d35f503de9c",
          "name": "crunchy_pgadmin"
        },
        {
          "digest": "sha256:8850ac1be18d01b7dee9bb0f61d453c787961985bc84df3bc8c1595962a00fab",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-pgbadger@sha256:8850ac1be18d01b7dee9bb0f61d453c787961985bc84df3bc8c1595962a00fab",
          "name": "crunchy_pgbadger"
        },
        {
          "digest": "sha256:a339798a1712a3a3bcf303e6df8186460edfb47aca0e2ae4cadf9db2207d72d6",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-pgbouncer@sha256:a339798a1712a3a3bcf303e6df8186460edfb47aca0e2ae4cadf9db2207d72d6",
          "name": "crunchy_pgbouncer"
        },
        {
          "digest": "sha256:ea787e002ce3aee1b8c54127816885b07c1f063515fa5b80ce957676f4e0a21b",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres-ha@sha256:ea787e002ce3aee1b8c54127816885b07c1f063515fa5b80ce957676f4e0a21b",
          "name": "crunchy_postgres_ha"
        },
        {
          "digest": "sha256:f0d2c8df4adb038af6be43089d96af02afd88b82568a2da342d8d97e1fa4ba8f",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres-gis-ha@sha256:f0d2c8df4adb038af6be43089d96af02afd88b82568a2da342d8d97e1fa4ba8f",
          "name": "crunchy_postgres_gis_ha"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.10",
      "version": "4.7.5",
      "version_original": "4.7.5"
    },
    {
      "_id": "629e4c3e5bfb3ed63084b484",
      "alm_examples": [
        {
          "api_version": "postgres-operator.crunchydata.com/v1beta1",
          "kind": "PostgresCluster",
          "metadata": {
            "name": "example"
          },
          "spec": {
            "instances": [
              {
                "dataVolumeClaimSpec": {
                  "accessModes": [
                    "ReadWriteOnce"
                  ],
                  "resources": {
                    "requests": {
                      "storage": "1Gi"
                    }
                  }
                },
                "replicas": 1
              }
            ],
            "postgresVersion": 13
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/crunchydata/postgres-operator-bundle@sha256:ce40b9bd94ed562963fd298acdad256ef5fe30397aa8ee82e88febfab61f96e4",
      "bundle_path_digest": "sha256:ce40b9bd94ed562963fd298acdad256ef5fe30397aa8ee82e88febfab61f96e4",
      "capabilities": [
        "Auto Pilot"
      ],
      "channel_name": "v5",
      "creation_date": "2022-06-06T18:49:34.429000+00:00",
      "csv_description": "[PGO](https://github.com/CrunchyData/postgres-operator), the\n[Postgres Operator](https://github.com/CrunchyData/postgres-operator) from\n[Crunchy Data](https://www.crunchydata.com), gives you a **declarative Postgres** solution that\nautomatically manages your [PostgreSQL](https://www.postgresql.org) clusters.\n\nDesigned for your GitOps workflows, it is [easy to get started](https://access.crunchydata.com/documentation/postgres-operator/v5/quickstart/)\nwith Postgres on Kubernetes with PGO. Within a few moments, you can have a production grade Postgres\ncluster complete with high availability, disaster recovery, and monitoring, all over secure TLS communications.\nEven better, PGO lets you easily customize your Postgres cluster to tailor it to your workload!\n\nWith conveniences like cloning Postgres clusters to using rolling updates to roll out disruptive\nchanges with minimal downtime, PGO is ready to support your Postgres data at every stage of your\nrelease pipeline. Built for resiliency and uptime, PGO will keep your desired Postgres in a desired\nstate so you do not need to worry about it.\n\nPGO is developed with many years of production experience in automating Postgres management on\nKubernetes, providing a seamless cloud native Postgres solution to keep your data always available.\n\n- **PostgreSQL Cluster Provisioning**: [Create, Scale, & Delete PostgreSQL clusters with ease][provisioning],\n  while fully customizing your Pods and PostgreSQL configuration!\n- **High-Availability**: Safe, automated failover backed by a [distributed consensus based high-availability solution][high-availability].\n  Uses [Pod Anti-Affinity][k8s-anti-affinity] to help resiliency; you can configure how aggressive this can be!\n  Failed primaries automatically heal, allowing for faster recovery time. You can even create regularly scheduled\n  backups as well and set your backup retention policy\n- **Disaster Recovery**: [Backups][backups] and [restores][disaster-recovery] leverage the open source [pgBackRest][] utility and\n  [includes support for full, incremental, and differential backups as well as efficient delta restores][backups].\n  Set how long you want your backups retained for. Works great with very large databases!\n- **Monitoring**: [Track the health of your PostgreSQL clusters][monitoring] using the open source [pgMonitor][] library.\n- **Clone**: [Create new clusters from your existing clusters or backups][clone] with efficient data cloning.\n- **TLS**: All connections are over [TLS][tls]. You can also [bring your own TLS infrastructure][tls] if you do not want to use the provided defaults.\n- **Connection Pooling**: Advanced [connection pooling][pool] support using [pgBouncer][].\n- **Affinity and Tolerations**: Have your PostgreSQL clusters deployed to [Kubernetes Nodes][k8s-nodes] of your preference.\n  Set your [pod anti-affinity][k8s-anti-affinity], node affinity, Pod tolerations and more rules to customize your deployment topology!\n- **Full Customizability**: Crunchy PostgreSQL for Kubernetes makes it easy to get your own PostgreSQL-as-a-Service up and running\n  and fully customize your deployments, including:\n    - Choose the resources for your Postgres cluster: [container resources and storage size][resize-cluster]. [Resize at any time][resize-cluster] with minimal disruption.\n    - Use your own container image repository, including support `imagePullSecrets` and private repositories\n    - [Customize your PostgreSQL configuration][customize-cluster]\n\nand much more!\n\n[backups]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/backups/\n[clone]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/disaster-recovery/#clone-a-postgres-cluster\n[customize-cluster]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/customize-cluster/\n[disaster-recovery]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/disaster-recovery/\n[high-availability]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/high-availability/\n[monitoring]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/monitoring/\n[pool]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/connection-pooling/\n[provisioning]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/create-cluster/\n[resize-cluster]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/resize-cluster/\n[tls]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/customize-cluster/#customize-tls\n\n[k8s-anti-affinity]: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#inter-pod-affinity-and-anti-affinity\n[k8s-nodes]: https://kubernetes.io/docs/concepts/architecture/nodes/\n\n[pgBackRest]: https://www.pgbackrest.org\n[pgBouncer]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/connection-pooling/\n[pgMonitor]: https://github.com/CrunchyData/pgmonitor\n\n\n## Post-Installation\n\n### Tutorial\n\nWant to [learn more about the PostgreSQL Operator][tutorial]? Browse through the [tutorial][] to learn more about what you can do!\n\n[tutorial]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial",
      "csv_display_name": "Crunchy Postgres for Kubernetes",
      "csv_metadata_description": "Production Postgres Made Easy",
      "csv_name": "postgresoperator.v5.0.5",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-11-23T08:50:25.178000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.10",
      "organization": "certified-operators",
      "package": "crunchy-postgres-operator",
      "provided_apis": [
        {
          "group": "postgres-operator.crunchydata.com",
          "kind": "PostgresCluster",
          "plural": "postgresclusters",
          "version": "v1beta1"
        }
      ],
      "provider": "Crunchy Data",
      "related_images": [
        {
          "digest": "sha256:779c528947a218c9a7ca1b2eb59987b20f42c93e75d6606c540e6fa32811b685",
          "image": "registry.connect.redhat.com/crunchydata/postgres-operator@sha256:779c528947a218c9a7ca1b2eb59987b20f42c93e75d6606c540e6fa32811b685",
          "name": "postgres-operator-779c528947a218c9a7ca1b2eb59987b20f42c93e75d6606c540e6fa32811b685-annotation"
        },
        {
          "digest": "sha256:779c528947a218c9a7ca1b2eb59987b20f42c93e75d6606c540e6fa32811b685",
          "image": "registry.connect.redhat.com/crunchydata/postgres-operator@sha256:779c528947a218c9a7ca1b2eb59987b20f42c93e75d6606c540e6fa32811b685",
          "name": "operator"
        },
        {
          "digest": "sha256:42807ed2f6b6594e9f023a843ee1de965b11afd62e12d6d5949fa4aba94a72ff",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-pgbackrest@sha256:42807ed2f6b6594e9f023a843ee1de965b11afd62e12d6d5949fa4aba94a72ff",
          "name": "pgbackrest"
        },
        {
          "digest": "sha256:d97c88c2d0804de1ee93fd99b647918b0b0b7c0ff3091254f315c4474e2623cf",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-pgbouncer@sha256:d97c88c2d0804de1ee93fd99b647918b0b0b7c0ff3091254f315c4474e2623cf",
          "name": "pgbouncer"
        },
        {
          "digest": "sha256:5043f230985d89e2bf1d41f6ca5831ce98a2c33b2e7c0521c0ccdbb5bd045230",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres-exporter@sha256:5043f230985d89e2bf1d41f6ca5831ce98a2c33b2e7c0521c0ccdbb5bd045230",
          "name": "pgexporter"
        },
        {
          "digest": "sha256:10f449d290003133c4a37482f575078250b9ab9d0efcb3541291e42140e5b2eb",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres@sha256:10f449d290003133c4a37482f575078250b9ab9d0efcb3541291e42140e5b2eb",
          "name": "postgres_12"
        },
        {
          "digest": "sha256:56a08b3d6c417bee9d670bf4063b05cfee5e4ab250e791cfd36dc10d8bdf1caa",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres@sha256:56a08b3d6c417bee9d670bf4063b05cfee5e4ab250e791cfd36dc10d8bdf1caa",
          "name": "postgres_13"
        },
        {
          "digest": "sha256:397ef823ac62b3d0d0f6b5e5d594bfcdb6b1ca1423a4c7ed6a80847a4bc6cb26",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres@sha256:397ef823ac62b3d0d0f6b5e5d594bfcdb6b1ca1423a4c7ed6a80847a4bc6cb26",
          "name": "postgres_14"
        },
        {
          "digest": "sha256:3ad82d4757699474d647606775ae6eed1527c0deb08690f5486ec993e4339b80",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres-gis@sha256:3ad82d4757699474d647606775ae6eed1527c0deb08690f5486ec993e4339b80",
          "name": "postgres_12_gis_2.5"
        },
        {
          "digest": "sha256:fd29e3f0f1ff6c81bf5821378cee6ac83954dd7b2f9e3ace00c18111d2bcda49",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres-gis@sha256:fd29e3f0f1ff6c81bf5821378cee6ac83954dd7b2f9e3ace00c18111d2bcda49",
          "name": "postgres_12_gis_3.0"
        },
        {
          "digest": "sha256:de806714dbf7ab548b18760f2181be67fe2dadacf5939027fd2ac4b2a94912d9",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres-gis@sha256:de806714dbf7ab548b18760f2181be67fe2dadacf5939027fd2ac4b2a94912d9",
          "name": "postgres_13_gis_3.0"
        },
        {
          "digest": "sha256:8bf4f028e941898a3f1ecfde8caaafab555e8547190df923e80c4cc7d2c13351",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres-gis@sha256:8bf4f028e941898a3f1ecfde8caaafab555e8547190df923e80c4cc7d2c13351",
          "name": "postgres_13_gis_3.1"
        },
        {
          "digest": "sha256:a35500c19df076b44566ef23c54f2adbe4bdbb6e6b9e01ccd0f30a2b679df776",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres-gis@sha256:a35500c19df076b44566ef23c54f2adbe4bdbb6e6b9e01ccd0f30a2b679df776",
          "name": "postgres_14_gis_3.1"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.10",
      "version": "5.0.5",
      "version_original": "5.0.5"
    },
    {
      "_id": "629e4e45665978f21d2b0678",
      "alm_examples": [
        {
          "api_version": "crunchydata.com/v1",
          "kind": "Pgcluster",
          "metadata": {
            "annotations": {
              "current-primary": "hippo"
            },
            "labels": {
              "crunchy-pgha-scope": "hippo",
              "deployment-name": "hippo",
              "name": "hippo",
              "namespace": "pgo",
              "pg-cluster": "hippo",
              "pgo-version": "4.7.5"
            },
            "name": "hippo"
          },
          "spec": {
            "BackrestStorage": {
              "accessmode": "ReadWriteMany",
              "matchLabels": "",
              "name": "",
              "size": "5Gi",
              "storageclass": "",
              "storagetype": "dynamic",
              "supplementalgroups": ""
            },
            "PrimaryStorage": {
              "accessmode": "ReadWriteMany",
              "matchLabels": "",
              "name": "hippo",
              "size": "5Gi",
              "storageclass": "",
              "storagetype": "dynamic",
              "supplementalgroups": ""
            },
            "ReplicaStorage": {
              "accessmode": "ReadWriteMany",
              "matchLabels": "",
              "name": "",
              "size": "5Gi",
              "storageclass": "",
              "storagetype": "dynamic",
              "supplementalgroups": ""
            },
            "ccpimage": "crunchy-postgres-ha",
            "ccpimagetag": "ubi8-13.6-4.7.5",
            "clustername": "hippo",
            "database": "hippo",
            "exporterport": "9187",
            "name": "hippo",
            "namespace": "pgo",
            "pgbadgerport": "10000",
            "podAntiAffinity": {
              "default": "preferred"
            },
            "port": "5432",
            "user": "hippo",
            "userlabels": {
              "pgo-version": "4.7.5"
            }
          }
        },
        {
          "api_version": "crunchydata.com/v1",
          "kind": "Pgreplica",
          "metadata": {
            "name": "example"
          },
          "spec": {}
        },
        {
          "api_version": "crunchydata.com/v1",
          "kind": "Pgpolicy",
          "metadata": {
            "name": "example"
          },
          "spec": {}
        },
        {
          "api_version": "crunchydata.com/v1",
          "kind": "Pgtask",
          "metadata": {
            "name": "example"
          },
          "spec": {}
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/crunchydata/postgres-operator-bundle@sha256:8ae562b0c37aeffd811263713d8577df0ce88dfc57aef83fb081cf957fbe2d1b",
      "bundle_path_digest": "sha256:8ae562b0c37aeffd811263713d8577df0ce88dfc57aef83fb081cf957fbe2d1b",
      "capabilities": [
        "Auto Pilot"
      ],
      "channel_name": "stable",
      "creation_date": "2022-06-06T18:58:13.657000+00:00",
      "csv_description": "Crunchy PostgreSQL for OpenShift lets you run your own production-grade PostgreSQL-as-a-Service on OpenShift!\n\nPowered by the Crunchy [PostgreSQL Operator](https://github.com/CrunchyData/postgres-operator), Crunchy PostgreSQL\nfor OpenShift automates and simplifies deploying and managing open source PostgreSQL clusters on OpenShift by\nproviding the essential features you need to keep your PostgreSQL clusters up and running, including:\n\n- **PostgreSQL Cluster Provisioning**: [Create, Scale, & Delete PostgreSQL clusters with ease][provisioning],\n  while fully customizing your Pods and PostgreSQL configuration!\n- **High-Availability**: Safe, automated failover backed by a [distributed consensus based high-availability solution][high-availability].\n  Uses [Pod Anti-Affinity][k8s-anti-affinity] to help resiliency; you can configure how aggressive this can be!\n  Failed primaries automatically heal, allowing for faster recovery time. You can even create regularly scheduled\n  backups as well and set your backup retention policy\n- **Disaster Recovery**: Backups and restores leverage the open source [pgBackRest][] utility\n  and [includes support for full, incremental, and differential backups as well as efficient delta restores][disaster-recovery].\n  Set how long you want your backups retained for. Works great with very large databases!\n- **Monitoring**: Track the health of your PostgreSQL clusters using the open source [pgMonitor][] library.\n- **Clone**: Create new clusters from your existing clusters or backups with a single [`pgo create cluster --restore-from`][pgo-create-cluster] command.\n- **TLS**: Secure communication between your applications and data servers by [enabling TLS for your PostgreSQL servers][pgo-task-tls], including the ability to enforce that all of your connections to use TLS.\n- **Connection Pooling**: Use [pgBouncer][] for connection pooling\n- **Affinity and Tolerations**: Have your PostgreSQL clusters deployed to [Kubernetes Nodes][k8s-nodes] of your preference with [node affinity][high-availability-node-affinity], or designate which nodes Kubernetes can schedule PostgreSQL instances to with Kubernetes [tolerations][high-availability-tolerations].\n- **Full Customizability**: Crunchy PostgreSQL for OpenShift makes it easy to get your own PostgreSQL-as-a-Service up and running on\n  and lets make further enhancements to customize your deployments, including:\n    - Selecting different storage classes for your primary, replica, and backup storage\n    - Select your own container resources class for each PostgreSQL cluster deployment; differentiate between resources applied for primary and replica clusters!\n    - Use your own container image repository, including support `imagePullSecrets` and private repositories\n    - Bring your own trusted certificate authority (CA) for use with the Operator API server\n    - Override your PostgreSQL configuration for each cluster\n\nand much more!\n\n[disaster-recovery]: https://access.crunchydata.com/documentation/postgres-operator/latest/architecture/disaster-recovery/\n[high-availability]: https://access.crunchydata.com/documentation/postgres-operator/latest/architecture/high-availability/\n[high-availability-node-affinity]: https://access.crunchydata.com/documentation/postgres-operator/latest/architecture/high-availability/#node-affinity\n[high-availability-tolerations]: https://access.crunchydata.com/documentation/postgres-operator/latest/architecture/high-availability/#tolerations\n[pgo-create-cluster]: https://access.crunchydata.com/documentation/postgres-operator/latest/pgo-client/reference/pgo_create_cluster/\n[pgo-task-tls]: https://access.crunchydata.com/documentation/postgres-operator/latest/tutorial/tls/\n[provisioning]: https://access.crunchydata.com/documentation/postgres-operator/latest/architecture/provisioning/\n\n[k8s-anti-affinity]: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#inter-pod-affinity-and-anti-affinity\n[k8s-nodes]: https://kubernetes.io/docs/concepts/architecture/nodes/\n\n[pgBackRest]: https://www.pgbackrest.org\n[pgBouncer]: https://access.crunchydata.com/documentation/postgres-operator/latest/tutorial/pgbouncer/\n[pgMonitor]: https://github.com/CrunchyData/pgmonitor\n\n## Pre-Installation\n\nThere are a few manual steps that the cluster administrator must perform prior to installing the PostgreSQL Operator.\nAt the very least, it must be provided with an initial configuration.\n\nFirst, select a namespace in which to install the PostgreSQL Operator. PostgreSQL clusters will also be deployed here.\nIf it does not exist, create it now.\n\n```\nexport PGO_OPERATOR_NAMESPACE=pgo\noc create namespace \"$PGO_OPERATOR_NAMESPACE\"\n```\n\n### Security\n\nFor the PostgreSQL Operator and PostgreSQL clusters to run in the recommended `restricted` [Security Context Constraint][],\nedit `conf/postgres-operator/pgo.yaml` and set `DisableFSGroup` to `true`.\n\n[Security Context Constraint]: https://docs.openshift.com/container-platform/latest/authentication/managing-security-context-constraints.html\n\n### Secrets (optional)\n\nIf you plan to use AWS S3 to store backups, you can configure your environment to automatically provide your AWS S3 credentials to all newly created PostgreSQL clusters:\n\n```\noc -n \"$PGO_OPERATOR_NAMESPACE\" create secret generic pgo-backrest-repo-config \\\n  --from-literal=aws-s3-key=\"<your-aws-s3-key>\" \\\n  --from-literal=aws-s3-key-secret=\"<your-aws-s3-key-secret>\"\noc -n \"$PGO_OPERATOR_NAMESPACE\" label secret pgo-backrest-repo-config vendor=crunchydata\n```\n\n### Certificates (optional)\n\nThe PostgreSQL Operator has an API that uses TLS to communicate securely with clients. If one is not provided, the API will automatically generated one for you.\n\nIf you have a certificate bundle validated by your organization, you can install it now.\n\n```\noc -n \"$PGO_OPERATOR_NAMESPACE\" create secret tls pgo.tls \\\n  --cert=/path/to/server.crt \\\n  --key=/path/to/server.key\n```\n\nOnce these resources are in place, the PostgreSQL Operator can be installed into the cluster.\n\n## Installation\n\nYou can now go ahead and install the PostgreSQL Operator from OperatorHub.\n\n### Security\n\nFor the PostgreSQL Operator and PostgreSQL clusters to run in the recommended `restricted` [Security Context Constraint][],\nedit the ConfigMap `pgo-config`, find the `pgo.yaml` entry, and set `DisableFSGroup` to `true`.\n\n[Security Context Constraint]: https://docs.openshift.com/container-platform/latest/authentication/managing-security-context-constraints.html\n\nYou will have to scale the `postgres-operator` Deployment down and up for the above change to take effect:\n\n```\noc -n pgo scale --replicas 0 deployment/postgres-operator\noc -n pgo scale --replicas 1 deployment/postgres-operator\n```\n\n## Post-Installation\n\n### Tutorial\n\nFor a guide on how to perform many of the daily functions of the PostgreSQL Operator, we recommend that you read the [Postgres Operator tutorial][pgo-tutorial]\n\n[pgo-tutorial]: https://access.crunchydata.com/documentation/postgres-operator/latest/tutorial/create-cluster/\n\nHowever, the below guide will show you how to create a Postgres cluster from a custom resource or from using the `pgo-client`.\n\n### Create a PostgreSQL Cluster from a Custom Resource\n\nThe fundamental workflow for interfacing with a PostgreSQL Operator Custom\nResource Definition is for creating a PostgreSQL cluster. There are several\nthat a PostgreSQL cluster requires to be deployed, including:\n\n- Secrets\n  - Information for setting up a pgBackRest repository\n  - PostgreSQL superuser bootstrap credentials\n  - PostgreSQL replication user bootstrap credentials\n  - PostgresQL standard user bootstrap credentials\n\nAdditionally, if you want to add some of the other sidecars, you may need to\ncreate additional secrets.\n\nThe good news is that if you do not provide these objects, the PostgreSQL\nOperator will create them for you to get your Postgres cluster up and running!\n\nThe following goes through how to create a PostgreSQL cluster called\n`hippo` by creating a new custom resource.\n\n```\n# this variable is the name of the cluster being created\nexport pgo_cluster_name=hippo\n# this variable is the namespace the cluster is being deployed into\nexport cluster_namespace=pgo\n# this variable is set to the location of your image repository\nexport cluster_image_prefix=registry.developers.crunchydata.com/crunchydata\n\ncat <<-EOF > \"${pgo_cluster_name}-pgcluster.yaml\"\napiVersion: crunchydata.com/v1\nkind: Pgcluster\nmetadata:\n  annotations:\n    current-primary: ${pgo_cluster_name}\n  labels:\n    crunchy-pgha-scope: ${pgo_cluster_name}\n    deployment-name: ${pgo_cluster_name}\n    name: ${pgo_cluster_name}\n    pg-cluster: ${pgo_cluster_name}\n    pgo-version: 4.7.5\n    pgouser: admin\n  name: ${pgo_cluster_name}\n  namespace: ${cluster_namespace}\nspec:\n  BackrestStorage:\n    accessmode: ReadWriteMany\n    matchLabels: \"\"\n    name: \"\"\n    size: 1G\n    storageclass: \"\"\n    storagetype: create\n    supplementalgroups: \"\"\n  PrimaryStorage:\n    accessmode: ReadWriteMany\n    matchLabels: \"\"\n    name: ${pgo_cluster_name}\n    size: 1G\n    storageclass: \"\"\n    storagetype: create\n    supplementalgroups: \"\"\n  ReplicaStorage:\n    accessmode: ReadWriteMany\n    matchLabels: \"\"\n    name: \"\"\n    size: 1G\n    storageclass: \"\"\n    storagetype: create\n    supplementalgroups: \"\"\n  annotations: {}\n  ccpimage: crunchy-postgres-ha\n  ccpimageprefix: ${cluster_image_prefix}\n  ccpimagetag: centos8-13.6-4.7.5\n  clustername: ${pgo_cluster_name}\n  database: ${pgo_cluster_name}\n  exporterport: \"9187\"\n  limits: {}\n  name: ${pgo_cluster_name}\n  pgDataSource:\n    restoreFrom: \"\"\n    restoreOpts: \"\"\n  pgbadgerport: \"10000\"\n  pgoimageprefix: ${cluster_image_prefix}\n  podAntiAffinity:\n    default: preferred\n    pgBackRest: preferred\n    pgBouncer: preferred\n  port: \"5432\"\n  tolerations: []\n  user: hippo\n  userlabels:\n    pgo-version: 4.7.5\nEOF\n\noc apply -f \"${pgo_cluster_name}-pgcluster.yaml\"\n```\n\nAnd that's all! The PostgreSQL Operator will go ahead and create the cluster.\n\nIf you have the PostgreSQL client `psql` installed on your host machine, you can\ntest connection to the PostgreSQL cluster using the following command:\n\n```\n# namespace that the cluster is running in\nexport PGO_OPERATOR_NAMESPACE=pgo\n# name of the cluster\nexport pgo_cluster_name=hippo\n# name of the user whose password we want to get\nexport pgo_cluster_username=hippo\n\n# get the password of the user and set it to a recognized psql environmental variable\nexport PGPASSWORD=$(oc -n \"${PGO_OPERATOR_NAMESPACE}\" get secrets \\\n  \"${pgo_cluster_name}-${pgo_cluster_username}-secret\" -o \"jsonpath={.data['password']}\" | base64 -d)\n\n# set up a port-forward either in a new terminal, or in the same terminal in the background:\noc -n pgo port-forward svc/hippo 5432:5432 &\n\npsql -h localhost -U \"${pgo_cluster_username}\" \"${pgo_cluster_name}\"\n```\n\n### Create a PostgreSQL Cluster the `pgo` Client\n\nOnce the PostgreSQL Operator is installed in your OpenShift cluster, you will need to do a few things\nto use the [PostgreSQL Operator Client][pgo-client].\n\n[pgo-client]: https://access.crunchydata.com/documentation/postgres-operator/latest/pgo-client/\n\nInstall the first set of client credentials and download the `pgo` binary and client certificates.\n\n```\ncurl https://raw.githubusercontent.com/CrunchyData/postgres-operator/v4.7.5/deploy/install-bootstrap-creds.sh > install-bootstrap-creds.sh\ncurl https://raw.githubusercontent.com/CrunchyData/postgres-operator/v4.7.5/installers/kubectl/client-setup.sh > client-setup.sh\n\nchmod +x install-bootstrap-creds.sh client-setup.sh\n\nPGO_CMD=oc ./install-bootstrap-creds.sh\nPGO_CMD=oc ./client-setup.sh\n```\n\nThe client needs to be able to reach the PostgreSQL Operator API from outside the OpenShift cluster.\nCreate an external service or forward a port locally.\n\n```\noc -n \"$PGO_OPERATOR_NAMESPACE\" expose deployment postgres-operator\noc -n \"$PGO_OPERATOR_NAMESPACE\" create route passthrough postgres-operator --service=postgres-operator\n\nexport PGO_APISERVER_URL=\"https://$(oc -n \"$PGO_OPERATOR_NAMESPACE\" get route postgres-operator -o jsonpath=\"{.spec.host}\")\"\n```\n_or_\n```\noc -n \"$PGO_OPERATOR_NAMESPACE\" port-forward deployment/postgres-operator 8443\n\nexport PGO_APISERVER_URL=\"https://127.0.0.1:8443\"\n```\n\nVerify connectivity using the `pgo` command.\n\n```\npgo version\n# pgo client version 4.7.5\n# pgo-apiserver version 4.7.5\n```\n\n\nYou can then create a cluster with the `pgo` client as simply as this:\n\n```\npgo create cluster -n pgo hippo\n```\n\nThe cluster may take a few moments to provision. You can verify that the cluster is up and running by using the `pgo test` command:\n\n```\npgo test cluster -n pgo hippo\n```\n\nIf you have the PostgreSQL client `psql` installed on your host machine, you can\ntest connection to the PostgreSQL cluster using the following command:\n\n```\n# namespace that the cluster is running in\nexport PGO_OPERATOR_NAMESPACE=pgo\n# name of the cluster\nexport pgo_cluster_name=hippo\n# name of the user whose password we want to get\nexport pgo_cluster_username=hippo\n\n# get the password of the user and set it to a recognized psql environmental variable\nexport PGPASSWORD=$(kubectl -n \"${PGO_OPERATOR_NAMESPACE}\" get secrets \\\n  \"${pgo_cluster_name}-${pgo_cluster_username}-secret\" -o \"jsonpath={.data['password']}\" | base64 -d)\n\n# set up a port-forward either in a new terminal, or in the same terminal in the background:\nkubectl -n pgo port-forward svc/hippo 5432:5432 &\n\npsql -h localhost -U \"${pgo_cluster_username}\" \"${pgo_cluster_name}\"\n```",
      "csv_display_name": "Crunchy PostgreSQL for OpenShift",
      "csv_metadata_description": "Enterprise open source PostgreSQL-as-a-Service",
      "csv_name": "postgresoperator.v4.7.5",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": true,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": false,
      "last_update_date": "2022-11-23T07:13:23.411000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.11",
      "organization": "certified-operators",
      "package": "crunchy-postgres-operator",
      "provided_apis": [
        {
          "group": "crunchydata.com",
          "kind": "Pgcluster",
          "version": "v1"
        },
        {
          "group": "crunchydata.com",
          "kind": "Pgpolicy",
          "version": "v1"
        },
        {
          "group": "crunchydata.com",
          "kind": "Pgreplica",
          "version": "v1"
        },
        {
          "group": "crunchydata.com",
          "kind": "Pgtask",
          "version": "v1"
        }
      ],
      "provider": "Crunchy Data",
      "related_images": [
        {
          "digest": "sha256:d9f91cb300615bcf39e610abd4741ff6be3d13418db4260f9409395a2a5c1ecb",
          "image": "registry.connect.redhat.com/crunchydata/postgres-operator@sha256:d9f91cb300615bcf39e610abd4741ff6be3d13418db4260f9409395a2a5c1ecb",
          "name": "postgres-operator-d9f91cb300615bcf39e610abd4741ff6be3d13418db4260f9409395a2a5c1ecb-annotation"
        },
        {
          "digest": "sha256:86b5b293c263667dfc5016fc587643261438bd8868cc1c523ccb77a7c4e1b85a",
          "image": "registry.connect.redhat.com/crunchydata/pgo-apiserver@sha256:86b5b293c263667dfc5016fc587643261438bd8868cc1c523ccb77a7c4e1b85a",
          "name": "apiserver"
        },
        {
          "digest": "sha256:d9f91cb300615bcf39e610abd4741ff6be3d13418db4260f9409395a2a5c1ecb",
          "image": "registry.connect.redhat.com/crunchydata/postgres-operator@sha256:d9f91cb300615bcf39e610abd4741ff6be3d13418db4260f9409395a2a5c1ecb",
          "name": "operator"
        },
        {
          "digest": "sha256:481c48699d7ba6aefc7ad27b43e4df54b8f8686f3445cf303a45bd43ceec4244",
          "image": "registry.connect.redhat.com/crunchydata/pgo-scheduler@sha256:481c48699d7ba6aefc7ad27b43e4df54b8f8686f3445cf303a45bd43ceec4244",
          "name": "scheduler"
        },
        {
          "digest": "sha256:bcc9f68cfce901352408e4645f7c6aa5d2157af93952b9124c3a20923e4f1288",
          "image": "registry.connect.redhat.com/crunchydata/pgo-event@sha256:bcc9f68cfce901352408e4645f7c6aa5d2157af93952b9124c3a20923e4f1288",
          "name": "event"
        },
        {
          "digest": "sha256:0016bf993af9ed5fdfdbbd56b1e743d1991ed390a4800cf1a04cdf55cafe119a",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-pgbackrest@sha256:0016bf993af9ed5fdfdbbd56b1e743d1991ed390a4800cf1a04cdf55cafe119a",
          "name": "pgo_backrest"
        },
        {
          "digest": "sha256:2991db135d2158fbb496afaabee221c432e13d11767e90130f4b2b5c2912a6b5",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-pgbackrest-repo@sha256:2991db135d2158fbb496afaabee221c432e13d11767e90130f4b2b5c2912a6b5",
          "name": "pgo_backrest_repo"
        },
        {
          "digest": "sha256:dabd59ff8e97a06e5e823c33805da6f66300c43c8b315cfd7ab0315bd92803e1",
          "image": "registry.connect.redhat.com/crunchydata/pgo-client@sha256:dabd59ff8e97a06e5e823c33805da6f66300c43c8b315cfd7ab0315bd92803e1",
          "name": "pgo_client"
        },
        {
          "digest": "sha256:c44507bfc3286017afef0697133595043256b8b595e1e2e470b6ff7598cff575",
          "image": "registry.connect.redhat.com/crunchydata/pgo-rmdata@sha256:c44507bfc3286017afef0697133595043256b8b595e1e2e470b6ff7598cff575",
          "name": "pgo_rmdata"
        },
        {
          "digest": "sha256:79d658ec93a10951cd06950e753f92332ea07ce191197213e14f4dc6042cc9f1",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres-exporter@sha256:79d658ec93a10951cd06950e753f92332ea07ce191197213e14f4dc6042cc9f1",
          "name": "crunchy_postgres_exporter"
        },
        {
          "digest": "sha256:77e508924d2358af0f3a8f597f7f59c38f7aecda2af9014ab9438d35f503de9c",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-pgadmin4@sha256:77e508924d2358af0f3a8f597f7f59c38f7aecda2af9014ab9438d35f503de9c",
          "name": "crunchy_pgadmin"
        },
        {
          "digest": "sha256:8850ac1be18d01b7dee9bb0f61d453c787961985bc84df3bc8c1595962a00fab",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-pgbadger@sha256:8850ac1be18d01b7dee9bb0f61d453c787961985bc84df3bc8c1595962a00fab",
          "name": "crunchy_pgbadger"
        },
        {
          "digest": "sha256:a339798a1712a3a3bcf303e6df8186460edfb47aca0e2ae4cadf9db2207d72d6",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-pgbouncer@sha256:a339798a1712a3a3bcf303e6df8186460edfb47aca0e2ae4cadf9db2207d72d6",
          "name": "crunchy_pgbouncer"
        },
        {
          "digest": "sha256:ea787e002ce3aee1b8c54127816885b07c1f063515fa5b80ce957676f4e0a21b",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres-ha@sha256:ea787e002ce3aee1b8c54127816885b07c1f063515fa5b80ce957676f4e0a21b",
          "name": "crunchy_postgres_ha"
        },
        {
          "digest": "sha256:f0d2c8df4adb038af6be43089d96af02afd88b82568a2da342d8d97e1fa4ba8f",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres-gis-ha@sha256:f0d2c8df4adb038af6be43089d96af02afd88b82568a2da342d8d97e1fa4ba8f",
          "name": "crunchy_postgres_gis_ha"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.11",
      "version": "4.7.5",
      "version_original": "4.7.5"
    },
    {
      "_id": "629e4fba665978f21d2b0682",
      "alm_examples": [
        {
          "api_version": "postgres-operator.crunchydata.com/v1beta1",
          "kind": "PostgresCluster",
          "metadata": {
            "name": "example"
          },
          "spec": {
            "instances": [
              {
                "dataVolumeClaimSpec": {
                  "accessModes": [
                    "ReadWriteOnce"
                  ],
                  "resources": {
                    "requests": {
                      "storage": "1Gi"
                    }
                  }
                },
                "replicas": 1
              }
            ],
            "postgresVersion": 13
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/crunchydata/postgres-operator-bundle@sha256:ce40b9bd94ed562963fd298acdad256ef5fe30397aa8ee82e88febfab61f96e4",
      "bundle_path_digest": "sha256:ce40b9bd94ed562963fd298acdad256ef5fe30397aa8ee82e88febfab61f96e4",
      "capabilities": [
        "Auto Pilot"
      ],
      "channel_name": "v5",
      "creation_date": "2022-06-06T19:04:25.999000+00:00",
      "csv_description": "[PGO](https://github.com/CrunchyData/postgres-operator), the\n[Postgres Operator](https://github.com/CrunchyData/postgres-operator) from\n[Crunchy Data](https://www.crunchydata.com), gives you a **declarative Postgres** solution that\nautomatically manages your [PostgreSQL](https://www.postgresql.org) clusters.\n\nDesigned for your GitOps workflows, it is [easy to get started](https://access.crunchydata.com/documentation/postgres-operator/v5/quickstart/)\nwith Postgres on Kubernetes with PGO. Within a few moments, you can have a production grade Postgres\ncluster complete with high availability, disaster recovery, and monitoring, all over secure TLS communications.\nEven better, PGO lets you easily customize your Postgres cluster to tailor it to your workload!\n\nWith conveniences like cloning Postgres clusters to using rolling updates to roll out disruptive\nchanges with minimal downtime, PGO is ready to support your Postgres data at every stage of your\nrelease pipeline. Built for resiliency and uptime, PGO will keep your desired Postgres in a desired\nstate so you do not need to worry about it.\n\nPGO is developed with many years of production experience in automating Postgres management on\nKubernetes, providing a seamless cloud native Postgres solution to keep your data always available.\n\n- **PostgreSQL Cluster Provisioning**: [Create, Scale, & Delete PostgreSQL clusters with ease][provisioning],\n  while fully customizing your Pods and PostgreSQL configuration!\n- **High-Availability**: Safe, automated failover backed by a [distributed consensus based high-availability solution][high-availability].\n  Uses [Pod Anti-Affinity][k8s-anti-affinity] to help resiliency; you can configure how aggressive this can be!\n  Failed primaries automatically heal, allowing for faster recovery time. You can even create regularly scheduled\n  backups as well and set your backup retention policy\n- **Disaster Recovery**: [Backups][backups] and [restores][disaster-recovery] leverage the open source [pgBackRest][] utility and\n  [includes support for full, incremental, and differential backups as well as efficient delta restores][backups].\n  Set how long you want your backups retained for. Works great with very large databases!\n- **Monitoring**: [Track the health of your PostgreSQL clusters][monitoring] using the open source [pgMonitor][] library.\n- **Clone**: [Create new clusters from your existing clusters or backups][clone] with efficient data cloning.\n- **TLS**: All connections are over [TLS][tls]. You can also [bring your own TLS infrastructure][tls] if you do not want to use the provided defaults.\n- **Connection Pooling**: Advanced [connection pooling][pool] support using [pgBouncer][].\n- **Affinity and Tolerations**: Have your PostgreSQL clusters deployed to [Kubernetes Nodes][k8s-nodes] of your preference.\n  Set your [pod anti-affinity][k8s-anti-affinity], node affinity, Pod tolerations and more rules to customize your deployment topology!\n- **Full Customizability**: Crunchy PostgreSQL for Kubernetes makes it easy to get your own PostgreSQL-as-a-Service up and running\n  and fully customize your deployments, including:\n    - Choose the resources for your Postgres cluster: [container resources and storage size][resize-cluster]. [Resize at any time][resize-cluster] with minimal disruption.\n    - Use your own container image repository, including support `imagePullSecrets` and private repositories\n    - [Customize your PostgreSQL configuration][customize-cluster]\n\nand much more!\n\n[backups]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/backups/\n[clone]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/disaster-recovery/#clone-a-postgres-cluster\n[customize-cluster]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/customize-cluster/\n[disaster-recovery]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/disaster-recovery/\n[high-availability]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/high-availability/\n[monitoring]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/monitoring/\n[pool]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/connection-pooling/\n[provisioning]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/create-cluster/\n[resize-cluster]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/resize-cluster/\n[tls]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/customize-cluster/#customize-tls\n\n[k8s-anti-affinity]: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#inter-pod-affinity-and-anti-affinity\n[k8s-nodes]: https://kubernetes.io/docs/concepts/architecture/nodes/\n\n[pgBackRest]: https://www.pgbackrest.org\n[pgBouncer]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/connection-pooling/\n[pgMonitor]: https://github.com/CrunchyData/pgmonitor\n\n\n## Post-Installation\n\n### Tutorial\n\nWant to [learn more about the PostgreSQL Operator][tutorial]? Browse through the [tutorial][] to learn more about what you can do!\n\n[tutorial]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial",
      "csv_display_name": "Crunchy Postgres for Kubernetes",
      "csv_metadata_description": "Production Postgres Made Easy",
      "csv_name": "postgresoperator.v5.0.5",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-11-23T07:13:51.975000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.11",
      "organization": "certified-operators",
      "package": "crunchy-postgres-operator",
      "provided_apis": [
        {
          "group": "postgres-operator.crunchydata.com",
          "kind": "PostgresCluster",
          "version": "v1beta1"
        }
      ],
      "provider": "Crunchy Data",
      "related_images": [
        {
          "digest": "sha256:779c528947a218c9a7ca1b2eb59987b20f42c93e75d6606c540e6fa32811b685",
          "image": "registry.connect.redhat.com/crunchydata/postgres-operator@sha256:779c528947a218c9a7ca1b2eb59987b20f42c93e75d6606c540e6fa32811b685",
          "name": "postgres-operator-779c528947a218c9a7ca1b2eb59987b20f42c93e75d6606c540e6fa32811b685-annotation"
        },
        {
          "digest": "sha256:779c528947a218c9a7ca1b2eb59987b20f42c93e75d6606c540e6fa32811b685",
          "image": "registry.connect.redhat.com/crunchydata/postgres-operator@sha256:779c528947a218c9a7ca1b2eb59987b20f42c93e75d6606c540e6fa32811b685",
          "name": "operator"
        },
        {
          "digest": "sha256:42807ed2f6b6594e9f023a843ee1de965b11afd62e12d6d5949fa4aba94a72ff",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-pgbackrest@sha256:42807ed2f6b6594e9f023a843ee1de965b11afd62e12d6d5949fa4aba94a72ff",
          "name": "pgbackrest"
        },
        {
          "digest": "sha256:d97c88c2d0804de1ee93fd99b647918b0b0b7c0ff3091254f315c4474e2623cf",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-pgbouncer@sha256:d97c88c2d0804de1ee93fd99b647918b0b0b7c0ff3091254f315c4474e2623cf",
          "name": "pgbouncer"
        },
        {
          "digest": "sha256:5043f230985d89e2bf1d41f6ca5831ce98a2c33b2e7c0521c0ccdbb5bd045230",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres-exporter@sha256:5043f230985d89e2bf1d41f6ca5831ce98a2c33b2e7c0521c0ccdbb5bd045230",
          "name": "pgexporter"
        },
        {
          "digest": "sha256:10f449d290003133c4a37482f575078250b9ab9d0efcb3541291e42140e5b2eb",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres@sha256:10f449d290003133c4a37482f575078250b9ab9d0efcb3541291e42140e5b2eb",
          "name": "postgres_12"
        },
        {
          "digest": "sha256:56a08b3d6c417bee9d670bf4063b05cfee5e4ab250e791cfd36dc10d8bdf1caa",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres@sha256:56a08b3d6c417bee9d670bf4063b05cfee5e4ab250e791cfd36dc10d8bdf1caa",
          "name": "postgres_13"
        },
        {
          "digest": "sha256:397ef823ac62b3d0d0f6b5e5d594bfcdb6b1ca1423a4c7ed6a80847a4bc6cb26",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres@sha256:397ef823ac62b3d0d0f6b5e5d594bfcdb6b1ca1423a4c7ed6a80847a4bc6cb26",
          "name": "postgres_14"
        },
        {
          "digest": "sha256:3ad82d4757699474d647606775ae6eed1527c0deb08690f5486ec993e4339b80",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres-gis@sha256:3ad82d4757699474d647606775ae6eed1527c0deb08690f5486ec993e4339b80",
          "name": "postgres_12_gis_2.5"
        },
        {
          "digest": "sha256:fd29e3f0f1ff6c81bf5821378cee6ac83954dd7b2f9e3ace00c18111d2bcda49",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres-gis@sha256:fd29e3f0f1ff6c81bf5821378cee6ac83954dd7b2f9e3ace00c18111d2bcda49",
          "name": "postgres_12_gis_3.0"
        },
        {
          "digest": "sha256:de806714dbf7ab548b18760f2181be67fe2dadacf5939027fd2ac4b2a94912d9",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres-gis@sha256:de806714dbf7ab548b18760f2181be67fe2dadacf5939027fd2ac4b2a94912d9",
          "name": "postgres_13_gis_3.0"
        },
        {
          "digest": "sha256:8bf4f028e941898a3f1ecfde8caaafab555e8547190df923e80c4cc7d2c13351",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres-gis@sha256:8bf4f028e941898a3f1ecfde8caaafab555e8547190df923e80c4cc7d2c13351",
          "name": "postgres_13_gis_3.1"
        },
        {
          "digest": "sha256:a35500c19df076b44566ef23c54f2adbe4bdbb6e6b9e01ccd0f30a2b679df776",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres-gis@sha256:a35500c19df076b44566ef23c54f2adbe4bdbb6e6b9e01ccd0f30a2b679df776",
          "name": "postgres_14_gis_3.1"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.11",
      "version": "5.0.5",
      "version_original": "5.0.5"
    },
    {
      "_id": "62a32228d1667dcf800b9c6a",
      "alm_examples": [
        {
          "api_version": "app.joget.com/v1alpha1",
          "kind": "JogetDX",
          "metadata": {
            "name": "example-joget"
          },
          "spec": {
            "size": 1
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/joget/joget-dx-operator-bundle@sha256:609d7eaba12b325ed9c3d932fc453cd06e2e4545877913a77b0f95ad6e7ea23d",
      "bundle_path_digest": "sha256:609d7eaba12b325ed9c3d932fc453cd06e2e4545877913a77b0f95ad6e7ea23d",
      "capabilities": [
        "Seamless Upgrades"
      ],
      "channel_name": "alpha",
      "creation_date": "2022-06-10T10:51:20.581000+00:00",
      "csv_description": "Joget DX is the next generation open source no-code / low-code application platform that combines the best of Rapid Application Development, Business Process Automation and Workflow Management. \nJoget DX empowers business users, non-coders or coders with a single platform to easily build, deliver, monitor and maintain enterprise applications.\n\nThis operator installs a Joget DX cluster running on JBoss EAP 7.\n\n### Features\n* Build full-fledged apps e.g. CRM, HR, Healthcare, etc\n* Drag and drop forms, lists, UI\n* Add workflow to automate processes\n* Extend via plugins\n* Apps are mobile optimized and cloud ready\n* Download ready-made apps from the Joget Marketplace\n\n### Before You Start\nDeploy a [MySQL](https://docs.openshift.com/online/pro/using_images/db_images/mysql.html) or [MariaDB](https://docs.openshift.com/online/pro/using_images/db_images/mariadb.html) database.\n\n### Post Deployment\nAccess the service URL and complete the one-time [Database Setup](https://dev.joget.org/community/display/DX7/Setting+Up+Database)\n\n### More Information\nMore information about Joget DX on JBoss EAP 7 is available in the [Joget Knowledge Base](https://dev.joget.org/community/display/DX7/Automated+Deployment+on+Red+Hat+OpenShift+with+the+Joget+Operator)\n",
      "csv_display_name": "Joget DX Operator",
      "csv_metadata_description": "No-code/low-code application platform to visually build, run and maintain apps",
      "csv_name": "joget-dx-openshift-operator.v0.0.30",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-11-23T07:12:14.690000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.11",
      "organization": "certified-operators",
      "package": "joget-dx-operator",
      "provided_apis": [
        {
          "group": "app.joget.com",
          "kind": "JogetDX",
          "version": "v1alpha1"
        }
      ],
      "provider": "Joget, Inc",
      "related_images": [
        {
          "digest": "sha256:79856e88008da33fc0323e197819fec39fbd6b6bf2b03a0bf799b517bcf8e749",
          "image": "registry.connect.redhat.com/joget/joget-dx-operator@sha256:79856e88008da33fc0323e197819fec39fbd6b6bf2b03a0bf799b517bcf8e749",
          "name": "joget-dx-operator-79856e88008da33fc0323e197819fec39fbd6b6bf2b03a0bf799b517bcf8e749-annotation"
        },
        {
          "digest": "sha256:79856e88008da33fc0323e197819fec39fbd6b6bf2b03a0bf799b517bcf8e749",
          "image": "registry.connect.redhat.com/joget/joget-dx-operator@sha256:79856e88008da33fc0323e197819fec39fbd6b6bf2b03a0bf799b517bcf8e749",
          "name": "operator"
        },
        {
          "digest": "sha256:0142dd0852a8d57411836c7b6fd61584b604291870a471c53694b7bd888469a4",
          "image": "registry.connect.redhat.com/joget/joget-dx7-eap7@sha256:0142dd0852a8d57411836c7b6fd61584b604291870a471c53694b7bd888469a4",
          "name": "joget"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.11",
      "version": "0.0.30",
      "version_original": "0.0.30"
    },
    {
      "_id": "62a32394e5356b6330e2d16c",
      "alm_examples": [
        {
          "api_version": "app.joget.com/v1alpha1",
          "kind": "JogetDX",
          "metadata": {
            "name": "example-joget"
          },
          "spec": {
            "size": 1
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/joget/joget-dx-operator-bundle@sha256:609d7eaba12b325ed9c3d932fc453cd06e2e4545877913a77b0f95ad6e7ea23d",
      "bundle_path_digest": "sha256:609d7eaba12b325ed9c3d932fc453cd06e2e4545877913a77b0f95ad6e7ea23d",
      "capabilities": [
        "Seamless Upgrades"
      ],
      "channel_name": "alpha",
      "creation_date": "2022-06-10T10:57:24.314000+00:00",
      "csv_description": "Joget DX is the next generation open source no-code / low-code application platform that combines the best of Rapid Application Development, Business Process Automation and Workflow Management. \nJoget DX empowers business users, non-coders or coders with a single platform to easily build, deliver, monitor and maintain enterprise applications.\n\nThis operator installs a Joget DX cluster running on JBoss EAP 7.\n\n### Features\n* Build full-fledged apps e.g. CRM, HR, Healthcare, etc\n* Drag and drop forms, lists, UI\n* Add workflow to automate processes\n* Extend via plugins\n* Apps are mobile optimized and cloud ready\n* Download ready-made apps from the Joget Marketplace\n\n### Before You Start\nDeploy a [MySQL](https://docs.openshift.com/online/pro/using_images/db_images/mysql.html) or [MariaDB](https://docs.openshift.com/online/pro/using_images/db_images/mariadb.html) database.\n\n### Post Deployment\nAccess the service URL and complete the one-time [Database Setup](https://dev.joget.org/community/display/DX7/Setting+Up+Database)\n\n### More Information\nMore information about Joget DX on JBoss EAP 7 is available in the [Joget Knowledge Base](https://dev.joget.org/community/display/DX7/Automated+Deployment+on+Red+Hat+OpenShift+with+the+Joget+Operator)\n",
      "csv_display_name": "Joget DX Operator",
      "csv_metadata_description": "No-code/low-code application platform to visually build, run and maintain apps",
      "csv_name": "joget-dx-openshift-operator.v0.0.30",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-11-23T07:18:24.965000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.9",
      "organization": "certified-operators",
      "package": "joget-dx-operator",
      "provided_apis": [
        {
          "group": "app.joget.com",
          "kind": "JogetDX",
          "plural": "jogetdx",
          "version": "v1alpha1"
        }
      ],
      "provider": "Joget, Inc",
      "related_images": [
        {
          "digest": "sha256:79856e88008da33fc0323e197819fec39fbd6b6bf2b03a0bf799b517bcf8e749",
          "image": "registry.connect.redhat.com/joget/joget-dx-operator@sha256:79856e88008da33fc0323e197819fec39fbd6b6bf2b03a0bf799b517bcf8e749",
          "name": "joget-dx-operator-79856e88008da33fc0323e197819fec39fbd6b6bf2b03a0bf799b517bcf8e749-annotation"
        },
        {
          "digest": "sha256:79856e88008da33fc0323e197819fec39fbd6b6bf2b03a0bf799b517bcf8e749",
          "image": "registry.connect.redhat.com/joget/joget-dx-operator@sha256:79856e88008da33fc0323e197819fec39fbd6b6bf2b03a0bf799b517bcf8e749",
          "name": "operator"
        },
        {
          "digest": "sha256:0142dd0852a8d57411836c7b6fd61584b604291870a471c53694b7bd888469a4",
          "image": "registry.connect.redhat.com/joget/joget-dx7-eap7@sha256:0142dd0852a8d57411836c7b6fd61584b604291870a471c53694b7bd888469a4",
          "name": "joget"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.9",
      "version": "0.0.30",
      "version_original": "0.0.30"
    },
    {
      "_id": "62a3245be5356b6330e2d16f",
      "alm_examples": [
        {
          "api_version": "app.joget.com/v1alpha1",
          "kind": "JogetDX",
          "metadata": {
            "name": "example-joget"
          },
          "spec": {
            "size": 1
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/joget/joget-dx-operator-bundle@sha256:609d7eaba12b325ed9c3d932fc453cd06e2e4545877913a77b0f95ad6e7ea23d",
      "bundle_path_digest": "sha256:609d7eaba12b325ed9c3d932fc453cd06e2e4545877913a77b0f95ad6e7ea23d",
      "capabilities": [
        "Seamless Upgrades"
      ],
      "channel_name": "alpha",
      "creation_date": "2022-06-10T11:00:43.401000+00:00",
      "csv_description": "Joget DX is the next generation open source no-code / low-code application platform that combines the best of Rapid Application Development, Business Process Automation and Workflow Management. \nJoget DX empowers business users, non-coders or coders with a single platform to easily build, deliver, monitor and maintain enterprise applications.\n\nThis operator installs a Joget DX cluster running on JBoss EAP 7.\n\n### Features\n* Build full-fledged apps e.g. CRM, HR, Healthcare, etc\n* Drag and drop forms, lists, UI\n* Add workflow to automate processes\n* Extend via plugins\n* Apps are mobile optimized and cloud ready\n* Download ready-made apps from the Joget Marketplace\n\n### Before You Start\nDeploy a [MySQL](https://docs.openshift.com/online/pro/using_images/db_images/mysql.html) or [MariaDB](https://docs.openshift.com/online/pro/using_images/db_images/mariadb.html) database.\n\n### Post Deployment\nAccess the service URL and complete the one-time [Database Setup](https://dev.joget.org/community/display/DX7/Setting+Up+Database)\n\n### More Information\nMore information about Joget DX on JBoss EAP 7 is available in the [Joget Knowledge Base](https://dev.joget.org/community/display/DX7/Automated+Deployment+on+Red+Hat+OpenShift+with+the+Joget+Operator)\n",
      "csv_display_name": "Joget DX Operator",
      "csv_metadata_description": "No-code/low-code application platform to visually build, run and maintain apps",
      "csv_name": "joget-dx-openshift-operator.v0.0.30",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-11-23T08:35:23.253000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.10",
      "organization": "certified-operators",
      "package": "joget-dx-operator",
      "provided_apis": [
        {
          "group": "app.joget.com",
          "kind": "JogetDX",
          "plural": "jogetdx",
          "version": "v1alpha1"
        }
      ],
      "provider": "Joget, Inc",
      "related_images": [
        {
          "digest": "sha256:79856e88008da33fc0323e197819fec39fbd6b6bf2b03a0bf799b517bcf8e749",
          "image": "registry.connect.redhat.com/joget/joget-dx-operator@sha256:79856e88008da33fc0323e197819fec39fbd6b6bf2b03a0bf799b517bcf8e749",
          "name": "joget-dx-operator-79856e88008da33fc0323e197819fec39fbd6b6bf2b03a0bf799b517bcf8e749-annotation"
        },
        {
          "digest": "sha256:79856e88008da33fc0323e197819fec39fbd6b6bf2b03a0bf799b517bcf8e749",
          "image": "registry.connect.redhat.com/joget/joget-dx-operator@sha256:79856e88008da33fc0323e197819fec39fbd6b6bf2b03a0bf799b517bcf8e749",
          "name": "operator"
        },
        {
          "digest": "sha256:0142dd0852a8d57411836c7b6fd61584b604291870a471c53694b7bd888469a4",
          "image": "registry.connect.redhat.com/joget/joget-dx7-eap7@sha256:0142dd0852a8d57411836c7b6fd61584b604291870a471c53694b7bd888469a4",
          "name": "joget"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.10",
      "version": "0.0.30",
      "version_original": "0.0.30"
    },
    {
      "_id": "62a326299ec0687ab6e7208a",
      "alm_examples": [
        {
          "api_version": "app.joget.com/v1alpha1",
          "kind": "JogetDX",
          "metadata": {
            "name": "example-joget"
          },
          "spec": {
            "size": 1
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/joget/joget-dx-operator-bundle@sha256:609d7eaba12b325ed9c3d932fc453cd06e2e4545877913a77b0f95ad6e7ea23d",
      "bundle_path_digest": "sha256:609d7eaba12b325ed9c3d932fc453cd06e2e4545877913a77b0f95ad6e7ea23d",
      "capabilities": [
        "Seamless Upgrades"
      ],
      "channel_name": "alpha",
      "creation_date": "2022-06-10T11:08:25.015000+00:00",
      "csv_description": "Joget DX is the next generation open source no-code / low-code application platform that combines the best of Rapid Application Development, Business Process Automation and Workflow Management. \nJoget DX empowers business users, non-coders or coders with a single platform to easily build, deliver, monitor and maintain enterprise applications.\n\nThis operator installs a Joget DX cluster running on JBoss EAP 7.\n\n### Features\n* Build full-fledged apps e.g. CRM, HR, Healthcare, etc\n* Drag and drop forms, lists, UI\n* Add workflow to automate processes\n* Extend via plugins\n* Apps are mobile optimized and cloud ready\n* Download ready-made apps from the Joget Marketplace\n\n### Before You Start\nDeploy a [MySQL](https://docs.openshift.com/online/pro/using_images/db_images/mysql.html) or [MariaDB](https://docs.openshift.com/online/pro/using_images/db_images/mariadb.html) database.\n\n### Post Deployment\nAccess the service URL and complete the one-time [Database Setup](https://dev.joget.org/community/display/DX7/Setting+Up+Database)\n\n### More Information\nMore information about Joget DX on JBoss EAP 7 is available in the [Joget Knowledge Base](https://dev.joget.org/community/display/DX7/Automated+Deployment+on+Red+Hat+OpenShift+with+the+Joget+Operator)\n",
      "csv_display_name": "Joget DX Operator",
      "csv_metadata_description": "No-code/low-code application platform to visually build, run and maintain apps",
      "csv_name": "joget-dx-openshift-operator.v0.0.30",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-11-23T07:32:49.082000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.8",
      "organization": "certified-operators",
      "package": "joget-dx-operator",
      "provided_apis": [
        {
          "group": "app.joget.com",
          "kind": "JogetDX",
          "plural": "jogetdx",
          "version": "v1alpha1"
        }
      ],
      "provider": "Joget, Inc",
      "related_images": [
        {
          "digest": "sha256:79856e88008da33fc0323e197819fec39fbd6b6bf2b03a0bf799b517bcf8e749",
          "image": "registry.connect.redhat.com/joget/joget-dx-operator@sha256:79856e88008da33fc0323e197819fec39fbd6b6bf2b03a0bf799b517bcf8e749",
          "name": "joget-dx-operator-79856e88008da33fc0323e197819fec39fbd6b6bf2b03a0bf799b517bcf8e749-annotation"
        },
        {
          "digest": "sha256:79856e88008da33fc0323e197819fec39fbd6b6bf2b03a0bf799b517bcf8e749",
          "image": "registry.connect.redhat.com/joget/joget-dx-operator@sha256:79856e88008da33fc0323e197819fec39fbd6b6bf2b03a0bf799b517bcf8e749",
          "name": "operator"
        },
        {
          "digest": "sha256:0142dd0852a8d57411836c7b6fd61584b604291870a471c53694b7bd888469a4",
          "image": "registry.connect.redhat.com/joget/joget-dx7-eap7@sha256:0142dd0852a8d57411836c7b6fd61584b604291870a471c53694b7bd888469a4",
          "name": "joget"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.8",
      "version": "0.0.30",
      "version_original": "0.0.30"
    },
    {
      "_id": "62a326364ceb377f81ad7d10",
      "alm_examples": [
        {
          "api_version": "app.joget.com/v1alpha1",
          "kind": "JogetDX",
          "metadata": {
            "name": "example-joget"
          },
          "spec": {
            "size": 1
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/joget/joget-dx-operator-bundle@sha256:609d7eaba12b325ed9c3d932fc453cd06e2e4545877913a77b0f95ad6e7ea23d",
      "bundle_path_digest": "sha256:609d7eaba12b325ed9c3d932fc453cd06e2e4545877913a77b0f95ad6e7ea23d",
      "capabilities": [
        "Seamless Upgrades"
      ],
      "channel_name": "alpha",
      "creation_date": "2022-06-10T11:08:38.925000+00:00",
      "csv_description": "Joget DX is the next generation open source no-code / low-code application platform that combines the best of Rapid Application Development, Business Process Automation and Workflow Management. \nJoget DX empowers business users, non-coders or coders with a single platform to easily build, deliver, monitor and maintain enterprise applications.\n\nThis operator installs a Joget DX cluster running on JBoss EAP 7.\n\n### Features\n* Build full-fledged apps e.g. CRM, HR, Healthcare, etc\n* Drag and drop forms, lists, UI\n* Add workflow to automate processes\n* Extend via plugins\n* Apps are mobile optimized and cloud ready\n* Download ready-made apps from the Joget Marketplace\n\n### Before You Start\nDeploy a [MySQL](https://docs.openshift.com/online/pro/using_images/db_images/mysql.html) or [MariaDB](https://docs.openshift.com/online/pro/using_images/db_images/mariadb.html) database.\n\n### Post Deployment\nAccess the service URL and complete the one-time [Database Setup](https://dev.joget.org/community/display/DX7/Setting+Up+Database)\n\n### More Information\nMore information about Joget DX on JBoss EAP 7 is available in the [Joget Knowledge Base](https://dev.joget.org/community/display/DX7/Automated+Deployment+on+Red+Hat+OpenShift+with+the+Joget+Operator)\n",
      "csv_display_name": "Joget DX Operator",
      "csv_metadata_description": "No-code/low-code application platform to visually build, run and maintain apps",
      "csv_name": "joget-dx-openshift-operator.v0.0.30",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-11-23T07:25:09.954000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.7",
      "organization": "certified-operators",
      "package": "joget-dx-operator",
      "provided_apis": [
        {
          "group": "app.joget.com",
          "kind": "JogetDX",
          "plural": "jogetdx",
          "version": "v1alpha1"
        }
      ],
      "provider": "Joget, Inc",
      "related_images": [
        {
          "digest": "sha256:79856e88008da33fc0323e197819fec39fbd6b6bf2b03a0bf799b517bcf8e749",
          "image": "registry.connect.redhat.com/joget/joget-dx-operator@sha256:79856e88008da33fc0323e197819fec39fbd6b6bf2b03a0bf799b517bcf8e749",
          "name": "joget-dx-operator-79856e88008da33fc0323e197819fec39fbd6b6bf2b03a0bf799b517bcf8e749-annotation"
        },
        {
          "digest": "sha256:79856e88008da33fc0323e197819fec39fbd6b6bf2b03a0bf799b517bcf8e749",
          "image": "registry.connect.redhat.com/joget/joget-dx-operator@sha256:79856e88008da33fc0323e197819fec39fbd6b6bf2b03a0bf799b517bcf8e749",
          "name": "operator"
        },
        {
          "digest": "sha256:0142dd0852a8d57411836c7b6fd61584b604291870a471c53694b7bd888469a4",
          "image": "registry.connect.redhat.com/joget/joget-dx7-eap7@sha256:0142dd0852a8d57411836c7b6fd61584b604291870a471c53694b7bd888469a4",
          "name": "joget"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.7",
      "version": "0.0.30",
      "version_original": "0.0.30"
    },
    {
      "_id": "62aaf8acdd92c1f1f129f3a1",
      "alm_examples": [
        {
          "api_version": "mellanox.com/v1alpha1",
          "kind": "HostDeviceNetwork",
          "metadata": {
            "name": "example-hostdevice-network"
          },
          "spec": {
            "ipam": "{\n  \"type\": \"whereabouts\",\n  \"range\": \"192.168.3.225/28\",\n  \"exclude\": [\n   \"192.168.3.229/30\",\n   \"192.168.3.236/32\"\n  ]\n}\n",
            "networkNamespace": "default",
            "resourceName": "hostdev"
          }
        },
        {
          "api_version": "mellanox.com/v1alpha1",
          "kind": "MacvlanNetwork",
          "metadata": {
            "name": "example-macvlannetwork"
          },
          "spec": {
            "ipam": "{\n  \"type\": \"whereabouts\",\n  \"range\": \"192.168.2.225/24\",\n  \"exclude\": [\n   \"192.168.2.229/30\",\n   \"192.168.2.236/32\"\n  ]\n}\n",
            "master": "ens2f0",
            "mode": "bridge",
            "mtu": 1500,
            "networkNamespace": "default"
          }
        },
        {
          "api_version": "mellanox.com/v1alpha1",
          "kind": "NicClusterPolicy",
          "metadata": {
            "name": "nic-cluster-policy"
          },
          "spec": {
            "ofedDriver": {
              "image": "mofed",
              "livenessProbe": {
                "initialDelaySeconds": 30,
                "periodSeconds": 30
              },
              "readinessProbe": {
                "initialDelaySeconds": 10,
                "periodSeconds": 30
              },
              "repository": "mellanox",
              "startupProbe": {
                "initialDelaySeconds": 10,
                "periodSeconds": 20
              },
              "version": "5.5-1.0.3.2"
            },
            "rdmaSharedDevicePlugin": {
              "config": "{\n  \"configList\": [\n    {\n      \"resourceName\": \"rdma_shared_device_a\",\n      \"rdmaHcaMax\": 1000,\n      \"selectors\": {\n        \"ifNames\": [\"ens2f0\"]\n      }\n    }\n  ]\n}\n",
              "image": "k8s-rdma-shared-dev-plugin",
              "repository": "nvcr.io/nvidia/cloud-native",
              "version": "v1.2.1-ubi"
            },
            "sriovDevicePlugin": {
              "config": "{\n  \"resourceList\": [\n      {\n          \"resourcePrefix\": \"nvidia.com\",\n          \"resourceName\": \"hostdev\",\n          \"selectors\": {\n              \"vendors\": [\"15b3\"],\n              \"isRdma\": true\n          }\n      }\n  ]\n}\n",
              "image": "sriov-device-plugin",
              "repository": "docker.io/nfvpe",
              "version": "v3.3"
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/nvidia-network-operator/nvidia-network-operator@sha256:5d0554c40fd2ad6d901b0a6384f48bacb478ba22345706448fd66258b4439c3d",
      "bundle_path_digest": "sha256:5d0554c40fd2ad6d901b0a6384f48bacb478ba22345706448fd66258b4439c3d",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "v1.2.1",
      "creation_date": "2022-06-16T09:32:28.908000+00:00",
      "csv_description": "## NVIDIA Network Operator\nThe NVIDIA Network Operator simplifies the provisioning and management of NVIDIA networking resources  in a Kubernetes cluster. The operator automatically installs the required host networking software - bringing together all the needed components to provide high-speed network connectivity. These components include the NVIDIA networking driver, Kubernetes device plugin, CNI plugins, IP address management (IPAM) plugin and others.\nThe NVIDIA Network Operator works in conjunction with the NVIDIA GPU Operator to deliver high-throughput, low-latency networking for scale-out, GPU computing clusters.\n\nThe Network Operator uses Node Feature Discovery (NFD) labels in order to properly schedule resources.\nNodes can be labelled manually or using the NFD Operator. An example of `NodeFeatureDiscovery`\nconfiguration is available in the documentation.\nThe following NFD labels are used:\n`feature.node.kubernetes.io/pci-15b3.present` for nodes containing NVIDIA Networking hardware.\n`feature.node.kubernetes.io/pci-10de.present` for nodes containing NVIDIA GPU hardware.\n\nThe Network Operator is open-source. For more information on contributions and release artifacts, see the [GitHub repo](https://github.com/Mellanox/network-operator/)\n",
      "csv_display_name": "NVIDIA Network Operator",
      "csv_metadata_description": "Deploy and manage NVIDIA networking resources in Kubernetes",
      "csv_name": "nvidia-network-operator.v1.2.0",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": false,
      "last_update_date": "2022-11-23T08:45:18.686000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.10",
      "organization": "certified-operators",
      "package": "nvidia-network-operator",
      "provided_apis": [
        {
          "group": "mellanox.com",
          "kind": "HostDeviceNetwork",
          "plural": "hostdevicenetworks",
          "version": "v1alpha1"
        },
        {
          "group": "mellanox.com",
          "kind": "MacvlanNetwork",
          "plural": "macvlannetworks",
          "version": "v1alpha1"
        },
        {
          "group": "mellanox.com",
          "kind": "NicClusterPolicy",
          "plural": "nicclusterpolicies",
          "version": "v1alpha1"
        }
      ],
      "provider": "NVIDIA",
      "related_images": [
        {
          "digest": "sha256:0a5108443c64fc013984be500e7db3d89c04418446f0bbc3241ed6e1c449b773",
          "image": "nvcr.io/nvidia/mellanox/mofed-5.6-1.0.3.3@sha256:0a5108443c64fc013984be500e7db3d89c04418446f0bbc3241ed6e1c449b773",
          "name": "mofed"
        },
        {
          "digest": "sha256:16a53286fecdb1e587d3c4c042078974674c3e86c9e98d7dae282f6eb4ee2d8c",
          "image": "ghcr.io/k8snetworkplumbingwg/sriov-network-device-plugin@sha256:16a53286fecdb1e587d3c4c042078974674c3e86c9e98d7dae282f6eb4ee2d8c",
          "name": "sriov-network-device-plugin"
        },
        {
          "digest": "sha256:941ad9ff5013e9e7ad5abeb0ea9f79d45379cfae88a628d923f87d2259bdd132",
          "image": "nvcr.io/nvidia/cloud-native/k8s-rdma-shared-dev-plugin@sha256:941ad9ff5013e9e7ad5abeb0ea9f79d45379cfae88a628d923f87d2259bdd132",
          "name": "rdma-shared-device-plugin"
        },
        {
          "digest": "sha256:db06cc4c084dd0253134f156dddaaf53ef1c3fb3cc809e5d81711baa4029ea4c",
          "image": "gcr.io/kubebuilder/kube-rbac-proxy@sha256:db06cc4c084dd0253134f156dddaaf53ef1c3fb3cc809e5d81711baa4029ea4c",
          "name": "kube-rbac-proxy"
        },
        {
          "digest": "sha256:90bd3c4d2620a7f1005afc7c1d101d31a64d2239b0bf33a701c4426a54e4e381",
          "image": "nvcr.io/nvidia/cloud-native/network-operator@sha256:90bd3c4d2620a7f1005afc7c1d101d31a64d2239b0bf33a701c4426a54e4e381",
          "name": "manager"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.10",
      "version": "1.2.0",
      "version_original": "1.2.0"
    },
    {
      "_id": "62aaf8ad240dc134424bc64a",
      "alm_examples": [
        {
          "api_version": "mellanox.com/v1alpha1",
          "kind": "HostDeviceNetwork",
          "metadata": {
            "name": "example-hostdevice-network"
          },
          "spec": {
            "ipam": "{\n  \"type\": \"whereabouts\",\n  \"range\": \"192.168.3.225/28\",\n  \"exclude\": [\n   \"192.168.3.229/30\",\n   \"192.168.3.236/32\"\n  ]\n}\n",
            "networkNamespace": "default",
            "resourceName": "hostdev"
          }
        },
        {
          "api_version": "mellanox.com/v1alpha1",
          "kind": "MacvlanNetwork",
          "metadata": {
            "name": "example-macvlannetwork"
          },
          "spec": {
            "ipam": "{\n  \"type\": \"whereabouts\",\n  \"range\": \"192.168.2.225/24\",\n  \"exclude\": [\n   \"192.168.2.229/30\",\n   \"192.168.2.236/32\"\n  ]\n}\n",
            "master": "ens2f0",
            "mode": "bridge",
            "mtu": 1500,
            "networkNamespace": "default"
          }
        },
        {
          "api_version": "mellanox.com/v1alpha1",
          "kind": "NicClusterPolicy",
          "metadata": {
            "name": "nic-cluster-policy"
          },
          "spec": {
            "ofedDriver": {
              "image": "mofed",
              "livenessProbe": {
                "initialDelaySeconds": 30,
                "periodSeconds": 30
              },
              "readinessProbe": {
                "initialDelaySeconds": 10,
                "periodSeconds": 30
              },
              "repository": "nvcr.io/nvidia/mellanox",
              "startupProbe": {
                "initialDelaySeconds": 10,
                "periodSeconds": 20
              },
              "version": "5.6-1.0.3.3"
            },
            "rdmaSharedDevicePlugin": {
              "config": "{\n  \"configList\": [\n    {\n      \"resourceName\": \"rdma_shared_device_a\",\n      \"rdmaHcaMax\": 1000,\n      \"selectors\": {\n        \"ifNames\": [\"ens2f0\"]\n      }\n    }\n  ]\n}\n",
              "image": "k8s-rdma-shared-dev-plugin",
              "repository": "nvcr.io/nvidia/cloud-native",
              "version": "v1.3.2"
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/nvidia-network-operator/nvidia-network-operator@sha256:92722308ad575e31059d00ea02213dbaf1a653f8d571aeef755e1a6b6ff60fc2",
      "bundle_path_digest": "sha256:92722308ad575e31059d00ea02213dbaf1a653f8d571aeef755e1a6b6ff60fc2",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "v1.2.1",
      "creation_date": "2022-06-16T09:32:29.284000+00:00",
      "csv_description": "## NVIDIA Network Operator\nThe NVIDIA Network Operator simplifies the provisioning and management of NVIDIA networking resources  in a Kubernetes cluster. The operator automatically installs the required host networking software - bringing together all the needed components to provide high-speed network connectivity. These components include the NVIDIA networking driver, Kubernetes device plugin, CNI plugins, IP address management (IPAM) plugin and others.\nThe NVIDIA Network Operator works in conjunction with the NVIDIA GPU Operator to deliver high-throughput, low-latency networking for scale-out, GPU computing clusters.\n\nThe Network Operator uses Node Feature Discovery (NFD) labels in order to properly schedule resources.\nNodes can be labelled manually or using the NFD Operator. An example of `NodeFeatureDiscovery`\nconfiguration is available in the documentation.\nThe following NFD labels are used:\n`feature.node.kubernetes.io/pci-15b3.present` for nodes containing NVIDIA Networking hardware.\n`feature.node.kubernetes.io/pci-10de.present` for nodes containing NVIDIA GPU hardware.\n\nThe Network Operator is open-source. For more information on contributions and release artifacts, see the [GitHub repo](https://github.com/Mellanox/network-operator/)\n",
      "csv_display_name": "NVIDIA Network Operator",
      "csv_metadata_description": "Deploy and manage NVIDIA networking resources in Kubernetes",
      "csv_name": "nvidia-network-operator.v1.2.1",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": false,
      "last_update_date": "2022-11-23T08:45:36.201000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.10",
      "organization": "certified-operators",
      "package": "nvidia-network-operator",
      "provided_apis": [
        {
          "group": "mellanox.com",
          "kind": "HostDeviceNetwork",
          "plural": "hostdevicenetworks",
          "version": "v1alpha1"
        },
        {
          "group": "mellanox.com",
          "kind": "MacvlanNetwork",
          "plural": "macvlannetworks",
          "version": "v1alpha1"
        },
        {
          "group": "mellanox.com",
          "kind": "NicClusterPolicy",
          "plural": "nicclusterpolicies",
          "version": "v1alpha1"
        }
      ],
      "provider": "NVIDIA",
      "related_images": [
        {
          "digest": "sha256:0a5108443c64fc013984be500e7db3d89c04418446f0bbc3241ed6e1c449b773",
          "image": "nvcr.io/nvidia/mellanox/mofed-5.6-1.0.3.3@sha256:0a5108443c64fc013984be500e7db3d89c04418446f0bbc3241ed6e1c449b773",
          "name": "mofed"
        },
        {
          "digest": "sha256:16a53286fecdb1e587d3c4c042078974674c3e86c9e98d7dae282f6eb4ee2d8c",
          "image": "ghcr.io/k8snetworkplumbingwg/sriov-network-device-plugin@sha256:16a53286fecdb1e587d3c4c042078974674c3e86c9e98d7dae282f6eb4ee2d8c",
          "name": "sriov-network-device-plugin"
        },
        {
          "digest": "sha256:941ad9ff5013e9e7ad5abeb0ea9f79d45379cfae88a628d923f87d2259bdd132",
          "image": "nvcr.io/nvidia/cloud-native/k8s-rdma-shared-dev-plugin@sha256:941ad9ff5013e9e7ad5abeb0ea9f79d45379cfae88a628d923f87d2259bdd132",
          "name": "rdma-shared-device-plugin"
        },
        {
          "digest": "sha256:db06cc4c084dd0253134f156dddaaf53ef1c3fb3cc809e5d81711baa4029ea4c",
          "image": "gcr.io/kubebuilder/kube-rbac-proxy@sha256:db06cc4c084dd0253134f156dddaaf53ef1c3fb3cc809e5d81711baa4029ea4c",
          "name": "kube-rbac-proxy"
        },
        {
          "digest": "sha256:90bd3c4d2620a7f1005afc7c1d101d31a64d2239b0bf33a701c4426a54e4e381",
          "image": "nvcr.io/nvidia/cloud-native/network-operator@sha256:90bd3c4d2620a7f1005afc7c1d101d31a64d2239b0bf33a701c4426a54e4e381",
          "name": "manager"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.10",
      "version": "1.2.1",
      "version_original": "1.2.1"
    },
    {
      "_id": "62abd817240dc134424c58c4",
      "alm_examples": [
        {
          "api_version": "charts.helm.k8s.io/v1",
          "kind": "Kubeturbo",
          "metadata": {
            "name": "kubeturbo-release"
          },
          "spec": {
            "serverMeta": {
              "turboServer": "https://Turbo_server_URL"
            },
            "targetConfig": {
              "targetName": "Cluster_Name"
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/turbonomic/kubeturbo-operator-bundle@sha256:23f06fa446df0253aeacaf9f063d959c91e56f8a210ac493dc47bc238bb84bfe",
      "bundle_path_digest": "sha256:23f06fa446df0253aeacaf9f063d959c91e56f8a210ac493dc47bc238bb84bfe",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "stable",
      "creation_date": "2022-06-17T01:25:43.789000+00:00",
      "csv_description": "### Application Resource Management for Kubernetes\nTurbonomic AI-powered Application Resource Management simultaneously optimizes performance, compliance, and cost in real time.\nSoftware manages the complete application stack, automatically. Applications are continually resourced to perform while satisfying business constraints.\n\nTurbonomic makes workloads smart \u2014 enabling them to self-manage and determines the specific actions that will drive continuous health:\n\n* Continuous placement for Pods (rescheduling)\n* Continuous scaling for applications and  the underlying cluster.\n\nIt assures application performance by giving workloads the resources they need when they need them.\n\n### How does it work?\nTurbonomic uses a container \u2014 KubeTurbo \u2014 that runs in your Kubernetes or Red Hat OpenShift cluster to discover and monitor your environment.\nKubeTurbo runs together with the default scheduler and sends this data back to the Turbonomic analytics engine.\nTurbonomic determines the right actions that drive continuous health, including continuous placement for Pods and continuous scaling for applications and the underlying cluster.",
      "csv_display_name": "Kubeturbo Operator",
      "csv_metadata_description": "Turbonomic Workload Automation for Multicloud simultaneously optimizes performance, compliance, and cost in real-time. Workloads are precisely resourced, automatically, to perform while satisfying business constraints.",
      "csv_name": "kubeturbo-operator.v8.5.5",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-11-23T08:38:21.688000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.10",
      "organization": "certified-operators",
      "package": "kubeturbo-certified",
      "provided_apis": [
        {
          "group": "charts.helm.k8s.io",
          "kind": "Kubeturbo",
          "plural": "kubeturbos",
          "version": "v1alpha1"
        },
        {
          "group": "charts.helm.k8s.io",
          "kind": "Kubeturbo",
          "plural": "kubeturbos",
          "version": "v1"
        }
      ],
      "provider": "Turbonomic, Inc.",
      "related_images": [
        {
          "digest": "sha256:e5cdbdeeaf639afe9782fceb9bf32c5e06e2b6ed4ded41e06abe8f49bda7ac47",
          "image": "registry.connect.redhat.com/turbonomic/kubeturbo-operator@sha256:e5cdbdeeaf639afe9782fceb9bf32c5e06e2b6ed4ded41e06abe8f49bda7ac47",
          "name": "kubeturbo-operator-e5cdbdeeaf639afe9782fceb9bf32c5e06e2b6ed4ded41e06abe8f49bda7ac47-annotation"
        },
        {
          "digest": "sha256:e5cdbdeeaf639afe9782fceb9bf32c5e06e2b6ed4ded41e06abe8f49bda7ac47",
          "image": "registry.connect.redhat.com/turbonomic/kubeturbo-operator@sha256:e5cdbdeeaf639afe9782fceb9bf32c5e06e2b6ed4ded41e06abe8f49bda7ac47",
          "name": "kubeturbo-operator"
        },
        {
          "digest": "sha256:e08debe4fcac4b377e86ed57c9587057e8f99d37c5bd8acfe94dfa17605d0759",
          "image": "registry.connect.redhat.com/turbonomic/kubeturbo@sha256:e08debe4fcac4b377e86ed57c9587057e8f99d37c5bd8acfe94dfa17605d0759",
          "name": "kubeturbo"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.10",
      "version": "8.5.5",
      "version_original": "8.5.5"
    },
    {
      "_id": "62abd9b8be1fd3a96e057ce3",
      "alm_examples": [
        {
          "api_version": "charts.helm.k8s.io/v1",
          "kind": "Kubeturbo",
          "metadata": {
            "name": "kubeturbo-release"
          },
          "spec": {
            "serverMeta": {
              "turboServer": "https://Turbo_server_URL"
            },
            "targetConfig": {
              "targetName": "Cluster_Name"
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/turbonomic/kubeturbo-operator-bundle@sha256:23f06fa446df0253aeacaf9f063d959c91e56f8a210ac493dc47bc238bb84bfe",
      "bundle_path_digest": "sha256:23f06fa446df0253aeacaf9f063d959c91e56f8a210ac493dc47bc238bb84bfe",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "stable",
      "creation_date": "2022-06-17T01:32:40.131000+00:00",
      "csv_description": "### Application Resource Management for Kubernetes\nTurbonomic AI-powered Application Resource Management simultaneously optimizes performance, compliance, and cost in real time.\nSoftware manages the complete application stack, automatically. Applications are continually resourced to perform while satisfying business constraints.\n\nTurbonomic makes workloads smart \u2014 enabling them to self-manage and determines the specific actions that will drive continuous health:\n\n* Continuous placement for Pods (rescheduling)\n* Continuous scaling for applications and  the underlying cluster.\n\nIt assures application performance by giving workloads the resources they need when they need them.\n\n### How does it work?\nTurbonomic uses a container \u2014 KubeTurbo \u2014 that runs in your Kubernetes or Red Hat OpenShift cluster to discover and monitor your environment.\nKubeTurbo runs together with the default scheduler and sends this data back to the Turbonomic analytics engine.\nTurbonomic determines the right actions that drive continuous health, including continuous placement for Pods and continuous scaling for applications and the underlying cluster.",
      "csv_display_name": "Kubeturbo Operator",
      "csv_metadata_description": "Turbonomic Workload Automation for Multicloud simultaneously optimizes performance, compliance, and cost in real-time. Workloads are precisely resourced, automatically, to perform while satisfying business constraints.",
      "csv_name": "kubeturbo-operator.v8.5.5",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-11-23T07:20:42.188000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.9",
      "organization": "certified-operators",
      "package": "kubeturbo-certified",
      "provided_apis": [
        {
          "group": "charts.helm.k8s.io",
          "kind": "Kubeturbo",
          "plural": "kubeturbos",
          "version": "v1"
        },
        {
          "group": "charts.helm.k8s.io",
          "kind": "Kubeturbo",
          "plural": "kubeturbos",
          "version": "v1alpha1"
        }
      ],
      "provider": "Turbonomic, Inc.",
      "related_images": [
        {
          "digest": "sha256:e5cdbdeeaf639afe9782fceb9bf32c5e06e2b6ed4ded41e06abe8f49bda7ac47",
          "image": "registry.connect.redhat.com/turbonomic/kubeturbo-operator@sha256:e5cdbdeeaf639afe9782fceb9bf32c5e06e2b6ed4ded41e06abe8f49bda7ac47",
          "name": "kubeturbo-operator-e5cdbdeeaf639afe9782fceb9bf32c5e06e2b6ed4ded41e06abe8f49bda7ac47-annotation"
        },
        {
          "digest": "sha256:e5cdbdeeaf639afe9782fceb9bf32c5e06e2b6ed4ded41e06abe8f49bda7ac47",
          "image": "registry.connect.redhat.com/turbonomic/kubeturbo-operator@sha256:e5cdbdeeaf639afe9782fceb9bf32c5e06e2b6ed4ded41e06abe8f49bda7ac47",
          "name": "kubeturbo-operator"
        },
        {
          "digest": "sha256:e08debe4fcac4b377e86ed57c9587057e8f99d37c5bd8acfe94dfa17605d0759",
          "image": "registry.connect.redhat.com/turbonomic/kubeturbo@sha256:e08debe4fcac4b377e86ed57c9587057e8f99d37c5bd8acfe94dfa17605d0759",
          "name": "kubeturbo"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.9",
      "version": "8.5.5",
      "version_original": "8.5.5"
    },
    {
      "_id": "62abdb9b240dc134424c609e",
      "alm_examples": [
        {
          "api_version": "charts.helm.k8s.io/v1",
          "kind": "Kubeturbo",
          "metadata": {
            "name": "kubeturbo-release"
          },
          "spec": {
            "serverMeta": {
              "turboServer": "https://Turbo_server_URL"
            },
            "targetConfig": {
              "targetName": "Cluster_Name"
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/turbonomic/kubeturbo-operator-bundle@sha256:23f06fa446df0253aeacaf9f063d959c91e56f8a210ac493dc47bc238bb84bfe",
      "bundle_path_digest": "sha256:23f06fa446df0253aeacaf9f063d959c91e56f8a210ac493dc47bc238bb84bfe",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "stable",
      "creation_date": "2022-06-17T01:40:43.413000+00:00",
      "csv_description": "### Application Resource Management for Kubernetes\nTurbonomic AI-powered Application Resource Management simultaneously optimizes performance, compliance, and cost in real time.\nSoftware manages the complete application stack, automatically. Applications are continually resourced to perform while satisfying business constraints.\n\nTurbonomic makes workloads smart \u2014 enabling them to self-manage and determines the specific actions that will drive continuous health:\n\n* Continuous placement for Pods (rescheduling)\n* Continuous scaling for applications and  the underlying cluster.\n\nIt assures application performance by giving workloads the resources they need when they need them.\n\n### How does it work?\nTurbonomic uses a container \u2014 KubeTurbo \u2014 that runs in your Kubernetes or Red Hat OpenShift cluster to discover and monitor your environment.\nKubeTurbo runs together with the default scheduler and sends this data back to the Turbonomic analytics engine.\nTurbonomic determines the right actions that drive continuous health, including continuous placement for Pods and continuous scaling for applications and the underlying cluster.",
      "csv_display_name": "Kubeturbo Operator",
      "csv_metadata_description": "Turbonomic Workload Automation for Multicloud simultaneously optimizes performance, compliance, and cost in real-time. Workloads are precisely resourced, automatically, to perform while satisfying business constraints.",
      "csv_name": "kubeturbo-operator.v8.5.5",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-11-23T06:59:13.169000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.11",
      "organization": "certified-operators",
      "package": "kubeturbo-certified",
      "provided_apis": [
        {
          "group": "charts.helm.k8s.io",
          "kind": "Kubeturbo",
          "version": "v1"
        },
        {
          "group": "charts.helm.k8s.io",
          "kind": "Kubeturbo",
          "version": "v1alpha1"
        }
      ],
      "provider": "Turbonomic, Inc.",
      "related_images": [
        {
          "digest": "sha256:e5cdbdeeaf639afe9782fceb9bf32c5e06e2b6ed4ded41e06abe8f49bda7ac47",
          "image": "registry.connect.redhat.com/turbonomic/kubeturbo-operator@sha256:e5cdbdeeaf639afe9782fceb9bf32c5e06e2b6ed4ded41e06abe8f49bda7ac47",
          "name": "kubeturbo-operator-e5cdbdeeaf639afe9782fceb9bf32c5e06e2b6ed4ded41e06abe8f49bda7ac47-annotation"
        },
        {
          "digest": "sha256:e5cdbdeeaf639afe9782fceb9bf32c5e06e2b6ed4ded41e06abe8f49bda7ac47",
          "image": "registry.connect.redhat.com/turbonomic/kubeturbo-operator@sha256:e5cdbdeeaf639afe9782fceb9bf32c5e06e2b6ed4ded41e06abe8f49bda7ac47",
          "name": "kubeturbo-operator"
        },
        {
          "digest": "sha256:e08debe4fcac4b377e86ed57c9587057e8f99d37c5bd8acfe94dfa17605d0759",
          "image": "registry.connect.redhat.com/turbonomic/kubeturbo@sha256:e08debe4fcac4b377e86ed57c9587057e8f99d37c5bd8acfe94dfa17605d0759",
          "name": "kubeturbo"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.11",
      "version": "8.5.5",
      "version_original": "8.5.5"
    },
    {
      "_id": "62abdde7dd92c1f1f12a9435",
      "alm_examples": [
        {
          "api_version": "charts.helm.k8s.io/v1",
          "kind": "Kubeturbo",
          "metadata": {
            "name": "kubeturbo-release"
          },
          "spec": {
            "serverMeta": {
              "turboServer": "https://Turbo_server_URL"
            },
            "targetConfig": {
              "targetName": "Cluster_Name"
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/turbonomic/kubeturbo-operator-bundle@sha256:23f06fa446df0253aeacaf9f063d959c91e56f8a210ac493dc47bc238bb84bfe",
      "bundle_path_digest": "sha256:23f06fa446df0253aeacaf9f063d959c91e56f8a210ac493dc47bc238bb84bfe",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "stable",
      "creation_date": "2022-06-17T01:50:31.129000+00:00",
      "csv_description": "### Application Resource Management for Kubernetes\nTurbonomic AI-powered Application Resource Management simultaneously optimizes performance, compliance, and cost in real time.\nSoftware manages the complete application stack, automatically. Applications are continually resourced to perform while satisfying business constraints.\n\nTurbonomic makes workloads smart \u2014 enabling them to self-manage and determines the specific actions that will drive continuous health:\n\n* Continuous placement for Pods (rescheduling)\n* Continuous scaling for applications and  the underlying cluster.\n\nIt assures application performance by giving workloads the resources they need when they need them.\n\n### How does it work?\nTurbonomic uses a container \u2014 KubeTurbo \u2014 that runs in your Kubernetes or Red Hat OpenShift cluster to discover and monitor your environment.\nKubeTurbo runs together with the default scheduler and sends this data back to the Turbonomic analytics engine.\nTurbonomic determines the right actions that drive continuous health, including continuous placement for Pods and continuous scaling for applications and the underlying cluster.",
      "csv_display_name": "Kubeturbo Operator",
      "csv_metadata_description": "Turbonomic Workload Automation for Multicloud simultaneously optimizes performance, compliance, and cost in real-time. Workloads are precisely resourced, automatically, to perform while satisfying business constraints.",
      "csv_name": "kubeturbo-operator.v8.5.5",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-11-23T07:32:12.022000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.8",
      "organization": "certified-operators",
      "package": "kubeturbo-certified",
      "provided_apis": [
        {
          "group": "charts.helm.k8s.io",
          "kind": "Kubeturbo",
          "plural": "kubeturbos",
          "version": "v1alpha1"
        },
        {
          "group": "charts.helm.k8s.io",
          "kind": "Kubeturbo",
          "plural": "kubeturbos",
          "version": "v1"
        }
      ],
      "provider": "Turbonomic, Inc.",
      "related_images": [
        {
          "digest": "sha256:e5cdbdeeaf639afe9782fceb9bf32c5e06e2b6ed4ded41e06abe8f49bda7ac47",
          "image": "registry.connect.redhat.com/turbonomic/kubeturbo-operator@sha256:e5cdbdeeaf639afe9782fceb9bf32c5e06e2b6ed4ded41e06abe8f49bda7ac47",
          "name": "kubeturbo-operator-e5cdbdeeaf639afe9782fceb9bf32c5e06e2b6ed4ded41e06abe8f49bda7ac47-annotation"
        },
        {
          "digest": "sha256:e5cdbdeeaf639afe9782fceb9bf32c5e06e2b6ed4ded41e06abe8f49bda7ac47",
          "image": "registry.connect.redhat.com/turbonomic/kubeturbo-operator@sha256:e5cdbdeeaf639afe9782fceb9bf32c5e06e2b6ed4ded41e06abe8f49bda7ac47",
          "name": "kubeturbo-operator"
        },
        {
          "digest": "sha256:e08debe4fcac4b377e86ed57c9587057e8f99d37c5bd8acfe94dfa17605d0759",
          "image": "registry.connect.redhat.com/turbonomic/kubeturbo@sha256:e08debe4fcac4b377e86ed57c9587057e8f99d37c5bd8acfe94dfa17605d0759",
          "name": "kubeturbo"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.8",
      "version": "8.5.5",
      "version_original": "8.5.5"
    },
    {
      "_id": "62abe028240dc134424c6bdc",
      "alm_examples": [
        {
          "api_version": "charts.helm.k8s.io/v1",
          "kind": "Kubeturbo",
          "metadata": {
            "name": "kubeturbo-release"
          },
          "spec": {
            "serverMeta": {
              "turboServer": "https://Turbo_server_URL"
            },
            "targetConfig": {
              "targetName": "Cluster_Name"
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/turbonomic/kubeturbo-operator-bundle@sha256:23f06fa446df0253aeacaf9f063d959c91e56f8a210ac493dc47bc238bb84bfe",
      "bundle_path_digest": "sha256:23f06fa446df0253aeacaf9f063d959c91e56f8a210ac493dc47bc238bb84bfe",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "stable",
      "creation_date": "2022-06-17T02:00:08.250000+00:00",
      "csv_description": "### Application Resource Management for Kubernetes\nTurbonomic AI-powered Application Resource Management simultaneously optimizes performance, compliance, and cost in real time.\nSoftware manages the complete application stack, automatically. Applications are continually resourced to perform while satisfying business constraints.\n\nTurbonomic makes workloads smart \u2014 enabling them to self-manage and determines the specific actions that will drive continuous health:\n\n* Continuous placement for Pods (rescheduling)\n* Continuous scaling for applications and  the underlying cluster.\n\nIt assures application performance by giving workloads the resources they need when they need them.\n\n### How does it work?\nTurbonomic uses a container \u2014 KubeTurbo \u2014 that runs in your Kubernetes or Red Hat OpenShift cluster to discover and monitor your environment.\nKubeTurbo runs together with the default scheduler and sends this data back to the Turbonomic analytics engine.\nTurbonomic determines the right actions that drive continuous health, including continuous placement for Pods and continuous scaling for applications and the underlying cluster.",
      "csv_display_name": "Kubeturbo Operator",
      "csv_metadata_description": "Turbonomic Workload Automation for Multicloud simultaneously optimizes performance, compliance, and cost in real-time. Workloads are precisely resourced, automatically, to perform while satisfying business constraints.",
      "csv_name": "kubeturbo-operator.v8.5.5",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-11-23T07:20:01.545000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.7",
      "organization": "certified-operators",
      "package": "kubeturbo-certified",
      "provided_apis": [
        {
          "group": "charts.helm.k8s.io",
          "kind": "Kubeturbo",
          "plural": "kubeturbos",
          "version": "v1alpha1"
        },
        {
          "group": "charts.helm.k8s.io",
          "kind": "Kubeturbo",
          "plural": "kubeturbos",
          "version": "v1"
        }
      ],
      "provider": "Turbonomic, Inc.",
      "related_images": [
        {
          "digest": "sha256:e5cdbdeeaf639afe9782fceb9bf32c5e06e2b6ed4ded41e06abe8f49bda7ac47",
          "image": "registry.connect.redhat.com/turbonomic/kubeturbo-operator@sha256:e5cdbdeeaf639afe9782fceb9bf32c5e06e2b6ed4ded41e06abe8f49bda7ac47",
          "name": "kubeturbo-operator-e5cdbdeeaf639afe9782fceb9bf32c5e06e2b6ed4ded41e06abe8f49bda7ac47-annotation"
        },
        {
          "digest": "sha256:e5cdbdeeaf639afe9782fceb9bf32c5e06e2b6ed4ded41e06abe8f49bda7ac47",
          "image": "registry.connect.redhat.com/turbonomic/kubeturbo-operator@sha256:e5cdbdeeaf639afe9782fceb9bf32c5e06e2b6ed4ded41e06abe8f49bda7ac47",
          "name": "kubeturbo-operator"
        },
        {
          "digest": "sha256:e08debe4fcac4b377e86ed57c9587057e8f99d37c5bd8acfe94dfa17605d0759",
          "image": "registry.connect.redhat.com/turbonomic/kubeturbo@sha256:e08debe4fcac4b377e86ed57c9587057e8f99d37c5bd8acfe94dfa17605d0759",
          "name": "kubeturbo"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.7",
      "version": "8.5.5",
      "version_original": "8.5.5"
    },
    {
      "_id": "62abe34bbe1fd3a96e0592ac",
      "alm_examples": [
        {
          "api_version": "charts.helm.k8s.io/v1",
          "kind": "Kubeturbo",
          "metadata": {
            "name": "kubeturbo-release"
          },
          "spec": {
            "serverMeta": {
              "turboServer": "https://Turbo_server_URL"
            },
            "targetConfig": {
              "targetName": "Cluster_Name"
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/turbonomic/kubeturbo-operator-bundle@sha256:23f06fa446df0253aeacaf9f063d959c91e56f8a210ac493dc47bc238bb84bfe",
      "bundle_path_digest": "sha256:23f06fa446df0253aeacaf9f063d959c91e56f8a210ac493dc47bc238bb84bfe",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "stable",
      "creation_date": "2022-06-17T02:13:31.499000+00:00",
      "csv_description": "### Application Resource Management for Kubernetes\nTurbonomic AI-powered Application Resource Management simultaneously optimizes performance, compliance, and cost in real time.\nSoftware manages the complete application stack, automatically. Applications are continually resourced to perform while satisfying business constraints.\n\nTurbonomic makes workloads smart \u2014 enabling them to self-manage and determines the specific actions that will drive continuous health:\n\n* Continuous placement for Pods (rescheduling)\n* Continuous scaling for applications and  the underlying cluster.\n\nIt assures application performance by giving workloads the resources they need when they need them.\n\n### How does it work?\nTurbonomic uses a container \u2014 KubeTurbo \u2014 that runs in your Kubernetes or Red Hat OpenShift cluster to discover and monitor your environment.\nKubeTurbo runs together with the default scheduler and sends this data back to the Turbonomic analytics engine.\nTurbonomic determines the right actions that drive continuous health, including continuous placement for Pods and continuous scaling for applications and the underlying cluster.",
      "csv_display_name": "Kubeturbo Operator",
      "csv_metadata_description": "Turbonomic Workload Automation for Multicloud simultaneously optimizes performance, compliance, and cost in real-time. Workloads are precisely resourced, automatically, to perform while satisfying business constraints.",
      "csv_name": "kubeturbo-operator.v8.5.5",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-11-23T07:32:02.584000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.6",
      "organization": "certified-operators",
      "package": "kubeturbo-certified",
      "provided_apis": [
        {
          "group": "charts.helm.k8s.io",
          "kind": "Kubeturbo",
          "plural": "kubeturbos",
          "version": "v1alpha1"
        },
        {
          "group": "charts.helm.k8s.io",
          "kind": "Kubeturbo",
          "plural": "kubeturbos",
          "version": "v1"
        }
      ],
      "provider": "Turbonomic, Inc.",
      "related_images": [
        {
          "digest": "sha256:e5cdbdeeaf639afe9782fceb9bf32c5e06e2b6ed4ded41e06abe8f49bda7ac47",
          "image": "registry.connect.redhat.com/turbonomic/kubeturbo-operator@sha256:e5cdbdeeaf639afe9782fceb9bf32c5e06e2b6ed4ded41e06abe8f49bda7ac47",
          "name": "kubeturbo-operator-e5cdbdeeaf639afe9782fceb9bf32c5e06e2b6ed4ded41e06abe8f49bda7ac47-annotation"
        },
        {
          "digest": "sha256:e5cdbdeeaf639afe9782fceb9bf32c5e06e2b6ed4ded41e06abe8f49bda7ac47",
          "image": "registry.connect.redhat.com/turbonomic/kubeturbo-operator@sha256:e5cdbdeeaf639afe9782fceb9bf32c5e06e2b6ed4ded41e06abe8f49bda7ac47",
          "name": "kubeturbo-operator"
        },
        {
          "digest": "sha256:e08debe4fcac4b377e86ed57c9587057e8f99d37c5bd8acfe94dfa17605d0759",
          "image": "registry.connect.redhat.com/turbonomic/kubeturbo@sha256:e08debe4fcac4b377e86ed57c9587057e8f99d37c5bd8acfe94dfa17605d0759",
          "name": "kubeturbo"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.6",
      "version": "8.5.5",
      "version_original": "8.5.5"
    },
    {
      "_id": "62b1b80ebe1fd3a96e06efd6",
      "alm_examples": [
        {
          "api_version": "cilium.io/v1alpha1",
          "kind": "CiliumConfig",
          "metadata": {
            "name": "cilium-openshift-default",
            "namespace": "placeholder"
          },
          "spec": {}
        }
      ],
      "annotations": {
        "infrastructure_features": [
          "disconnected"
        ],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/isovalent/cilium-ee-olm-metadata@sha256:2c5164ec8369a7aff720ffd3d43d2f698adec22ea4b69a7df7833d5714e79db3",
      "bundle_path_digest": "sha256:2c5164ec8369a7aff720ffd3d43d2f698adec22ea4b69a7df7833d5714e79db3",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "1.11",
      "creation_date": "2022-06-21T12:22:38.250000+00:00",
      "csv_description": "Cilium Enterprise - eBPF-based Networking, Security, and Observability",
      "csv_display_name": "Cilium Enterprise",
      "csv_metadata_description": "",
      "csv_name": "cilium.v1.11.5-xeb3a68e",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-11-23T07:06:32.913000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.9",
      "organization": "certified-operators",
      "package": "cilium-enterprise",
      "provided_apis": [
        {
          "group": "cilium.io",
          "kind": "CiliumConfig",
          "plural": "ciliumconfigs",
          "version": "v1alpha1"
        }
      ],
      "provider": "Isovalent",
      "related_images": [
        {
          "digest": "sha256:b357efdc6b328f607aaeac4ea629fc8d8b0e70c3734548b0f14855303c99dee5",
          "image": "quay.io/isovalent/cilium@sha256:b357efdc6b328f607aaeac4ea629fc8d8b0e70c3734548b0f14855303c99dee5",
          "name": "cilium"
        },
        {
          "digest": "sha256:77feb78e0a0d1d072ff13dc4d7e1f784a0f51fa6ab88ff78bc7954ebb4f59b7a",
          "image": "quay.io/isovalent/hubble-relay@sha256:77feb78e0a0d1d072ff13dc4d7e1f784a0f51fa6ab88ff78bc7954ebb4f59b7a",
          "name": "hubble-relay"
        },
        {
          "digest": "sha256:464a67c1a04a2df8bd529783f1a0169df9a8f313af9c9e12bd61a3b5c482460e",
          "image": "quay.io/isovalent/operator-generic@sha256:464a67c1a04a2df8bd529783f1a0169df9a8f313af9c9e12bd61a3b5c482460e",
          "name": "cilium-operator"
        },
        {
          "digest": "sha256:b357efdc6b328f607aaeac4ea629fc8d8b0e70c3734548b0f14855303c99dee5",
          "image": "quay.io/isovalent/cilium@sha256:b357efdc6b328f607aaeac4ea629fc8d8b0e70c3734548b0f14855303c99dee5",
          "name": "preflight"
        },
        {
          "digest": "sha256:97b10d36bc9903c74c66a5ab95960184258999b6510b413cc5dde7d4168cfb7e",
          "image": "quay.io/isovalent/clustermesh-apiserver@sha256:97b10d36bc9903c74c66a5ab95960184258999b6510b413cc5dde7d4168cfb7e",
          "name": "clustermesh"
        },
        {
          "digest": "sha256:0c2b71bb3469990e7990e7e26243617aa344b5a69a4ce465740b8577f9d48ab9",
          "image": "quay.io/cilium/certgen@sha256:0c2b71bb3469990e7990e7e26243617aa344b5a69a4ce465740b8577f9d48ab9",
          "name": "certgen"
        },
        {
          "digest": "sha256:e892cba6dafcdf6704df3135a0b38d11894a5f276ace40683e74cb62b9c311d8",
          "image": "quay.io/isovalent/hubble-ui-enterprise-backend@sha256:e892cba6dafcdf6704df3135a0b38d11894a5f276ace40683e74cb62b9c311d8",
          "name": "hubble-ui-backend"
        },
        {
          "digest": "sha256:c9921c65892a5293a4f1bef787636d6ae58cec1af65da06862a47e3450afdc6e",
          "image": "quay.io/isovalent/hubble-ui-enterprise@sha256:c9921c65892a5293a4f1bef787636d6ae58cec1af65da06862a47e3450afdc6e",
          "name": "hubble-ui-frontend"
        },
        {
          "digest": "sha256:ecd26b74a01f2b547ddaed4d32d35f8f5e09c378d5c1fc6cfa63f0adf659ac2b",
          "image": "quay.io/oauth2-proxy/oauth2-proxy@sha256:ecd26b74a01f2b547ddaed4d32d35f8f5e09c378d5c1fc6cfa63f0adf659ac2b",
          "name": "hubble-ui-oauth"
        },
        {
          "digest": "sha256:04b8327f7f992693c2cb483b999041ed8f92efc8e14f2a5f3ab95574a65ea2dc",
          "image": "quay.io/cilium/cilium-etcd-operator@sha256:04b8327f7f992693c2cb483b999041ed8f92efc8e14f2a5f3ab95574a65ea2dc",
          "name": "etcd-operator"
        },
        {
          "digest": "sha256:1daf817f34000399fcb5da9a94cb299e2810d2c7a52e51de22ba0d4783b6ce84",
          "image": "quay.io/cilium/startup-script@sha256:1daf817f34000399fcb5da9a94cb299e2810d2c7a52e51de22ba0d4783b6ce84",
          "name": "nodeinit"
        },
        {
          "digest": "sha256:04833b601fa130512450afa45c4fe484fee1293634f34c7ddc231bd193c74017",
          "image": "quay.io/coreos/etcd@sha256:04833b601fa130512450afa45c4fe484fee1293634f34c7ddc231bd193c74017",
          "name": "clustermesh-etcd"
        },
        {
          "digest": "sha256:1e1d3e6c199546b2398f4166ff64210c04075bb407c8f07c5be8d3db08560d4a",
          "image": "quay.io/isovalent/hubble-enterprise@sha256:1e1d3e6c199546b2398f4166ff64210c04075bb407c8f07c5be8d3db08560d4a",
          "name": "hubble-enterprise"
        },
        {
          "digest": "sha256:7d6865502729b67f12a3a734f1e9db14775c2a1867910e60922a194767e3b25b",
          "image": "quay.io/isovalent/hubble-enterprise-metadata@sha256:7d6865502729b67f12a3a734f1e9db14775c2a1867910e60922a194767e3b25b",
          "name": "hubble-enterprise-metadata"
        },
        {
          "digest": "sha256:0d5e6a5d9c07780c18e5aa81d8f1c95c399bff2d36a44fa5f61ac89a788ca83b",
          "image": "quay.io/isovalent/hubble-enterprise-operator@sha256:0d5e6a5d9c07780c18e5aa81d8f1c95c399bff2d36a44fa5f61ac89a788ca83b",
          "name": "hubble-enterprise-operator"
        },
        {
          "digest": "sha256:f1b1475a3c89950481264f5f8ca63eea3b911a45dc83fdf15122d695a0fba9b3",
          "image": "quay.io/cilium/hubble-export-stdout@sha256:f1b1475a3c89950481264f5f8ca63eea3b911a45dc83fdf15122d695a0fba9b3",
          "name": "hubble-export"
        },
        {
          "digest": "sha256:05cb13f533f2c89b98a47cd58badda07add54077cd22283ca5028cb39f2e4486",
          "image": "quay.io/isovalent/hubble-export-fluentd@sha256:05cb13f533f2c89b98a47cd58badda07add54077cd22283ca5028cb39f2e4486",
          "name": "hubble-export-fluentd"
        },
        {
          "digest": "sha256:ce4f71f7f13301133b7d471b87fb81108b2c2da7fc29865ef96bc8431b2be72b",
          "image": "quay.io/isovalent/hubble-export-s3@sha256:ce4f71f7f13301133b7d471b87fb81108b2c2da7fc29865ef96bc8431b2be72b",
          "name": "hubble-export-s3"
        },
        {
          "digest": "sha256:03f3304c157024356a6de13c3d91b4506b8e54d587920d80ebcc76cbcfeaeda6",
          "image": "quay.io/isovalent/hubble-rbac@sha256:03f3304c157024356a6de13c3d91b4506b8e54d587920d80ebcc76cbcfeaeda6",
          "name": "hubble-rbac"
        },
        {
          "digest": "sha256:e867d348af25d628982adac2c6ed195632418ee2dc687312dceb7133594efd12",
          "image": "registry.connect.redhat.com/isovalent/cilium-ee-olm@sha256:e867d348af25d628982adac2c6ed195632418ee2dc687312dceb7133594efd12",
          "name": "operator"
        },
        {
          "digest": "sha256:77feb78e0a0d1d072ff13dc4d7e1f784a0f51fa6ab88ff78bc7954ebb4f59b7a",
          "image": "quay.io/isovalent/hubble-relay@sha256:77feb78e0a0d1d072ff13dc4d7e1f784a0f51fa6ab88ff78bc7954ebb4f59b7a",
          "name": "hubble_relay"
        },
        {
          "digest": "sha256:464a67c1a04a2df8bd529783f1a0169df9a8f313af9c9e12bd61a3b5c482460e",
          "image": "quay.io/isovalent/operator-generic@sha256:464a67c1a04a2df8bd529783f1a0169df9a8f313af9c9e12bd61a3b5c482460e",
          "name": "cilium_operator"
        },
        {
          "digest": "sha256:e892cba6dafcdf6704df3135a0b38d11894a5f276ace40683e74cb62b9c311d8",
          "image": "quay.io/isovalent/hubble-ui-enterprise-backend@sha256:e892cba6dafcdf6704df3135a0b38d11894a5f276ace40683e74cb62b9c311d8",
          "name": "hubble_ui_be"
        },
        {
          "digest": "sha256:c9921c65892a5293a4f1bef787636d6ae58cec1af65da06862a47e3450afdc6e",
          "image": "quay.io/isovalent/hubble-ui-enterprise@sha256:c9921c65892a5293a4f1bef787636d6ae58cec1af65da06862a47e3450afdc6e",
          "name": "hubble_ui_fe"
        },
        {
          "digest": "sha256:ecd26b74a01f2b547ddaed4d32d35f8f5e09c378d5c1fc6cfa63f0adf659ac2b",
          "image": "quay.io/oauth2-proxy/oauth2-proxy@sha256:ecd26b74a01f2b547ddaed4d32d35f8f5e09c378d5c1fc6cfa63f0adf659ac2b",
          "name": "hubble_ui_oauth"
        },
        {
          "digest": "sha256:04b8327f7f992693c2cb483b999041ed8f92efc8e14f2a5f3ab95574a65ea2dc",
          "image": "quay.io/cilium/cilium-etcd-operator@sha256:04b8327f7f992693c2cb483b999041ed8f92efc8e14f2a5f3ab95574a65ea2dc",
          "name": "etcd_operator"
        },
        {
          "digest": "sha256:04833b601fa130512450afa45c4fe484fee1293634f34c7ddc231bd193c74017",
          "image": "quay.io/coreos/etcd@sha256:04833b601fa130512450afa45c4fe484fee1293634f34c7ddc231bd193c74017",
          "name": "clustermesh_etcd"
        },
        {
          "digest": "sha256:1e1d3e6c199546b2398f4166ff64210c04075bb407c8f07c5be8d3db08560d4a",
          "image": "quay.io/isovalent/hubble-enterprise@sha256:1e1d3e6c199546b2398f4166ff64210c04075bb407c8f07c5be8d3db08560d4a",
          "name": "hubble_enterprise"
        },
        {
          "digest": "sha256:7d6865502729b67f12a3a734f1e9db14775c2a1867910e60922a194767e3b25b",
          "image": "quay.io/isovalent/hubble-enterprise-metadata@sha256:7d6865502729b67f12a3a734f1e9db14775c2a1867910e60922a194767e3b25b",
          "name": "hubble_enterprise_metadata"
        },
        {
          "digest": "sha256:0d5e6a5d9c07780c18e5aa81d8f1c95c399bff2d36a44fa5f61ac89a788ca83b",
          "image": "quay.io/isovalent/hubble-enterprise-operator@sha256:0d5e6a5d9c07780c18e5aa81d8f1c95c399bff2d36a44fa5f61ac89a788ca83b",
          "name": "hubble_enterprise_operator"
        },
        {
          "digest": "sha256:f1b1475a3c89950481264f5f8ca63eea3b911a45dc83fdf15122d695a0fba9b3",
          "image": "quay.io/cilium/hubble-export-stdout@sha256:f1b1475a3c89950481264f5f8ca63eea3b911a45dc83fdf15122d695a0fba9b3",
          "name": "hubble_export"
        },
        {
          "digest": "sha256:05cb13f533f2c89b98a47cd58badda07add54077cd22283ca5028cb39f2e4486",
          "image": "quay.io/isovalent/hubble-export-fluentd@sha256:05cb13f533f2c89b98a47cd58badda07add54077cd22283ca5028cb39f2e4486",
          "name": "hubble_export_fluentd"
        },
        {
          "digest": "sha256:ce4f71f7f13301133b7d471b87fb81108b2c2da7fc29865ef96bc8431b2be72b",
          "image": "quay.io/isovalent/hubble-export-s3@sha256:ce4f71f7f13301133b7d471b87fb81108b2c2da7fc29865ef96bc8431b2be72b",
          "name": "hubble_export_s3"
        },
        {
          "digest": "sha256:03f3304c157024356a6de13c3d91b4506b8e54d587920d80ebcc76cbcfeaeda6",
          "image": "quay.io/isovalent/hubble-rbac@sha256:03f3304c157024356a6de13c3d91b4506b8e54d587920d80ebcc76cbcfeaeda6",
          "name": "hubble_rbac"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.9",
      "version": "1.11.5+xeb3a68e",
      "version_original": "1.11.5+xeb3a68e"
    },
    {
      "_id": "62b1b862150d8182de902ec4",
      "alm_examples": [
        {
          "api_version": "cilium.io/v1alpha1",
          "kind": "CiliumConfig",
          "metadata": {
            "name": "cilium-openshift-default",
            "namespace": "placeholder"
          },
          "spec": {}
        }
      ],
      "annotations": {
        "infrastructure_features": [
          "disconnected"
        ],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/isovalent/cilium-ee-olm-metadata@sha256:2c5164ec8369a7aff720ffd3d43d2f698adec22ea4b69a7df7833d5714e79db3",
      "bundle_path_digest": "sha256:2c5164ec8369a7aff720ffd3d43d2f698adec22ea4b69a7df7833d5714e79db3",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "1.11",
      "creation_date": "2022-06-21T12:24:02.408000+00:00",
      "csv_description": "Cilium Enterprise - eBPF-based Networking, Security, and Observability",
      "csv_display_name": "Cilium Enterprise",
      "csv_metadata_description": "",
      "csv_name": "cilium.v1.11.5-xeb3a68e",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-11-23T08:21:32.815000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.10",
      "organization": "certified-operators",
      "package": "cilium-enterprise",
      "provided_apis": [
        {
          "group": "cilium.io",
          "kind": "CiliumConfig",
          "plural": "ciliumconfigs",
          "version": "v1alpha1"
        }
      ],
      "provider": "Isovalent",
      "related_images": [
        {
          "digest": "sha256:b357efdc6b328f607aaeac4ea629fc8d8b0e70c3734548b0f14855303c99dee5",
          "image": "quay.io/isovalent/cilium@sha256:b357efdc6b328f607aaeac4ea629fc8d8b0e70c3734548b0f14855303c99dee5",
          "name": "cilium"
        },
        {
          "digest": "sha256:77feb78e0a0d1d072ff13dc4d7e1f784a0f51fa6ab88ff78bc7954ebb4f59b7a",
          "image": "quay.io/isovalent/hubble-relay@sha256:77feb78e0a0d1d072ff13dc4d7e1f784a0f51fa6ab88ff78bc7954ebb4f59b7a",
          "name": "hubble-relay"
        },
        {
          "digest": "sha256:464a67c1a04a2df8bd529783f1a0169df9a8f313af9c9e12bd61a3b5c482460e",
          "image": "quay.io/isovalent/operator-generic@sha256:464a67c1a04a2df8bd529783f1a0169df9a8f313af9c9e12bd61a3b5c482460e",
          "name": "cilium-operator"
        },
        {
          "digest": "sha256:b357efdc6b328f607aaeac4ea629fc8d8b0e70c3734548b0f14855303c99dee5",
          "image": "quay.io/isovalent/cilium@sha256:b357efdc6b328f607aaeac4ea629fc8d8b0e70c3734548b0f14855303c99dee5",
          "name": "preflight"
        },
        {
          "digest": "sha256:97b10d36bc9903c74c66a5ab95960184258999b6510b413cc5dde7d4168cfb7e",
          "image": "quay.io/isovalent/clustermesh-apiserver@sha256:97b10d36bc9903c74c66a5ab95960184258999b6510b413cc5dde7d4168cfb7e",
          "name": "clustermesh"
        },
        {
          "digest": "sha256:0c2b71bb3469990e7990e7e26243617aa344b5a69a4ce465740b8577f9d48ab9",
          "image": "quay.io/cilium/certgen@sha256:0c2b71bb3469990e7990e7e26243617aa344b5a69a4ce465740b8577f9d48ab9",
          "name": "certgen"
        },
        {
          "digest": "sha256:e892cba6dafcdf6704df3135a0b38d11894a5f276ace40683e74cb62b9c311d8",
          "image": "quay.io/isovalent/hubble-ui-enterprise-backend@sha256:e892cba6dafcdf6704df3135a0b38d11894a5f276ace40683e74cb62b9c311d8",
          "name": "hubble-ui-backend"
        },
        {
          "digest": "sha256:c9921c65892a5293a4f1bef787636d6ae58cec1af65da06862a47e3450afdc6e",
          "image": "quay.io/isovalent/hubble-ui-enterprise@sha256:c9921c65892a5293a4f1bef787636d6ae58cec1af65da06862a47e3450afdc6e",
          "name": "hubble-ui-frontend"
        },
        {
          "digest": "sha256:ecd26b74a01f2b547ddaed4d32d35f8f5e09c378d5c1fc6cfa63f0adf659ac2b",
          "image": "quay.io/oauth2-proxy/oauth2-proxy@sha256:ecd26b74a01f2b547ddaed4d32d35f8f5e09c378d5c1fc6cfa63f0adf659ac2b",
          "name": "hubble-ui-oauth"
        },
        {
          "digest": "sha256:04b8327f7f992693c2cb483b999041ed8f92efc8e14f2a5f3ab95574a65ea2dc",
          "image": "quay.io/cilium/cilium-etcd-operator@sha256:04b8327f7f992693c2cb483b999041ed8f92efc8e14f2a5f3ab95574a65ea2dc",
          "name": "etcd-operator"
        },
        {
          "digest": "sha256:1daf817f34000399fcb5da9a94cb299e2810d2c7a52e51de22ba0d4783b6ce84",
          "image": "quay.io/cilium/startup-script@sha256:1daf817f34000399fcb5da9a94cb299e2810d2c7a52e51de22ba0d4783b6ce84",
          "name": "nodeinit"
        },
        {
          "digest": "sha256:04833b601fa130512450afa45c4fe484fee1293634f34c7ddc231bd193c74017",
          "image": "quay.io/coreos/etcd@sha256:04833b601fa130512450afa45c4fe484fee1293634f34c7ddc231bd193c74017",
          "name": "clustermesh-etcd"
        },
        {
          "digest": "sha256:1e1d3e6c199546b2398f4166ff64210c04075bb407c8f07c5be8d3db08560d4a",
          "image": "quay.io/isovalent/hubble-enterprise@sha256:1e1d3e6c199546b2398f4166ff64210c04075bb407c8f07c5be8d3db08560d4a",
          "name": "hubble-enterprise"
        },
        {
          "digest": "sha256:7d6865502729b67f12a3a734f1e9db14775c2a1867910e60922a194767e3b25b",
          "image": "quay.io/isovalent/hubble-enterprise-metadata@sha256:7d6865502729b67f12a3a734f1e9db14775c2a1867910e60922a194767e3b25b",
          "name": "hubble-enterprise-metadata"
        },
        {
          "digest": "sha256:0d5e6a5d9c07780c18e5aa81d8f1c95c399bff2d36a44fa5f61ac89a788ca83b",
          "image": "quay.io/isovalent/hubble-enterprise-operator@sha256:0d5e6a5d9c07780c18e5aa81d8f1c95c399bff2d36a44fa5f61ac89a788ca83b",
          "name": "hubble-enterprise-operator"
        },
        {
          "digest": "sha256:f1b1475a3c89950481264f5f8ca63eea3b911a45dc83fdf15122d695a0fba9b3",
          "image": "quay.io/cilium/hubble-export-stdout@sha256:f1b1475a3c89950481264f5f8ca63eea3b911a45dc83fdf15122d695a0fba9b3",
          "name": "hubble-export"
        },
        {
          "digest": "sha256:05cb13f533f2c89b98a47cd58badda07add54077cd22283ca5028cb39f2e4486",
          "image": "quay.io/isovalent/hubble-export-fluentd@sha256:05cb13f533f2c89b98a47cd58badda07add54077cd22283ca5028cb39f2e4486",
          "name": "hubble-export-fluentd"
        },
        {
          "digest": "sha256:ce4f71f7f13301133b7d471b87fb81108b2c2da7fc29865ef96bc8431b2be72b",
          "image": "quay.io/isovalent/hubble-export-s3@sha256:ce4f71f7f13301133b7d471b87fb81108b2c2da7fc29865ef96bc8431b2be72b",
          "name": "hubble-export-s3"
        },
        {
          "digest": "sha256:03f3304c157024356a6de13c3d91b4506b8e54d587920d80ebcc76cbcfeaeda6",
          "image": "quay.io/isovalent/hubble-rbac@sha256:03f3304c157024356a6de13c3d91b4506b8e54d587920d80ebcc76cbcfeaeda6",
          "name": "hubble-rbac"
        },
        {
          "digest": "sha256:e867d348af25d628982adac2c6ed195632418ee2dc687312dceb7133594efd12",
          "image": "registry.connect.redhat.com/isovalent/cilium-ee-olm@sha256:e867d348af25d628982adac2c6ed195632418ee2dc687312dceb7133594efd12",
          "name": "operator"
        },
        {
          "digest": "sha256:77feb78e0a0d1d072ff13dc4d7e1f784a0f51fa6ab88ff78bc7954ebb4f59b7a",
          "image": "quay.io/isovalent/hubble-relay@sha256:77feb78e0a0d1d072ff13dc4d7e1f784a0f51fa6ab88ff78bc7954ebb4f59b7a",
          "name": "hubble_relay"
        },
        {
          "digest": "sha256:464a67c1a04a2df8bd529783f1a0169df9a8f313af9c9e12bd61a3b5c482460e",
          "image": "quay.io/isovalent/operator-generic@sha256:464a67c1a04a2df8bd529783f1a0169df9a8f313af9c9e12bd61a3b5c482460e",
          "name": "cilium_operator"
        },
        {
          "digest": "sha256:e892cba6dafcdf6704df3135a0b38d11894a5f276ace40683e74cb62b9c311d8",
          "image": "quay.io/isovalent/hubble-ui-enterprise-backend@sha256:e892cba6dafcdf6704df3135a0b38d11894a5f276ace40683e74cb62b9c311d8",
          "name": "hubble_ui_be"
        },
        {
          "digest": "sha256:c9921c65892a5293a4f1bef787636d6ae58cec1af65da06862a47e3450afdc6e",
          "image": "quay.io/isovalent/hubble-ui-enterprise@sha256:c9921c65892a5293a4f1bef787636d6ae58cec1af65da06862a47e3450afdc6e",
          "name": "hubble_ui_fe"
        },
        {
          "digest": "sha256:ecd26b74a01f2b547ddaed4d32d35f8f5e09c378d5c1fc6cfa63f0adf659ac2b",
          "image": "quay.io/oauth2-proxy/oauth2-proxy@sha256:ecd26b74a01f2b547ddaed4d32d35f8f5e09c378d5c1fc6cfa63f0adf659ac2b",
          "name": "hubble_ui_oauth"
        },
        {
          "digest": "sha256:04b8327f7f992693c2cb483b999041ed8f92efc8e14f2a5f3ab95574a65ea2dc",
          "image": "quay.io/cilium/cilium-etcd-operator@sha256:04b8327f7f992693c2cb483b999041ed8f92efc8e14f2a5f3ab95574a65ea2dc",
          "name": "etcd_operator"
        },
        {
          "digest": "sha256:04833b601fa130512450afa45c4fe484fee1293634f34c7ddc231bd193c74017",
          "image": "quay.io/coreos/etcd@sha256:04833b601fa130512450afa45c4fe484fee1293634f34c7ddc231bd193c74017",
          "name": "clustermesh_etcd"
        },
        {
          "digest": "sha256:1e1d3e6c199546b2398f4166ff64210c04075bb407c8f07c5be8d3db08560d4a",
          "image": "quay.io/isovalent/hubble-enterprise@sha256:1e1d3e6c199546b2398f4166ff64210c04075bb407c8f07c5be8d3db08560d4a",
          "name": "hubble_enterprise"
        },
        {
          "digest": "sha256:7d6865502729b67f12a3a734f1e9db14775c2a1867910e60922a194767e3b25b",
          "image": "quay.io/isovalent/hubble-enterprise-metadata@sha256:7d6865502729b67f12a3a734f1e9db14775c2a1867910e60922a194767e3b25b",
          "name": "hubble_enterprise_metadata"
        },
        {
          "digest": "sha256:0d5e6a5d9c07780c18e5aa81d8f1c95c399bff2d36a44fa5f61ac89a788ca83b",
          "image": "quay.io/isovalent/hubble-enterprise-operator@sha256:0d5e6a5d9c07780c18e5aa81d8f1c95c399bff2d36a44fa5f61ac89a788ca83b",
          "name": "hubble_enterprise_operator"
        },
        {
          "digest": "sha256:f1b1475a3c89950481264f5f8ca63eea3b911a45dc83fdf15122d695a0fba9b3",
          "image": "quay.io/cilium/hubble-export-stdout@sha256:f1b1475a3c89950481264f5f8ca63eea3b911a45dc83fdf15122d695a0fba9b3",
          "name": "hubble_export"
        },
        {
          "digest": "sha256:05cb13f533f2c89b98a47cd58badda07add54077cd22283ca5028cb39f2e4486",
          "image": "quay.io/isovalent/hubble-export-fluentd@sha256:05cb13f533f2c89b98a47cd58badda07add54077cd22283ca5028cb39f2e4486",
          "name": "hubble_export_fluentd"
        },
        {
          "digest": "sha256:ce4f71f7f13301133b7d471b87fb81108b2c2da7fc29865ef96bc8431b2be72b",
          "image": "quay.io/isovalent/hubble-export-s3@sha256:ce4f71f7f13301133b7d471b87fb81108b2c2da7fc29865ef96bc8431b2be72b",
          "name": "hubble_export_s3"
        },
        {
          "digest": "sha256:03f3304c157024356a6de13c3d91b4506b8e54d587920d80ebcc76cbcfeaeda6",
          "image": "quay.io/isovalent/hubble-rbac@sha256:03f3304c157024356a6de13c3d91b4506b8e54d587920d80ebcc76cbcfeaeda6",
          "name": "hubble_rbac"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.10",
      "version": "1.11.5+xeb3a68e",
      "version_original": "1.11.5+xeb3a68e"
    },
    {
      "_id": "62b1ff65240dc134424de468",
      "alm_examples": [
        {
          "api_version": "sriovfec.intel.com/v2",
          "kind": "SriovFecClusterConfig",
          "metadata": {
            "name": "acc100SampleConfig",
            "namespace": "vran-acceleration-operators"
          },
          "spec": {
            "acceleratorSelector": {
              "deviceID": "someDevice",
              "driver": "someDriver",
              "maxVirtualFunctions": 2,
              "pciAddress": "somePciAddress",
              "vendorID": "someVendor"
            },
            "drainSkip": false,
            "nodeSelector": {
              "expectedLabel1": "valueOfExpectedLabel1",
              "expectedLabelN": "valueOfExpectedLabelN"
            },
            "physicalFunction": {
              "bbDevConfig": {
                "acc100": {
                  "downlink4G": {
                    "aqDepthLog2": 4,
                    "numAqsPerGroups": 16,
                    "numQueueGroups": 0
                  },
                  "downlink5G": {
                    "aqDepthLog2": 4,
                    "numAqsPerGroups": 16,
                    "numQueueGroups": 4
                  },
                  "maxQueueSize": 1024,
                  "numVfBundles": 16,
                  "pfMode": true,
                  "uplink4G": {
                    "aqDepthLog2": 4,
                    "numAqsPerGroups": 16,
                    "numQueueGroups": 0
                  },
                  "uplink5G": {
                    "aqDepthLog2": 4,
                    "numAqsPerGroups": 16,
                    "numQueueGroups": 4
                  }
                }
              },
              "pfDriver": "pci-pf-stub",
              "vfAmount": 2,
              "vfDriver": "vfio-pci"
            },
            "priority": 100
          }
        },
        {
          "api_version": "sriovfec.intel.com/v2",
          "kind": "SriovFecClusterConfig",
          "metadata": {
            "name": "n3000SampleConfig",
            "namespace": "vran-acceleration-operators"
          },
          "spec": {
            "acceleratorSelector": {
              "deviceID": "someDevice",
              "driver": "someDriver",
              "maxVirtualFunctions": 2,
              "pciAddress": "somePciAddress",
              "vendorID": "someVendor"
            },
            "drainSkip": false,
            "nodeSelector": {
              "expectedLabel1": "valueOfExpectedLabel1",
              "expectedLabelN": "valueOfExpectedLabelN"
            },
            "physicalFunction": {
              "bbDevConfig": {
                "n3000": {
                  "downlink": {
                    "bandwidth": 3,
                    "loadBalance": 128,
                    "queues": {
                      "vf0": 16,
                      "vf1": 16,
                      "vf2": 0,
                      "vf3": 0,
                      "vf4": 0,
                      "vf5": 0,
                      "vf6": 0,
                      "vf7": 0
                    }
                  },
                  "flrTimeout": 610,
                  "networkType": "FPGA_5GNR",
                  "pfMode": true,
                  "uplink": {
                    "bandwidth": 3,
                    "loadBalance": 128,
                    "queues": {
                      "vf0": 16,
                      "vf1": 16,
                      "vf2": 0,
                      "vf3": 0,
                      "vf4": 0,
                      "vf5": 0,
                      "vf6": 0,
                      "vf7": 0
                    }
                  }
                }
              },
              "pfDriver": "pci-pf-stub",
              "vfAmount": 2,
              "vfDriver": "vfio-pci"
            },
            "priority": 100
          }
        },
        {
          "api_version": "sriovfec.intel.com/v2",
          "kind": "SriovFecNodeConfig",
          "metadata": {
            "name": "acc100-worker",
            "namespace": "vran-acceleration-operators"
          },
          "spec": {
            "drainSkip": false,
            "physicalFunctions": [
              {
                "bbDevConfig": {
                  "acc100": {
                    "downlink4G": {
                      "aqDepthLog2": 4,
                      "numAqsPerGroups": 16,
                      "numQueueGroups": 0
                    },
                    "downlink5G": {
                      "aqDepthLog2": 4,
                      "numAqsPerGroups": 16,
                      "numQueueGroups": 4
                    },
                    "maxQueueSize": 1024,
                    "numVfBundles": 16,
                    "pfMode": true,
                    "uplink4G": {
                      "aqDepthLog2": 4,
                      "numAqsPerGroups": 16,
                      "numQueueGroups": 0
                    },
                    "uplink5G": {
                      "aqDepthLog2": 4,
                      "numAqsPerGroups": 16,
                      "numQueueGroups": 4
                    }
                  }
                },
                "pci_addr": "somePciAddress",
                "pf_driver": "pci-pf-stub",
                "vf_amount": 2,
                "vf_driver": "vfio-pci"
              }
            ]
          }
        },
        {
          "api_version": "sriovfec.intel.com/v2",
          "kind": "SriovFecNodeConfig",
          "metadata": {
            "name": "n3000-worker",
            "namespace": "vran-acceleration-operators"
          },
          "spec": {
            "drainSkip": false,
            "physicalFunctions": [
              {
                "bbDevConfig": {
                  "n3000": {
                    "downlink": {
                      "bandwidth": 3,
                      "loadBalance": 128,
                      "queues": {
                        "vf0": 16,
                        "vf1": 16,
                        "vf2": 0,
                        "vf3": 0,
                        "vf4": 0,
                        "vf5": 0,
                        "vf6": 0,
                        "vf7": 0
                      }
                    },
                    "flrTimeout": 610,
                    "networkType": "FPGA_5GNR",
                    "pfMode": true,
                    "uplink": {
                      "bandwidth": 3,
                      "loadBalance": 128,
                      "queues": {
                        "vf0": 16,
                        "vf1": 16,
                        "vf2": 0,
                        "vf3": 0,
                        "vf4": 0,
                        "vf5": 0,
                        "vf6": 0,
                        "vf7": 0
                      }
                    }
                  }
                },
                "pci_addr": "somePciAddress",
                "pf_driver": "pci-pf-stub",
                "vf_amount": 2,
                "vf_driver": "vfio-pci"
              }
            ]
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/intel/sriov-fec-operator-bundle@sha256:cdfa252bcdaa95a68ba24a2ab91d6cca74897146c22c8e2b29e6b603c5081d42",
      "bundle_path_digest": "sha256:cdfa252bcdaa95a68ba24a2ab91d6cca74897146c22c8e2b29e6b603c5081d42",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "stable",
      "creation_date": "2022-06-21T17:27:01.411000+00:00",
      "csv_description": "The Intel\u00ae FPGA Programmable Acceleration Card N3000 (Intel\u00ae FPGA PAC N3000) is a highly customizable FPGA SmartNIC which enables high-throughput, low latency and high-bandwith applications.  It allows the optimization of data plane performance to reduce total cost of ownership while maintaining a high degree of flexibility.  The Intel FPGA PAC N3000 plays a key role in accelerating 5G and network functions virtualization (NFV) adoption for ecosystem partners such as telecommunications equipment manufacturers (TEMs) virtual network functions (VNF) vendors, system integrators and telcos, to bring scalable and high-performance solutions to market. The Intel FPGA PAC N3000 includes a variant that is design to be Network Equipment Building System (NEBS)-friendly, and features a Root-of-Trust device that helps protect systems from FPGA host security exploits. This document explains how the FPGA resource can be used on the Smart Edge Open (SEO) platform for accelerating network functions and edge application workloads. We use the Intel\u00ae FPGA PAC N3000 as a reference FPGA PAC and use LTE/5G Forward Error Correction (FEC) as an example workload that accelerates the 5G or 4G L1 base station network function. The same concept and mechanism is applicable for application acceleration workloads like AI and ML on FPGA for Inference applications. The Intel\u00ae FPGA PAC N3000 is a full-duplex, 100 Gbps in-system, re-programmable acceleration card for multi-workload networking application acceleration. It has an optimal memory mixture designed for network functions, with an integrated network interface card (NIC) in a small form factor that enables high throughput, low latency, and low power per bit for a custom networking pipeline. The ACC100 supports the O-RAN adopted DPDK BBDev API - an API which Intel contributed to the opensource community to enable choice and TTM for FEC acceleration solutions. The FlexRAN software reference architecture supports the ACC100 which enables users to quickly evaluate and build platforms for the wide range of vRAN networks. Reduces platform power, E2E latency and Intel\u00ae CPU core count requirements as well as increases cell capacity than existing programmable accelerator. Accelerates both 4G and 5G data concurrently.\tLowers development cost using commercial off the shelf (COTS) servers. Accommodates space-constrained implementations via a low-profile PCIe card form factor. Enables a variety of flexible FlexRAN deployments from small cell to macro to Massive MIMO networks. Supports extended temperature for the most challenging of RAN deployment scenario\u2019s.",
      "csv_display_name": "SEO SR-IOV Operator for Wireless FEC Accelerators",
      "csv_metadata_description": "An operator for Intel Wireless FEC Accelerator to orchestrate and manage the resources/devices exposed by a range of Intel's vRAN FEC acceleration devices/hardware within the OpenShift cluster.",
      "csv_name": "sriov-fec.v2.3.0",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-11-23T07:43:13.869000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.8",
      "organization": "certified-operators",
      "package": "sriov-fec",
      "provided_apis": [
        {
          "group": "sriovfec.intel.com",
          "kind": "SriovFecClusterConfig",
          "plural": "sriovfecclusterconfigs",
          "version": "v2"
        },
        {
          "group": "sriovfec.intel.com",
          "kind": "SriovFecNodeConfig",
          "plural": "sriovfecnodeconfigs",
          "version": "v1"
        },
        {
          "group": "sriovfec.intel.com",
          "kind": "SriovFecNodeConfig",
          "plural": "sriovfecnodeconfigs",
          "version": "v2"
        },
        {
          "group": "sriovfec.intel.com",
          "kind": "SriovFecClusterConfig",
          "plural": "sriovfecclusterconfigs",
          "version": "v1"
        }
      ],
      "provider": "Intel Corporation",
      "related_images": [
        {
          "digest": "sha256:dede7827589da291005a4e403f85b0ff0b19447f05e3dd9c72b25ae8a54936eb",
          "image": "registry.connect.redhat.com/intel/sriov-fec-daemon@sha256:dede7827589da291005a4e403f85b0ff0b19447f05e3dd9c72b25ae8a54936eb",
          "name": "sriov-fec-daemon"
        },
        {
          "digest": "sha256:5055121216aa2e2a8240d1a1a30850077e77c4828d94e1bd02045d640faf1f1d",
          "image": "registry.connect.redhat.com/intel/n3000-labeler@sha256:5055121216aa2e2a8240d1a1a30850077e77c4828d94e1bd02045d640faf1f1d",
          "name": "sriov-fec-labeler"
        },
        {
          "digest": "sha256:5ad615f1256d1d23b159a5a331a31103692dd0293b6f128b9095d0b34c0f6586",
          "image": "registry.redhat.io/openshift4/ose-sriov-network-device-plugin@sha256:5ad615f1256d1d23b159a5a331a31103692dd0293b6f128b9095d0b34c0f6586",
          "name": "sriov-network-device-plugin"
        },
        {
          "digest": "sha256:558b236c3938024280600939798a0aad84949fce4de758763b6ecbd57d00f48b",
          "image": "registry.connect.redhat.com/intel/sriov-fec-operator@sha256:558b236c3938024280600939798a0aad84949fce4de758763b6ecbd57d00f48b",
          "name": "sriov-fec-operator"
        },
        {
          "digest": "sha256:86e5fa1fa294987114be200890c2e516501e424aee0fb98ece25c95e7716295b",
          "image": "registry.redhat.io/openshift4/ose-kube-rbac-proxy@sha256:86e5fa1fa294987114be200890c2e516501e424aee0fb98ece25c95e7716295b",
          "name": "kube-rbac-proxy"
        },
        {
          "digest": "sha256:558b236c3938024280600939798a0aad84949fce4de758763b6ecbd57d00f48b",
          "image": "registry.connect.redhat.com/intel/sriov-fec-operator@sha256:558b236c3938024280600939798a0aad84949fce4de758763b6ecbd57d00f48b",
          "name": "sriov-fec-operator-558b236c3938024280600939798a0aad84949fce4de758763b6ecbd57d00f48b-annotation"
        },
        {
          "digest": "sha256:558b236c3938024280600939798a0aad84949fce4de758763b6ecbd57d00f48b",
          "image": "registry.connect.redhat.com/intel/sriov-fec-operator@sha256:558b236c3938024280600939798a0aad84949fce4de758763b6ecbd57d00f48b",
          "name": "manager"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.8",
      "version": "2.3.0",
      "version_original": "2.3.0"
    },
    {
      "_id": "62b22be5150d8182de904c43",
      "alm_examples": [
        {
          "api_version": "cilium.io/v1alpha1",
          "kind": "CiliumConfig",
          "metadata": {
            "name": "cilium-openshift-default",
            "namespace": "placeholder"
          },
          "spec": {}
        }
      ],
      "annotations": {
        "infrastructure_features": [
          "disconnected"
        ],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/isovalent/cilium-ee-olm-metadata@sha256:2c5164ec8369a7aff720ffd3d43d2f698adec22ea4b69a7df7833d5714e79db3",
      "bundle_path_digest": "sha256:2c5164ec8369a7aff720ffd3d43d2f698adec22ea4b69a7df7833d5714e79db3",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "1.11",
      "creation_date": "2022-06-21T20:36:53.555000+00:00",
      "csv_description": "Cilium Enterprise - eBPF-based Networking, Security, and Observability",
      "csv_display_name": "Cilium Enterprise",
      "csv_metadata_description": "",
      "csv_name": "cilium.v1.11.5-xeb3a68e",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-11-23T06:56:08.560000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.11",
      "organization": "certified-operators",
      "package": "cilium-enterprise",
      "provided_apis": [
        {
          "group": "cilium.io",
          "kind": "CiliumConfig",
          "version": "v1alpha1"
        }
      ],
      "provider": "Isovalent",
      "related_images": [
        {
          "digest": "sha256:b357efdc6b328f607aaeac4ea629fc8d8b0e70c3734548b0f14855303c99dee5",
          "image": "quay.io/isovalent/cilium@sha256:b357efdc6b328f607aaeac4ea629fc8d8b0e70c3734548b0f14855303c99dee5",
          "name": "cilium"
        },
        {
          "digest": "sha256:77feb78e0a0d1d072ff13dc4d7e1f784a0f51fa6ab88ff78bc7954ebb4f59b7a",
          "image": "quay.io/isovalent/hubble-relay@sha256:77feb78e0a0d1d072ff13dc4d7e1f784a0f51fa6ab88ff78bc7954ebb4f59b7a",
          "name": "hubble-relay"
        },
        {
          "digest": "sha256:464a67c1a04a2df8bd529783f1a0169df9a8f313af9c9e12bd61a3b5c482460e",
          "image": "quay.io/isovalent/operator-generic@sha256:464a67c1a04a2df8bd529783f1a0169df9a8f313af9c9e12bd61a3b5c482460e",
          "name": "cilium-operator"
        },
        {
          "digest": "sha256:b357efdc6b328f607aaeac4ea629fc8d8b0e70c3734548b0f14855303c99dee5",
          "image": "quay.io/isovalent/cilium@sha256:b357efdc6b328f607aaeac4ea629fc8d8b0e70c3734548b0f14855303c99dee5",
          "name": "preflight"
        },
        {
          "digest": "sha256:97b10d36bc9903c74c66a5ab95960184258999b6510b413cc5dde7d4168cfb7e",
          "image": "quay.io/isovalent/clustermesh-apiserver@sha256:97b10d36bc9903c74c66a5ab95960184258999b6510b413cc5dde7d4168cfb7e",
          "name": "clustermesh"
        },
        {
          "digest": "sha256:0c2b71bb3469990e7990e7e26243617aa344b5a69a4ce465740b8577f9d48ab9",
          "image": "quay.io/cilium/certgen@sha256:0c2b71bb3469990e7990e7e26243617aa344b5a69a4ce465740b8577f9d48ab9",
          "name": "certgen"
        },
        {
          "digest": "sha256:e892cba6dafcdf6704df3135a0b38d11894a5f276ace40683e74cb62b9c311d8",
          "image": "quay.io/isovalent/hubble-ui-enterprise-backend@sha256:e892cba6dafcdf6704df3135a0b38d11894a5f276ace40683e74cb62b9c311d8",
          "name": "hubble-ui-backend"
        },
        {
          "digest": "sha256:c9921c65892a5293a4f1bef787636d6ae58cec1af65da06862a47e3450afdc6e",
          "image": "quay.io/isovalent/hubble-ui-enterprise@sha256:c9921c65892a5293a4f1bef787636d6ae58cec1af65da06862a47e3450afdc6e",
          "name": "hubble-ui-frontend"
        },
        {
          "digest": "sha256:ecd26b74a01f2b547ddaed4d32d35f8f5e09c378d5c1fc6cfa63f0adf659ac2b",
          "image": "quay.io/oauth2-proxy/oauth2-proxy@sha256:ecd26b74a01f2b547ddaed4d32d35f8f5e09c378d5c1fc6cfa63f0adf659ac2b",
          "name": "hubble-ui-oauth"
        },
        {
          "digest": "sha256:04b8327f7f992693c2cb483b999041ed8f92efc8e14f2a5f3ab95574a65ea2dc",
          "image": "quay.io/cilium/cilium-etcd-operator@sha256:04b8327f7f992693c2cb483b999041ed8f92efc8e14f2a5f3ab95574a65ea2dc",
          "name": "etcd-operator"
        },
        {
          "digest": "sha256:1daf817f34000399fcb5da9a94cb299e2810d2c7a52e51de22ba0d4783b6ce84",
          "image": "quay.io/cilium/startup-script@sha256:1daf817f34000399fcb5da9a94cb299e2810d2c7a52e51de22ba0d4783b6ce84",
          "name": "nodeinit"
        },
        {
          "digest": "sha256:04833b601fa130512450afa45c4fe484fee1293634f34c7ddc231bd193c74017",
          "image": "quay.io/coreos/etcd@sha256:04833b601fa130512450afa45c4fe484fee1293634f34c7ddc231bd193c74017",
          "name": "clustermesh-etcd"
        },
        {
          "digest": "sha256:1e1d3e6c199546b2398f4166ff64210c04075bb407c8f07c5be8d3db08560d4a",
          "image": "quay.io/isovalent/hubble-enterprise@sha256:1e1d3e6c199546b2398f4166ff64210c04075bb407c8f07c5be8d3db08560d4a",
          "name": "hubble-enterprise"
        },
        {
          "digest": "sha256:7d6865502729b67f12a3a734f1e9db14775c2a1867910e60922a194767e3b25b",
          "image": "quay.io/isovalent/hubble-enterprise-metadata@sha256:7d6865502729b67f12a3a734f1e9db14775c2a1867910e60922a194767e3b25b",
          "name": "hubble-enterprise-metadata"
        },
        {
          "digest": "sha256:0d5e6a5d9c07780c18e5aa81d8f1c95c399bff2d36a44fa5f61ac89a788ca83b",
          "image": "quay.io/isovalent/hubble-enterprise-operator@sha256:0d5e6a5d9c07780c18e5aa81d8f1c95c399bff2d36a44fa5f61ac89a788ca83b",
          "name": "hubble-enterprise-operator"
        },
        {
          "digest": "sha256:f1b1475a3c89950481264f5f8ca63eea3b911a45dc83fdf15122d695a0fba9b3",
          "image": "quay.io/cilium/hubble-export-stdout@sha256:f1b1475a3c89950481264f5f8ca63eea3b911a45dc83fdf15122d695a0fba9b3",
          "name": "hubble-export"
        },
        {
          "digest": "sha256:05cb13f533f2c89b98a47cd58badda07add54077cd22283ca5028cb39f2e4486",
          "image": "quay.io/isovalent/hubble-export-fluentd@sha256:05cb13f533f2c89b98a47cd58badda07add54077cd22283ca5028cb39f2e4486",
          "name": "hubble-export-fluentd"
        },
        {
          "digest": "sha256:ce4f71f7f13301133b7d471b87fb81108b2c2da7fc29865ef96bc8431b2be72b",
          "image": "quay.io/isovalent/hubble-export-s3@sha256:ce4f71f7f13301133b7d471b87fb81108b2c2da7fc29865ef96bc8431b2be72b",
          "name": "hubble-export-s3"
        },
        {
          "digest": "sha256:03f3304c157024356a6de13c3d91b4506b8e54d587920d80ebcc76cbcfeaeda6",
          "image": "quay.io/isovalent/hubble-rbac@sha256:03f3304c157024356a6de13c3d91b4506b8e54d587920d80ebcc76cbcfeaeda6",
          "name": "hubble-rbac"
        },
        {
          "digest": "sha256:e867d348af25d628982adac2c6ed195632418ee2dc687312dceb7133594efd12",
          "image": "registry.connect.redhat.com/isovalent/cilium-ee-olm@sha256:e867d348af25d628982adac2c6ed195632418ee2dc687312dceb7133594efd12",
          "name": "operator"
        },
        {
          "digest": "sha256:77feb78e0a0d1d072ff13dc4d7e1f784a0f51fa6ab88ff78bc7954ebb4f59b7a",
          "image": "quay.io/isovalent/hubble-relay@sha256:77feb78e0a0d1d072ff13dc4d7e1f784a0f51fa6ab88ff78bc7954ebb4f59b7a",
          "name": "hubble_relay"
        },
        {
          "digest": "sha256:464a67c1a04a2df8bd529783f1a0169df9a8f313af9c9e12bd61a3b5c482460e",
          "image": "quay.io/isovalent/operator-generic@sha256:464a67c1a04a2df8bd529783f1a0169df9a8f313af9c9e12bd61a3b5c482460e",
          "name": "cilium_operator"
        },
        {
          "digest": "sha256:e892cba6dafcdf6704df3135a0b38d11894a5f276ace40683e74cb62b9c311d8",
          "image": "quay.io/isovalent/hubble-ui-enterprise-backend@sha256:e892cba6dafcdf6704df3135a0b38d11894a5f276ace40683e74cb62b9c311d8",
          "name": "hubble_ui_be"
        },
        {
          "digest": "sha256:c9921c65892a5293a4f1bef787636d6ae58cec1af65da06862a47e3450afdc6e",
          "image": "quay.io/isovalent/hubble-ui-enterprise@sha256:c9921c65892a5293a4f1bef787636d6ae58cec1af65da06862a47e3450afdc6e",
          "name": "hubble_ui_fe"
        },
        {
          "digest": "sha256:ecd26b74a01f2b547ddaed4d32d35f8f5e09c378d5c1fc6cfa63f0adf659ac2b",
          "image": "quay.io/oauth2-proxy/oauth2-proxy@sha256:ecd26b74a01f2b547ddaed4d32d35f8f5e09c378d5c1fc6cfa63f0adf659ac2b",
          "name": "hubble_ui_oauth"
        },
        {
          "digest": "sha256:04b8327f7f992693c2cb483b999041ed8f92efc8e14f2a5f3ab95574a65ea2dc",
          "image": "quay.io/cilium/cilium-etcd-operator@sha256:04b8327f7f992693c2cb483b999041ed8f92efc8e14f2a5f3ab95574a65ea2dc",
          "name": "etcd_operator"
        },
        {
          "digest": "sha256:04833b601fa130512450afa45c4fe484fee1293634f34c7ddc231bd193c74017",
          "image": "quay.io/coreos/etcd@sha256:04833b601fa130512450afa45c4fe484fee1293634f34c7ddc231bd193c74017",
          "name": "clustermesh_etcd"
        },
        {
          "digest": "sha256:1e1d3e6c199546b2398f4166ff64210c04075bb407c8f07c5be8d3db08560d4a",
          "image": "quay.io/isovalent/hubble-enterprise@sha256:1e1d3e6c199546b2398f4166ff64210c04075bb407c8f07c5be8d3db08560d4a",
          "name": "hubble_enterprise"
        },
        {
          "digest": "sha256:7d6865502729b67f12a3a734f1e9db14775c2a1867910e60922a194767e3b25b",
          "image": "quay.io/isovalent/hubble-enterprise-metadata@sha256:7d6865502729b67f12a3a734f1e9db14775c2a1867910e60922a194767e3b25b",
          "name": "hubble_enterprise_metadata"
        },
        {
          "digest": "sha256:0d5e6a5d9c07780c18e5aa81d8f1c95c399bff2d36a44fa5f61ac89a788ca83b",
          "image": "quay.io/isovalent/hubble-enterprise-operator@sha256:0d5e6a5d9c07780c18e5aa81d8f1c95c399bff2d36a44fa5f61ac89a788ca83b",
          "name": "hubble_enterprise_operator"
        },
        {
          "digest": "sha256:f1b1475a3c89950481264f5f8ca63eea3b911a45dc83fdf15122d695a0fba9b3",
          "image": "quay.io/cilium/hubble-export-stdout@sha256:f1b1475a3c89950481264f5f8ca63eea3b911a45dc83fdf15122d695a0fba9b3",
          "name": "hubble_export"
        },
        {
          "digest": "sha256:05cb13f533f2c89b98a47cd58badda07add54077cd22283ca5028cb39f2e4486",
          "image": "quay.io/isovalent/hubble-export-fluentd@sha256:05cb13f533f2c89b98a47cd58badda07add54077cd22283ca5028cb39f2e4486",
          "name": "hubble_export_fluentd"
        },
        {
          "digest": "sha256:ce4f71f7f13301133b7d471b87fb81108b2c2da7fc29865ef96bc8431b2be72b",
          "image": "quay.io/isovalent/hubble-export-s3@sha256:ce4f71f7f13301133b7d471b87fb81108b2c2da7fc29865ef96bc8431b2be72b",
          "name": "hubble_export_s3"
        },
        {
          "digest": "sha256:03f3304c157024356a6de13c3d91b4506b8e54d587920d80ebcc76cbcfeaeda6",
          "image": "quay.io/isovalent/hubble-rbac@sha256:03f3304c157024356a6de13c3d91b4506b8e54d587920d80ebcc76cbcfeaeda6",
          "name": "hubble_rbac"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.11",
      "version": "1.11.5+xeb3a68e",
      "version_original": "1.11.5+xeb3a68e"
    },
    {
      "_id": "62b2b17fabc56faca879eefa",
      "alm_examples": [
        {
          "api_version": "core.libopenstorage.org/v1",
          "kind": "StorageCluster",
          "metadata": {
            "annotations": {
              "portworx.io/is-openshift": "true"
            },
            "name": "portworx",
            "namespace": "test-operator"
          },
          "spec": {}
        },
        {
          "api_version": "core.libopenstorage.org/v1",
          "kind": "StorageNode",
          "metadata": {
            "name": "example",
            "namespace": "test-operator"
          },
          "spec": {}
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/portworx/portworx-certified-bundle@sha256:2a717d5d940a152105e8f5dd46b3e527c19132fec7eb0982955f52c46166dfca",
      "bundle_path_digest": "sha256:2a717d5d940a152105e8f5dd46b3e527c19132fec7eb0982955f52c46166dfca",
      "capabilities": [
        "Auto Pilot"
      ],
      "channel_name": "alpha",
      "creation_date": "2022-06-22T06:06:55.379000+00:00",
      "csv_description": "Portworx-Enterprise is the most widely-used and reliable cloud-native\nstorage solution for production workloads and provides high-availability,\ndata protection and security for containerized applications.\n\nPortworx Enterprise enables you to migrate entire applications, including\ndata, between clusters in a single data center or cloud, or between clouds,\nwith a single kubectl command.\n\nThe cloud native storage and data management platform that enterprises trust\nto manage data in containers now has an operator which simplifies the install,\nconfiguration, upgrades and manages the Portworx Enterprise cluster lifecycle.\n\nLearn more about the Portworx Enterprise\n[the data platform for Kubernetes](https://portworx.com/products/introduction)\n\nTo learn more about the platform features, please visit our\n[product features page](https://portworx.com/products/features)\n\n### About Portworx\n\nPortworx is the solution for running stateful containers in production,\ndesigned with DevOps in mind. With Portworx, users can manage any database\nor stateful service on any infrastructure using any container scheduler,\nincluding Kubernetes, Mesosphere DC/OS, and Docker Swarm. Portworx solves\nthe five most common problems DevOps teams encounter when running stateful\nservices in production: persistence, high availability, data automation,\nsecurity, and support for multiple data stores and infrastructure.\n\n### How to install StorageCluster\n\nTo customize your cluster's configuration (specification), use the\n[Spec Generator](https://central.portworx.com/) from PX-Central.\n\n### Prerequisite\n\nEnsure ports 17001-17020 on worker nodes are reachable from master and other worker nodes.\n\n### Tutorials\n\n* [Portworx Enterprise on Openshift](https://portworx.com/openshift)\n\n* [Stateful applications on Kubernetes](https://docs.portworx.com/portworx-install-with-kubernetes/application-install-with-kubernetes)\n\n* [Portworx Enterprise on Kubernetes](https://docs.portworx.com/portworx-install-with-kubernetes)\n\n* [Kafka on Kubernetes](https://portworx.com/kafka-kubernetes)\n\n* [Elastisearch on Kubernetes](https://portworx.com/elasticsearch-kubernetes)\n\n* [PostgreSQL on Kubernetes](https://portworx.com/postgres-kubernetes/)\n\n* [MongoDB on Kubernetes](https://portworx.com/mongodb-kubernetes/)\n\n* [Cassandra on Kubernetes](https://portworx.com/cassandra-kubernetes/)\n\n* [Kubernetes backup and recovery](https://portworx.com/kubernetes-backup/)\n\n* [Disaster Recovery for Kubernetes](https://portworx.com/kubernetes-disaster-recovery/)\n\n### Uninstall\n\nDeleting the StorageCluster object for Portworx cluster does not stop Portworx\nservice running on the nodes, to avoid application downtime.\n\nTo uninstall Portworx completely without wiping the data, you should add the\nfollowing delete strategy to the StorageCluster spec:\n```\nspec:\n  deleteStrategy:\n    type: Uninstall\n```\n**Caution:** To uninstall Portworx and **wipe all the data**, you should use the following\ndelete strategy:\n```\nspec:\n  deleteStrategy:\n    type: UninstallAndWipe\n```\n",
      "csv_display_name": "Portworx Enterprise",
      "csv_metadata_description": "Cloud native storage solution for production workloads",
      "csv_name": "portworx-operator.v1.8.1",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": true,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": false,
      "last_update_date": "2022-11-23T06:45:21.962000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.11",
      "organization": "certified-operators",
      "package": "portworx-certified",
      "provided_apis": [
        {
          "group": "core.libopenstorage.org",
          "kind": "StorageCluster",
          "version": "v1"
        },
        {
          "group": "core.libopenstorage.org",
          "kind": "StorageCluster",
          "version": "v1alpha1"
        },
        {
          "group": "core.libopenstorage.org",
          "kind": "StorageNode",
          "version": "v1"
        },
        {
          "group": "core.libopenstorage.org",
          "kind": "StorageNode",
          "version": "v1alpha1"
        }
      ],
      "provider": "Portworx",
      "related_images": [
        {
          "digest": "sha256:bb93cb53d57ed8277480f5cfb827b6b2041e639c8fceae2e501e91c201d7e0d9",
          "image": "registry.connect.redhat.com/portworx/openstorage-operator@sha256:bb93cb53d57ed8277480f5cfb827b6b2041e639c8fceae2e501e91c201d7e0d9",
          "name": "openstorage-operator-bb93cb53d57ed8277480f5cfb827b6b2041e639c8fceae2e501e91c201d7e0d9-annotation"
        },
        {
          "digest": "sha256:bb93cb53d57ed8277480f5cfb827b6b2041e639c8fceae2e501e91c201d7e0d9",
          "image": "registry.connect.redhat.com/portworx/openstorage-operator@sha256:bb93cb53d57ed8277480f5cfb827b6b2041e639c8fceae2e501e91c201d7e0d9",
          "name": "portworx-operator"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.11",
      "version": "1.8.1",
      "version_original": "1.8.1"
    },
    {
      "_id": "62b2b1823b84323f9e3d7387",
      "alm_examples": [
        {
          "api_version": "core.libopenstorage.org/v1",
          "kind": "StorageCluster",
          "metadata": {
            "annotations": {
              "portworx.io/is-openshift": "true"
            },
            "name": "portworx",
            "namespace": "test-operator"
          },
          "spec": {}
        },
        {
          "api_version": "core.libopenstorage.org/v1",
          "kind": "StorageNode",
          "metadata": {
            "name": "example",
            "namespace": "test-operator"
          },
          "spec": {}
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/portworx/portworx-certified-bundle@sha256:2a717d5d940a152105e8f5dd46b3e527c19132fec7eb0982955f52c46166dfca",
      "bundle_path_digest": "sha256:2a717d5d940a152105e8f5dd46b3e527c19132fec7eb0982955f52c46166dfca",
      "capabilities": [
        "Auto Pilot"
      ],
      "channel_name": "stable",
      "creation_date": "2022-06-22T06:06:58.731000+00:00",
      "csv_description": "Portworx-Enterprise is the most widely-used and reliable cloud-native\nstorage solution for production workloads and provides high-availability,\ndata protection and security for containerized applications.\n\nPortworx Enterprise enables you to migrate entire applications, including\ndata, between clusters in a single data center or cloud, or between clouds,\nwith a single kubectl command.\n\nThe cloud native storage and data management platform that enterprises trust\nto manage data in containers now has an operator which simplifies the install,\nconfiguration, upgrades and manages the Portworx Enterprise cluster lifecycle.\n\nLearn more about the Portworx Enterprise\n[the data platform for Kubernetes](https://portworx.com/products/introduction)\n\nTo learn more about the platform features, please visit our\n[product features page](https://portworx.com/products/features)\n\n### About Portworx\n\nPortworx is the solution for running stateful containers in production,\ndesigned with DevOps in mind. With Portworx, users can manage any database\nor stateful service on any infrastructure using any container scheduler,\nincluding Kubernetes, Mesosphere DC/OS, and Docker Swarm. Portworx solves\nthe five most common problems DevOps teams encounter when running stateful\nservices in production: persistence, high availability, data automation,\nsecurity, and support for multiple data stores and infrastructure.\n\n### How to install StorageCluster\n\nTo customize your cluster's configuration (specification), use the\n[Spec Generator](https://central.portworx.com/) from PX-Central.\n\n### Prerequisite\n\nEnsure ports 17001-17020 on worker nodes are reachable from master and other worker nodes.\n\n### Tutorials\n\n* [Portworx Enterprise on Openshift](https://portworx.com/openshift)\n\n* [Stateful applications on Kubernetes](https://docs.portworx.com/portworx-install-with-kubernetes/application-install-with-kubernetes)\n\n* [Portworx Enterprise on Kubernetes](https://docs.portworx.com/portworx-install-with-kubernetes)\n\n* [Kafka on Kubernetes](https://portworx.com/kafka-kubernetes)\n\n* [Elastisearch on Kubernetes](https://portworx.com/elasticsearch-kubernetes)\n\n* [PostgreSQL on Kubernetes](https://portworx.com/postgres-kubernetes/)\n\n* [MongoDB on Kubernetes](https://portworx.com/mongodb-kubernetes/)\n\n* [Cassandra on Kubernetes](https://portworx.com/cassandra-kubernetes/)\n\n* [Kubernetes backup and recovery](https://portworx.com/kubernetes-backup/)\n\n* [Disaster Recovery for Kubernetes](https://portworx.com/kubernetes-disaster-recovery/)\n\n### Uninstall\n\nDeleting the StorageCluster object for Portworx cluster does not stop Portworx\nservice running on the nodes, to avoid application downtime.\n\nTo uninstall Portworx completely without wiping the data, you should add the\nfollowing delete strategy to the StorageCluster spec:\n```\nspec:\n  deleteStrategy:\n    type: Uninstall\n```\n**Caution:** To uninstall Portworx and **wipe all the data**, you should use the following\ndelete strategy:\n```\nspec:\n  deleteStrategy:\n    type: UninstallAndWipe\n```\n",
      "csv_display_name": "Portworx Enterprise",
      "csv_metadata_description": "Cloud native storage solution for production workloads",
      "csv_name": "portworx-operator.v1.8.1",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": true,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-11-23T06:45:59.695000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.11",
      "organization": "certified-operators",
      "package": "portworx-certified",
      "provided_apis": [
        {
          "group": "core.libopenstorage.org",
          "kind": "StorageCluster",
          "version": "v1"
        },
        {
          "group": "core.libopenstorage.org",
          "kind": "StorageCluster",
          "version": "v1alpha1"
        },
        {
          "group": "core.libopenstorage.org",
          "kind": "StorageNode",
          "version": "v1"
        },
        {
          "group": "core.libopenstorage.org",
          "kind": "StorageNode",
          "version": "v1alpha1"
        }
      ],
      "provider": "Portworx",
      "related_images": [
        {
          "digest": "sha256:bb93cb53d57ed8277480f5cfb827b6b2041e639c8fceae2e501e91c201d7e0d9",
          "image": "registry.connect.redhat.com/portworx/openstorage-operator@sha256:bb93cb53d57ed8277480f5cfb827b6b2041e639c8fceae2e501e91c201d7e0d9",
          "name": "openstorage-operator-bb93cb53d57ed8277480f5cfb827b6b2041e639c8fceae2e501e91c201d7e0d9-annotation"
        },
        {
          "digest": "sha256:bb93cb53d57ed8277480f5cfb827b6b2041e639c8fceae2e501e91c201d7e0d9",
          "image": "registry.connect.redhat.com/portworx/openstorage-operator@sha256:bb93cb53d57ed8277480f5cfb827b6b2041e639c8fceae2e501e91c201d7e0d9",
          "name": "portworx-operator"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.11",
      "version": "1.8.1",
      "version_original": "1.8.1"
    },
    {
      "_id": "62b2b532f9bec1296a00006b",
      "alm_examples": [
        {
          "api_version": "core.libopenstorage.org/v1",
          "kind": "StorageCluster",
          "metadata": {
            "annotations": {
              "portworx.io/is-openshift": "true"
            },
            "name": "portworx",
            "namespace": "test-operator"
          },
          "spec": {}
        },
        {
          "api_version": "core.libopenstorage.org/v1",
          "kind": "StorageNode",
          "metadata": {
            "name": "example",
            "namespace": "test-operator"
          },
          "spec": {}
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/portworx/portworx-certified-bundle@sha256:2a717d5d940a152105e8f5dd46b3e527c19132fec7eb0982955f52c46166dfca",
      "bundle_path_digest": "sha256:2a717d5d940a152105e8f5dd46b3e527c19132fec7eb0982955f52c46166dfca",
      "capabilities": [
        "Auto Pilot"
      ],
      "channel_name": "alpha",
      "creation_date": "2022-06-22T06:22:42.865000+00:00",
      "csv_description": "Portworx-Enterprise is the most widely-used and reliable cloud-native\nstorage solution for production workloads and provides high-availability,\ndata protection and security for containerized applications.\n\nPortworx Enterprise enables you to migrate entire applications, including\ndata, between clusters in a single data center or cloud, or between clouds,\nwith a single kubectl command.\n\nThe cloud native storage and data management platform that enterprises trust\nto manage data in containers now has an operator which simplifies the install,\nconfiguration, upgrades and manages the Portworx Enterprise cluster lifecycle.\n\nLearn more about the Portworx Enterprise\n[the data platform for Kubernetes](https://portworx.com/products/introduction)\n\nTo learn more about the platform features, please visit our\n[product features page](https://portworx.com/products/features)\n\n### About Portworx\n\nPortworx is the solution for running stateful containers in production,\ndesigned with DevOps in mind. With Portworx, users can manage any database\nor stateful service on any infrastructure using any container scheduler,\nincluding Kubernetes, Mesosphere DC/OS, and Docker Swarm. Portworx solves\nthe five most common problems DevOps teams encounter when running stateful\nservices in production: persistence, high availability, data automation,\nsecurity, and support for multiple data stores and infrastructure.\n\n### How to install StorageCluster\n\nTo customize your cluster's configuration (specification), use the\n[Spec Generator](https://central.portworx.com/) from PX-Central.\n\n### Prerequisite\n\nEnsure ports 17001-17020 on worker nodes are reachable from master and other worker nodes.\n\n### Tutorials\n\n* [Portworx Enterprise on Openshift](https://portworx.com/openshift)\n\n* [Stateful applications on Kubernetes](https://docs.portworx.com/portworx-install-with-kubernetes/application-install-with-kubernetes)\n\n* [Portworx Enterprise on Kubernetes](https://docs.portworx.com/portworx-install-with-kubernetes)\n\n* [Kafka on Kubernetes](https://portworx.com/kafka-kubernetes)\n\n* [Elastisearch on Kubernetes](https://portworx.com/elasticsearch-kubernetes)\n\n* [PostgreSQL on Kubernetes](https://portworx.com/postgres-kubernetes/)\n\n* [MongoDB on Kubernetes](https://portworx.com/mongodb-kubernetes/)\n\n* [Cassandra on Kubernetes](https://portworx.com/cassandra-kubernetes/)\n\n* [Kubernetes backup and recovery](https://portworx.com/kubernetes-backup/)\n\n* [Disaster Recovery for Kubernetes](https://portworx.com/kubernetes-disaster-recovery/)\n\n### Uninstall\n\nDeleting the StorageCluster object for Portworx cluster does not stop Portworx\nservice running on the nodes, to avoid application downtime.\n\nTo uninstall Portworx completely without wiping the data, you should add the\nfollowing delete strategy to the StorageCluster spec:\n```\nspec:\n  deleteStrategy:\n    type: Uninstall\n```\n**Caution:** To uninstall Portworx and **wipe all the data**, you should use the following\ndelete strategy:\n```\nspec:\n  deleteStrategy:\n    type: UninstallAndWipe\n```\n",
      "csv_display_name": "Portworx Enterprise",
      "csv_metadata_description": "Cloud native storage solution for production workloads",
      "csv_name": "portworx-operator.v1.8.1",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": true,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": false,
      "last_update_date": "2022-11-23T08:49:19.994000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.10",
      "organization": "certified-operators",
      "package": "portworx-certified",
      "provided_apis": [
        {
          "group": "core.libopenstorage.org",
          "kind": "StorageNode",
          "plural": "storagenodes",
          "version": "v1alpha1"
        },
        {
          "group": "core.libopenstorage.org",
          "kind": "StorageCluster",
          "plural": "storageclusters",
          "version": "v1"
        },
        {
          "group": "core.libopenstorage.org",
          "kind": "StorageCluster",
          "plural": "storageclusters",
          "version": "v1alpha1"
        },
        {
          "group": "core.libopenstorage.org",
          "kind": "StorageNode",
          "plural": "storagenodes",
          "version": "v1"
        }
      ],
      "provider": "Portworx",
      "related_images": [
        {
          "digest": "sha256:bb93cb53d57ed8277480f5cfb827b6b2041e639c8fceae2e501e91c201d7e0d9",
          "image": "registry.connect.redhat.com/portworx/openstorage-operator@sha256:bb93cb53d57ed8277480f5cfb827b6b2041e639c8fceae2e501e91c201d7e0d9",
          "name": "openstorage-operator-bb93cb53d57ed8277480f5cfb827b6b2041e639c8fceae2e501e91c201d7e0d9-annotation"
        },
        {
          "digest": "sha256:bb93cb53d57ed8277480f5cfb827b6b2041e639c8fceae2e501e91c201d7e0d9",
          "image": "registry.connect.redhat.com/portworx/openstorage-operator@sha256:bb93cb53d57ed8277480f5cfb827b6b2041e639c8fceae2e501e91c201d7e0d9",
          "name": "portworx-operator"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.10",
      "version": "1.8.1",
      "version_original": "1.8.1"
    },
    {
      "_id": "62b2b5373b84323f9e3d73ee",
      "alm_examples": [
        {
          "api_version": "core.libopenstorage.org/v1",
          "kind": "StorageCluster",
          "metadata": {
            "annotations": {
              "portworx.io/is-openshift": "true"
            },
            "name": "portworx",
            "namespace": "test-operator"
          },
          "spec": {}
        },
        {
          "api_version": "core.libopenstorage.org/v1",
          "kind": "StorageNode",
          "metadata": {
            "name": "example",
            "namespace": "test-operator"
          },
          "spec": {}
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/portworx/portworx-certified-bundle@sha256:2a717d5d940a152105e8f5dd46b3e527c19132fec7eb0982955f52c46166dfca",
      "bundle_path_digest": "sha256:2a717d5d940a152105e8f5dd46b3e527c19132fec7eb0982955f52c46166dfca",
      "capabilities": [
        "Auto Pilot"
      ],
      "channel_name": "stable",
      "creation_date": "2022-06-22T06:22:47.348000+00:00",
      "csv_description": "Portworx-Enterprise is the most widely-used and reliable cloud-native\nstorage solution for production workloads and provides high-availability,\ndata protection and security for containerized applications.\n\nPortworx Enterprise enables you to migrate entire applications, including\ndata, between clusters in a single data center or cloud, or between clouds,\nwith a single kubectl command.\n\nThe cloud native storage and data management platform that enterprises trust\nto manage data in containers now has an operator which simplifies the install,\nconfiguration, upgrades and manages the Portworx Enterprise cluster lifecycle.\n\nLearn more about the Portworx Enterprise\n[the data platform for Kubernetes](https://portworx.com/products/introduction)\n\nTo learn more about the platform features, please visit our\n[product features page](https://portworx.com/products/features)\n\n### About Portworx\n\nPortworx is the solution for running stateful containers in production,\ndesigned with DevOps in mind. With Portworx, users can manage any database\nor stateful service on any infrastructure using any container scheduler,\nincluding Kubernetes, Mesosphere DC/OS, and Docker Swarm. Portworx solves\nthe five most common problems DevOps teams encounter when running stateful\nservices in production: persistence, high availability, data automation,\nsecurity, and support for multiple data stores and infrastructure.\n\n### How to install StorageCluster\n\nTo customize your cluster's configuration (specification), use the\n[Spec Generator](https://central.portworx.com/) from PX-Central.\n\n### Prerequisite\n\nEnsure ports 17001-17020 on worker nodes are reachable from master and other worker nodes.\n\n### Tutorials\n\n* [Portworx Enterprise on Openshift](https://portworx.com/openshift)\n\n* [Stateful applications on Kubernetes](https://docs.portworx.com/portworx-install-with-kubernetes/application-install-with-kubernetes)\n\n* [Portworx Enterprise on Kubernetes](https://docs.portworx.com/portworx-install-with-kubernetes)\n\n* [Kafka on Kubernetes](https://portworx.com/kafka-kubernetes)\n\n* [Elastisearch on Kubernetes](https://portworx.com/elasticsearch-kubernetes)\n\n* [PostgreSQL on Kubernetes](https://portworx.com/postgres-kubernetes/)\n\n* [MongoDB on Kubernetes](https://portworx.com/mongodb-kubernetes/)\n\n* [Cassandra on Kubernetes](https://portworx.com/cassandra-kubernetes/)\n\n* [Kubernetes backup and recovery](https://portworx.com/kubernetes-backup/)\n\n* [Disaster Recovery for Kubernetes](https://portworx.com/kubernetes-disaster-recovery/)\n\n### Uninstall\n\nDeleting the StorageCluster object for Portworx cluster does not stop Portworx\nservice running on the nodes, to avoid application downtime.\n\nTo uninstall Portworx completely without wiping the data, you should add the\nfollowing delete strategy to the StorageCluster spec:\n```\nspec:\n  deleteStrategy:\n    type: Uninstall\n```\n**Caution:** To uninstall Portworx and **wipe all the data**, you should use the following\ndelete strategy:\n```\nspec:\n  deleteStrategy:\n    type: UninstallAndWipe\n```\n",
      "csv_display_name": "Portworx Enterprise",
      "csv_metadata_description": "Cloud native storage solution for production workloads",
      "csv_name": "portworx-operator.v1.8.1",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": true,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-11-23T08:49:29.552000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.10",
      "organization": "certified-operators",
      "package": "portworx-certified",
      "provided_apis": [
        {
          "group": "core.libopenstorage.org",
          "kind": "StorageNode",
          "plural": "storagenodes",
          "version": "v1alpha1"
        },
        {
          "group": "core.libopenstorage.org",
          "kind": "StorageCluster",
          "plural": "storageclusters",
          "version": "v1"
        },
        {
          "group": "core.libopenstorage.org",
          "kind": "StorageCluster",
          "plural": "storageclusters",
          "version": "v1alpha1"
        },
        {
          "group": "core.libopenstorage.org",
          "kind": "StorageNode",
          "plural": "storagenodes",
          "version": "v1"
        }
      ],
      "provider": "Portworx",
      "related_images": [
        {
          "digest": "sha256:bb93cb53d57ed8277480f5cfb827b6b2041e639c8fceae2e501e91c201d7e0d9",
          "image": "registry.connect.redhat.com/portworx/openstorage-operator@sha256:bb93cb53d57ed8277480f5cfb827b6b2041e639c8fceae2e501e91c201d7e0d9",
          "name": "openstorage-operator-bb93cb53d57ed8277480f5cfb827b6b2041e639c8fceae2e501e91c201d7e0d9-annotation"
        },
        {
          "digest": "sha256:bb93cb53d57ed8277480f5cfb827b6b2041e639c8fceae2e501e91c201d7e0d9",
          "image": "registry.connect.redhat.com/portworx/openstorage-operator@sha256:bb93cb53d57ed8277480f5cfb827b6b2041e639c8fceae2e501e91c201d7e0d9",
          "name": "portworx-operator"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.10",
      "version": "1.8.1",
      "version_original": "1.8.1"
    },
    {
      "_id": "62b2b572f9bec1296a00006f",
      "alm_examples": [
        {
          "api_version": "core.libopenstorage.org/v1",
          "kind": "StorageCluster",
          "metadata": {
            "annotations": {
              "portworx.io/is-openshift": "true"
            },
            "name": "portworx",
            "namespace": "test-operator"
          },
          "spec": {}
        },
        {
          "api_version": "core.libopenstorage.org/v1",
          "kind": "StorageNode",
          "metadata": {
            "name": "example",
            "namespace": "test-operator"
          },
          "spec": {}
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/portworx/portworx-certified-bundle@sha256:2a717d5d940a152105e8f5dd46b3e527c19132fec7eb0982955f52c46166dfca",
      "bundle_path_digest": "sha256:2a717d5d940a152105e8f5dd46b3e527c19132fec7eb0982955f52c46166dfca",
      "capabilities": [
        "Auto Pilot"
      ],
      "channel_name": "alpha",
      "creation_date": "2022-06-22T06:23:46.165000+00:00",
      "csv_description": "Portworx-Enterprise is the most widely-used and reliable cloud-native\nstorage solution for production workloads and provides high-availability,\ndata protection and security for containerized applications.\n\nPortworx Enterprise enables you to migrate entire applications, including\ndata, between clusters in a single data center or cloud, or between clouds,\nwith a single kubectl command.\n\nThe cloud native storage and data management platform that enterprises trust\nto manage data in containers now has an operator which simplifies the install,\nconfiguration, upgrades and manages the Portworx Enterprise cluster lifecycle.\n\nLearn more about the Portworx Enterprise\n[the data platform for Kubernetes](https://portworx.com/products/introduction)\n\nTo learn more about the platform features, please visit our\n[product features page](https://portworx.com/products/features)\n\n### About Portworx\n\nPortworx is the solution for running stateful containers in production,\ndesigned with DevOps in mind. With Portworx, users can manage any database\nor stateful service on any infrastructure using any container scheduler,\nincluding Kubernetes, Mesosphere DC/OS, and Docker Swarm. Portworx solves\nthe five most common problems DevOps teams encounter when running stateful\nservices in production: persistence, high availability, data automation,\nsecurity, and support for multiple data stores and infrastructure.\n\n### How to install StorageCluster\n\nTo customize your cluster's configuration (specification), use the\n[Spec Generator](https://central.portworx.com/) from PX-Central.\n\n### Prerequisite\n\nEnsure ports 17001-17020 on worker nodes are reachable from master and other worker nodes.\n\n### Tutorials\n\n* [Portworx Enterprise on Openshift](https://portworx.com/openshift)\n\n* [Stateful applications on Kubernetes](https://docs.portworx.com/portworx-install-with-kubernetes/application-install-with-kubernetes)\n\n* [Portworx Enterprise on Kubernetes](https://docs.portworx.com/portworx-install-with-kubernetes)\n\n* [Kafka on Kubernetes](https://portworx.com/kafka-kubernetes)\n\n* [Elastisearch on Kubernetes](https://portworx.com/elasticsearch-kubernetes)\n\n* [PostgreSQL on Kubernetes](https://portworx.com/postgres-kubernetes/)\n\n* [MongoDB on Kubernetes](https://portworx.com/mongodb-kubernetes/)\n\n* [Cassandra on Kubernetes](https://portworx.com/cassandra-kubernetes/)\n\n* [Kubernetes backup and recovery](https://portworx.com/kubernetes-backup/)\n\n* [Disaster Recovery for Kubernetes](https://portworx.com/kubernetes-disaster-recovery/)\n\n### Uninstall\n\nDeleting the StorageCluster object for Portworx cluster does not stop Portworx\nservice running on the nodes, to avoid application downtime.\n\nTo uninstall Portworx completely without wiping the data, you should add the\nfollowing delete strategy to the StorageCluster spec:\n```\nspec:\n  deleteStrategy:\n    type: Uninstall\n```\n**Caution:** To uninstall Portworx and **wipe all the data**, you should use the following\ndelete strategy:\n```\nspec:\n  deleteStrategy:\n    type: UninstallAndWipe\n```\n",
      "csv_display_name": "Portworx Enterprise",
      "csv_metadata_description": "Cloud native storage solution for production workloads",
      "csv_name": "portworx-operator.v1.8.1",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": true,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": false,
      "last_update_date": "2022-11-23T07:25:43.950000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.9",
      "organization": "certified-operators",
      "package": "portworx-certified",
      "provided_apis": [
        {
          "group": "core.libopenstorage.org",
          "kind": "StorageNode",
          "plural": "storagenodes",
          "version": "v1alpha1"
        },
        {
          "group": "core.libopenstorage.org",
          "kind": "StorageCluster",
          "plural": "storageclusters",
          "version": "v1"
        },
        {
          "group": "core.libopenstorage.org",
          "kind": "StorageCluster",
          "plural": "storageclusters",
          "version": "v1alpha1"
        },
        {
          "group": "core.libopenstorage.org",
          "kind": "StorageNode",
          "plural": "storagenodes",
          "version": "v1"
        }
      ],
      "provider": "Portworx",
      "related_images": [
        {
          "digest": "sha256:bb93cb53d57ed8277480f5cfb827b6b2041e639c8fceae2e501e91c201d7e0d9",
          "image": "registry.connect.redhat.com/portworx/openstorage-operator@sha256:bb93cb53d57ed8277480f5cfb827b6b2041e639c8fceae2e501e91c201d7e0d9",
          "name": "openstorage-operator-bb93cb53d57ed8277480f5cfb827b6b2041e639c8fceae2e501e91c201d7e0d9-annotation"
        },
        {
          "digest": "sha256:bb93cb53d57ed8277480f5cfb827b6b2041e639c8fceae2e501e91c201d7e0d9",
          "image": "registry.connect.redhat.com/portworx/openstorage-operator@sha256:bb93cb53d57ed8277480f5cfb827b6b2041e639c8fceae2e501e91c201d7e0d9",
          "name": "portworx-operator"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.9",
      "version": "1.8.1",
      "version_original": "1.8.1"
    },
    {
      "_id": "62b2b575abc56faca879ef66",
      "alm_examples": [
        {
          "api_version": "core.libopenstorage.org/v1",
          "kind": "StorageCluster",
          "metadata": {
            "annotations": {
              "portworx.io/is-openshift": "true"
            },
            "name": "portworx",
            "namespace": "test-operator"
          },
          "spec": {}
        },
        {
          "api_version": "core.libopenstorage.org/v1",
          "kind": "StorageNode",
          "metadata": {
            "name": "example",
            "namespace": "test-operator"
          },
          "spec": {}
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/portworx/portworx-certified-bundle@sha256:2a717d5d940a152105e8f5dd46b3e527c19132fec7eb0982955f52c46166dfca",
      "bundle_path_digest": "sha256:2a717d5d940a152105e8f5dd46b3e527c19132fec7eb0982955f52c46166dfca",
      "capabilities": [
        "Auto Pilot"
      ],
      "channel_name": "stable",
      "creation_date": "2022-06-22T06:23:49.610000+00:00",
      "csv_description": "Portworx-Enterprise is the most widely-used and reliable cloud-native\nstorage solution for production workloads and provides high-availability,\ndata protection and security for containerized applications.\n\nPortworx Enterprise enables you to migrate entire applications, including\ndata, between clusters in a single data center or cloud, or between clouds,\nwith a single kubectl command.\n\nThe cloud native storage and data management platform that enterprises trust\nto manage data in containers now has an operator which simplifies the install,\nconfiguration, upgrades and manages the Portworx Enterprise cluster lifecycle.\n\nLearn more about the Portworx Enterprise\n[the data platform for Kubernetes](https://portworx.com/products/introduction)\n\nTo learn more about the platform features, please visit our\n[product features page](https://portworx.com/products/features)\n\n### About Portworx\n\nPortworx is the solution for running stateful containers in production,\ndesigned with DevOps in mind. With Portworx, users can manage any database\nor stateful service on any infrastructure using any container scheduler,\nincluding Kubernetes, Mesosphere DC/OS, and Docker Swarm. Portworx solves\nthe five most common problems DevOps teams encounter when running stateful\nservices in production: persistence, high availability, data automation,\nsecurity, and support for multiple data stores and infrastructure.\n\n### How to install StorageCluster\n\nTo customize your cluster's configuration (specification), use the\n[Spec Generator](https://central.portworx.com/) from PX-Central.\n\n### Prerequisite\n\nEnsure ports 17001-17020 on worker nodes are reachable from master and other worker nodes.\n\n### Tutorials\n\n* [Portworx Enterprise on Openshift](https://portworx.com/openshift)\n\n* [Stateful applications on Kubernetes](https://docs.portworx.com/portworx-install-with-kubernetes/application-install-with-kubernetes)\n\n* [Portworx Enterprise on Kubernetes](https://docs.portworx.com/portworx-install-with-kubernetes)\n\n* [Kafka on Kubernetes](https://portworx.com/kafka-kubernetes)\n\n* [Elastisearch on Kubernetes](https://portworx.com/elasticsearch-kubernetes)\n\n* [PostgreSQL on Kubernetes](https://portworx.com/postgres-kubernetes/)\n\n* [MongoDB on Kubernetes](https://portworx.com/mongodb-kubernetes/)\n\n* [Cassandra on Kubernetes](https://portworx.com/cassandra-kubernetes/)\n\n* [Kubernetes backup and recovery](https://portworx.com/kubernetes-backup/)\n\n* [Disaster Recovery for Kubernetes](https://portworx.com/kubernetes-disaster-recovery/)\n\n### Uninstall\n\nDeleting the StorageCluster object for Portworx cluster does not stop Portworx\nservice running on the nodes, to avoid application downtime.\n\nTo uninstall Portworx completely without wiping the data, you should add the\nfollowing delete strategy to the StorageCluster spec:\n```\nspec:\n  deleteStrategy:\n    type: Uninstall\n```\n**Caution:** To uninstall Portworx and **wipe all the data**, you should use the following\ndelete strategy:\n```\nspec:\n  deleteStrategy:\n    type: UninstallAndWipe\n```\n",
      "csv_display_name": "Portworx Enterprise",
      "csv_metadata_description": "Cloud native storage solution for production workloads",
      "csv_name": "portworx-operator.v1.8.1",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": true,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-11-23T07:25:49.205000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.9",
      "organization": "certified-operators",
      "package": "portworx-certified",
      "provided_apis": [
        {
          "group": "core.libopenstorage.org",
          "kind": "StorageNode",
          "plural": "storagenodes",
          "version": "v1alpha1"
        },
        {
          "group": "core.libopenstorage.org",
          "kind": "StorageCluster",
          "plural": "storageclusters",
          "version": "v1"
        },
        {
          "group": "core.libopenstorage.org",
          "kind": "StorageCluster",
          "plural": "storageclusters",
          "version": "v1alpha1"
        },
        {
          "group": "core.libopenstorage.org",
          "kind": "StorageNode",
          "plural": "storagenodes",
          "version": "v1"
        }
      ],
      "provider": "Portworx",
      "related_images": [
        {
          "digest": "sha256:bb93cb53d57ed8277480f5cfb827b6b2041e639c8fceae2e501e91c201d7e0d9",
          "image": "registry.connect.redhat.com/portworx/openstorage-operator@sha256:bb93cb53d57ed8277480f5cfb827b6b2041e639c8fceae2e501e91c201d7e0d9",
          "name": "openstorage-operator-bb93cb53d57ed8277480f5cfb827b6b2041e639c8fceae2e501e91c201d7e0d9-annotation"
        },
        {
          "digest": "sha256:bb93cb53d57ed8277480f5cfb827b6b2041e639c8fceae2e501e91c201d7e0d9",
          "image": "registry.connect.redhat.com/portworx/openstorage-operator@sha256:bb93cb53d57ed8277480f5cfb827b6b2041e639c8fceae2e501e91c201d7e0d9",
          "name": "portworx-operator"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.9",
      "version": "1.8.1",
      "version_original": "1.8.1"
    },
    {
      "_id": "62b2b658409164f59aaf84e1",
      "alm_examples": [
        {
          "api_version": "core.libopenstorage.org/v1",
          "kind": "StorageCluster",
          "metadata": {
            "annotations": {
              "portworx.io/is-openshift": "true"
            },
            "name": "portworx",
            "namespace": "test-operator"
          },
          "spec": {}
        },
        {
          "api_version": "core.libopenstorage.org/v1",
          "kind": "StorageNode",
          "metadata": {
            "name": "example",
            "namespace": "test-operator"
          },
          "spec": {}
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/portworx/portworx-certified-bundle@sha256:2a717d5d940a152105e8f5dd46b3e527c19132fec7eb0982955f52c46166dfca",
      "bundle_path_digest": "sha256:2a717d5d940a152105e8f5dd46b3e527c19132fec7eb0982955f52c46166dfca",
      "capabilities": [
        "Auto Pilot"
      ],
      "channel_name": "stable",
      "creation_date": "2022-06-22T06:27:36.606000+00:00",
      "csv_description": "Portworx-Enterprise is the most widely-used and reliable cloud-native\nstorage solution for production workloads and provides high-availability,\ndata protection and security for containerized applications.\n\nPortworx Enterprise enables you to migrate entire applications, including\ndata, between clusters in a single data center or cloud, or between clouds,\nwith a single kubectl command.\n\nThe cloud native storage and data management platform that enterprises trust\nto manage data in containers now has an operator which simplifies the install,\nconfiguration, upgrades and manages the Portworx Enterprise cluster lifecycle.\n\nLearn more about the Portworx Enterprise\n[the data platform for Kubernetes](https://portworx.com/products/introduction)\n\nTo learn more about the platform features, please visit our\n[product features page](https://portworx.com/products/features)\n\n### About Portworx\n\nPortworx is the solution for running stateful containers in production,\ndesigned with DevOps in mind. With Portworx, users can manage any database\nor stateful service on any infrastructure using any container scheduler,\nincluding Kubernetes, Mesosphere DC/OS, and Docker Swarm. Portworx solves\nthe five most common problems DevOps teams encounter when running stateful\nservices in production: persistence, high availability, data automation,\nsecurity, and support for multiple data stores and infrastructure.\n\n### How to install StorageCluster\n\nTo customize your cluster's configuration (specification), use the\n[Spec Generator](https://central.portworx.com/) from PX-Central.\n\n### Prerequisite\n\nEnsure ports 17001-17020 on worker nodes are reachable from master and other worker nodes.\n\n### Tutorials\n\n* [Portworx Enterprise on Openshift](https://portworx.com/openshift)\n\n* [Stateful applications on Kubernetes](https://docs.portworx.com/portworx-install-with-kubernetes/application-install-with-kubernetes)\n\n* [Portworx Enterprise on Kubernetes](https://docs.portworx.com/portworx-install-with-kubernetes)\n\n* [Kafka on Kubernetes](https://portworx.com/kafka-kubernetes)\n\n* [Elastisearch on Kubernetes](https://portworx.com/elasticsearch-kubernetes)\n\n* [PostgreSQL on Kubernetes](https://portworx.com/postgres-kubernetes/)\n\n* [MongoDB on Kubernetes](https://portworx.com/mongodb-kubernetes/)\n\n* [Cassandra on Kubernetes](https://portworx.com/cassandra-kubernetes/)\n\n* [Kubernetes backup and recovery](https://portworx.com/kubernetes-backup/)\n\n* [Disaster Recovery for Kubernetes](https://portworx.com/kubernetes-disaster-recovery/)\n\n### Uninstall\n\nDeleting the StorageCluster object for Portworx cluster does not stop Portworx\nservice running on the nodes, to avoid application downtime.\n\nTo uninstall Portworx completely without wiping the data, you should add the\nfollowing delete strategy to the StorageCluster spec:\n```\nspec:\n  deleteStrategy:\n    type: Uninstall\n```\n**Caution:** To uninstall Portworx and **wipe all the data**, you should use the following\ndelete strategy:\n```\nspec:\n  deleteStrategy:\n    type: UninstallAndWipe\n```\n",
      "csv_display_name": "Portworx Enterprise",
      "csv_metadata_description": "Cloud native storage solution for production workloads",
      "csv_name": "portworx-operator.v1.8.1",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": true,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-11-23T07:42:51.521000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.8",
      "organization": "certified-operators",
      "package": "portworx-certified",
      "provided_apis": [
        {
          "group": "core.libopenstorage.org",
          "kind": "StorageCluster",
          "plural": "storageclusters",
          "version": "v1"
        },
        {
          "group": "core.libopenstorage.org",
          "kind": "StorageCluster",
          "plural": "storageclusters",
          "version": "v1alpha1"
        },
        {
          "group": "core.libopenstorage.org",
          "kind": "StorageNode",
          "plural": "storagenodes",
          "version": "v1"
        },
        {
          "group": "core.libopenstorage.org",
          "kind": "StorageNode",
          "plural": "storagenodes",
          "version": "v1alpha1"
        }
      ],
      "provider": "Portworx",
      "related_images": [
        {
          "digest": "sha256:bb93cb53d57ed8277480f5cfb827b6b2041e639c8fceae2e501e91c201d7e0d9",
          "image": "registry.connect.redhat.com/portworx/openstorage-operator@sha256:bb93cb53d57ed8277480f5cfb827b6b2041e639c8fceae2e501e91c201d7e0d9",
          "name": "openstorage-operator-bb93cb53d57ed8277480f5cfb827b6b2041e639c8fceae2e501e91c201d7e0d9-annotation"
        },
        {
          "digest": "sha256:bb93cb53d57ed8277480f5cfb827b6b2041e639c8fceae2e501e91c201d7e0d9",
          "image": "registry.connect.redhat.com/portworx/openstorage-operator@sha256:bb93cb53d57ed8277480f5cfb827b6b2041e639c8fceae2e501e91c201d7e0d9",
          "name": "portworx-operator"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.8",
      "version": "1.8.1",
      "version_original": "1.8.1"
    },
    {
      "_id": "62b2b65cabc56faca879ef69",
      "alm_examples": [
        {
          "api_version": "core.libopenstorage.org/v1",
          "kind": "StorageCluster",
          "metadata": {
            "annotations": {
              "portworx.io/is-openshift": "true"
            },
            "name": "portworx",
            "namespace": "test-operator"
          },
          "spec": {}
        },
        {
          "api_version": "core.libopenstorage.org/v1",
          "kind": "StorageNode",
          "metadata": {
            "name": "example",
            "namespace": "test-operator"
          },
          "spec": {}
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/portworx/portworx-certified-bundle@sha256:2a717d5d940a152105e8f5dd46b3e527c19132fec7eb0982955f52c46166dfca",
      "bundle_path_digest": "sha256:2a717d5d940a152105e8f5dd46b3e527c19132fec7eb0982955f52c46166dfca",
      "capabilities": [
        "Auto Pilot"
      ],
      "channel_name": "alpha",
      "creation_date": "2022-06-22T06:27:40.694000+00:00",
      "csv_description": "Portworx-Enterprise is the most widely-used and reliable cloud-native\nstorage solution for production workloads and provides high-availability,\ndata protection and security for containerized applications.\n\nPortworx Enterprise enables you to migrate entire applications, including\ndata, between clusters in a single data center or cloud, or between clouds,\nwith a single kubectl command.\n\nThe cloud native storage and data management platform that enterprises trust\nto manage data in containers now has an operator which simplifies the install,\nconfiguration, upgrades and manages the Portworx Enterprise cluster lifecycle.\n\nLearn more about the Portworx Enterprise\n[the data platform for Kubernetes](https://portworx.com/products/introduction)\n\nTo learn more about the platform features, please visit our\n[product features page](https://portworx.com/products/features)\n\n### About Portworx\n\nPortworx is the solution for running stateful containers in production,\ndesigned with DevOps in mind. With Portworx, users can manage any database\nor stateful service on any infrastructure using any container scheduler,\nincluding Kubernetes, Mesosphere DC/OS, and Docker Swarm. Portworx solves\nthe five most common problems DevOps teams encounter when running stateful\nservices in production: persistence, high availability, data automation,\nsecurity, and support for multiple data stores and infrastructure.\n\n### How to install StorageCluster\n\nTo customize your cluster's configuration (specification), use the\n[Spec Generator](https://central.portworx.com/) from PX-Central.\n\n### Prerequisite\n\nEnsure ports 17001-17020 on worker nodes are reachable from master and other worker nodes.\n\n### Tutorials\n\n* [Portworx Enterprise on Openshift](https://portworx.com/openshift)\n\n* [Stateful applications on Kubernetes](https://docs.portworx.com/portworx-install-with-kubernetes/application-install-with-kubernetes)\n\n* [Portworx Enterprise on Kubernetes](https://docs.portworx.com/portworx-install-with-kubernetes)\n\n* [Kafka on Kubernetes](https://portworx.com/kafka-kubernetes)\n\n* [Elastisearch on Kubernetes](https://portworx.com/elasticsearch-kubernetes)\n\n* [PostgreSQL on Kubernetes](https://portworx.com/postgres-kubernetes/)\n\n* [MongoDB on Kubernetes](https://portworx.com/mongodb-kubernetes/)\n\n* [Cassandra on Kubernetes](https://portworx.com/cassandra-kubernetes/)\n\n* [Kubernetes backup and recovery](https://portworx.com/kubernetes-backup/)\n\n* [Disaster Recovery for Kubernetes](https://portworx.com/kubernetes-disaster-recovery/)\n\n### Uninstall\n\nDeleting the StorageCluster object for Portworx cluster does not stop Portworx\nservice running on the nodes, to avoid application downtime.\n\nTo uninstall Portworx completely without wiping the data, you should add the\nfollowing delete strategy to the StorageCluster spec:\n```\nspec:\n  deleteStrategy:\n    type: Uninstall\n```\n**Caution:** To uninstall Portworx and **wipe all the data**, you should use the following\ndelete strategy:\n```\nspec:\n  deleteStrategy:\n    type: UninstallAndWipe\n```\n",
      "csv_display_name": "Portworx Enterprise",
      "csv_metadata_description": "Cloud native storage solution for production workloads",
      "csv_name": "portworx-operator.v1.8.1",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": true,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": false,
      "last_update_date": "2022-11-23T07:33:51.086000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.8",
      "organization": "certified-operators",
      "package": "portworx-certified",
      "provided_apis": [
        {
          "group": "core.libopenstorage.org",
          "kind": "StorageCluster",
          "plural": "storageclusters",
          "version": "v1"
        },
        {
          "group": "core.libopenstorage.org",
          "kind": "StorageCluster",
          "plural": "storageclusters",
          "version": "v1alpha1"
        },
        {
          "group": "core.libopenstorage.org",
          "kind": "StorageNode",
          "plural": "storagenodes",
          "version": "v1"
        },
        {
          "group": "core.libopenstorage.org",
          "kind": "StorageNode",
          "plural": "storagenodes",
          "version": "v1alpha1"
        }
      ],
      "provider": "Portworx",
      "related_images": [
        {
          "digest": "sha256:bb93cb53d57ed8277480f5cfb827b6b2041e639c8fceae2e501e91c201d7e0d9",
          "image": "registry.connect.redhat.com/portworx/openstorage-operator@sha256:bb93cb53d57ed8277480f5cfb827b6b2041e639c8fceae2e501e91c201d7e0d9",
          "name": "openstorage-operator-bb93cb53d57ed8277480f5cfb827b6b2041e639c8fceae2e501e91c201d7e0d9-annotation"
        },
        {
          "digest": "sha256:bb93cb53d57ed8277480f5cfb827b6b2041e639c8fceae2e501e91c201d7e0d9",
          "image": "registry.connect.redhat.com/portworx/openstorage-operator@sha256:bb93cb53d57ed8277480f5cfb827b6b2041e639c8fceae2e501e91c201d7e0d9",
          "name": "portworx-operator"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.8",
      "version": "1.8.1",
      "version_original": "1.8.1"
    },
    {
      "_id": "62b2b666f9bec1296a000073",
      "alm_examples": [
        {
          "api_version": "core.libopenstorage.org/v1",
          "kind": "StorageCluster",
          "metadata": {
            "annotations": {
              "portworx.io/is-openshift": "true"
            },
            "name": "portworx",
            "namespace": "test-operator"
          },
          "spec": {}
        },
        {
          "api_version": "core.libopenstorage.org/v1",
          "kind": "StorageNode",
          "metadata": {
            "name": "example",
            "namespace": "test-operator"
          },
          "spec": {}
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/portworx/portworx-certified-bundle@sha256:2a717d5d940a152105e8f5dd46b3e527c19132fec7eb0982955f52c46166dfca",
      "bundle_path_digest": "sha256:2a717d5d940a152105e8f5dd46b3e527c19132fec7eb0982955f52c46166dfca",
      "capabilities": [
        "Auto Pilot"
      ],
      "channel_name": "stable",
      "creation_date": "2022-06-22T06:27:50.358000+00:00",
      "csv_description": "Portworx-Enterprise is the most widely-used and reliable cloud-native\nstorage solution for production workloads and provides high-availability,\ndata protection and security for containerized applications.\n\nPortworx Enterprise enables you to migrate entire applications, including\ndata, between clusters in a single data center or cloud, or between clouds,\nwith a single kubectl command.\n\nThe cloud native storage and data management platform that enterprises trust\nto manage data in containers now has an operator which simplifies the install,\nconfiguration, upgrades and manages the Portworx Enterprise cluster lifecycle.\n\nLearn more about the Portworx Enterprise\n[the data platform for Kubernetes](https://portworx.com/products/introduction)\n\nTo learn more about the platform features, please visit our\n[product features page](https://portworx.com/products/features)\n\n### About Portworx\n\nPortworx is the solution for running stateful containers in production,\ndesigned with DevOps in mind. With Portworx, users can manage any database\nor stateful service on any infrastructure using any container scheduler,\nincluding Kubernetes, Mesosphere DC/OS, and Docker Swarm. Portworx solves\nthe five most common problems DevOps teams encounter when running stateful\nservices in production: persistence, high availability, data automation,\nsecurity, and support for multiple data stores and infrastructure.\n\n### How to install StorageCluster\n\nTo customize your cluster's configuration (specification), use the\n[Spec Generator](https://central.portworx.com/) from PX-Central.\n\n### Prerequisite\n\nEnsure ports 17001-17020 on worker nodes are reachable from master and other worker nodes.\n\n### Tutorials\n\n* [Portworx Enterprise on Openshift](https://portworx.com/openshift)\n\n* [Stateful applications on Kubernetes](https://docs.portworx.com/portworx-install-with-kubernetes/application-install-with-kubernetes)\n\n* [Portworx Enterprise on Kubernetes](https://docs.portworx.com/portworx-install-with-kubernetes)\n\n* [Kafka on Kubernetes](https://portworx.com/kafka-kubernetes)\n\n* [Elastisearch on Kubernetes](https://portworx.com/elasticsearch-kubernetes)\n\n* [PostgreSQL on Kubernetes](https://portworx.com/postgres-kubernetes/)\n\n* [MongoDB on Kubernetes](https://portworx.com/mongodb-kubernetes/)\n\n* [Cassandra on Kubernetes](https://portworx.com/cassandra-kubernetes/)\n\n* [Kubernetes backup and recovery](https://portworx.com/kubernetes-backup/)\n\n* [Disaster Recovery for Kubernetes](https://portworx.com/kubernetes-disaster-recovery/)\n\n### Uninstall\n\nDeleting the StorageCluster object for Portworx cluster does not stop Portworx\nservice running on the nodes, to avoid application downtime.\n\nTo uninstall Portworx completely without wiping the data, you should add the\nfollowing delete strategy to the StorageCluster spec:\n```\nspec:\n  deleteStrategy:\n    type: Uninstall\n```\n**Caution:** To uninstall Portworx and **wipe all the data**, you should use the following\ndelete strategy:\n```\nspec:\n  deleteStrategy:\n    type: UninstallAndWipe\n```\n",
      "csv_display_name": "Portworx Enterprise",
      "csv_metadata_description": "Cloud native storage solution for production workloads",
      "csv_name": "portworx-operator.v1.8.1",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": true,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-11-23T06:47:24.022000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.7",
      "organization": "certified-operators",
      "package": "portworx-certified",
      "provided_apis": [
        {
          "group": "core.libopenstorage.org",
          "kind": "StorageNode",
          "plural": "storagenodes",
          "version": "v1"
        },
        {
          "group": "core.libopenstorage.org",
          "kind": "StorageNode",
          "plural": "storagenodes",
          "version": "v1alpha1"
        },
        {
          "group": "core.libopenstorage.org",
          "kind": "StorageCluster",
          "plural": "storageclusters",
          "version": "v1"
        },
        {
          "group": "core.libopenstorage.org",
          "kind": "StorageCluster",
          "plural": "storageclusters",
          "version": "v1alpha1"
        }
      ],
      "provider": "Portworx",
      "related_images": [
        {
          "digest": "sha256:bb93cb53d57ed8277480f5cfb827b6b2041e639c8fceae2e501e91c201d7e0d9",
          "image": "registry.connect.redhat.com/portworx/openstorage-operator@sha256:bb93cb53d57ed8277480f5cfb827b6b2041e639c8fceae2e501e91c201d7e0d9",
          "name": "openstorage-operator-bb93cb53d57ed8277480f5cfb827b6b2041e639c8fceae2e501e91c201d7e0d9-annotation"
        },
        {
          "digest": "sha256:bb93cb53d57ed8277480f5cfb827b6b2041e639c8fceae2e501e91c201d7e0d9",
          "image": "registry.connect.redhat.com/portworx/openstorage-operator@sha256:bb93cb53d57ed8277480f5cfb827b6b2041e639c8fceae2e501e91c201d7e0d9",
          "name": "portworx-operator"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.7",
      "version": "1.8.1",
      "version_original": "1.8.1"
    },
    {
      "_id": "62b2b66a409164f59aaf84e3",
      "alm_examples": [
        {
          "api_version": "core.libopenstorage.org/v1",
          "kind": "StorageCluster",
          "metadata": {
            "annotations": {
              "portworx.io/is-openshift": "true"
            },
            "name": "portworx",
            "namespace": "test-operator"
          },
          "spec": {}
        },
        {
          "api_version": "core.libopenstorage.org/v1",
          "kind": "StorageNode",
          "metadata": {
            "name": "example",
            "namespace": "test-operator"
          },
          "spec": {}
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/portworx/portworx-certified-bundle@sha256:2a717d5d940a152105e8f5dd46b3e527c19132fec7eb0982955f52c46166dfca",
      "bundle_path_digest": "sha256:2a717d5d940a152105e8f5dd46b3e527c19132fec7eb0982955f52c46166dfca",
      "capabilities": [
        "Auto Pilot"
      ],
      "channel_name": "alpha",
      "creation_date": "2022-06-22T06:27:54.242000+00:00",
      "csv_description": "Portworx-Enterprise is the most widely-used and reliable cloud-native\nstorage solution for production workloads and provides high-availability,\ndata protection and security for containerized applications.\n\nPortworx Enterprise enables you to migrate entire applications, including\ndata, between clusters in a single data center or cloud, or between clouds,\nwith a single kubectl command.\n\nThe cloud native storage and data management platform that enterprises trust\nto manage data in containers now has an operator which simplifies the install,\nconfiguration, upgrades and manages the Portworx Enterprise cluster lifecycle.\n\nLearn more about the Portworx Enterprise\n[the data platform for Kubernetes](https://portworx.com/products/introduction)\n\nTo learn more about the platform features, please visit our\n[product features page](https://portworx.com/products/features)\n\n### About Portworx\n\nPortworx is the solution for running stateful containers in production,\ndesigned with DevOps in mind. With Portworx, users can manage any database\nor stateful service on any infrastructure using any container scheduler,\nincluding Kubernetes, Mesosphere DC/OS, and Docker Swarm. Portworx solves\nthe five most common problems DevOps teams encounter when running stateful\nservices in production: persistence, high availability, data automation,\nsecurity, and support for multiple data stores and infrastructure.\n\n### How to install StorageCluster\n\nTo customize your cluster's configuration (specification), use the\n[Spec Generator](https://central.portworx.com/) from PX-Central.\n\n### Prerequisite\n\nEnsure ports 17001-17020 on worker nodes are reachable from master and other worker nodes.\n\n### Tutorials\n\n* [Portworx Enterprise on Openshift](https://portworx.com/openshift)\n\n* [Stateful applications on Kubernetes](https://docs.portworx.com/portworx-install-with-kubernetes/application-install-with-kubernetes)\n\n* [Portworx Enterprise on Kubernetes](https://docs.portworx.com/portworx-install-with-kubernetes)\n\n* [Kafka on Kubernetes](https://portworx.com/kafka-kubernetes)\n\n* [Elastisearch on Kubernetes](https://portworx.com/elasticsearch-kubernetes)\n\n* [PostgreSQL on Kubernetes](https://portworx.com/postgres-kubernetes/)\n\n* [MongoDB on Kubernetes](https://portworx.com/mongodb-kubernetes/)\n\n* [Cassandra on Kubernetes](https://portworx.com/cassandra-kubernetes/)\n\n* [Kubernetes backup and recovery](https://portworx.com/kubernetes-backup/)\n\n* [Disaster Recovery for Kubernetes](https://portworx.com/kubernetes-disaster-recovery/)\n\n### Uninstall\n\nDeleting the StorageCluster object for Portworx cluster does not stop Portworx\nservice running on the nodes, to avoid application downtime.\n\nTo uninstall Portworx completely without wiping the data, you should add the\nfollowing delete strategy to the StorageCluster spec:\n```\nspec:\n  deleteStrategy:\n    type: Uninstall\n```\n**Caution:** To uninstall Portworx and **wipe all the data**, you should use the following\ndelete strategy:\n```\nspec:\n  deleteStrategy:\n    type: UninstallAndWipe\n```\n",
      "csv_display_name": "Portworx Enterprise",
      "csv_metadata_description": "Cloud native storage solution for production workloads",
      "csv_name": "portworx-operator.v1.8.1",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": true,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": false,
      "last_update_date": "2022-11-23T06:45:02.157000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.7",
      "organization": "certified-operators",
      "package": "portworx-certified",
      "provided_apis": [
        {
          "group": "core.libopenstorage.org",
          "kind": "StorageNode",
          "plural": "storagenodes",
          "version": "v1"
        },
        {
          "group": "core.libopenstorage.org",
          "kind": "StorageNode",
          "plural": "storagenodes",
          "version": "v1alpha1"
        },
        {
          "group": "core.libopenstorage.org",
          "kind": "StorageCluster",
          "plural": "storageclusters",
          "version": "v1"
        },
        {
          "group": "core.libopenstorage.org",
          "kind": "StorageCluster",
          "plural": "storageclusters",
          "version": "v1alpha1"
        }
      ],
      "provider": "Portworx",
      "related_images": [
        {
          "digest": "sha256:bb93cb53d57ed8277480f5cfb827b6b2041e639c8fceae2e501e91c201d7e0d9",
          "image": "registry.connect.redhat.com/portworx/openstorage-operator@sha256:bb93cb53d57ed8277480f5cfb827b6b2041e639c8fceae2e501e91c201d7e0d9",
          "name": "openstorage-operator-bb93cb53d57ed8277480f5cfb827b6b2041e639c8fceae2e501e91c201d7e0d9-annotation"
        },
        {
          "digest": "sha256:bb93cb53d57ed8277480f5cfb827b6b2041e639c8fceae2e501e91c201d7e0d9",
          "image": "registry.connect.redhat.com/portworx/openstorage-operator@sha256:bb93cb53d57ed8277480f5cfb827b6b2041e639c8fceae2e501e91c201d7e0d9",
          "name": "portworx-operator"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.7",
      "version": "1.8.1",
      "version_original": "1.8.1"
    },
    {
      "_id": "62b2b8a4409164f59aaf84f0",
      "alm_examples": [
        {
          "api_version": "core.libopenstorage.org/v1",
          "kind": "StorageCluster",
          "metadata": {
            "annotations": {
              "portworx.io/is-openshift": "true"
            },
            "name": "portworx",
            "namespace": "test-operator"
          },
          "spec": {}
        },
        {
          "api_version": "core.libopenstorage.org/v1",
          "kind": "StorageNode",
          "metadata": {
            "name": "example",
            "namespace": "test-operator"
          },
          "spec": {}
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/portworx/portworx-certified-bundle@sha256:2a717d5d940a152105e8f5dd46b3e527c19132fec7eb0982955f52c46166dfca",
      "bundle_path_digest": "sha256:2a717d5d940a152105e8f5dd46b3e527c19132fec7eb0982955f52c46166dfca",
      "capabilities": [
        "Auto Pilot"
      ],
      "channel_name": "stable",
      "creation_date": "2022-06-22T06:37:24.246000+00:00",
      "csv_description": "Portworx-Enterprise is the most widely-used and reliable cloud-native\nstorage solution for production workloads and provides high-availability,\ndata protection and security for containerized applications.\n\nPortworx Enterprise enables you to migrate entire applications, including\ndata, between clusters in a single data center or cloud, or between clouds,\nwith a single kubectl command.\n\nThe cloud native storage and data management platform that enterprises trust\nto manage data in containers now has an operator which simplifies the install,\nconfiguration, upgrades and manages the Portworx Enterprise cluster lifecycle.\n\nLearn more about the Portworx Enterprise\n[the data platform for Kubernetes](https://portworx.com/products/introduction)\n\nTo learn more about the platform features, please visit our\n[product features page](https://portworx.com/products/features)\n\n### About Portworx\n\nPortworx is the solution for running stateful containers in production,\ndesigned with DevOps in mind. With Portworx, users can manage any database\nor stateful service on any infrastructure using any container scheduler,\nincluding Kubernetes, Mesosphere DC/OS, and Docker Swarm. Portworx solves\nthe five most common problems DevOps teams encounter when running stateful\nservices in production: persistence, high availability, data automation,\nsecurity, and support for multiple data stores and infrastructure.\n\n### How to install StorageCluster\n\nTo customize your cluster's configuration (specification), use the\n[Spec Generator](https://central.portworx.com/) from PX-Central.\n\n### Prerequisite\n\nEnsure ports 17001-17020 on worker nodes are reachable from master and other worker nodes.\n\n### Tutorials\n\n* [Portworx Enterprise on Openshift](https://portworx.com/openshift)\n\n* [Stateful applications on Kubernetes](https://docs.portworx.com/portworx-install-with-kubernetes/application-install-with-kubernetes)\n\n* [Portworx Enterprise on Kubernetes](https://docs.portworx.com/portworx-install-with-kubernetes)\n\n* [Kafka on Kubernetes](https://portworx.com/kafka-kubernetes)\n\n* [Elastisearch on Kubernetes](https://portworx.com/elasticsearch-kubernetes)\n\n* [PostgreSQL on Kubernetes](https://portworx.com/postgres-kubernetes/)\n\n* [MongoDB on Kubernetes](https://portworx.com/mongodb-kubernetes/)\n\n* [Cassandra on Kubernetes](https://portworx.com/cassandra-kubernetes/)\n\n* [Kubernetes backup and recovery](https://portworx.com/kubernetes-backup/)\n\n* [Disaster Recovery for Kubernetes](https://portworx.com/kubernetes-disaster-recovery/)\n\n### Uninstall\n\nDeleting the StorageCluster object for Portworx cluster does not stop Portworx\nservice running on the nodes, to avoid application downtime.\n\nTo uninstall Portworx completely without wiping the data, you should add the\nfollowing delete strategy to the StorageCluster spec:\n```\nspec:\n  deleteStrategy:\n    type: Uninstall\n```\n**Caution:** To uninstall Portworx and **wipe all the data**, you should use the following\ndelete strategy:\n```\nspec:\n  deleteStrategy:\n    type: UninstallAndWipe\n```\n",
      "csv_display_name": "Portworx Enterprise",
      "csv_metadata_description": "Cloud native storage solution for production workloads",
      "csv_name": "portworx-operator.v1.8.1",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": true,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-11-23T07:29:29.624000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.6",
      "organization": "certified-operators",
      "package": "portworx-certified",
      "provided_apis": [
        {
          "group": "core.libopenstorage.org",
          "kind": "StorageNode",
          "plural": "storagenodes",
          "version": "v1alpha1"
        },
        {
          "group": "core.libopenstorage.org",
          "kind": "StorageCluster",
          "plural": "storageclusters",
          "version": "v1"
        },
        {
          "group": "core.libopenstorage.org",
          "kind": "StorageCluster",
          "plural": "storageclusters",
          "version": "v1alpha1"
        },
        {
          "group": "core.libopenstorage.org",
          "kind": "StorageNode",
          "plural": "storagenodes",
          "version": "v1"
        }
      ],
      "provider": "Portworx",
      "related_images": [
        {
          "digest": "sha256:bb93cb53d57ed8277480f5cfb827b6b2041e639c8fceae2e501e91c201d7e0d9",
          "image": "registry.connect.redhat.com/portworx/openstorage-operator@sha256:bb93cb53d57ed8277480f5cfb827b6b2041e639c8fceae2e501e91c201d7e0d9",
          "name": "openstorage-operator-bb93cb53d57ed8277480f5cfb827b6b2041e639c8fceae2e501e91c201d7e0d9-annotation"
        },
        {
          "digest": "sha256:bb93cb53d57ed8277480f5cfb827b6b2041e639c8fceae2e501e91c201d7e0d9",
          "image": "registry.connect.redhat.com/portworx/openstorage-operator@sha256:bb93cb53d57ed8277480f5cfb827b6b2041e639c8fceae2e501e91c201d7e0d9",
          "name": "portworx-operator"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.6",
      "version": "1.8.1",
      "version_original": "1.8.1"
    },
    {
      "_id": "62b2b8a83b84323f9e3d73fc",
      "alm_examples": [
        {
          "api_version": "core.libopenstorage.org/v1",
          "kind": "StorageCluster",
          "metadata": {
            "annotations": {
              "portworx.io/is-openshift": "true"
            },
            "name": "portworx",
            "namespace": "test-operator"
          },
          "spec": {}
        },
        {
          "api_version": "core.libopenstorage.org/v1",
          "kind": "StorageNode",
          "metadata": {
            "name": "example",
            "namespace": "test-operator"
          },
          "spec": {}
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/portworx/portworx-certified-bundle@sha256:2a717d5d940a152105e8f5dd46b3e527c19132fec7eb0982955f52c46166dfca",
      "bundle_path_digest": "sha256:2a717d5d940a152105e8f5dd46b3e527c19132fec7eb0982955f52c46166dfca",
      "capabilities": [
        "Auto Pilot"
      ],
      "channel_name": "alpha",
      "creation_date": "2022-06-22T06:37:28.062000+00:00",
      "csv_description": "Portworx-Enterprise is the most widely-used and reliable cloud-native\nstorage solution for production workloads and provides high-availability,\ndata protection and security for containerized applications.\n\nPortworx Enterprise enables you to migrate entire applications, including\ndata, between clusters in a single data center or cloud, or between clouds,\nwith a single kubectl command.\n\nThe cloud native storage and data management platform that enterprises trust\nto manage data in containers now has an operator which simplifies the install,\nconfiguration, upgrades and manages the Portworx Enterprise cluster lifecycle.\n\nLearn more about the Portworx Enterprise\n[the data platform for Kubernetes](https://portworx.com/products/introduction)\n\nTo learn more about the platform features, please visit our\n[product features page](https://portworx.com/products/features)\n\n### About Portworx\n\nPortworx is the solution for running stateful containers in production,\ndesigned with DevOps in mind. With Portworx, users can manage any database\nor stateful service on any infrastructure using any container scheduler,\nincluding Kubernetes, Mesosphere DC/OS, and Docker Swarm. Portworx solves\nthe five most common problems DevOps teams encounter when running stateful\nservices in production: persistence, high availability, data automation,\nsecurity, and support for multiple data stores and infrastructure.\n\n### How to install StorageCluster\n\nTo customize your cluster's configuration (specification), use the\n[Spec Generator](https://central.portworx.com/) from PX-Central.\n\n### Prerequisite\n\nEnsure ports 17001-17020 on worker nodes are reachable from master and other worker nodes.\n\n### Tutorials\n\n* [Portworx Enterprise on Openshift](https://portworx.com/openshift)\n\n* [Stateful applications on Kubernetes](https://docs.portworx.com/portworx-install-with-kubernetes/application-install-with-kubernetes)\n\n* [Portworx Enterprise on Kubernetes](https://docs.portworx.com/portworx-install-with-kubernetes)\n\n* [Kafka on Kubernetes](https://portworx.com/kafka-kubernetes)\n\n* [Elastisearch on Kubernetes](https://portworx.com/elasticsearch-kubernetes)\n\n* [PostgreSQL on Kubernetes](https://portworx.com/postgres-kubernetes/)\n\n* [MongoDB on Kubernetes](https://portworx.com/mongodb-kubernetes/)\n\n* [Cassandra on Kubernetes](https://portworx.com/cassandra-kubernetes/)\n\n* [Kubernetes backup and recovery](https://portworx.com/kubernetes-backup/)\n\n* [Disaster Recovery for Kubernetes](https://portworx.com/kubernetes-disaster-recovery/)\n\n### Uninstall\n\nDeleting the StorageCluster object for Portworx cluster does not stop Portworx\nservice running on the nodes, to avoid application downtime.\n\nTo uninstall Portworx completely without wiping the data, you should add the\nfollowing delete strategy to the StorageCluster spec:\n```\nspec:\n  deleteStrategy:\n    type: Uninstall\n```\n**Caution:** To uninstall Portworx and **wipe all the data**, you should use the following\ndelete strategy:\n```\nspec:\n  deleteStrategy:\n    type: UninstallAndWipe\n```\n",
      "csv_display_name": "Portworx Enterprise",
      "csv_metadata_description": "Cloud native storage solution for production workloads",
      "csv_name": "portworx-operator.v1.8.1",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": true,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": false,
      "last_update_date": "2022-11-23T07:25:48.975000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.6",
      "organization": "certified-operators",
      "package": "portworx-certified",
      "provided_apis": [
        {
          "group": "core.libopenstorage.org",
          "kind": "StorageNode",
          "plural": "storagenodes",
          "version": "v1alpha1"
        },
        {
          "group": "core.libopenstorage.org",
          "kind": "StorageCluster",
          "plural": "storageclusters",
          "version": "v1"
        },
        {
          "group": "core.libopenstorage.org",
          "kind": "StorageCluster",
          "plural": "storageclusters",
          "version": "v1alpha1"
        },
        {
          "group": "core.libopenstorage.org",
          "kind": "StorageNode",
          "plural": "storagenodes",
          "version": "v1"
        }
      ],
      "provider": "Portworx",
      "related_images": [
        {
          "digest": "sha256:bb93cb53d57ed8277480f5cfb827b6b2041e639c8fceae2e501e91c201d7e0d9",
          "image": "registry.connect.redhat.com/portworx/openstorage-operator@sha256:bb93cb53d57ed8277480f5cfb827b6b2041e639c8fceae2e501e91c201d7e0d9",
          "name": "openstorage-operator-bb93cb53d57ed8277480f5cfb827b6b2041e639c8fceae2e501e91c201d7e0d9-annotation"
        },
        {
          "digest": "sha256:bb93cb53d57ed8277480f5cfb827b6b2041e639c8fceae2e501e91c201d7e0d9",
          "image": "registry.connect.redhat.com/portworx/openstorage-operator@sha256:bb93cb53d57ed8277480f5cfb827b6b2041e639c8fceae2e501e91c201d7e0d9",
          "name": "portworx-operator"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.6",
      "version": "1.8.1",
      "version_original": "1.8.1"
    },
    {
      "_id": "62b35a90dd92c1f1f12c2fe8",
      "alm_examples": [
        {
          "api_version": "postgres-operator.crunchydata.com/v1beta1",
          "kind": "PostgresCluster",
          "metadata": {
            "name": "example"
          },
          "spec": {
            "instances": [
              {
                "dataVolumeClaimSpec": {
                  "accessModes": [
                    "ReadWriteOnce"
                  ],
                  "resources": {
                    "requests": {
                      "storage": "1Gi"
                    }
                  }
                },
                "replicas": 1
              }
            ],
            "postgresVersion": 13
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/crunchydata/postgres-operator-bundle@sha256:4fdda557b9826951dce70c57792d906f1366c51c114dbcd8c7c9d2217d35c902",
      "bundle_path_digest": "sha256:4fdda557b9826951dce70c57792d906f1366c51c114dbcd8c7c9d2217d35c902",
      "capabilities": [
        "Auto Pilot"
      ],
      "channel_name": "v5",
      "creation_date": "2022-06-22T18:08:16.912000+00:00",
      "csv_description": "[PGO](https://github.com/CrunchyData/postgres-operator), the\n[Postgres Operator](https://github.com/CrunchyData/postgres-operator) from\n[Crunchy Data](https://www.crunchydata.com), gives you a **declarative Postgres** solution that\nautomatically manages your [PostgreSQL](https://www.postgresql.org) clusters.\n\nDesigned for your GitOps workflows, it is [easy to get started](https://access.crunchydata.com/documentation/postgres-operator/v5/quickstart/)\nwith Postgres on Kubernetes with PGO. Within a few moments, you can have a production grade Postgres\ncluster complete with high availability, disaster recovery, and monitoring, all over secure TLS communications.\nEven better, PGO lets you easily customize your Postgres cluster to tailor it to your workload!\n\nWith conveniences like cloning Postgres clusters to using rolling updates to roll out disruptive\nchanges with minimal downtime, PGO is ready to support your Postgres data at every stage of your\nrelease pipeline. Built for resiliency and uptime, PGO will keep your desired Postgres in a desired\nstate so you do not need to worry about it.\n\nPGO is developed with many years of production experience in automating Postgres management on\nKubernetes, providing a seamless cloud native Postgres solution to keep your data always available.\n\n- **PostgreSQL Cluster Provisioning**: [Create, Scale, & Delete PostgreSQL clusters with ease][provisioning],\n  while fully customizing your Pods and PostgreSQL configuration!\n- **High-Availability**: Safe, automated failover backed by a [distributed consensus based high-availability solution][high-availability].\n  Uses [Pod Anti-Affinity][k8s-anti-affinity] to help resiliency; you can configure how aggressive this can be!\n  Failed primaries automatically heal, allowing for faster recovery time. You can even create regularly scheduled\n  backups as well and set your backup retention policy\n- **Disaster Recovery**: [Backups][backups] and [restores][disaster-recovery] leverage the open source [pgBackRest][] utility and\n  [includes support for full, incremental, and differential backups as well as efficient delta restores][backups].\n  Set how long you want your backups retained for. Works great with very large databases!\n- **Monitoring**: [Track the health of your PostgreSQL clusters][monitoring] using the open source [pgMonitor][] library.\n- **Clone**: [Create new clusters from your existing clusters or backups][clone] with efficient data cloning.\n- **TLS**: All connections are over [TLS][tls]. You can also [bring your own TLS infrastructure][tls] if you do not want to use the provided defaults.\n- **Connection Pooling**: Advanced [connection pooling][pool] support using [pgBouncer][].\n- **Affinity and Tolerations**: Have your PostgreSQL clusters deployed to [Kubernetes Nodes][k8s-nodes] of your preference.\n  Set your [pod anti-affinity][k8s-anti-affinity], node affinity, Pod tolerations and more rules to customize your deployment topology!\n- **Full Customizability**: Crunchy PostgreSQL for Kubernetes makes it easy to get your own PostgreSQL-as-a-Service up and running\n  and fully customize your deployments, including:\n    - Choose the resources for your Postgres cluster: [container resources and storage size][resize-cluster]. [Resize at any time][resize-cluster] with minimal disruption.\n    - Use your own container image repository, including support `imagePullSecrets` and private repositories\n    - [Customize your PostgreSQL configuration][customize-cluster]\n\nand much more!\n\n[backups]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/backups/\n[clone]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/disaster-recovery/#clone-a-postgres-cluster\n[customize-cluster]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/customize-cluster/\n[disaster-recovery]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/disaster-recovery/\n[high-availability]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/high-availability/\n[monitoring]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/monitoring/\n[pool]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/connection-pooling/\n[provisioning]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/create-cluster/\n[resize-cluster]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/resize-cluster/\n[tls]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/customize-cluster/#customize-tls\n\n[k8s-anti-affinity]: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#inter-pod-affinity-and-anti-affinity\n[k8s-nodes]: https://kubernetes.io/docs/concepts/architecture/nodes/\n\n[pgBackRest]: https://www.pgbackrest.org\n[pgBouncer]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/connection-pooling/\n[pgMonitor]: https://github.com/CrunchyData/pgmonitor\n\n\n## Post-Installation\n\n### Tutorial\n\nWant to [learn more about the PostgreSQL Operator][tutorial]? Browse through the [tutorial][] to learn more about what you can do!\n\n[tutorial]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial",
      "csv_display_name": "Crunchy Postgres for Kubernetes",
      "csv_metadata_description": "Production Postgres Made Easy",
      "csv_name": "postgresoperator.v5.1.1",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-11-23T07:13:32.987000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.11",
      "organization": "certified-operators",
      "package": "crunchy-postgres-operator",
      "provided_apis": [
        {
          "group": "postgres-operator.crunchydata.com",
          "kind": "PostgresCluster",
          "version": "v1beta1"
        }
      ],
      "provider": "Crunchy Data",
      "related_images": [
        {
          "digest": "sha256:7113af9768b12c74373ad350178c7a2db56df9da7ccb08b5c6b6cbd32220e134",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-pgadmin4@sha256:7113af9768b12c74373ad350178c7a2db56df9da7ccb08b5c6b6cbd32220e134",
          "name": "PGADMIN"
        },
        {
          "digest": "sha256:ddce52cf45f3a7d850163687836d565dc1b0671424b2044778645d108995a480",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-pgbackrest@sha256:ddce52cf45f3a7d850163687836d565dc1b0671424b2044778645d108995a480",
          "name": "PGBACKREST"
        },
        {
          "digest": "sha256:b7aeb28b7aacf76aad8b67037d47c64400d31a281076557f4dfd72a7e09d2a9e",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-pgbouncer@sha256:b7aeb28b7aacf76aad8b67037d47c64400d31a281076557f4dfd72a7e09d2a9e",
          "name": "PGBOUNCER"
        },
        {
          "digest": "sha256:694a2e85412f82fe444c7a605c2a16c5b6fe07689e7eb919477ae0f19a54e610",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres-exporter@sha256:694a2e85412f82fe444c7a605c2a16c5b6fe07689e7eb919477ae0f19a54e610",
          "name": "PGEXPORTER"
        },
        {
          "digest": "sha256:af0557c9dce647181b8071f5b18f51818a460ea116cbdd22c65a2f0129cf1408",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres@sha256:af0557c9dce647181b8071f5b18f51818a460ea116cbdd22c65a2f0129cf1408",
          "name": "POSTGRES_13"
        },
        {
          "digest": "sha256:030dd4c2535b4ec725006efff074faafc835bd87caeaf0e8cd11d7af885e601c",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres@sha256:030dd4c2535b4ec725006efff074faafc835bd87caeaf0e8cd11d7af885e601c",
          "name": "POSTGRES_14"
        },
        {
          "digest": "sha256:430a39a4815f59e9ee92b1db6fb1ed516bc69a40500bf7e8d9d43b5972f0d058",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres-gis@sha256:430a39a4815f59e9ee92b1db6fb1ed516bc69a40500bf7e8d9d43b5972f0d058",
          "name": "POSTGRES_13_GIS_3.0"
        },
        {
          "digest": "sha256:20baa15efefeae142b8ab9034f1956203f6ff2397b1ca53ffe86ddbf95977891",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres-gis@sha256:20baa15efefeae142b8ab9034f1956203f6ff2397b1ca53ffe86ddbf95977891",
          "name": "POSTGRES_13_GIS_3.1"
        },
        {
          "digest": "sha256:1de1072082f58449d9d181ddaf0026ffe00c5961e982aef6558a7ac4314581e2",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres-gis@sha256:1de1072082f58449d9d181ddaf0026ffe00c5961e982aef6558a7ac4314581e2",
          "name": "POSTGRES_14_GIS_3.1"
        },
        {
          "digest": "sha256:e26176f740c5b1fef9dbcc11554062209c22b41f591c9d31d74681b686f6598b",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres-gis@sha256:e26176f740c5b1fef9dbcc11554062209c22b41f591c9d31d74681b686f6598b",
          "name": "POSTGRES_14_GIS_3.2"
        },
        {
          "digest": "sha256:b4dba56b93e7e9eb82ba8fb2dcdcf7cbe08dc16dfcbd8d42bd6d8c86488f6495",
          "image": "registry.connect.redhat.com/crunchydata/postgres-operator@sha256:b4dba56b93e7e9eb82ba8fb2dcdcf7cbe08dc16dfcbd8d42bd6d8c86488f6495",
          "name": "postgres-operator"
        },
        {
          "digest": "sha256:b4dba56b93e7e9eb82ba8fb2dcdcf7cbe08dc16dfcbd8d42bd6d8c86488f6495",
          "image": "registry.connect.redhat.com/crunchydata/postgres-operator@sha256:b4dba56b93e7e9eb82ba8fb2dcdcf7cbe08dc16dfcbd8d42bd6d8c86488f6495",
          "name": "postgres-operator-b4dba56b93e7e9eb82ba8fb2dcdcf7cbe08dc16dfcbd8d42bd6d8c86488f6495-annotation"
        },
        {
          "digest": "sha256:b4dba56b93e7e9eb82ba8fb2dcdcf7cbe08dc16dfcbd8d42bd6d8c86488f6495",
          "image": "registry.connect.redhat.com/crunchydata/postgres-operator@sha256:b4dba56b93e7e9eb82ba8fb2dcdcf7cbe08dc16dfcbd8d42bd6d8c86488f6495",
          "name": "operator"
        },
        {
          "digest": "sha256:7113af9768b12c74373ad350178c7a2db56df9da7ccb08b5c6b6cbd32220e134",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-pgadmin4@sha256:7113af9768b12c74373ad350178c7a2db56df9da7ccb08b5c6b6cbd32220e134",
          "name": "pgadmin"
        },
        {
          "digest": "sha256:ddce52cf45f3a7d850163687836d565dc1b0671424b2044778645d108995a480",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-pgbackrest@sha256:ddce52cf45f3a7d850163687836d565dc1b0671424b2044778645d108995a480",
          "name": "pgbackrest"
        },
        {
          "digest": "sha256:b7aeb28b7aacf76aad8b67037d47c64400d31a281076557f4dfd72a7e09d2a9e",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-pgbouncer@sha256:b7aeb28b7aacf76aad8b67037d47c64400d31a281076557f4dfd72a7e09d2a9e",
          "name": "pgbouncer"
        },
        {
          "digest": "sha256:694a2e85412f82fe444c7a605c2a16c5b6fe07689e7eb919477ae0f19a54e610",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres-exporter@sha256:694a2e85412f82fe444c7a605c2a16c5b6fe07689e7eb919477ae0f19a54e610",
          "name": "pgexporter"
        },
        {
          "digest": "sha256:af0557c9dce647181b8071f5b18f51818a460ea116cbdd22c65a2f0129cf1408",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres@sha256:af0557c9dce647181b8071f5b18f51818a460ea116cbdd22c65a2f0129cf1408",
          "name": "postgres_13"
        },
        {
          "digest": "sha256:030dd4c2535b4ec725006efff074faafc835bd87caeaf0e8cd11d7af885e601c",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres@sha256:030dd4c2535b4ec725006efff074faafc835bd87caeaf0e8cd11d7af885e601c",
          "name": "postgres_14"
        },
        {
          "digest": "sha256:430a39a4815f59e9ee92b1db6fb1ed516bc69a40500bf7e8d9d43b5972f0d058",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres-gis@sha256:430a39a4815f59e9ee92b1db6fb1ed516bc69a40500bf7e8d9d43b5972f0d058",
          "name": "postgres_13_gis_3.0"
        },
        {
          "digest": "sha256:20baa15efefeae142b8ab9034f1956203f6ff2397b1ca53ffe86ddbf95977891",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres-gis@sha256:20baa15efefeae142b8ab9034f1956203f6ff2397b1ca53ffe86ddbf95977891",
          "name": "postgres_13_gis_3.1"
        },
        {
          "digest": "sha256:1de1072082f58449d9d181ddaf0026ffe00c5961e982aef6558a7ac4314581e2",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres-gis@sha256:1de1072082f58449d9d181ddaf0026ffe00c5961e982aef6558a7ac4314581e2",
          "name": "postgres_14_gis_3.1"
        },
        {
          "digest": "sha256:e26176f740c5b1fef9dbcc11554062209c22b41f591c9d31d74681b686f6598b",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres-gis@sha256:e26176f740c5b1fef9dbcc11554062209c22b41f591c9d31d74681b686f6598b",
          "name": "postgres_14_gis_3.2"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.11",
      "version": "5.1.1",
      "version_original": "5.1.1"
    },
    {
      "_id": "62b35a96150d8182de906018",
      "alm_examples": [
        {
          "api_version": "postgres-operator.crunchydata.com/v1beta1",
          "kind": "PostgresCluster",
          "metadata": {
            "name": "example"
          },
          "spec": {
            "instances": [
              {
                "dataVolumeClaimSpec": {
                  "accessModes": [
                    "ReadWriteOnce"
                  ],
                  "resources": {
                    "requests": {
                      "storage": "1Gi"
                    }
                  }
                },
                "replicas": 1
              }
            ],
            "postgresVersion": 13
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/crunchydata/postgres-operator-bundle@sha256:4fdda557b9826951dce70c57792d906f1366c51c114dbcd8c7c9d2217d35c902",
      "bundle_path_digest": "sha256:4fdda557b9826951dce70c57792d906f1366c51c114dbcd8c7c9d2217d35c902",
      "capabilities": [
        "Auto Pilot"
      ],
      "channel_name": "v5",
      "creation_date": "2022-06-22T18:08:22.522000+00:00",
      "csv_description": "[PGO](https://github.com/CrunchyData/postgres-operator), the\n[Postgres Operator](https://github.com/CrunchyData/postgres-operator) from\n[Crunchy Data](https://www.crunchydata.com), gives you a **declarative Postgres** solution that\nautomatically manages your [PostgreSQL](https://www.postgresql.org) clusters.\n\nDesigned for your GitOps workflows, it is [easy to get started](https://access.crunchydata.com/documentation/postgres-operator/v5/quickstart/)\nwith Postgres on Kubernetes with PGO. Within a few moments, you can have a production grade Postgres\ncluster complete with high availability, disaster recovery, and monitoring, all over secure TLS communications.\nEven better, PGO lets you easily customize your Postgres cluster to tailor it to your workload!\n\nWith conveniences like cloning Postgres clusters to using rolling updates to roll out disruptive\nchanges with minimal downtime, PGO is ready to support your Postgres data at every stage of your\nrelease pipeline. Built for resiliency and uptime, PGO will keep your desired Postgres in a desired\nstate so you do not need to worry about it.\n\nPGO is developed with many years of production experience in automating Postgres management on\nKubernetes, providing a seamless cloud native Postgres solution to keep your data always available.\n\n- **PostgreSQL Cluster Provisioning**: [Create, Scale, & Delete PostgreSQL clusters with ease][provisioning],\n  while fully customizing your Pods and PostgreSQL configuration!\n- **High-Availability**: Safe, automated failover backed by a [distributed consensus based high-availability solution][high-availability].\n  Uses [Pod Anti-Affinity][k8s-anti-affinity] to help resiliency; you can configure how aggressive this can be!\n  Failed primaries automatically heal, allowing for faster recovery time. You can even create regularly scheduled\n  backups as well and set your backup retention policy\n- **Disaster Recovery**: [Backups][backups] and [restores][disaster-recovery] leverage the open source [pgBackRest][] utility and\n  [includes support for full, incremental, and differential backups as well as efficient delta restores][backups].\n  Set how long you want your backups retained for. Works great with very large databases!\n- **Monitoring**: [Track the health of your PostgreSQL clusters][monitoring] using the open source [pgMonitor][] library.\n- **Clone**: [Create new clusters from your existing clusters or backups][clone] with efficient data cloning.\n- **TLS**: All connections are over [TLS][tls]. You can also [bring your own TLS infrastructure][tls] if you do not want to use the provided defaults.\n- **Connection Pooling**: Advanced [connection pooling][pool] support using [pgBouncer][].\n- **Affinity and Tolerations**: Have your PostgreSQL clusters deployed to [Kubernetes Nodes][k8s-nodes] of your preference.\n  Set your [pod anti-affinity][k8s-anti-affinity], node affinity, Pod tolerations and more rules to customize your deployment topology!\n- **Full Customizability**: Crunchy PostgreSQL for Kubernetes makes it easy to get your own PostgreSQL-as-a-Service up and running\n  and fully customize your deployments, including:\n    - Choose the resources for your Postgres cluster: [container resources and storage size][resize-cluster]. [Resize at any time][resize-cluster] with minimal disruption.\n    - Use your own container image repository, including support `imagePullSecrets` and private repositories\n    - [Customize your PostgreSQL configuration][customize-cluster]\n\nand much more!\n\n[backups]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/backups/\n[clone]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/disaster-recovery/#clone-a-postgres-cluster\n[customize-cluster]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/customize-cluster/\n[disaster-recovery]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/disaster-recovery/\n[high-availability]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/high-availability/\n[monitoring]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/monitoring/\n[pool]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/connection-pooling/\n[provisioning]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/create-cluster/\n[resize-cluster]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/resize-cluster/\n[tls]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/customize-cluster/#customize-tls\n\n[k8s-anti-affinity]: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#inter-pod-affinity-and-anti-affinity\n[k8s-nodes]: https://kubernetes.io/docs/concepts/architecture/nodes/\n\n[pgBackRest]: https://www.pgbackrest.org\n[pgBouncer]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/connection-pooling/\n[pgMonitor]: https://github.com/CrunchyData/pgmonitor\n\n\n## Post-Installation\n\n### Tutorial\n\nWant to [learn more about the PostgreSQL Operator][tutorial]? Browse through the [tutorial][] to learn more about what you can do!\n\n[tutorial]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial",
      "csv_display_name": "Crunchy Postgres for Kubernetes",
      "csv_metadata_description": "Production Postgres Made Easy",
      "csv_name": "postgresoperator.v5.1.1",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-11-23T08:50:44.397000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.10",
      "organization": "certified-operators",
      "package": "crunchy-postgres-operator",
      "provided_apis": [
        {
          "group": "postgres-operator.crunchydata.com",
          "kind": "PostgresCluster",
          "plural": "postgresclusters",
          "version": "v1beta1"
        }
      ],
      "provider": "Crunchy Data",
      "related_images": [
        {
          "digest": "sha256:7113af9768b12c74373ad350178c7a2db56df9da7ccb08b5c6b6cbd32220e134",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-pgadmin4@sha256:7113af9768b12c74373ad350178c7a2db56df9da7ccb08b5c6b6cbd32220e134",
          "name": "PGADMIN"
        },
        {
          "digest": "sha256:ddce52cf45f3a7d850163687836d565dc1b0671424b2044778645d108995a480",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-pgbackrest@sha256:ddce52cf45f3a7d850163687836d565dc1b0671424b2044778645d108995a480",
          "name": "PGBACKREST"
        },
        {
          "digest": "sha256:b7aeb28b7aacf76aad8b67037d47c64400d31a281076557f4dfd72a7e09d2a9e",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-pgbouncer@sha256:b7aeb28b7aacf76aad8b67037d47c64400d31a281076557f4dfd72a7e09d2a9e",
          "name": "PGBOUNCER"
        },
        {
          "digest": "sha256:694a2e85412f82fe444c7a605c2a16c5b6fe07689e7eb919477ae0f19a54e610",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres-exporter@sha256:694a2e85412f82fe444c7a605c2a16c5b6fe07689e7eb919477ae0f19a54e610",
          "name": "PGEXPORTER"
        },
        {
          "digest": "sha256:af0557c9dce647181b8071f5b18f51818a460ea116cbdd22c65a2f0129cf1408",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres@sha256:af0557c9dce647181b8071f5b18f51818a460ea116cbdd22c65a2f0129cf1408",
          "name": "POSTGRES_13"
        },
        {
          "digest": "sha256:030dd4c2535b4ec725006efff074faafc835bd87caeaf0e8cd11d7af885e601c",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres@sha256:030dd4c2535b4ec725006efff074faafc835bd87caeaf0e8cd11d7af885e601c",
          "name": "POSTGRES_14"
        },
        {
          "digest": "sha256:430a39a4815f59e9ee92b1db6fb1ed516bc69a40500bf7e8d9d43b5972f0d058",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres-gis@sha256:430a39a4815f59e9ee92b1db6fb1ed516bc69a40500bf7e8d9d43b5972f0d058",
          "name": "POSTGRES_13_GIS_3.0"
        },
        {
          "digest": "sha256:20baa15efefeae142b8ab9034f1956203f6ff2397b1ca53ffe86ddbf95977891",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres-gis@sha256:20baa15efefeae142b8ab9034f1956203f6ff2397b1ca53ffe86ddbf95977891",
          "name": "POSTGRES_13_GIS_3.1"
        },
        {
          "digest": "sha256:1de1072082f58449d9d181ddaf0026ffe00c5961e982aef6558a7ac4314581e2",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres-gis@sha256:1de1072082f58449d9d181ddaf0026ffe00c5961e982aef6558a7ac4314581e2",
          "name": "POSTGRES_14_GIS_3.1"
        },
        {
          "digest": "sha256:e26176f740c5b1fef9dbcc11554062209c22b41f591c9d31d74681b686f6598b",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres-gis@sha256:e26176f740c5b1fef9dbcc11554062209c22b41f591c9d31d74681b686f6598b",
          "name": "POSTGRES_14_GIS_3.2"
        },
        {
          "digest": "sha256:b4dba56b93e7e9eb82ba8fb2dcdcf7cbe08dc16dfcbd8d42bd6d8c86488f6495",
          "image": "registry.connect.redhat.com/crunchydata/postgres-operator@sha256:b4dba56b93e7e9eb82ba8fb2dcdcf7cbe08dc16dfcbd8d42bd6d8c86488f6495",
          "name": "postgres-operator"
        },
        {
          "digest": "sha256:b4dba56b93e7e9eb82ba8fb2dcdcf7cbe08dc16dfcbd8d42bd6d8c86488f6495",
          "image": "registry.connect.redhat.com/crunchydata/postgres-operator@sha256:b4dba56b93e7e9eb82ba8fb2dcdcf7cbe08dc16dfcbd8d42bd6d8c86488f6495",
          "name": "postgres-operator-b4dba56b93e7e9eb82ba8fb2dcdcf7cbe08dc16dfcbd8d42bd6d8c86488f6495-annotation"
        },
        {
          "digest": "sha256:b4dba56b93e7e9eb82ba8fb2dcdcf7cbe08dc16dfcbd8d42bd6d8c86488f6495",
          "image": "registry.connect.redhat.com/crunchydata/postgres-operator@sha256:b4dba56b93e7e9eb82ba8fb2dcdcf7cbe08dc16dfcbd8d42bd6d8c86488f6495",
          "name": "operator"
        },
        {
          "digest": "sha256:7113af9768b12c74373ad350178c7a2db56df9da7ccb08b5c6b6cbd32220e134",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-pgadmin4@sha256:7113af9768b12c74373ad350178c7a2db56df9da7ccb08b5c6b6cbd32220e134",
          "name": "pgadmin"
        },
        {
          "digest": "sha256:ddce52cf45f3a7d850163687836d565dc1b0671424b2044778645d108995a480",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-pgbackrest@sha256:ddce52cf45f3a7d850163687836d565dc1b0671424b2044778645d108995a480",
          "name": "pgbackrest"
        },
        {
          "digest": "sha256:b7aeb28b7aacf76aad8b67037d47c64400d31a281076557f4dfd72a7e09d2a9e",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-pgbouncer@sha256:b7aeb28b7aacf76aad8b67037d47c64400d31a281076557f4dfd72a7e09d2a9e",
          "name": "pgbouncer"
        },
        {
          "digest": "sha256:694a2e85412f82fe444c7a605c2a16c5b6fe07689e7eb919477ae0f19a54e610",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres-exporter@sha256:694a2e85412f82fe444c7a605c2a16c5b6fe07689e7eb919477ae0f19a54e610",
          "name": "pgexporter"
        },
        {
          "digest": "sha256:af0557c9dce647181b8071f5b18f51818a460ea116cbdd22c65a2f0129cf1408",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres@sha256:af0557c9dce647181b8071f5b18f51818a460ea116cbdd22c65a2f0129cf1408",
          "name": "postgres_13"
        },
        {
          "digest": "sha256:030dd4c2535b4ec725006efff074faafc835bd87caeaf0e8cd11d7af885e601c",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres@sha256:030dd4c2535b4ec725006efff074faafc835bd87caeaf0e8cd11d7af885e601c",
          "name": "postgres_14"
        },
        {
          "digest": "sha256:430a39a4815f59e9ee92b1db6fb1ed516bc69a40500bf7e8d9d43b5972f0d058",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres-gis@sha256:430a39a4815f59e9ee92b1db6fb1ed516bc69a40500bf7e8d9d43b5972f0d058",
          "name": "postgres_13_gis_3.0"
        },
        {
          "digest": "sha256:20baa15efefeae142b8ab9034f1956203f6ff2397b1ca53ffe86ddbf95977891",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres-gis@sha256:20baa15efefeae142b8ab9034f1956203f6ff2397b1ca53ffe86ddbf95977891",
          "name": "postgres_13_gis_3.1"
        },
        {
          "digest": "sha256:1de1072082f58449d9d181ddaf0026ffe00c5961e982aef6558a7ac4314581e2",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres-gis@sha256:1de1072082f58449d9d181ddaf0026ffe00c5961e982aef6558a7ac4314581e2",
          "name": "postgres_14_gis_3.1"
        },
        {
          "digest": "sha256:e26176f740c5b1fef9dbcc11554062209c22b41f591c9d31d74681b686f6598b",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres-gis@sha256:e26176f740c5b1fef9dbcc11554062209c22b41f591c9d31d74681b686f6598b",
          "name": "postgres_14_gis_3.2"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.10",
      "version": "5.1.1",
      "version_original": "5.1.1"
    },
    {
      "_id": "62b35b3f240dc134424e031d",
      "alm_examples": [
        {
          "api_version": "postgres-operator.crunchydata.com/v1beta1",
          "kind": "PostgresCluster",
          "metadata": {
            "name": "example"
          },
          "spec": {
            "instances": [
              {
                "dataVolumeClaimSpec": {
                  "accessModes": [
                    "ReadWriteOnce"
                  ],
                  "resources": {
                    "requests": {
                      "storage": "1Gi"
                    }
                  }
                },
                "replicas": 1
              }
            ],
            "postgresVersion": 13
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/crunchydata/postgres-operator-bundle@sha256:4fdda557b9826951dce70c57792d906f1366c51c114dbcd8c7c9d2217d35c902",
      "bundle_path_digest": "sha256:4fdda557b9826951dce70c57792d906f1366c51c114dbcd8c7c9d2217d35c902",
      "capabilities": [
        "Auto Pilot"
      ],
      "channel_name": "v5",
      "creation_date": "2022-06-22T18:11:11.598000+00:00",
      "csv_description": "[PGO](https://github.com/CrunchyData/postgres-operator), the\n[Postgres Operator](https://github.com/CrunchyData/postgres-operator) from\n[Crunchy Data](https://www.crunchydata.com), gives you a **declarative Postgres** solution that\nautomatically manages your [PostgreSQL](https://www.postgresql.org) clusters.\n\nDesigned for your GitOps workflows, it is [easy to get started](https://access.crunchydata.com/documentation/postgres-operator/v5/quickstart/)\nwith Postgres on Kubernetes with PGO. Within a few moments, you can have a production grade Postgres\ncluster complete with high availability, disaster recovery, and monitoring, all over secure TLS communications.\nEven better, PGO lets you easily customize your Postgres cluster to tailor it to your workload!\n\nWith conveniences like cloning Postgres clusters to using rolling updates to roll out disruptive\nchanges with minimal downtime, PGO is ready to support your Postgres data at every stage of your\nrelease pipeline. Built for resiliency and uptime, PGO will keep your desired Postgres in a desired\nstate so you do not need to worry about it.\n\nPGO is developed with many years of production experience in automating Postgres management on\nKubernetes, providing a seamless cloud native Postgres solution to keep your data always available.\n\n- **PostgreSQL Cluster Provisioning**: [Create, Scale, & Delete PostgreSQL clusters with ease][provisioning],\n  while fully customizing your Pods and PostgreSQL configuration!\n- **High-Availability**: Safe, automated failover backed by a [distributed consensus based high-availability solution][high-availability].\n  Uses [Pod Anti-Affinity][k8s-anti-affinity] to help resiliency; you can configure how aggressive this can be!\n  Failed primaries automatically heal, allowing for faster recovery time. You can even create regularly scheduled\n  backups as well and set your backup retention policy\n- **Disaster Recovery**: [Backups][backups] and [restores][disaster-recovery] leverage the open source [pgBackRest][] utility and\n  [includes support for full, incremental, and differential backups as well as efficient delta restores][backups].\n  Set how long you want your backups retained for. Works great with very large databases!\n- **Monitoring**: [Track the health of your PostgreSQL clusters][monitoring] using the open source [pgMonitor][] library.\n- **Clone**: [Create new clusters from your existing clusters or backups][clone] with efficient data cloning.\n- **TLS**: All connections are over [TLS][tls]. You can also [bring your own TLS infrastructure][tls] if you do not want to use the provided defaults.\n- **Connection Pooling**: Advanced [connection pooling][pool] support using [pgBouncer][].\n- **Affinity and Tolerations**: Have your PostgreSQL clusters deployed to [Kubernetes Nodes][k8s-nodes] of your preference.\n  Set your [pod anti-affinity][k8s-anti-affinity], node affinity, Pod tolerations and more rules to customize your deployment topology!\n- **Full Customizability**: Crunchy PostgreSQL for Kubernetes makes it easy to get your own PostgreSQL-as-a-Service up and running\n  and fully customize your deployments, including:\n    - Choose the resources for your Postgres cluster: [container resources and storage size][resize-cluster]. [Resize at any time][resize-cluster] with minimal disruption.\n    - Use your own container image repository, including support `imagePullSecrets` and private repositories\n    - [Customize your PostgreSQL configuration][customize-cluster]\n\nand much more!\n\n[backups]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/backups/\n[clone]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/disaster-recovery/#clone-a-postgres-cluster\n[customize-cluster]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/customize-cluster/\n[disaster-recovery]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/disaster-recovery/\n[high-availability]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/high-availability/\n[monitoring]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/monitoring/\n[pool]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/connection-pooling/\n[provisioning]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/create-cluster/\n[resize-cluster]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/resize-cluster/\n[tls]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/customize-cluster/#customize-tls\n\n[k8s-anti-affinity]: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#inter-pod-affinity-and-anti-affinity\n[k8s-nodes]: https://kubernetes.io/docs/concepts/architecture/nodes/\n\n[pgBackRest]: https://www.pgbackrest.org\n[pgBouncer]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/connection-pooling/\n[pgMonitor]: https://github.com/CrunchyData/pgmonitor\n\n\n## Post-Installation\n\n### Tutorial\n\nWant to [learn more about the PostgreSQL Operator][tutorial]? Browse through the [tutorial][] to learn more about what you can do!\n\n[tutorial]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial",
      "csv_display_name": "Crunchy Postgres for Kubernetes",
      "csv_metadata_description": "Production Postgres Made Easy",
      "csv_name": "postgresoperator.v5.1.1",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-11-23T07:26:13.163000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.9",
      "organization": "certified-operators",
      "package": "crunchy-postgres-operator",
      "provided_apis": [
        {
          "group": "postgres-operator.crunchydata.com",
          "kind": "PostgresCluster",
          "plural": "postgresclusters",
          "version": "v1beta1"
        }
      ],
      "provider": "Crunchy Data",
      "related_images": [
        {
          "digest": "sha256:7113af9768b12c74373ad350178c7a2db56df9da7ccb08b5c6b6cbd32220e134",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-pgadmin4@sha256:7113af9768b12c74373ad350178c7a2db56df9da7ccb08b5c6b6cbd32220e134",
          "name": "PGADMIN"
        },
        {
          "digest": "sha256:ddce52cf45f3a7d850163687836d565dc1b0671424b2044778645d108995a480",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-pgbackrest@sha256:ddce52cf45f3a7d850163687836d565dc1b0671424b2044778645d108995a480",
          "name": "PGBACKREST"
        },
        {
          "digest": "sha256:b7aeb28b7aacf76aad8b67037d47c64400d31a281076557f4dfd72a7e09d2a9e",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-pgbouncer@sha256:b7aeb28b7aacf76aad8b67037d47c64400d31a281076557f4dfd72a7e09d2a9e",
          "name": "PGBOUNCER"
        },
        {
          "digest": "sha256:694a2e85412f82fe444c7a605c2a16c5b6fe07689e7eb919477ae0f19a54e610",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres-exporter@sha256:694a2e85412f82fe444c7a605c2a16c5b6fe07689e7eb919477ae0f19a54e610",
          "name": "PGEXPORTER"
        },
        {
          "digest": "sha256:af0557c9dce647181b8071f5b18f51818a460ea116cbdd22c65a2f0129cf1408",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres@sha256:af0557c9dce647181b8071f5b18f51818a460ea116cbdd22c65a2f0129cf1408",
          "name": "POSTGRES_13"
        },
        {
          "digest": "sha256:030dd4c2535b4ec725006efff074faafc835bd87caeaf0e8cd11d7af885e601c",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres@sha256:030dd4c2535b4ec725006efff074faafc835bd87caeaf0e8cd11d7af885e601c",
          "name": "POSTGRES_14"
        },
        {
          "digest": "sha256:430a39a4815f59e9ee92b1db6fb1ed516bc69a40500bf7e8d9d43b5972f0d058",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres-gis@sha256:430a39a4815f59e9ee92b1db6fb1ed516bc69a40500bf7e8d9d43b5972f0d058",
          "name": "POSTGRES_13_GIS_3.0"
        },
        {
          "digest": "sha256:20baa15efefeae142b8ab9034f1956203f6ff2397b1ca53ffe86ddbf95977891",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres-gis@sha256:20baa15efefeae142b8ab9034f1956203f6ff2397b1ca53ffe86ddbf95977891",
          "name": "POSTGRES_13_GIS_3.1"
        },
        {
          "digest": "sha256:1de1072082f58449d9d181ddaf0026ffe00c5961e982aef6558a7ac4314581e2",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres-gis@sha256:1de1072082f58449d9d181ddaf0026ffe00c5961e982aef6558a7ac4314581e2",
          "name": "POSTGRES_14_GIS_3.1"
        },
        {
          "digest": "sha256:e26176f740c5b1fef9dbcc11554062209c22b41f591c9d31d74681b686f6598b",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres-gis@sha256:e26176f740c5b1fef9dbcc11554062209c22b41f591c9d31d74681b686f6598b",
          "name": "POSTGRES_14_GIS_3.2"
        },
        {
          "digest": "sha256:b4dba56b93e7e9eb82ba8fb2dcdcf7cbe08dc16dfcbd8d42bd6d8c86488f6495",
          "image": "registry.connect.redhat.com/crunchydata/postgres-operator@sha256:b4dba56b93e7e9eb82ba8fb2dcdcf7cbe08dc16dfcbd8d42bd6d8c86488f6495",
          "name": "postgres-operator"
        },
        {
          "digest": "sha256:b4dba56b93e7e9eb82ba8fb2dcdcf7cbe08dc16dfcbd8d42bd6d8c86488f6495",
          "image": "registry.connect.redhat.com/crunchydata/postgres-operator@sha256:b4dba56b93e7e9eb82ba8fb2dcdcf7cbe08dc16dfcbd8d42bd6d8c86488f6495",
          "name": "postgres-operator-b4dba56b93e7e9eb82ba8fb2dcdcf7cbe08dc16dfcbd8d42bd6d8c86488f6495-annotation"
        },
        {
          "digest": "sha256:b4dba56b93e7e9eb82ba8fb2dcdcf7cbe08dc16dfcbd8d42bd6d8c86488f6495",
          "image": "registry.connect.redhat.com/crunchydata/postgres-operator@sha256:b4dba56b93e7e9eb82ba8fb2dcdcf7cbe08dc16dfcbd8d42bd6d8c86488f6495",
          "name": "operator"
        },
        {
          "digest": "sha256:7113af9768b12c74373ad350178c7a2db56df9da7ccb08b5c6b6cbd32220e134",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-pgadmin4@sha256:7113af9768b12c74373ad350178c7a2db56df9da7ccb08b5c6b6cbd32220e134",
          "name": "pgadmin"
        },
        {
          "digest": "sha256:ddce52cf45f3a7d850163687836d565dc1b0671424b2044778645d108995a480",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-pgbackrest@sha256:ddce52cf45f3a7d850163687836d565dc1b0671424b2044778645d108995a480",
          "name": "pgbackrest"
        },
        {
          "digest": "sha256:b7aeb28b7aacf76aad8b67037d47c64400d31a281076557f4dfd72a7e09d2a9e",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-pgbouncer@sha256:b7aeb28b7aacf76aad8b67037d47c64400d31a281076557f4dfd72a7e09d2a9e",
          "name": "pgbouncer"
        },
        {
          "digest": "sha256:694a2e85412f82fe444c7a605c2a16c5b6fe07689e7eb919477ae0f19a54e610",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres-exporter@sha256:694a2e85412f82fe444c7a605c2a16c5b6fe07689e7eb919477ae0f19a54e610",
          "name": "pgexporter"
        },
        {
          "digest": "sha256:af0557c9dce647181b8071f5b18f51818a460ea116cbdd22c65a2f0129cf1408",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres@sha256:af0557c9dce647181b8071f5b18f51818a460ea116cbdd22c65a2f0129cf1408",
          "name": "postgres_13"
        },
        {
          "digest": "sha256:030dd4c2535b4ec725006efff074faafc835bd87caeaf0e8cd11d7af885e601c",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres@sha256:030dd4c2535b4ec725006efff074faafc835bd87caeaf0e8cd11d7af885e601c",
          "name": "postgres_14"
        },
        {
          "digest": "sha256:430a39a4815f59e9ee92b1db6fb1ed516bc69a40500bf7e8d9d43b5972f0d058",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres-gis@sha256:430a39a4815f59e9ee92b1db6fb1ed516bc69a40500bf7e8d9d43b5972f0d058",
          "name": "postgres_13_gis_3.0"
        },
        {
          "digest": "sha256:20baa15efefeae142b8ab9034f1956203f6ff2397b1ca53ffe86ddbf95977891",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres-gis@sha256:20baa15efefeae142b8ab9034f1956203f6ff2397b1ca53ffe86ddbf95977891",
          "name": "postgres_13_gis_3.1"
        },
        {
          "digest": "sha256:1de1072082f58449d9d181ddaf0026ffe00c5961e982aef6558a7ac4314581e2",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres-gis@sha256:1de1072082f58449d9d181ddaf0026ffe00c5961e982aef6558a7ac4314581e2",
          "name": "postgres_14_gis_3.1"
        },
        {
          "digest": "sha256:e26176f740c5b1fef9dbcc11554062209c22b41f591c9d31d74681b686f6598b",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres-gis@sha256:e26176f740c5b1fef9dbcc11554062209c22b41f591c9d31d74681b686f6598b",
          "name": "postgres_14_gis_3.2"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.9",
      "version": "5.1.1",
      "version_original": "5.1.1"
    },
    {
      "_id": "62b35ddd240dc134424e0336",
      "alm_examples": [
        {
          "api_version": "postgres-operator.crunchydata.com/v1beta1",
          "kind": "PostgresCluster",
          "metadata": {
            "name": "example"
          },
          "spec": {
            "instances": [
              {
                "dataVolumeClaimSpec": {
                  "accessModes": [
                    "ReadWriteOnce"
                  ],
                  "resources": {
                    "requests": {
                      "storage": "1Gi"
                    }
                  }
                },
                "replicas": 1
              }
            ],
            "postgresVersion": 13
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/crunchydata/postgres-operator-bundle@sha256:4fdda557b9826951dce70c57792d906f1366c51c114dbcd8c7c9d2217d35c902",
      "bundle_path_digest": "sha256:4fdda557b9826951dce70c57792d906f1366c51c114dbcd8c7c9d2217d35c902",
      "capabilities": [
        "Auto Pilot"
      ],
      "channel_name": "v5",
      "creation_date": "2022-06-22T18:22:21.566000+00:00",
      "csv_description": "[PGO](https://github.com/CrunchyData/postgres-operator), the\n[Postgres Operator](https://github.com/CrunchyData/postgres-operator) from\n[Crunchy Data](https://www.crunchydata.com), gives you a **declarative Postgres** solution that\nautomatically manages your [PostgreSQL](https://www.postgresql.org) clusters.\n\nDesigned for your GitOps workflows, it is [easy to get started](https://access.crunchydata.com/documentation/postgres-operator/v5/quickstart/)\nwith Postgres on Kubernetes with PGO. Within a few moments, you can have a production grade Postgres\ncluster complete with high availability, disaster recovery, and monitoring, all over secure TLS communications.\nEven better, PGO lets you easily customize your Postgres cluster to tailor it to your workload!\n\nWith conveniences like cloning Postgres clusters to using rolling updates to roll out disruptive\nchanges with minimal downtime, PGO is ready to support your Postgres data at every stage of your\nrelease pipeline. Built for resiliency and uptime, PGO will keep your desired Postgres in a desired\nstate so you do not need to worry about it.\n\nPGO is developed with many years of production experience in automating Postgres management on\nKubernetes, providing a seamless cloud native Postgres solution to keep your data always available.\n\n- **PostgreSQL Cluster Provisioning**: [Create, Scale, & Delete PostgreSQL clusters with ease][provisioning],\n  while fully customizing your Pods and PostgreSQL configuration!\n- **High-Availability**: Safe, automated failover backed by a [distributed consensus based high-availability solution][high-availability].\n  Uses [Pod Anti-Affinity][k8s-anti-affinity] to help resiliency; you can configure how aggressive this can be!\n  Failed primaries automatically heal, allowing for faster recovery time. You can even create regularly scheduled\n  backups as well and set your backup retention policy\n- **Disaster Recovery**: [Backups][backups] and [restores][disaster-recovery] leverage the open source [pgBackRest][] utility and\n  [includes support for full, incremental, and differential backups as well as efficient delta restores][backups].\n  Set how long you want your backups retained for. Works great with very large databases!\n- **Monitoring**: [Track the health of your PostgreSQL clusters][monitoring] using the open source [pgMonitor][] library.\n- **Clone**: [Create new clusters from your existing clusters or backups][clone] with efficient data cloning.\n- **TLS**: All connections are over [TLS][tls]. You can also [bring your own TLS infrastructure][tls] if you do not want to use the provided defaults.\n- **Connection Pooling**: Advanced [connection pooling][pool] support using [pgBouncer][].\n- **Affinity and Tolerations**: Have your PostgreSQL clusters deployed to [Kubernetes Nodes][k8s-nodes] of your preference.\n  Set your [pod anti-affinity][k8s-anti-affinity], node affinity, Pod tolerations and more rules to customize your deployment topology!\n- **Full Customizability**: Crunchy PostgreSQL for Kubernetes makes it easy to get your own PostgreSQL-as-a-Service up and running\n  and fully customize your deployments, including:\n    - Choose the resources for your Postgres cluster: [container resources and storage size][resize-cluster]. [Resize at any time][resize-cluster] with minimal disruption.\n    - Use your own container image repository, including support `imagePullSecrets` and private repositories\n    - [Customize your PostgreSQL configuration][customize-cluster]\n\nand much more!\n\n[backups]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/backups/\n[clone]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/disaster-recovery/#clone-a-postgres-cluster\n[customize-cluster]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/customize-cluster/\n[disaster-recovery]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/disaster-recovery/\n[high-availability]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/high-availability/\n[monitoring]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/monitoring/\n[pool]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/connection-pooling/\n[provisioning]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/create-cluster/\n[resize-cluster]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/resize-cluster/\n[tls]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/customize-cluster/#customize-tls\n\n[k8s-anti-affinity]: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#inter-pod-affinity-and-anti-affinity\n[k8s-nodes]: https://kubernetes.io/docs/concepts/architecture/nodes/\n\n[pgBackRest]: https://www.pgbackrest.org\n[pgBouncer]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/connection-pooling/\n[pgMonitor]: https://github.com/CrunchyData/pgmonitor\n\n\n## Post-Installation\n\n### Tutorial\n\nWant to [learn more about the PostgreSQL Operator][tutorial]? Browse through the [tutorial][] to learn more about what you can do!\n\n[tutorial]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial",
      "csv_display_name": "Crunchy Postgres for Kubernetes",
      "csv_metadata_description": "Production Postgres Made Easy",
      "csv_name": "postgresoperator.v5.1.1",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-11-23T07:27:57.870000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.6",
      "organization": "certified-operators",
      "package": "crunchy-postgres-operator",
      "provided_apis": [
        {
          "group": "postgres-operator.crunchydata.com",
          "kind": "PostgresCluster",
          "plural": "postgresclusters",
          "version": "v1beta1"
        }
      ],
      "provider": "Crunchy Data",
      "related_images": [
        {
          "digest": "sha256:7113af9768b12c74373ad350178c7a2db56df9da7ccb08b5c6b6cbd32220e134",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-pgadmin4@sha256:7113af9768b12c74373ad350178c7a2db56df9da7ccb08b5c6b6cbd32220e134",
          "name": "PGADMIN"
        },
        {
          "digest": "sha256:ddce52cf45f3a7d850163687836d565dc1b0671424b2044778645d108995a480",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-pgbackrest@sha256:ddce52cf45f3a7d850163687836d565dc1b0671424b2044778645d108995a480",
          "name": "PGBACKREST"
        },
        {
          "digest": "sha256:b7aeb28b7aacf76aad8b67037d47c64400d31a281076557f4dfd72a7e09d2a9e",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-pgbouncer@sha256:b7aeb28b7aacf76aad8b67037d47c64400d31a281076557f4dfd72a7e09d2a9e",
          "name": "PGBOUNCER"
        },
        {
          "digest": "sha256:694a2e85412f82fe444c7a605c2a16c5b6fe07689e7eb919477ae0f19a54e610",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres-exporter@sha256:694a2e85412f82fe444c7a605c2a16c5b6fe07689e7eb919477ae0f19a54e610",
          "name": "PGEXPORTER"
        },
        {
          "digest": "sha256:af0557c9dce647181b8071f5b18f51818a460ea116cbdd22c65a2f0129cf1408",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres@sha256:af0557c9dce647181b8071f5b18f51818a460ea116cbdd22c65a2f0129cf1408",
          "name": "POSTGRES_13"
        },
        {
          "digest": "sha256:030dd4c2535b4ec725006efff074faafc835bd87caeaf0e8cd11d7af885e601c",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres@sha256:030dd4c2535b4ec725006efff074faafc835bd87caeaf0e8cd11d7af885e601c",
          "name": "POSTGRES_14"
        },
        {
          "digest": "sha256:430a39a4815f59e9ee92b1db6fb1ed516bc69a40500bf7e8d9d43b5972f0d058",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres-gis@sha256:430a39a4815f59e9ee92b1db6fb1ed516bc69a40500bf7e8d9d43b5972f0d058",
          "name": "POSTGRES_13_GIS_3.0"
        },
        {
          "digest": "sha256:20baa15efefeae142b8ab9034f1956203f6ff2397b1ca53ffe86ddbf95977891",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres-gis@sha256:20baa15efefeae142b8ab9034f1956203f6ff2397b1ca53ffe86ddbf95977891",
          "name": "POSTGRES_13_GIS_3.1"
        },
        {
          "digest": "sha256:1de1072082f58449d9d181ddaf0026ffe00c5961e982aef6558a7ac4314581e2",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres-gis@sha256:1de1072082f58449d9d181ddaf0026ffe00c5961e982aef6558a7ac4314581e2",
          "name": "POSTGRES_14_GIS_3.1"
        },
        {
          "digest": "sha256:e26176f740c5b1fef9dbcc11554062209c22b41f591c9d31d74681b686f6598b",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres-gis@sha256:e26176f740c5b1fef9dbcc11554062209c22b41f591c9d31d74681b686f6598b",
          "name": "POSTGRES_14_GIS_3.2"
        },
        {
          "digest": "sha256:b4dba56b93e7e9eb82ba8fb2dcdcf7cbe08dc16dfcbd8d42bd6d8c86488f6495",
          "image": "registry.connect.redhat.com/crunchydata/postgres-operator@sha256:b4dba56b93e7e9eb82ba8fb2dcdcf7cbe08dc16dfcbd8d42bd6d8c86488f6495",
          "name": "postgres-operator"
        },
        {
          "digest": "sha256:b4dba56b93e7e9eb82ba8fb2dcdcf7cbe08dc16dfcbd8d42bd6d8c86488f6495",
          "image": "registry.connect.redhat.com/crunchydata/postgres-operator@sha256:b4dba56b93e7e9eb82ba8fb2dcdcf7cbe08dc16dfcbd8d42bd6d8c86488f6495",
          "name": "postgres-operator-b4dba56b93e7e9eb82ba8fb2dcdcf7cbe08dc16dfcbd8d42bd6d8c86488f6495-annotation"
        },
        {
          "digest": "sha256:b4dba56b93e7e9eb82ba8fb2dcdcf7cbe08dc16dfcbd8d42bd6d8c86488f6495",
          "image": "registry.connect.redhat.com/crunchydata/postgres-operator@sha256:b4dba56b93e7e9eb82ba8fb2dcdcf7cbe08dc16dfcbd8d42bd6d8c86488f6495",
          "name": "operator"
        },
        {
          "digest": "sha256:7113af9768b12c74373ad350178c7a2db56df9da7ccb08b5c6b6cbd32220e134",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-pgadmin4@sha256:7113af9768b12c74373ad350178c7a2db56df9da7ccb08b5c6b6cbd32220e134",
          "name": "pgadmin"
        },
        {
          "digest": "sha256:ddce52cf45f3a7d850163687836d565dc1b0671424b2044778645d108995a480",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-pgbackrest@sha256:ddce52cf45f3a7d850163687836d565dc1b0671424b2044778645d108995a480",
          "name": "pgbackrest"
        },
        {
          "digest": "sha256:b7aeb28b7aacf76aad8b67037d47c64400d31a281076557f4dfd72a7e09d2a9e",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-pgbouncer@sha256:b7aeb28b7aacf76aad8b67037d47c64400d31a281076557f4dfd72a7e09d2a9e",
          "name": "pgbouncer"
        },
        {
          "digest": "sha256:694a2e85412f82fe444c7a605c2a16c5b6fe07689e7eb919477ae0f19a54e610",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres-exporter@sha256:694a2e85412f82fe444c7a605c2a16c5b6fe07689e7eb919477ae0f19a54e610",
          "name": "pgexporter"
        },
        {
          "digest": "sha256:af0557c9dce647181b8071f5b18f51818a460ea116cbdd22c65a2f0129cf1408",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres@sha256:af0557c9dce647181b8071f5b18f51818a460ea116cbdd22c65a2f0129cf1408",
          "name": "postgres_13"
        },
        {
          "digest": "sha256:030dd4c2535b4ec725006efff074faafc835bd87caeaf0e8cd11d7af885e601c",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres@sha256:030dd4c2535b4ec725006efff074faafc835bd87caeaf0e8cd11d7af885e601c",
          "name": "postgres_14"
        },
        {
          "digest": "sha256:430a39a4815f59e9ee92b1db6fb1ed516bc69a40500bf7e8d9d43b5972f0d058",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres-gis@sha256:430a39a4815f59e9ee92b1db6fb1ed516bc69a40500bf7e8d9d43b5972f0d058",
          "name": "postgres_13_gis_3.0"
        },
        {
          "digest": "sha256:20baa15efefeae142b8ab9034f1956203f6ff2397b1ca53ffe86ddbf95977891",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres-gis@sha256:20baa15efefeae142b8ab9034f1956203f6ff2397b1ca53ffe86ddbf95977891",
          "name": "postgres_13_gis_3.1"
        },
        {
          "digest": "sha256:1de1072082f58449d9d181ddaf0026ffe00c5961e982aef6558a7ac4314581e2",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres-gis@sha256:1de1072082f58449d9d181ddaf0026ffe00c5961e982aef6558a7ac4314581e2",
          "name": "postgres_14_gis_3.1"
        },
        {
          "digest": "sha256:e26176f740c5b1fef9dbcc11554062209c22b41f591c9d31d74681b686f6598b",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres-gis@sha256:e26176f740c5b1fef9dbcc11554062209c22b41f591c9d31d74681b686f6598b",
          "name": "postgres_14_gis_3.2"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.6",
      "version": "5.1.1",
      "version_original": "5.1.1"
    },
    {
      "_id": "62b35e0bdd92c1f1f12c300c",
      "alm_examples": [
        {
          "api_version": "postgres-operator.crunchydata.com/v1beta1",
          "kind": "PostgresCluster",
          "metadata": {
            "name": "example"
          },
          "spec": {
            "instances": [
              {
                "dataVolumeClaimSpec": {
                  "accessModes": [
                    "ReadWriteOnce"
                  ],
                  "resources": {
                    "requests": {
                      "storage": "1Gi"
                    }
                  }
                },
                "replicas": 1
              }
            ],
            "postgresVersion": 13
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/crunchydata/postgres-operator-bundle@sha256:4fdda557b9826951dce70c57792d906f1366c51c114dbcd8c7c9d2217d35c902",
      "bundle_path_digest": "sha256:4fdda557b9826951dce70c57792d906f1366c51c114dbcd8c7c9d2217d35c902",
      "capabilities": [
        "Auto Pilot"
      ],
      "channel_name": "v5",
      "creation_date": "2022-06-22T18:23:07.486000+00:00",
      "csv_description": "[PGO](https://github.com/CrunchyData/postgres-operator), the\n[Postgres Operator](https://github.com/CrunchyData/postgres-operator) from\n[Crunchy Data](https://www.crunchydata.com), gives you a **declarative Postgres** solution that\nautomatically manages your [PostgreSQL](https://www.postgresql.org) clusters.\n\nDesigned for your GitOps workflows, it is [easy to get started](https://access.crunchydata.com/documentation/postgres-operator/v5/quickstart/)\nwith Postgres on Kubernetes with PGO. Within a few moments, you can have a production grade Postgres\ncluster complete with high availability, disaster recovery, and monitoring, all over secure TLS communications.\nEven better, PGO lets you easily customize your Postgres cluster to tailor it to your workload!\n\nWith conveniences like cloning Postgres clusters to using rolling updates to roll out disruptive\nchanges with minimal downtime, PGO is ready to support your Postgres data at every stage of your\nrelease pipeline. Built for resiliency and uptime, PGO will keep your desired Postgres in a desired\nstate so you do not need to worry about it.\n\nPGO is developed with many years of production experience in automating Postgres management on\nKubernetes, providing a seamless cloud native Postgres solution to keep your data always available.\n\n- **PostgreSQL Cluster Provisioning**: [Create, Scale, & Delete PostgreSQL clusters with ease][provisioning],\n  while fully customizing your Pods and PostgreSQL configuration!\n- **High-Availability**: Safe, automated failover backed by a [distributed consensus based high-availability solution][high-availability].\n  Uses [Pod Anti-Affinity][k8s-anti-affinity] to help resiliency; you can configure how aggressive this can be!\n  Failed primaries automatically heal, allowing for faster recovery time. You can even create regularly scheduled\n  backups as well and set your backup retention policy\n- **Disaster Recovery**: [Backups][backups] and [restores][disaster-recovery] leverage the open source [pgBackRest][] utility and\n  [includes support for full, incremental, and differential backups as well as efficient delta restores][backups].\n  Set how long you want your backups retained for. Works great with very large databases!\n- **Monitoring**: [Track the health of your PostgreSQL clusters][monitoring] using the open source [pgMonitor][] library.\n- **Clone**: [Create new clusters from your existing clusters or backups][clone] with efficient data cloning.\n- **TLS**: All connections are over [TLS][tls]. You can also [bring your own TLS infrastructure][tls] if you do not want to use the provided defaults.\n- **Connection Pooling**: Advanced [connection pooling][pool] support using [pgBouncer][].\n- **Affinity and Tolerations**: Have your PostgreSQL clusters deployed to [Kubernetes Nodes][k8s-nodes] of your preference.\n  Set your [pod anti-affinity][k8s-anti-affinity], node affinity, Pod tolerations and more rules to customize your deployment topology!\n- **Full Customizability**: Crunchy PostgreSQL for Kubernetes makes it easy to get your own PostgreSQL-as-a-Service up and running\n  and fully customize your deployments, including:\n    - Choose the resources for your Postgres cluster: [container resources and storage size][resize-cluster]. [Resize at any time][resize-cluster] with minimal disruption.\n    - Use your own container image repository, including support `imagePullSecrets` and private repositories\n    - [Customize your PostgreSQL configuration][customize-cluster]\n\nand much more!\n\n[backups]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/backups/\n[clone]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/disaster-recovery/#clone-a-postgres-cluster\n[customize-cluster]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/customize-cluster/\n[disaster-recovery]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/disaster-recovery/\n[high-availability]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/high-availability/\n[monitoring]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/monitoring/\n[pool]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/connection-pooling/\n[provisioning]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/create-cluster/\n[resize-cluster]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/resize-cluster/\n[tls]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/customize-cluster/#customize-tls\n\n[k8s-anti-affinity]: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#inter-pod-affinity-and-anti-affinity\n[k8s-nodes]: https://kubernetes.io/docs/concepts/architecture/nodes/\n\n[pgBackRest]: https://www.pgbackrest.org\n[pgBouncer]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/connection-pooling/\n[pgMonitor]: https://github.com/CrunchyData/pgmonitor\n\n\n## Post-Installation\n\n### Tutorial\n\nWant to [learn more about the PostgreSQL Operator][tutorial]? Browse through the [tutorial][] to learn more about what you can do!\n\n[tutorial]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial",
      "csv_display_name": "Crunchy Postgres for Kubernetes",
      "csv_metadata_description": "Production Postgres Made Easy",
      "csv_name": "postgresoperator.v5.1.1",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-11-23T07:29:17.484000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.7",
      "organization": "certified-operators",
      "package": "crunchy-postgres-operator",
      "provided_apis": [
        {
          "group": "postgres-operator.crunchydata.com",
          "kind": "PostgresCluster",
          "plural": "postgresclusters",
          "version": "v1beta1"
        }
      ],
      "provider": "Crunchy Data",
      "related_images": [
        {
          "digest": "sha256:7113af9768b12c74373ad350178c7a2db56df9da7ccb08b5c6b6cbd32220e134",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-pgadmin4@sha256:7113af9768b12c74373ad350178c7a2db56df9da7ccb08b5c6b6cbd32220e134",
          "name": "PGADMIN"
        },
        {
          "digest": "sha256:ddce52cf45f3a7d850163687836d565dc1b0671424b2044778645d108995a480",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-pgbackrest@sha256:ddce52cf45f3a7d850163687836d565dc1b0671424b2044778645d108995a480",
          "name": "PGBACKREST"
        },
        {
          "digest": "sha256:b7aeb28b7aacf76aad8b67037d47c64400d31a281076557f4dfd72a7e09d2a9e",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-pgbouncer@sha256:b7aeb28b7aacf76aad8b67037d47c64400d31a281076557f4dfd72a7e09d2a9e",
          "name": "PGBOUNCER"
        },
        {
          "digest": "sha256:694a2e85412f82fe444c7a605c2a16c5b6fe07689e7eb919477ae0f19a54e610",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres-exporter@sha256:694a2e85412f82fe444c7a605c2a16c5b6fe07689e7eb919477ae0f19a54e610",
          "name": "PGEXPORTER"
        },
        {
          "digest": "sha256:af0557c9dce647181b8071f5b18f51818a460ea116cbdd22c65a2f0129cf1408",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres@sha256:af0557c9dce647181b8071f5b18f51818a460ea116cbdd22c65a2f0129cf1408",
          "name": "POSTGRES_13"
        },
        {
          "digest": "sha256:030dd4c2535b4ec725006efff074faafc835bd87caeaf0e8cd11d7af885e601c",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres@sha256:030dd4c2535b4ec725006efff074faafc835bd87caeaf0e8cd11d7af885e601c",
          "name": "POSTGRES_14"
        },
        {
          "digest": "sha256:430a39a4815f59e9ee92b1db6fb1ed516bc69a40500bf7e8d9d43b5972f0d058",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres-gis@sha256:430a39a4815f59e9ee92b1db6fb1ed516bc69a40500bf7e8d9d43b5972f0d058",
          "name": "POSTGRES_13_GIS_3.0"
        },
        {
          "digest": "sha256:20baa15efefeae142b8ab9034f1956203f6ff2397b1ca53ffe86ddbf95977891",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres-gis@sha256:20baa15efefeae142b8ab9034f1956203f6ff2397b1ca53ffe86ddbf95977891",
          "name": "POSTGRES_13_GIS_3.1"
        },
        {
          "digest": "sha256:1de1072082f58449d9d181ddaf0026ffe00c5961e982aef6558a7ac4314581e2",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres-gis@sha256:1de1072082f58449d9d181ddaf0026ffe00c5961e982aef6558a7ac4314581e2",
          "name": "POSTGRES_14_GIS_3.1"
        },
        {
          "digest": "sha256:e26176f740c5b1fef9dbcc11554062209c22b41f591c9d31d74681b686f6598b",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres-gis@sha256:e26176f740c5b1fef9dbcc11554062209c22b41f591c9d31d74681b686f6598b",
          "name": "POSTGRES_14_GIS_3.2"
        },
        {
          "digest": "sha256:b4dba56b93e7e9eb82ba8fb2dcdcf7cbe08dc16dfcbd8d42bd6d8c86488f6495",
          "image": "registry.connect.redhat.com/crunchydata/postgres-operator@sha256:b4dba56b93e7e9eb82ba8fb2dcdcf7cbe08dc16dfcbd8d42bd6d8c86488f6495",
          "name": "postgres-operator"
        },
        {
          "digest": "sha256:b4dba56b93e7e9eb82ba8fb2dcdcf7cbe08dc16dfcbd8d42bd6d8c86488f6495",
          "image": "registry.connect.redhat.com/crunchydata/postgres-operator@sha256:b4dba56b93e7e9eb82ba8fb2dcdcf7cbe08dc16dfcbd8d42bd6d8c86488f6495",
          "name": "postgres-operator-b4dba56b93e7e9eb82ba8fb2dcdcf7cbe08dc16dfcbd8d42bd6d8c86488f6495-annotation"
        },
        {
          "digest": "sha256:b4dba56b93e7e9eb82ba8fb2dcdcf7cbe08dc16dfcbd8d42bd6d8c86488f6495",
          "image": "registry.connect.redhat.com/crunchydata/postgres-operator@sha256:b4dba56b93e7e9eb82ba8fb2dcdcf7cbe08dc16dfcbd8d42bd6d8c86488f6495",
          "name": "operator"
        },
        {
          "digest": "sha256:7113af9768b12c74373ad350178c7a2db56df9da7ccb08b5c6b6cbd32220e134",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-pgadmin4@sha256:7113af9768b12c74373ad350178c7a2db56df9da7ccb08b5c6b6cbd32220e134",
          "name": "pgadmin"
        },
        {
          "digest": "sha256:ddce52cf45f3a7d850163687836d565dc1b0671424b2044778645d108995a480",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-pgbackrest@sha256:ddce52cf45f3a7d850163687836d565dc1b0671424b2044778645d108995a480",
          "name": "pgbackrest"
        },
        {
          "digest": "sha256:b7aeb28b7aacf76aad8b67037d47c64400d31a281076557f4dfd72a7e09d2a9e",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-pgbouncer@sha256:b7aeb28b7aacf76aad8b67037d47c64400d31a281076557f4dfd72a7e09d2a9e",
          "name": "pgbouncer"
        },
        {
          "digest": "sha256:694a2e85412f82fe444c7a605c2a16c5b6fe07689e7eb919477ae0f19a54e610",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres-exporter@sha256:694a2e85412f82fe444c7a605c2a16c5b6fe07689e7eb919477ae0f19a54e610",
          "name": "pgexporter"
        },
        {
          "digest": "sha256:af0557c9dce647181b8071f5b18f51818a460ea116cbdd22c65a2f0129cf1408",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres@sha256:af0557c9dce647181b8071f5b18f51818a460ea116cbdd22c65a2f0129cf1408",
          "name": "postgres_13"
        },
        {
          "digest": "sha256:030dd4c2535b4ec725006efff074faafc835bd87caeaf0e8cd11d7af885e601c",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres@sha256:030dd4c2535b4ec725006efff074faafc835bd87caeaf0e8cd11d7af885e601c",
          "name": "postgres_14"
        },
        {
          "digest": "sha256:430a39a4815f59e9ee92b1db6fb1ed516bc69a40500bf7e8d9d43b5972f0d058",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres-gis@sha256:430a39a4815f59e9ee92b1db6fb1ed516bc69a40500bf7e8d9d43b5972f0d058",
          "name": "postgres_13_gis_3.0"
        },
        {
          "digest": "sha256:20baa15efefeae142b8ab9034f1956203f6ff2397b1ca53ffe86ddbf95977891",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres-gis@sha256:20baa15efefeae142b8ab9034f1956203f6ff2397b1ca53ffe86ddbf95977891",
          "name": "postgres_13_gis_3.1"
        },
        {
          "digest": "sha256:1de1072082f58449d9d181ddaf0026ffe00c5961e982aef6558a7ac4314581e2",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres-gis@sha256:1de1072082f58449d9d181ddaf0026ffe00c5961e982aef6558a7ac4314581e2",
          "name": "postgres_14_gis_3.1"
        },
        {
          "digest": "sha256:e26176f740c5b1fef9dbcc11554062209c22b41f591c9d31d74681b686f6598b",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres-gis@sha256:e26176f740c5b1fef9dbcc11554062209c22b41f591c9d31d74681b686f6598b",
          "name": "postgres_14_gis_3.2"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.7",
      "version": "5.1.1",
      "version_original": "5.1.1"
    },
    {
      "_id": "62b35ed5dd92c1f1f12c3012",
      "alm_examples": [
        {
          "api_version": "postgres-operator.crunchydata.com/v1beta1",
          "kind": "PostgresCluster",
          "metadata": {
            "name": "example"
          },
          "spec": {
            "instances": [
              {
                "dataVolumeClaimSpec": {
                  "accessModes": [
                    "ReadWriteOnce"
                  ],
                  "resources": {
                    "requests": {
                      "storage": "1Gi"
                    }
                  }
                },
                "replicas": 1
              }
            ],
            "postgresVersion": 13
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/crunchydata/postgres-operator-bundle@sha256:4fdda557b9826951dce70c57792d906f1366c51c114dbcd8c7c9d2217d35c902",
      "bundle_path_digest": "sha256:4fdda557b9826951dce70c57792d906f1366c51c114dbcd8c7c9d2217d35c902",
      "capabilities": [
        "Auto Pilot"
      ],
      "channel_name": "v5",
      "creation_date": "2022-06-22T18:26:29.500000+00:00",
      "csv_description": "[PGO](https://github.com/CrunchyData/postgres-operator), the\n[Postgres Operator](https://github.com/CrunchyData/postgres-operator) from\n[Crunchy Data](https://www.crunchydata.com), gives you a **declarative Postgres** solution that\nautomatically manages your [PostgreSQL](https://www.postgresql.org) clusters.\n\nDesigned for your GitOps workflows, it is [easy to get started](https://access.crunchydata.com/documentation/postgres-operator/v5/quickstart/)\nwith Postgres on Kubernetes with PGO. Within a few moments, you can have a production grade Postgres\ncluster complete with high availability, disaster recovery, and monitoring, all over secure TLS communications.\nEven better, PGO lets you easily customize your Postgres cluster to tailor it to your workload!\n\nWith conveniences like cloning Postgres clusters to using rolling updates to roll out disruptive\nchanges with minimal downtime, PGO is ready to support your Postgres data at every stage of your\nrelease pipeline. Built for resiliency and uptime, PGO will keep your desired Postgres in a desired\nstate so you do not need to worry about it.\n\nPGO is developed with many years of production experience in automating Postgres management on\nKubernetes, providing a seamless cloud native Postgres solution to keep your data always available.\n\n- **PostgreSQL Cluster Provisioning**: [Create, Scale, & Delete PostgreSQL clusters with ease][provisioning],\n  while fully customizing your Pods and PostgreSQL configuration!\n- **High-Availability**: Safe, automated failover backed by a [distributed consensus based high-availability solution][high-availability].\n  Uses [Pod Anti-Affinity][k8s-anti-affinity] to help resiliency; you can configure how aggressive this can be!\n  Failed primaries automatically heal, allowing for faster recovery time. You can even create regularly scheduled\n  backups as well and set your backup retention policy\n- **Disaster Recovery**: [Backups][backups] and [restores][disaster-recovery] leverage the open source [pgBackRest][] utility and\n  [includes support for full, incremental, and differential backups as well as efficient delta restores][backups].\n  Set how long you want your backups retained for. Works great with very large databases!\n- **Monitoring**: [Track the health of your PostgreSQL clusters][monitoring] using the open source [pgMonitor][] library.\n- **Clone**: [Create new clusters from your existing clusters or backups][clone] with efficient data cloning.\n- **TLS**: All connections are over [TLS][tls]. You can also [bring your own TLS infrastructure][tls] if you do not want to use the provided defaults.\n- **Connection Pooling**: Advanced [connection pooling][pool] support using [pgBouncer][].\n- **Affinity and Tolerations**: Have your PostgreSQL clusters deployed to [Kubernetes Nodes][k8s-nodes] of your preference.\n  Set your [pod anti-affinity][k8s-anti-affinity], node affinity, Pod tolerations and more rules to customize your deployment topology!\n- **Full Customizability**: Crunchy PostgreSQL for Kubernetes makes it easy to get your own PostgreSQL-as-a-Service up and running\n  and fully customize your deployments, including:\n    - Choose the resources for your Postgres cluster: [container resources and storage size][resize-cluster]. [Resize at any time][resize-cluster] with minimal disruption.\n    - Use your own container image repository, including support `imagePullSecrets` and private repositories\n    - [Customize your PostgreSQL configuration][customize-cluster]\n\nand much more!\n\n[backups]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/backups/\n[clone]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/disaster-recovery/#clone-a-postgres-cluster\n[customize-cluster]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/customize-cluster/\n[disaster-recovery]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/disaster-recovery/\n[high-availability]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/high-availability/\n[monitoring]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/monitoring/\n[pool]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/connection-pooling/\n[provisioning]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/create-cluster/\n[resize-cluster]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/resize-cluster/\n[tls]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/customize-cluster/#customize-tls\n\n[k8s-anti-affinity]: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#inter-pod-affinity-and-anti-affinity\n[k8s-nodes]: https://kubernetes.io/docs/concepts/architecture/nodes/\n\n[pgBackRest]: https://www.pgbackrest.org\n[pgBouncer]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/connection-pooling/\n[pgMonitor]: https://github.com/CrunchyData/pgmonitor\n\n\n## Post-Installation\n\n### Tutorial\n\nWant to [learn more about the PostgreSQL Operator][tutorial]? Browse through the [tutorial][] to learn more about what you can do!\n\n[tutorial]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial",
      "csv_display_name": "Crunchy Postgres for Kubernetes",
      "csv_metadata_description": "Production Postgres Made Easy",
      "csv_name": "postgresoperator.v5.1.1",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-11-23T07:37:39.946000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.8",
      "organization": "certified-operators",
      "package": "crunchy-postgres-operator",
      "provided_apis": [
        {
          "group": "postgres-operator.crunchydata.com",
          "kind": "PostgresCluster",
          "plural": "postgresclusters",
          "version": "v1beta1"
        }
      ],
      "provider": "Crunchy Data",
      "related_images": [
        {
          "digest": "sha256:7113af9768b12c74373ad350178c7a2db56df9da7ccb08b5c6b6cbd32220e134",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-pgadmin4@sha256:7113af9768b12c74373ad350178c7a2db56df9da7ccb08b5c6b6cbd32220e134",
          "name": "PGADMIN"
        },
        {
          "digest": "sha256:ddce52cf45f3a7d850163687836d565dc1b0671424b2044778645d108995a480",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-pgbackrest@sha256:ddce52cf45f3a7d850163687836d565dc1b0671424b2044778645d108995a480",
          "name": "PGBACKREST"
        },
        {
          "digest": "sha256:b7aeb28b7aacf76aad8b67037d47c64400d31a281076557f4dfd72a7e09d2a9e",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-pgbouncer@sha256:b7aeb28b7aacf76aad8b67037d47c64400d31a281076557f4dfd72a7e09d2a9e",
          "name": "PGBOUNCER"
        },
        {
          "digest": "sha256:694a2e85412f82fe444c7a605c2a16c5b6fe07689e7eb919477ae0f19a54e610",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres-exporter@sha256:694a2e85412f82fe444c7a605c2a16c5b6fe07689e7eb919477ae0f19a54e610",
          "name": "PGEXPORTER"
        },
        {
          "digest": "sha256:af0557c9dce647181b8071f5b18f51818a460ea116cbdd22c65a2f0129cf1408",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres@sha256:af0557c9dce647181b8071f5b18f51818a460ea116cbdd22c65a2f0129cf1408",
          "name": "POSTGRES_13"
        },
        {
          "digest": "sha256:030dd4c2535b4ec725006efff074faafc835bd87caeaf0e8cd11d7af885e601c",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres@sha256:030dd4c2535b4ec725006efff074faafc835bd87caeaf0e8cd11d7af885e601c",
          "name": "POSTGRES_14"
        },
        {
          "digest": "sha256:430a39a4815f59e9ee92b1db6fb1ed516bc69a40500bf7e8d9d43b5972f0d058",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres-gis@sha256:430a39a4815f59e9ee92b1db6fb1ed516bc69a40500bf7e8d9d43b5972f0d058",
          "name": "POSTGRES_13_GIS_3.0"
        },
        {
          "digest": "sha256:20baa15efefeae142b8ab9034f1956203f6ff2397b1ca53ffe86ddbf95977891",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres-gis@sha256:20baa15efefeae142b8ab9034f1956203f6ff2397b1ca53ffe86ddbf95977891",
          "name": "POSTGRES_13_GIS_3.1"
        },
        {
          "digest": "sha256:1de1072082f58449d9d181ddaf0026ffe00c5961e982aef6558a7ac4314581e2",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres-gis@sha256:1de1072082f58449d9d181ddaf0026ffe00c5961e982aef6558a7ac4314581e2",
          "name": "POSTGRES_14_GIS_3.1"
        },
        {
          "digest": "sha256:e26176f740c5b1fef9dbcc11554062209c22b41f591c9d31d74681b686f6598b",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres-gis@sha256:e26176f740c5b1fef9dbcc11554062209c22b41f591c9d31d74681b686f6598b",
          "name": "POSTGRES_14_GIS_3.2"
        },
        {
          "digest": "sha256:b4dba56b93e7e9eb82ba8fb2dcdcf7cbe08dc16dfcbd8d42bd6d8c86488f6495",
          "image": "registry.connect.redhat.com/crunchydata/postgres-operator@sha256:b4dba56b93e7e9eb82ba8fb2dcdcf7cbe08dc16dfcbd8d42bd6d8c86488f6495",
          "name": "postgres-operator"
        },
        {
          "digest": "sha256:b4dba56b93e7e9eb82ba8fb2dcdcf7cbe08dc16dfcbd8d42bd6d8c86488f6495",
          "image": "registry.connect.redhat.com/crunchydata/postgres-operator@sha256:b4dba56b93e7e9eb82ba8fb2dcdcf7cbe08dc16dfcbd8d42bd6d8c86488f6495",
          "name": "postgres-operator-b4dba56b93e7e9eb82ba8fb2dcdcf7cbe08dc16dfcbd8d42bd6d8c86488f6495-annotation"
        },
        {
          "digest": "sha256:b4dba56b93e7e9eb82ba8fb2dcdcf7cbe08dc16dfcbd8d42bd6d8c86488f6495",
          "image": "registry.connect.redhat.com/crunchydata/postgres-operator@sha256:b4dba56b93e7e9eb82ba8fb2dcdcf7cbe08dc16dfcbd8d42bd6d8c86488f6495",
          "name": "operator"
        },
        {
          "digest": "sha256:7113af9768b12c74373ad350178c7a2db56df9da7ccb08b5c6b6cbd32220e134",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-pgadmin4@sha256:7113af9768b12c74373ad350178c7a2db56df9da7ccb08b5c6b6cbd32220e134",
          "name": "pgadmin"
        },
        {
          "digest": "sha256:ddce52cf45f3a7d850163687836d565dc1b0671424b2044778645d108995a480",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-pgbackrest@sha256:ddce52cf45f3a7d850163687836d565dc1b0671424b2044778645d108995a480",
          "name": "pgbackrest"
        },
        {
          "digest": "sha256:b7aeb28b7aacf76aad8b67037d47c64400d31a281076557f4dfd72a7e09d2a9e",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-pgbouncer@sha256:b7aeb28b7aacf76aad8b67037d47c64400d31a281076557f4dfd72a7e09d2a9e",
          "name": "pgbouncer"
        },
        {
          "digest": "sha256:694a2e85412f82fe444c7a605c2a16c5b6fe07689e7eb919477ae0f19a54e610",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres-exporter@sha256:694a2e85412f82fe444c7a605c2a16c5b6fe07689e7eb919477ae0f19a54e610",
          "name": "pgexporter"
        },
        {
          "digest": "sha256:af0557c9dce647181b8071f5b18f51818a460ea116cbdd22c65a2f0129cf1408",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres@sha256:af0557c9dce647181b8071f5b18f51818a460ea116cbdd22c65a2f0129cf1408",
          "name": "postgres_13"
        },
        {
          "digest": "sha256:030dd4c2535b4ec725006efff074faafc835bd87caeaf0e8cd11d7af885e601c",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres@sha256:030dd4c2535b4ec725006efff074faafc835bd87caeaf0e8cd11d7af885e601c",
          "name": "postgres_14"
        },
        {
          "digest": "sha256:430a39a4815f59e9ee92b1db6fb1ed516bc69a40500bf7e8d9d43b5972f0d058",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres-gis@sha256:430a39a4815f59e9ee92b1db6fb1ed516bc69a40500bf7e8d9d43b5972f0d058",
          "name": "postgres_13_gis_3.0"
        },
        {
          "digest": "sha256:20baa15efefeae142b8ab9034f1956203f6ff2397b1ca53ffe86ddbf95977891",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres-gis@sha256:20baa15efefeae142b8ab9034f1956203f6ff2397b1ca53ffe86ddbf95977891",
          "name": "postgres_13_gis_3.1"
        },
        {
          "digest": "sha256:1de1072082f58449d9d181ddaf0026ffe00c5961e982aef6558a7ac4314581e2",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres-gis@sha256:1de1072082f58449d9d181ddaf0026ffe00c5961e982aef6558a7ac4314581e2",
          "name": "postgres_14_gis_3.1"
        },
        {
          "digest": "sha256:e26176f740c5b1fef9dbcc11554062209c22b41f591c9d31d74681b686f6598b",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres-gis@sha256:e26176f740c5b1fef9dbcc11554062209c22b41f591c9d31d74681b686f6598b",
          "name": "postgres_14_gis_3.2"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.8",
      "version": "5.1.1",
      "version_original": "5.1.1"
    },
    {
      "_id": "62b4a15a240dc134424e1ac1",
      "alm_examples": [
        {
          "api_version": "appdirector.randoli.ca/v1beta1",
          "kind": "ClusterWideGuvnor",
          "metadata": {
            "name": "guvnor"
          },
          "spec": {
            "guvnorCertsSecretName": "guvnor-certs",
            "guvnorSecretName": "guvnor-secret",
            "namespace": "app-director-example-app"
          }
        },
        {
          "api_version": "appdirector.randoli.ca/v1beta1",
          "kind": "Guvnor",
          "metadata": {
            "name": "guvnor",
            "namespace": "app-director-example-app"
          },
          "spec": {
            "guvnorCertsSecretName": "guvnor-certs",
            "guvnorSecretName": "guvnor-secret"
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/randoli/app-director-operator@sha256:1b4615dbd13dc87f9f72a3689bf4fa9f1d3b66c29504422b605beb8d1fe5c431",
      "bundle_path_digest": "sha256:1b4615dbd13dc87f9f72a3689bf4fa9f1d3b66c29504422b605beb8d1fe5c431",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "0.7.3",
      "creation_date": "2022-06-23T17:22:34.968000+00:00",
      "csv_description": "App Director\u00ae Operator manages the Guvnor agent that connects with the App Director\u00ae Platform to receive deployments and send back events. The agent can be scoped to a namespace or cluster-wide. More information can be found at https://www.randoli.ca/app-director-overview. Tekton Pipelines are a pre-requisite for this operator to work. If not already installed, the operator will install the correct version on initial start-up.",
      "csv_display_name": "App-Director",
      "csv_metadata_description": "Define, Manage and Deploy Apps Across Multiple OpenShift Clusters",
      "csv_name": "app-director-operator.v0.7.3",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": false,
          "type": "OwnNamespace"
        },
        {
          "supported": false,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": false,
      "last_update_date": "2022-11-23T08:19:24.901000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.10",
      "organization": "certified-operators",
      "package": "app-director-operator",
      "provided_apis": [
        {
          "group": "appdirector.randoli.ca",
          "kind": "ClusterWideGuvnor",
          "plural": "clusterwideguvnors",
          "version": "v1beta1"
        },
        {
          "group": "appdirector.randoli.ca",
          "kind": "Guvnor",
          "plural": "guvnors",
          "version": "v1beta1"
        }
      ],
      "provider": "Randoli",
      "related_images": [
        {
          "digest": "sha256:09b93f604beb8443432bcb48a6c9291cfaa2232b3ad491fa9c2999a2d24cc78d",
          "image": "docker.io/randoli/app-director-operator:0.7.3@sha256:09b93f604beb8443432bcb48a6c9291cfaa2232b3ad491fa9c2999a2d24cc78d",
          "name": "app-director-operator:0.7.3-09b93f604beb8443432bcb48a6c9291cfaa2232b3ad491fa9c2999a2d24cc78d-annotation"
        },
        {
          "digest": "sha256:e10d1d982dd653db74ca87a1d1ad017bc5ef1aeb651bdea089debf16485b080b",
          "image": "gcr.io/kubebuilder/kube-rbac-proxy:v0.5.0@sha256:e10d1d982dd653db74ca87a1d1ad017bc5ef1aeb651bdea089debf16485b080b",
          "name": "kube-rbac-proxy"
        },
        {
          "digest": "sha256:09b93f604beb8443432bcb48a6c9291cfaa2232b3ad491fa9c2999a2d24cc78d",
          "image": "docker.io/randoli/app-director-operator:0.7.3@sha256:09b93f604beb8443432bcb48a6c9291cfaa2232b3ad491fa9c2999a2d24cc78d",
          "name": "manager"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.10",
      "version": "0.7.3",
      "version_original": "0.7.3"
    },
    {
      "_id": "62b4a1a5240dc134424e1ae7",
      "alm_examples": [
        {
          "api_version": "appdirector.randoli.ca/v1beta1",
          "kind": "ClusterWideGuvnor",
          "metadata": {
            "name": "guvnor"
          },
          "spec": {
            "guvnorCertsSecretName": "guvnor-certs",
            "guvnorSecretName": "guvnor-secret",
            "namespace": "app-director-example-app"
          }
        },
        {
          "api_version": "appdirector.randoli.ca/v1beta1",
          "kind": "Guvnor",
          "metadata": {
            "name": "guvnor",
            "namespace": "app-director-example-app"
          },
          "spec": {
            "guvnorCertsSecretName": "guvnor-certs",
            "guvnorSecretName": "guvnor-secret"
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/randoli/app-director-operator@sha256:1b4615dbd13dc87f9f72a3689bf4fa9f1d3b66c29504422b605beb8d1fe5c431",
      "bundle_path_digest": "sha256:1b4615dbd13dc87f9f72a3689bf4fa9f1d3b66c29504422b605beb8d1fe5c431",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "0.7.3",
      "creation_date": "2022-06-23T17:23:49.347000+00:00",
      "csv_description": "App Director\u00ae Operator manages the Guvnor agent that connects with the App Director\u00ae Platform to receive deployments and send back events. The agent can be scoped to a namespace or cluster-wide. More information can be found at https://www.randoli.ca/app-director-overview. Tekton Pipelines are a pre-requisite for this operator to work. If not already installed, the operator will install the correct version on initial start-up.",
      "csv_display_name": "App-Director",
      "csv_metadata_description": "Define, Manage and Deploy Apps Across Multiple OpenShift Clusters",
      "csv_name": "app-director-operator.v0.7.3",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": false,
          "type": "OwnNamespace"
        },
        {
          "supported": false,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": false,
      "last_update_date": "2022-11-23T07:03:43.963000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.9",
      "organization": "certified-operators",
      "package": "app-director-operator",
      "provided_apis": [
        {
          "group": "appdirector.randoli.ca",
          "kind": "ClusterWideGuvnor",
          "plural": "clusterwideguvnors",
          "version": "v1beta1"
        },
        {
          "group": "appdirector.randoli.ca",
          "kind": "Guvnor",
          "plural": "guvnors",
          "version": "v1beta1"
        }
      ],
      "provider": "Randoli",
      "related_images": [
        {
          "digest": "sha256:09b93f604beb8443432bcb48a6c9291cfaa2232b3ad491fa9c2999a2d24cc78d",
          "image": "docker.io/randoli/app-director-operator:0.7.3@sha256:09b93f604beb8443432bcb48a6c9291cfaa2232b3ad491fa9c2999a2d24cc78d",
          "name": "app-director-operator:0.7.3-09b93f604beb8443432bcb48a6c9291cfaa2232b3ad491fa9c2999a2d24cc78d-annotation"
        },
        {
          "digest": "sha256:e10d1d982dd653db74ca87a1d1ad017bc5ef1aeb651bdea089debf16485b080b",
          "image": "gcr.io/kubebuilder/kube-rbac-proxy:v0.5.0@sha256:e10d1d982dd653db74ca87a1d1ad017bc5ef1aeb651bdea089debf16485b080b",
          "name": "kube-rbac-proxy"
        },
        {
          "digest": "sha256:09b93f604beb8443432bcb48a6c9291cfaa2232b3ad491fa9c2999a2d24cc78d",
          "image": "docker.io/randoli/app-director-operator:0.7.3@sha256:09b93f604beb8443432bcb48a6c9291cfaa2232b3ad491fa9c2999a2d24cc78d",
          "name": "manager"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.9",
      "version": "0.7.3",
      "version_original": "0.7.3"
    },
    {
      "_id": "62b4a4fc150d8182de9079a7",
      "alm_examples": [
        {
          "api_version": "appdirector.randoli.ca/v1beta1",
          "kind": "ClusterWideGuvnor",
          "metadata": {
            "name": "guvnor"
          },
          "spec": {
            "guvnorCertsSecretName": "guvnor-certs",
            "guvnorSecretName": "guvnor-secret",
            "namespace": "app-director-example-app"
          }
        },
        {
          "api_version": "appdirector.randoli.ca/v1beta1",
          "kind": "Guvnor",
          "metadata": {
            "name": "guvnor",
            "namespace": "app-director-example-app"
          },
          "spec": {
            "guvnorCertsSecretName": "guvnor-certs",
            "guvnorSecretName": "guvnor-secret"
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/randoli/app-director-operator@sha256:1b4615dbd13dc87f9f72a3689bf4fa9f1d3b66c29504422b605beb8d1fe5c431",
      "bundle_path_digest": "sha256:1b4615dbd13dc87f9f72a3689bf4fa9f1d3b66c29504422b605beb8d1fe5c431",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "0.7.3",
      "creation_date": "2022-06-23T17:38:04.555000+00:00",
      "csv_description": "App Director\u00ae Operator manages the Guvnor agent that connects with the App Director\u00ae Platform to receive deployments and send back events. The agent can be scoped to a namespace or cluster-wide. More information can be found at https://www.randoli.ca/app-director-overview. Tekton Pipelines are a pre-requisite for this operator to work. If not already installed, the operator will install the correct version on initial start-up.",
      "csv_display_name": "App-Director",
      "csv_metadata_description": "Define, Manage and Deploy Apps Across Multiple OpenShift Clusters",
      "csv_name": "app-director-operator.v0.7.3",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": false,
          "type": "OwnNamespace"
        },
        {
          "supported": false,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": false,
      "last_update_date": "2022-11-23T07:44:47.756000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.8",
      "organization": "certified-operators",
      "package": "app-director-operator",
      "provided_apis": [
        {
          "group": "appdirector.randoli.ca",
          "kind": "ClusterWideGuvnor",
          "plural": "clusterwideguvnors",
          "version": "v1beta1"
        },
        {
          "group": "appdirector.randoli.ca",
          "kind": "Guvnor",
          "plural": "guvnors",
          "version": "v1beta1"
        }
      ],
      "provider": "Randoli",
      "related_images": [
        {
          "digest": "sha256:09b93f604beb8443432bcb48a6c9291cfaa2232b3ad491fa9c2999a2d24cc78d",
          "image": "docker.io/randoli/app-director-operator:0.7.3@sha256:09b93f604beb8443432bcb48a6c9291cfaa2232b3ad491fa9c2999a2d24cc78d",
          "name": "app-director-operator:0.7.3-09b93f604beb8443432bcb48a6c9291cfaa2232b3ad491fa9c2999a2d24cc78d-annotation"
        },
        {
          "digest": "sha256:e10d1d982dd653db74ca87a1d1ad017bc5ef1aeb651bdea089debf16485b080b",
          "image": "gcr.io/kubebuilder/kube-rbac-proxy:v0.5.0@sha256:e10d1d982dd653db74ca87a1d1ad017bc5ef1aeb651bdea089debf16485b080b",
          "name": "kube-rbac-proxy"
        },
        {
          "digest": "sha256:09b93f604beb8443432bcb48a6c9291cfaa2232b3ad491fa9c2999a2d24cc78d",
          "image": "docker.io/randoli/app-director-operator:0.7.3@sha256:09b93f604beb8443432bcb48a6c9291cfaa2232b3ad491fa9c2999a2d24cc78d",
          "name": "manager"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.8",
      "version": "0.7.3",
      "version_original": "0.7.3"
    },
    {
      "_id": "62b4a664be1fd3a96e073bc6",
      "alm_examples": [
        {
          "api_version": "appdirector.randoli.ca/v1beta1",
          "kind": "ClusterWideGuvnor",
          "metadata": {
            "name": "guvnor"
          },
          "spec": {
            "guvnorCertsSecretName": "guvnor-certs",
            "guvnorSecretName": "guvnor-secret",
            "namespace": "app-director-example-app"
          }
        },
        {
          "api_version": "appdirector.randoli.ca/v1beta1",
          "kind": "Guvnor",
          "metadata": {
            "name": "guvnor",
            "namespace": "app-director-example-app"
          },
          "spec": {
            "guvnorCertsSecretName": "guvnor-certs",
            "guvnorSecretName": "guvnor-secret"
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/randoli/app-director-operator@sha256:1b4615dbd13dc87f9f72a3689bf4fa9f1d3b66c29504422b605beb8d1fe5c431",
      "bundle_path_digest": "sha256:1b4615dbd13dc87f9f72a3689bf4fa9f1d3b66c29504422b605beb8d1fe5c431",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "0.7.3",
      "creation_date": "2022-06-23T17:44:04.244000+00:00",
      "csv_description": "App Director\u00ae Operator manages the Guvnor agent that connects with the App Director\u00ae Platform to receive deployments and send back events. The agent can be scoped to a namespace or cluster-wide. More information can be found at https://www.randoli.ca/app-director-overview. Tekton Pipelines are a pre-requisite for this operator to work. If not already installed, the operator will install the correct version on initial start-up.",
      "csv_display_name": "App-Director",
      "csv_metadata_description": "Define, Manage and Deploy Apps Across Multiple OpenShift Clusters",
      "csv_name": "app-director-operator.v0.7.3",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": false,
          "type": "OwnNamespace"
        },
        {
          "supported": false,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": false,
      "last_update_date": "2022-11-23T07:33:39.658000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.6",
      "organization": "certified-operators",
      "package": "app-director-operator",
      "provided_apis": [
        {
          "group": "appdirector.randoli.ca",
          "kind": "ClusterWideGuvnor",
          "plural": "clusterwideguvnors",
          "version": "v1beta1"
        },
        {
          "group": "appdirector.randoli.ca",
          "kind": "Guvnor",
          "plural": "guvnors",
          "version": "v1beta1"
        }
      ],
      "provider": "Randoli",
      "related_images": [
        {
          "digest": "sha256:09b93f604beb8443432bcb48a6c9291cfaa2232b3ad491fa9c2999a2d24cc78d",
          "image": "docker.io/randoli/app-director-operator:0.7.3@sha256:09b93f604beb8443432bcb48a6c9291cfaa2232b3ad491fa9c2999a2d24cc78d",
          "name": "app-director-operator:0.7.3-09b93f604beb8443432bcb48a6c9291cfaa2232b3ad491fa9c2999a2d24cc78d-annotation"
        },
        {
          "digest": "sha256:e10d1d982dd653db74ca87a1d1ad017bc5ef1aeb651bdea089debf16485b080b",
          "image": "gcr.io/kubebuilder/kube-rbac-proxy:v0.5.0@sha256:e10d1d982dd653db74ca87a1d1ad017bc5ef1aeb651bdea089debf16485b080b",
          "name": "kube-rbac-proxy"
        },
        {
          "digest": "sha256:09b93f604beb8443432bcb48a6c9291cfaa2232b3ad491fa9c2999a2d24cc78d",
          "image": "docker.io/randoli/app-director-operator:0.7.3@sha256:09b93f604beb8443432bcb48a6c9291cfaa2232b3ad491fa9c2999a2d24cc78d",
          "name": "manager"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.6",
      "version": "0.7.3",
      "version_original": "0.7.3"
    },
    {
      "_id": "62b4a679150d8182de907a23",
      "alm_examples": [
        {
          "api_version": "appdirector.randoli.ca/v1beta1",
          "kind": "ClusterWideGuvnor",
          "metadata": {
            "name": "guvnor"
          },
          "spec": {
            "guvnorCertsSecretName": "guvnor-certs",
            "guvnorSecretName": "guvnor-secret",
            "namespace": "app-director-example-app"
          }
        },
        {
          "api_version": "appdirector.randoli.ca/v1beta1",
          "kind": "Guvnor",
          "metadata": {
            "name": "guvnor",
            "namespace": "app-director-example-app"
          },
          "spec": {
            "guvnorCertsSecretName": "guvnor-certs",
            "guvnorSecretName": "guvnor-secret"
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/randoli/app-director-operator@sha256:1b4615dbd13dc87f9f72a3689bf4fa9f1d3b66c29504422b605beb8d1fe5c431",
      "bundle_path_digest": "sha256:1b4615dbd13dc87f9f72a3689bf4fa9f1d3b66c29504422b605beb8d1fe5c431",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "0.7.3",
      "creation_date": "2022-06-23T17:44:25.405000+00:00",
      "csv_description": "App Director\u00ae Operator manages the Guvnor agent that connects with the App Director\u00ae Platform to receive deployments and send back events. The agent can be scoped to a namespace or cluster-wide. More information can be found at https://www.randoli.ca/app-director-overview. Tekton Pipelines are a pre-requisite for this operator to work. If not already installed, the operator will install the correct version on initial start-up.",
      "csv_display_name": "App-Director",
      "csv_metadata_description": "Define, Manage and Deploy Apps Across Multiple OpenShift Clusters",
      "csv_name": "app-director-operator.v0.7.3",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": false,
          "type": "OwnNamespace"
        },
        {
          "supported": false,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": false,
      "last_update_date": "2022-11-23T07:30:01.078000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.7",
      "organization": "certified-operators",
      "package": "app-director-operator",
      "provided_apis": [
        {
          "group": "appdirector.randoli.ca",
          "kind": "ClusterWideGuvnor",
          "plural": "clusterwideguvnors",
          "version": "v1beta1"
        },
        {
          "group": "appdirector.randoli.ca",
          "kind": "Guvnor",
          "plural": "guvnors",
          "version": "v1beta1"
        }
      ],
      "provider": "Randoli",
      "related_images": [
        {
          "digest": "sha256:09b93f604beb8443432bcb48a6c9291cfaa2232b3ad491fa9c2999a2d24cc78d",
          "image": "docker.io/randoli/app-director-operator:0.7.3@sha256:09b93f604beb8443432bcb48a6c9291cfaa2232b3ad491fa9c2999a2d24cc78d",
          "name": "app-director-operator:0.7.3-09b93f604beb8443432bcb48a6c9291cfaa2232b3ad491fa9c2999a2d24cc78d-annotation"
        },
        {
          "digest": "sha256:e10d1d982dd653db74ca87a1d1ad017bc5ef1aeb651bdea089debf16485b080b",
          "image": "gcr.io/kubebuilder/kube-rbac-proxy:v0.5.0@sha256:e10d1d982dd653db74ca87a1d1ad017bc5ef1aeb651bdea089debf16485b080b",
          "name": "kube-rbac-proxy"
        },
        {
          "digest": "sha256:09b93f604beb8443432bcb48a6c9291cfaa2232b3ad491fa9c2999a2d24cc78d",
          "image": "docker.io/randoli/app-director-operator:0.7.3@sha256:09b93f604beb8443432bcb48a6c9291cfaa2232b3ad491fa9c2999a2d24cc78d",
          "name": "manager"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.7",
      "version": "0.7.3",
      "version_original": "0.7.3"
    },
    {
      "_id": "62bad63c655931047b684467",
      "alm_examples": [
        {
          "api_version": "mongodb.com/v1",
          "kind": "MongoDB",
          "metadata": {
            "name": "my-replica-set"
          },
          "spec": {
            "credentials": "my-credentials",
            "members": 3,
            "opsManager": {
              "configMapRef": {
                "name": "my-project"
              }
            },
            "persistent": true,
            "type": "ReplicaSet",
            "version": "4.4.0-ent"
          }
        },
        {
          "api_version": "mongodb.com/v1",
          "kind": "MongoDB",
          "metadata": {
            "name": "sample-sharded-cluster"
          },
          "spec": {
            "configServerCount": 3,
            "credentials": "my-credentials",
            "mongodsPerShardCount": 3,
            "mongosCount": 2,
            "opsManager": {
              "configMapRef": {
                "name": "my-project"
              }
            },
            "persistent": true,
            "shardCount": 1,
            "type": "ShardedCluster",
            "version": "4.4.0-ent"
          }
        },
        {
          "api_version": "mongodb.com/v1",
          "kind": "MongoDBMulti",
          "metadata": {
            "name": "multi-replica-set"
          },
          "spec": {
            "clusterSpecList": {
              "clusterSpecs": [
                {
                  "clusterName": "e2e.cluster1.mongokubernetes.com",
                  "members": 2
                },
                {
                  "clusterName": "e2e.cluster2.mongokubernetes.com",
                  "members": 1
                },
                {
                  "clusterName": "e2e.cluster3.mongokubernetes.com",
                  "members": 2
                }
              ]
            },
            "credentials": "my-credentials",
            "duplicateServiceObjects": false,
            "opsManager": {
              "configMapRef": {
                "name": "my-project"
              }
            },
            "persistent": false,
            "type": "ReplicaSet",
            "version": "4.4.0-ent"
          }
        },
        {
          "api_version": "mongodb.com/v1",
          "kind": "MongoDBOpsManager",
          "metadata": {
            "name": "ops-manager"
          },
          "spec": {
            "adminCredentials": "ops-manager-admin",
            "applicationDatabase": {
              "members": 3,
              "persistent": true,
              "podSpec": {
                "cpu": 1
              }
            },
            "configuration": {
              "mms.fromEmailAddr": "admin@thecompany.com"
            },
            "externalConnectivity": {
              "type": "LoadBalancer"
            },
            "version": "4.4.1"
          }
        },
        {
          "api_version": "mongodb.com/v1",
          "kind": "MongoDBUser",
          "metadata": {
            "name": "my-replica-set-x509-user"
          },
          "spec": {
            "db": "$external",
            "mongodbResourceRef": {
              "name": "my-replica-set"
            },
            "roles": [
              {
                "db": "admin",
                "name": "dbOwner"
              }
            ],
            "username": "CN=my-replica-set-x509-user,OU=cloud,O=MongoDB,L=New York,ST=New York,C=US"
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/mongodb/enterprise-operator-bundle@sha256:82fdada2703e13083559ff1b8e8467a8e075d00e2153c0ee7323e4b713964694",
      "bundle_path_digest": "sha256:82fdada2703e13083559ff1b8e8467a8e075d00e2153c0ee7323e4b713964694",
      "capabilities": [
        "Deep Insights"
      ],
      "channel_name": "stable",
      "creation_date": "2022-06-28T10:21:48.637000+00:00",
      "csv_description": "The MongoDB Enterprise Kubernetes Operator enables easy deploys of MongoDB\ninto Kubernetes clusters, using our management, monitoring and backup\nplatforms, Ops Manager and Cloud Manager.\n\n## Before You Start\n\nTo start using the operator you''ll need an account in MongoDB Cloud Manager or\na MongoDB Ops Manager deployment.\n\n* [Create a Secret with your OpsManager API key](https://docs.mongodb.com/kubernetes-operator/stable/tutorial/create-operator-credentials/#procedure)\n\n* [Create a ConfigMap with your OpsManager project ID and URL](https://docs.mongodb.com/kubernetes-operator/stable/tutorial/create-project-using-configmap/)\n\nBy installing this integration, you will be able to deploy MongoDB instances\nwith a single simple command.\n\n## Required Parameters\n\n* `opsManager` or `cloudManager` - Enter the name of the ConfigMap containing project information\n* `credentials` - Enter the name of the Secret containing your OpsManager credentials\n* `type` - Enter MongoDB Deployment Types (\"Standalone\", \"ReplicaSet\", \"ShardedCluster\"\n\n## Supported MongoDB Deployment Types ##\n\n* Standalone: An instance of mongod that is running as a single server and\nnot as part of a replica set, this is, it does not do any kind of\nreplication.\n\n* Replica Set: A replica set in MongoDB is a group of mongod processes that\nmaintain the same data set. Replica sets provide redundancy and high\navailability, and are the basis for all production deployments. This section\nintroduces replication in MongoDB as well as the components and architecture\nof replica sets. The section also provides tutorials for common tasks\nrelated to replica sets.\n\n* Sharded Cluster: The set of nodes comprising a sharded MongoDB deployment.\nA sharded cluster consists of config servers, shards, and one or more mongos\nrouting processes. Sharding is a A database architecture that partitions\ndata by key ranges and distributes the data among two or more database\ninstances. Sharding enables horizontal scaling.\n\n## Requirements for deploying MongoDB OpsManager\n\n* In order to deploy resources of type MongoDB OpsManager, you will need to\ncreate a secret containing the [credentials](https://docs.mongodb.com/kubernetes-operator/stable/tutorial/plan-om-resource/#om-rsrc-prereqs)\nfor the Global Onwer user\n\n## Security ##\n\nThe operator can enable TLS for all traffic between servers and also between\nclients and servers. Before enabling `security.tls.enabled` to `true` you\nshould create your certificates.  or you can leave the operator to create all\nthe certificates for you. The operator ability to create certs is been\ndepricted due to Kuberentes API changes.\n\nFor more information, please read the official MongoDB\nKubernetes Operator  [docs](https://docs.mongodb.com/kubernetes-operator/stable/).\n",
      "csv_display_name": "MongoDB Enterprise Operator",
      "csv_metadata_description": "The MongoDB Enterprise Kubernetes Operator enables easy deploys of MongoDB into Kubernetes clusters, using our management, monitoring and backup platforms, Ops Manager and Cloud Manager.",
      "csv_name": "mongodb-enterprise.v1.16.2",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-11-23T07:05:55.398000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.11",
      "organization": "certified-operators",
      "package": "mongodb-enterprise",
      "provided_apis": [
        {
          "group": "mongodb.com",
          "kind": "MongoDB",
          "version": "v1"
        },
        {
          "group": "mongodb.com",
          "kind": "MongoDBMulti",
          "version": "v1"
        },
        {
          "group": "mongodb.com",
          "kind": "MongoDBOpsManager",
          "version": "v1"
        },
        {
          "group": "mongodb.com",
          "kind": "MongoDBUser",
          "version": "v1"
        }
      ],
      "provider": "MongoDB, Inc",
      "related_images": [
        {
          "digest": "sha256:fe45e0de1221ea740602a3c55d6e45c2f8af46f26963b8f82ffd608079335026",
          "image": "registry.connect.redhat.com/mongodb/enterprise-operator@sha256:fe45e0de1221ea740602a3c55d6e45c2f8af46f26963b8f82ffd608079335026",
          "name": "mongodb-enterprise-operator"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.11",
      "version": "1.16.2",
      "version_original": "1.16.2"
    },
    {
      "_id": "62bad65fb490e03bc2bc7dbb",
      "alm_examples": [
        {
          "api_version": "mongodb.com/v1",
          "kind": "MongoDB",
          "metadata": {
            "name": "my-replica-set"
          },
          "spec": {
            "credentials": "my-credentials",
            "members": 3,
            "opsManager": {
              "configMapRef": {
                "name": "my-project"
              }
            },
            "persistent": true,
            "type": "ReplicaSet",
            "version": "4.4.0-ent"
          }
        },
        {
          "api_version": "mongodb.com/v1",
          "kind": "MongoDB",
          "metadata": {
            "name": "sample-sharded-cluster"
          },
          "spec": {
            "configServerCount": 3,
            "credentials": "my-credentials",
            "mongodsPerShardCount": 3,
            "mongosCount": 2,
            "opsManager": {
              "configMapRef": {
                "name": "my-project"
              }
            },
            "persistent": true,
            "shardCount": 1,
            "type": "ShardedCluster",
            "version": "4.4.0-ent"
          }
        },
        {
          "api_version": "mongodb.com/v1",
          "kind": "MongoDBMulti",
          "metadata": {
            "name": "multi-replica-set"
          },
          "spec": {
            "clusterSpecList": {
              "clusterSpecs": [
                {
                  "clusterName": "e2e.cluster1.mongokubernetes.com",
                  "members": 2
                },
                {
                  "clusterName": "e2e.cluster2.mongokubernetes.com",
                  "members": 1
                },
                {
                  "clusterName": "e2e.cluster3.mongokubernetes.com",
                  "members": 2
                }
              ]
            },
            "credentials": "my-credentials",
            "duplicateServiceObjects": false,
            "opsManager": {
              "configMapRef": {
                "name": "my-project"
              }
            },
            "persistent": false,
            "type": "ReplicaSet",
            "version": "4.4.0-ent"
          }
        },
        {
          "api_version": "mongodb.com/v1",
          "kind": "MongoDBOpsManager",
          "metadata": {
            "name": "ops-manager"
          },
          "spec": {
            "adminCredentials": "ops-manager-admin",
            "applicationDatabase": {
              "members": 3,
              "persistent": true,
              "podSpec": {
                "cpu": 1
              }
            },
            "configuration": {
              "mms.fromEmailAddr": "admin@thecompany.com"
            },
            "externalConnectivity": {
              "type": "LoadBalancer"
            },
            "version": "4.4.1"
          }
        },
        {
          "api_version": "mongodb.com/v1",
          "kind": "MongoDBUser",
          "metadata": {
            "name": "my-replica-set-x509-user"
          },
          "spec": {
            "db": "$external",
            "mongodbResourceRef": {
              "name": "my-replica-set"
            },
            "roles": [
              {
                "db": "admin",
                "name": "dbOwner"
              }
            ],
            "username": "CN=my-replica-set-x509-user,OU=cloud,O=MongoDB,L=New York,ST=New York,C=US"
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/mongodb/enterprise-operator-bundle@sha256:82fdada2703e13083559ff1b8e8467a8e075d00e2153c0ee7323e4b713964694",
      "bundle_path_digest": "sha256:82fdada2703e13083559ff1b8e8467a8e075d00e2153c0ee7323e4b713964694",
      "capabilities": [
        "Deep Insights"
      ],
      "channel_name": "stable",
      "creation_date": "2022-06-28T10:22:23.945000+00:00",
      "csv_description": "The MongoDB Enterprise Kubernetes Operator enables easy deploys of MongoDB\ninto Kubernetes clusters, using our management, monitoring and backup\nplatforms, Ops Manager and Cloud Manager.\n\n## Before You Start\n\nTo start using the operator you''ll need an account in MongoDB Cloud Manager or\na MongoDB Ops Manager deployment.\n\n* [Create a Secret with your OpsManager API key](https://docs.mongodb.com/kubernetes-operator/stable/tutorial/create-operator-credentials/#procedure)\n\n* [Create a ConfigMap with your OpsManager project ID and URL](https://docs.mongodb.com/kubernetes-operator/stable/tutorial/create-project-using-configmap/)\n\nBy installing this integration, you will be able to deploy MongoDB instances\nwith a single simple command.\n\n## Required Parameters\n\n* `opsManager` or `cloudManager` - Enter the name of the ConfigMap containing project information\n* `credentials` - Enter the name of the Secret containing your OpsManager credentials\n* `type` - Enter MongoDB Deployment Types (\"Standalone\", \"ReplicaSet\", \"ShardedCluster\"\n\n## Supported MongoDB Deployment Types ##\n\n* Standalone: An instance of mongod that is running as a single server and\nnot as part of a replica set, this is, it does not do any kind of\nreplication.\n\n* Replica Set: A replica set in MongoDB is a group of mongod processes that\nmaintain the same data set. Replica sets provide redundancy and high\navailability, and are the basis for all production deployments. This section\nintroduces replication in MongoDB as well as the components and architecture\nof replica sets. The section also provides tutorials for common tasks\nrelated to replica sets.\n\n* Sharded Cluster: The set of nodes comprising a sharded MongoDB deployment.\nA sharded cluster consists of config servers, shards, and one or more mongos\nrouting processes. Sharding is a A database architecture that partitions\ndata by key ranges and distributes the data among two or more database\ninstances. Sharding enables horizontal scaling.\n\n## Requirements for deploying MongoDB OpsManager\n\n* In order to deploy resources of type MongoDB OpsManager, you will need to\ncreate a secret containing the [credentials](https://docs.mongodb.com/kubernetes-operator/stable/tutorial/plan-om-resource/#om-rsrc-prereqs)\nfor the Global Onwer user\n\n## Security ##\n\nThe operator can enable TLS for all traffic between servers and also between\nclients and servers. Before enabling `security.tls.enabled` to `true` you\nshould create your certificates.  or you can leave the operator to create all\nthe certificates for you. The operator ability to create certs is been\ndepricted due to Kuberentes API changes.\n\nFor more information, please read the official MongoDB\nKubernetes Operator  [docs](https://docs.mongodb.com/kubernetes-operator/stable/).\n",
      "csv_display_name": "MongoDB Enterprise Operator",
      "csv_metadata_description": "The MongoDB Enterprise Kubernetes Operator enables easy deploys of MongoDB into Kubernetes clusters, using our management, monitoring and backup platforms, Ops Manager and Cloud Manager.",
      "csv_name": "mongodb-enterprise.v1.16.2",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-11-23T07:23:01.948000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.9",
      "organization": "certified-operators",
      "package": "mongodb-enterprise",
      "provided_apis": [
        {
          "group": "mongodb.com",
          "kind": "MongoDB",
          "plural": "mongodb",
          "version": "v1"
        },
        {
          "group": "mongodb.com",
          "kind": "MongoDBMulti",
          "plural": "mongodbmulti",
          "version": "v1"
        },
        {
          "group": "mongodb.com",
          "kind": "MongoDBUser",
          "plural": "mongodbusers",
          "version": "v1"
        },
        {
          "group": "mongodb.com",
          "kind": "MongoDBOpsManager",
          "plural": "opsmanagers",
          "version": "v1"
        }
      ],
      "provider": "MongoDB, Inc",
      "related_images": [
        {
          "digest": "sha256:fe45e0de1221ea740602a3c55d6e45c2f8af46f26963b8f82ffd608079335026",
          "image": "registry.connect.redhat.com/mongodb/enterprise-operator@sha256:fe45e0de1221ea740602a3c55d6e45c2f8af46f26963b8f82ffd608079335026",
          "name": "mongodb-enterprise-operator"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.9",
      "version": "1.16.2",
      "version_original": "1.16.2"
    },
    {
      "_id": "62bad66ab490e03bc2bc7dc7",
      "alm_examples": [
        {
          "api_version": "mongodb.com/v1",
          "kind": "MongoDB",
          "metadata": {
            "name": "my-replica-set"
          },
          "spec": {
            "credentials": "my-credentials",
            "members": 3,
            "opsManager": {
              "configMapRef": {
                "name": "my-project"
              }
            },
            "persistent": true,
            "type": "ReplicaSet",
            "version": "4.4.0-ent"
          }
        },
        {
          "api_version": "mongodb.com/v1",
          "kind": "MongoDB",
          "metadata": {
            "name": "sample-sharded-cluster"
          },
          "spec": {
            "configServerCount": 3,
            "credentials": "my-credentials",
            "mongodsPerShardCount": 3,
            "mongosCount": 2,
            "opsManager": {
              "configMapRef": {
                "name": "my-project"
              }
            },
            "persistent": true,
            "shardCount": 1,
            "type": "ShardedCluster",
            "version": "4.4.0-ent"
          }
        },
        {
          "api_version": "mongodb.com/v1",
          "kind": "MongoDBMulti",
          "metadata": {
            "name": "multi-replica-set"
          },
          "spec": {
            "clusterSpecList": {
              "clusterSpecs": [
                {
                  "clusterName": "e2e.cluster1.mongokubernetes.com",
                  "members": 2
                },
                {
                  "clusterName": "e2e.cluster2.mongokubernetes.com",
                  "members": 1
                },
                {
                  "clusterName": "e2e.cluster3.mongokubernetes.com",
                  "members": 2
                }
              ]
            },
            "credentials": "my-credentials",
            "duplicateServiceObjects": false,
            "opsManager": {
              "configMapRef": {
                "name": "my-project"
              }
            },
            "persistent": false,
            "type": "ReplicaSet",
            "version": "4.4.0-ent"
          }
        },
        {
          "api_version": "mongodb.com/v1",
          "kind": "MongoDBOpsManager",
          "metadata": {
            "name": "ops-manager"
          },
          "spec": {
            "adminCredentials": "ops-manager-admin",
            "applicationDatabase": {
              "members": 3,
              "persistent": true,
              "podSpec": {
                "cpu": 1
              }
            },
            "configuration": {
              "mms.fromEmailAddr": "admin@thecompany.com"
            },
            "externalConnectivity": {
              "type": "LoadBalancer"
            },
            "version": "4.4.1"
          }
        },
        {
          "api_version": "mongodb.com/v1",
          "kind": "MongoDBUser",
          "metadata": {
            "name": "my-replica-set-x509-user"
          },
          "spec": {
            "db": "$external",
            "mongodbResourceRef": {
              "name": "my-replica-set"
            },
            "roles": [
              {
                "db": "admin",
                "name": "dbOwner"
              }
            ],
            "username": "CN=my-replica-set-x509-user,OU=cloud,O=MongoDB,L=New York,ST=New York,C=US"
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/mongodb/enterprise-operator-bundle@sha256:82fdada2703e13083559ff1b8e8467a8e075d00e2153c0ee7323e4b713964694",
      "bundle_path_digest": "sha256:82fdada2703e13083559ff1b8e8467a8e075d00e2153c0ee7323e4b713964694",
      "capabilities": [
        "Deep Insights"
      ],
      "channel_name": "stable",
      "creation_date": "2022-06-28T10:22:34.111000+00:00",
      "csv_description": "The MongoDB Enterprise Kubernetes Operator enables easy deploys of MongoDB\ninto Kubernetes clusters, using our management, monitoring and backup\nplatforms, Ops Manager and Cloud Manager.\n\n## Before You Start\n\nTo start using the operator you''ll need an account in MongoDB Cloud Manager or\na MongoDB Ops Manager deployment.\n\n* [Create a Secret with your OpsManager API key](https://docs.mongodb.com/kubernetes-operator/stable/tutorial/create-operator-credentials/#procedure)\n\n* [Create a ConfigMap with your OpsManager project ID and URL](https://docs.mongodb.com/kubernetes-operator/stable/tutorial/create-project-using-configmap/)\n\nBy installing this integration, you will be able to deploy MongoDB instances\nwith a single simple command.\n\n## Required Parameters\n\n* `opsManager` or `cloudManager` - Enter the name of the ConfigMap containing project information\n* `credentials` - Enter the name of the Secret containing your OpsManager credentials\n* `type` - Enter MongoDB Deployment Types (\"Standalone\", \"ReplicaSet\", \"ShardedCluster\"\n\n## Supported MongoDB Deployment Types ##\n\n* Standalone: An instance of mongod that is running as a single server and\nnot as part of a replica set, this is, it does not do any kind of\nreplication.\n\n* Replica Set: A replica set in MongoDB is a group of mongod processes that\nmaintain the same data set. Replica sets provide redundancy and high\navailability, and are the basis for all production deployments. This section\nintroduces replication in MongoDB as well as the components and architecture\nof replica sets. The section also provides tutorials for common tasks\nrelated to replica sets.\n\n* Sharded Cluster: The set of nodes comprising a sharded MongoDB deployment.\nA sharded cluster consists of config servers, shards, and one or more mongos\nrouting processes. Sharding is a A database architecture that partitions\ndata by key ranges and distributes the data among two or more database\ninstances. Sharding enables horizontal scaling.\n\n## Requirements for deploying MongoDB OpsManager\n\n* In order to deploy resources of type MongoDB OpsManager, you will need to\ncreate a secret containing the [credentials](https://docs.mongodb.com/kubernetes-operator/stable/tutorial/plan-om-resource/#om-rsrc-prereqs)\nfor the Global Onwer user\n\n## Security ##\n\nThe operator can enable TLS for all traffic between servers and also between\nclients and servers. Before enabling `security.tls.enabled` to `true` you\nshould create your certificates.  or you can leave the operator to create all\nthe certificates for you. The operator ability to create certs is been\ndepricted due to Kuberentes API changes.\n\nFor more information, please read the official MongoDB\nKubernetes Operator  [docs](https://docs.mongodb.com/kubernetes-operator/stable/).\n",
      "csv_display_name": "MongoDB Enterprise Operator",
      "csv_metadata_description": "The MongoDB Enterprise Kubernetes Operator enables easy deploys of MongoDB into Kubernetes clusters, using our management, monitoring and backup platforms, Ops Manager and Cloud Manager.",
      "csv_name": "mongodb-enterprise.v1.16.2",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-11-23T08:42:19.201000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.10",
      "organization": "certified-operators",
      "package": "mongodb-enterprise",
      "provided_apis": [
        {
          "group": "mongodb.com",
          "kind": "MongoDB",
          "plural": "mongodb",
          "version": "v1"
        },
        {
          "group": "mongodb.com",
          "kind": "MongoDBMulti",
          "plural": "mongodbmulti",
          "version": "v1"
        },
        {
          "group": "mongodb.com",
          "kind": "MongoDBUser",
          "plural": "mongodbusers",
          "version": "v1"
        },
        {
          "group": "mongodb.com",
          "kind": "MongoDBOpsManager",
          "plural": "opsmanagers",
          "version": "v1"
        }
      ],
      "provider": "MongoDB, Inc",
      "related_images": [
        {
          "digest": "sha256:fe45e0de1221ea740602a3c55d6e45c2f8af46f26963b8f82ffd608079335026",
          "image": "registry.connect.redhat.com/mongodb/enterprise-operator@sha256:fe45e0de1221ea740602a3c55d6e45c2f8af46f26963b8f82ffd608079335026",
          "name": "mongodb-enterprise-operator"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.10",
      "version": "1.16.2",
      "version_original": "1.16.2"
    },
    {
      "_id": "62bad6fdb490e03bc2bc7e17",
      "alm_examples": [
        {
          "api_version": "mongodb.com/v1",
          "kind": "MongoDB",
          "metadata": {
            "name": "my-replica-set"
          },
          "spec": {
            "credentials": "my-credentials",
            "members": 3,
            "opsManager": {
              "configMapRef": {
                "name": "my-project"
              }
            },
            "persistent": true,
            "type": "ReplicaSet",
            "version": "4.4.0-ent"
          }
        },
        {
          "api_version": "mongodb.com/v1",
          "kind": "MongoDB",
          "metadata": {
            "name": "sample-sharded-cluster"
          },
          "spec": {
            "configServerCount": 3,
            "credentials": "my-credentials",
            "mongodsPerShardCount": 3,
            "mongosCount": 2,
            "opsManager": {
              "configMapRef": {
                "name": "my-project"
              }
            },
            "persistent": true,
            "shardCount": 1,
            "type": "ShardedCluster",
            "version": "4.4.0-ent"
          }
        },
        {
          "api_version": "mongodb.com/v1",
          "kind": "MongoDBMulti",
          "metadata": {
            "name": "multi-replica-set"
          },
          "spec": {
            "clusterSpecList": {
              "clusterSpecs": [
                {
                  "clusterName": "e2e.cluster1.mongokubernetes.com",
                  "members": 2
                },
                {
                  "clusterName": "e2e.cluster2.mongokubernetes.com",
                  "members": 1
                },
                {
                  "clusterName": "e2e.cluster3.mongokubernetes.com",
                  "members": 2
                }
              ]
            },
            "credentials": "my-credentials",
            "duplicateServiceObjects": false,
            "opsManager": {
              "configMapRef": {
                "name": "my-project"
              }
            },
            "persistent": false,
            "type": "ReplicaSet",
            "version": "4.4.0-ent"
          }
        },
        {
          "api_version": "mongodb.com/v1",
          "kind": "MongoDBOpsManager",
          "metadata": {
            "name": "ops-manager"
          },
          "spec": {
            "adminCredentials": "ops-manager-admin",
            "applicationDatabase": {
              "members": 3,
              "persistent": true,
              "podSpec": {
                "cpu": 1
              }
            },
            "configuration": {
              "mms.fromEmailAddr": "admin@thecompany.com"
            },
            "externalConnectivity": {
              "type": "LoadBalancer"
            },
            "version": "4.4.1"
          }
        },
        {
          "api_version": "mongodb.com/v1",
          "kind": "MongoDBUser",
          "metadata": {
            "name": "my-replica-set-x509-user"
          },
          "spec": {
            "db": "$external",
            "mongodbResourceRef": {
              "name": "my-replica-set"
            },
            "roles": [
              {
                "db": "admin",
                "name": "dbOwner"
              }
            ],
            "username": "CN=my-replica-set-x509-user,OU=cloud,O=MongoDB,L=New York,ST=New York,C=US"
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/mongodb/enterprise-operator-bundle@sha256:82fdada2703e13083559ff1b8e8467a8e075d00e2153c0ee7323e4b713964694",
      "bundle_path_digest": "sha256:82fdada2703e13083559ff1b8e8467a8e075d00e2153c0ee7323e4b713964694",
      "capabilities": [
        "Deep Insights"
      ],
      "channel_name": "stable",
      "creation_date": "2022-06-28T10:25:01.498000+00:00",
      "csv_description": "The MongoDB Enterprise Kubernetes Operator enables easy deploys of MongoDB\ninto Kubernetes clusters, using our management, monitoring and backup\nplatforms, Ops Manager and Cloud Manager.\n\n## Before You Start\n\nTo start using the operator you''ll need an account in MongoDB Cloud Manager or\na MongoDB Ops Manager deployment.\n\n* [Create a Secret with your OpsManager API key](https://docs.mongodb.com/kubernetes-operator/stable/tutorial/create-operator-credentials/#procedure)\n\n* [Create a ConfigMap with your OpsManager project ID and URL](https://docs.mongodb.com/kubernetes-operator/stable/tutorial/create-project-using-configmap/)\n\nBy installing this integration, you will be able to deploy MongoDB instances\nwith a single simple command.\n\n## Required Parameters\n\n* `opsManager` or `cloudManager` - Enter the name of the ConfigMap containing project information\n* `credentials` - Enter the name of the Secret containing your OpsManager credentials\n* `type` - Enter MongoDB Deployment Types (\"Standalone\", \"ReplicaSet\", \"ShardedCluster\"\n\n## Supported MongoDB Deployment Types ##\n\n* Standalone: An instance of mongod that is running as a single server and\nnot as part of a replica set, this is, it does not do any kind of\nreplication.\n\n* Replica Set: A replica set in MongoDB is a group of mongod processes that\nmaintain the same data set. Replica sets provide redundancy and high\navailability, and are the basis for all production deployments. This section\nintroduces replication in MongoDB as well as the components and architecture\nof replica sets. The section also provides tutorials for common tasks\nrelated to replica sets.\n\n* Sharded Cluster: The set of nodes comprising a sharded MongoDB deployment.\nA sharded cluster consists of config servers, shards, and one or more mongos\nrouting processes. Sharding is a A database architecture that partitions\ndata by key ranges and distributes the data among two or more database\ninstances. Sharding enables horizontal scaling.\n\n## Requirements for deploying MongoDB OpsManager\n\n* In order to deploy resources of type MongoDB OpsManager, you will need to\ncreate a secret containing the [credentials](https://docs.mongodb.com/kubernetes-operator/stable/tutorial/plan-om-resource/#om-rsrc-prereqs)\nfor the Global Onwer user\n\n## Security ##\n\nThe operator can enable TLS for all traffic between servers and also between\nclients and servers. Before enabling `security.tls.enabled` to `true` you\nshould create your certificates.  or you can leave the operator to create all\nthe certificates for you. The operator ability to create certs is been\ndepricted due to Kuberentes API changes.\n\nFor more information, please read the official MongoDB\nKubernetes Operator  [docs](https://docs.mongodb.com/kubernetes-operator/stable/).\n",
      "csv_display_name": "MongoDB Enterprise Operator",
      "csv_metadata_description": "The MongoDB Enterprise Kubernetes Operator enables easy deploys of MongoDB into Kubernetes clusters, using our management, monitoring and backup platforms, Ops Manager and Cloud Manager.",
      "csv_name": "mongodb-enterprise.v1.16.2",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-11-23T07:26:28.357000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.6",
      "organization": "certified-operators",
      "package": "mongodb-enterprise",
      "provided_apis": [
        {
          "group": "mongodb.com",
          "kind": "MongoDBMulti",
          "plural": "mongodbmulti",
          "version": "v1"
        },
        {
          "group": "mongodb.com",
          "kind": "MongoDBUser",
          "plural": "mongodbusers",
          "version": "v1"
        },
        {
          "group": "mongodb.com",
          "kind": "MongoDBOpsManager",
          "plural": "opsmanagers",
          "version": "v1"
        },
        {
          "group": "mongodb.com",
          "kind": "MongoDB",
          "plural": "mongodb",
          "version": "v1"
        }
      ],
      "provider": "MongoDB, Inc",
      "related_images": [
        {
          "digest": "sha256:fe45e0de1221ea740602a3c55d6e45c2f8af46f26963b8f82ffd608079335026",
          "image": "registry.connect.redhat.com/mongodb/enterprise-operator@sha256:fe45e0de1221ea740602a3c55d6e45c2f8af46f26963b8f82ffd608079335026",
          "name": "mongodb-enterprise-operator"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.6",
      "version": "1.16.2",
      "version_original": "1.16.2"
    },
    {
      "_id": "62bad72909a31e274257de77",
      "alm_examples": [
        {
          "api_version": "mongodb.com/v1",
          "kind": "MongoDB",
          "metadata": {
            "name": "my-replica-set"
          },
          "spec": {
            "credentials": "my-credentials",
            "members": 3,
            "opsManager": {
              "configMapRef": {
                "name": "my-project"
              }
            },
            "persistent": true,
            "type": "ReplicaSet",
            "version": "4.4.0-ent"
          }
        },
        {
          "api_version": "mongodb.com/v1",
          "kind": "MongoDB",
          "metadata": {
            "name": "sample-sharded-cluster"
          },
          "spec": {
            "configServerCount": 3,
            "credentials": "my-credentials",
            "mongodsPerShardCount": 3,
            "mongosCount": 2,
            "opsManager": {
              "configMapRef": {
                "name": "my-project"
              }
            },
            "persistent": true,
            "shardCount": 1,
            "type": "ShardedCluster",
            "version": "4.4.0-ent"
          }
        },
        {
          "api_version": "mongodb.com/v1",
          "kind": "MongoDBMulti",
          "metadata": {
            "name": "multi-replica-set"
          },
          "spec": {
            "clusterSpecList": {
              "clusterSpecs": [
                {
                  "clusterName": "e2e.cluster1.mongokubernetes.com",
                  "members": 2
                },
                {
                  "clusterName": "e2e.cluster2.mongokubernetes.com",
                  "members": 1
                },
                {
                  "clusterName": "e2e.cluster3.mongokubernetes.com",
                  "members": 2
                }
              ]
            },
            "credentials": "my-credentials",
            "duplicateServiceObjects": false,
            "opsManager": {
              "configMapRef": {
                "name": "my-project"
              }
            },
            "persistent": false,
            "type": "ReplicaSet",
            "version": "4.4.0-ent"
          }
        },
        {
          "api_version": "mongodb.com/v1",
          "kind": "MongoDBOpsManager",
          "metadata": {
            "name": "ops-manager"
          },
          "spec": {
            "adminCredentials": "ops-manager-admin",
            "applicationDatabase": {
              "members": 3,
              "persistent": true,
              "podSpec": {
                "cpu": 1
              }
            },
            "configuration": {
              "mms.fromEmailAddr": "admin@thecompany.com"
            },
            "externalConnectivity": {
              "type": "LoadBalancer"
            },
            "version": "4.4.1"
          }
        },
        {
          "api_version": "mongodb.com/v1",
          "kind": "MongoDBUser",
          "metadata": {
            "name": "my-replica-set-x509-user"
          },
          "spec": {
            "db": "$external",
            "mongodbResourceRef": {
              "name": "my-replica-set"
            },
            "roles": [
              {
                "db": "admin",
                "name": "dbOwner"
              }
            ],
            "username": "CN=my-replica-set-x509-user,OU=cloud,O=MongoDB,L=New York,ST=New York,C=US"
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/mongodb/enterprise-operator-bundle@sha256:82fdada2703e13083559ff1b8e8467a8e075d00e2153c0ee7323e4b713964694",
      "bundle_path_digest": "sha256:82fdada2703e13083559ff1b8e8467a8e075d00e2153c0ee7323e4b713964694",
      "capabilities": [
        "Deep Insights"
      ],
      "channel_name": "stable",
      "creation_date": "2022-06-28T10:25:45.133000+00:00",
      "csv_description": "The MongoDB Enterprise Kubernetes Operator enables easy deploys of MongoDB\ninto Kubernetes clusters, using our management, monitoring and backup\nplatforms, Ops Manager and Cloud Manager.\n\n## Before You Start\n\nTo start using the operator you''ll need an account in MongoDB Cloud Manager or\na MongoDB Ops Manager deployment.\n\n* [Create a Secret with your OpsManager API key](https://docs.mongodb.com/kubernetes-operator/stable/tutorial/create-operator-credentials/#procedure)\n\n* [Create a ConfigMap with your OpsManager project ID and URL](https://docs.mongodb.com/kubernetes-operator/stable/tutorial/create-project-using-configmap/)\n\nBy installing this integration, you will be able to deploy MongoDB instances\nwith a single simple command.\n\n## Required Parameters\n\n* `opsManager` or `cloudManager` - Enter the name of the ConfigMap containing project information\n* `credentials` - Enter the name of the Secret containing your OpsManager credentials\n* `type` - Enter MongoDB Deployment Types (\"Standalone\", \"ReplicaSet\", \"ShardedCluster\"\n\n## Supported MongoDB Deployment Types ##\n\n* Standalone: An instance of mongod that is running as a single server and\nnot as part of a replica set, this is, it does not do any kind of\nreplication.\n\n* Replica Set: A replica set in MongoDB is a group of mongod processes that\nmaintain the same data set. Replica sets provide redundancy and high\navailability, and are the basis for all production deployments. This section\nintroduces replication in MongoDB as well as the components and architecture\nof replica sets. The section also provides tutorials for common tasks\nrelated to replica sets.\n\n* Sharded Cluster: The set of nodes comprising a sharded MongoDB deployment.\nA sharded cluster consists of config servers, shards, and one or more mongos\nrouting processes. Sharding is a A database architecture that partitions\ndata by key ranges and distributes the data among two or more database\ninstances. Sharding enables horizontal scaling.\n\n## Requirements for deploying MongoDB OpsManager\n\n* In order to deploy resources of type MongoDB OpsManager, you will need to\ncreate a secret containing the [credentials](https://docs.mongodb.com/kubernetes-operator/stable/tutorial/plan-om-resource/#om-rsrc-prereqs)\nfor the Global Onwer user\n\n## Security ##\n\nThe operator can enable TLS for all traffic between servers and also between\nclients and servers. Before enabling `security.tls.enabled` to `true` you\nshould create your certificates.  or you can leave the operator to create all\nthe certificates for you. The operator ability to create certs is been\ndepricted due to Kuberentes API changes.\n\nFor more information, please read the official MongoDB\nKubernetes Operator  [docs](https://docs.mongodb.com/kubernetes-operator/stable/).\n",
      "csv_display_name": "MongoDB Enterprise Operator",
      "csv_metadata_description": "The MongoDB Enterprise Kubernetes Operator enables easy deploys of MongoDB into Kubernetes clusters, using our management, monitoring and backup platforms, Ops Manager and Cloud Manager.",
      "csv_name": "mongodb-enterprise.v1.16.2",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-11-23T06:38:57.205000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.7",
      "organization": "certified-operators",
      "package": "mongodb-enterprise",
      "provided_apis": [
        {
          "group": "mongodb.com",
          "kind": "MongoDBOpsManager",
          "plural": "opsmanagers",
          "version": "v1"
        },
        {
          "group": "mongodb.com",
          "kind": "MongoDB",
          "plural": "mongodb",
          "version": "v1"
        },
        {
          "group": "mongodb.com",
          "kind": "MongoDBMulti",
          "plural": "mongodbmulti",
          "version": "v1"
        },
        {
          "group": "mongodb.com",
          "kind": "MongoDBUser",
          "plural": "mongodbusers",
          "version": "v1"
        }
      ],
      "provider": "MongoDB, Inc",
      "related_images": [
        {
          "digest": "sha256:fe45e0de1221ea740602a3c55d6e45c2f8af46f26963b8f82ffd608079335026",
          "image": "registry.connect.redhat.com/mongodb/enterprise-operator@sha256:fe45e0de1221ea740602a3c55d6e45c2f8af46f26963b8f82ffd608079335026",
          "name": "mongodb-enterprise-operator"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.7",
      "version": "1.16.2",
      "version_original": "1.16.2"
    },
    {
      "_id": "62bad758655931047b684504",
      "alm_examples": [
        {
          "api_version": "mongodb.com/v1",
          "kind": "MongoDB",
          "metadata": {
            "name": "my-replica-set"
          },
          "spec": {
            "credentials": "my-credentials",
            "members": 3,
            "opsManager": {
              "configMapRef": {
                "name": "my-project"
              }
            },
            "persistent": true,
            "type": "ReplicaSet",
            "version": "4.4.0-ent"
          }
        },
        {
          "api_version": "mongodb.com/v1",
          "kind": "MongoDB",
          "metadata": {
            "name": "sample-sharded-cluster"
          },
          "spec": {
            "configServerCount": 3,
            "credentials": "my-credentials",
            "mongodsPerShardCount": 3,
            "mongosCount": 2,
            "opsManager": {
              "configMapRef": {
                "name": "my-project"
              }
            },
            "persistent": true,
            "shardCount": 1,
            "type": "ShardedCluster",
            "version": "4.4.0-ent"
          }
        },
        {
          "api_version": "mongodb.com/v1",
          "kind": "MongoDBMulti",
          "metadata": {
            "name": "multi-replica-set"
          },
          "spec": {
            "clusterSpecList": {
              "clusterSpecs": [
                {
                  "clusterName": "e2e.cluster1.mongokubernetes.com",
                  "members": 2
                },
                {
                  "clusterName": "e2e.cluster2.mongokubernetes.com",
                  "members": 1
                },
                {
                  "clusterName": "e2e.cluster3.mongokubernetes.com",
                  "members": 2
                }
              ]
            },
            "credentials": "my-credentials",
            "duplicateServiceObjects": false,
            "opsManager": {
              "configMapRef": {
                "name": "my-project"
              }
            },
            "persistent": false,
            "type": "ReplicaSet",
            "version": "4.4.0-ent"
          }
        },
        {
          "api_version": "mongodb.com/v1",
          "kind": "MongoDBOpsManager",
          "metadata": {
            "name": "ops-manager"
          },
          "spec": {
            "adminCredentials": "ops-manager-admin",
            "applicationDatabase": {
              "members": 3,
              "persistent": true,
              "podSpec": {
                "cpu": 1
              }
            },
            "configuration": {
              "mms.fromEmailAddr": "admin@thecompany.com"
            },
            "externalConnectivity": {
              "type": "LoadBalancer"
            },
            "version": "4.4.1"
          }
        },
        {
          "api_version": "mongodb.com/v1",
          "kind": "MongoDBUser",
          "metadata": {
            "name": "my-replica-set-x509-user"
          },
          "spec": {
            "db": "$external",
            "mongodbResourceRef": {
              "name": "my-replica-set"
            },
            "roles": [
              {
                "db": "admin",
                "name": "dbOwner"
              }
            ],
            "username": "CN=my-replica-set-x509-user,OU=cloud,O=MongoDB,L=New York,ST=New York,C=US"
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/mongodb/enterprise-operator-bundle@sha256:82fdada2703e13083559ff1b8e8467a8e075d00e2153c0ee7323e4b713964694",
      "bundle_path_digest": "sha256:82fdada2703e13083559ff1b8e8467a8e075d00e2153c0ee7323e4b713964694",
      "capabilities": [
        "Deep Insights"
      ],
      "channel_name": "stable",
      "creation_date": "2022-06-28T10:26:32.353000+00:00",
      "csv_description": "The MongoDB Enterprise Kubernetes Operator enables easy deploys of MongoDB\ninto Kubernetes clusters, using our management, monitoring and backup\nplatforms, Ops Manager and Cloud Manager.\n\n## Before You Start\n\nTo start using the operator you''ll need an account in MongoDB Cloud Manager or\na MongoDB Ops Manager deployment.\n\n* [Create a Secret with your OpsManager API key](https://docs.mongodb.com/kubernetes-operator/stable/tutorial/create-operator-credentials/#procedure)\n\n* [Create a ConfigMap with your OpsManager project ID and URL](https://docs.mongodb.com/kubernetes-operator/stable/tutorial/create-project-using-configmap/)\n\nBy installing this integration, you will be able to deploy MongoDB instances\nwith a single simple command.\n\n## Required Parameters\n\n* `opsManager` or `cloudManager` - Enter the name of the ConfigMap containing project information\n* `credentials` - Enter the name of the Secret containing your OpsManager credentials\n* `type` - Enter MongoDB Deployment Types (\"Standalone\", \"ReplicaSet\", \"ShardedCluster\"\n\n## Supported MongoDB Deployment Types ##\n\n* Standalone: An instance of mongod that is running as a single server and\nnot as part of a replica set, this is, it does not do any kind of\nreplication.\n\n* Replica Set: A replica set in MongoDB is a group of mongod processes that\nmaintain the same data set. Replica sets provide redundancy and high\navailability, and are the basis for all production deployments. This section\nintroduces replication in MongoDB as well as the components and architecture\nof replica sets. The section also provides tutorials for common tasks\nrelated to replica sets.\n\n* Sharded Cluster: The set of nodes comprising a sharded MongoDB deployment.\nA sharded cluster consists of config servers, shards, and one or more mongos\nrouting processes. Sharding is a A database architecture that partitions\ndata by key ranges and distributes the data among two or more database\ninstances. Sharding enables horizontal scaling.\n\n## Requirements for deploying MongoDB OpsManager\n\n* In order to deploy resources of type MongoDB OpsManager, you will need to\ncreate a secret containing the [credentials](https://docs.mongodb.com/kubernetes-operator/stable/tutorial/plan-om-resource/#om-rsrc-prereqs)\nfor the Global Onwer user\n\n## Security ##\n\nThe operator can enable TLS for all traffic between servers and also between\nclients and servers. Before enabling `security.tls.enabled` to `true` you\nshould create your certificates.  or you can leave the operator to create all\nthe certificates for you. The operator ability to create certs is been\ndepricted due to Kuberentes API changes.\n\nFor more information, please read the official MongoDB\nKubernetes Operator  [docs](https://docs.mongodb.com/kubernetes-operator/stable/).\n",
      "csv_display_name": "MongoDB Enterprise Operator",
      "csv_metadata_description": "The MongoDB Enterprise Kubernetes Operator enables easy deploys of MongoDB into Kubernetes clusters, using our management, monitoring and backup platforms, Ops Manager and Cloud Manager.",
      "csv_name": "mongodb-enterprise.v1.16.2",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-11-23T07:45:19.716000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.8",
      "organization": "certified-operators",
      "package": "mongodb-enterprise",
      "provided_apis": [
        {
          "group": "mongodb.com",
          "kind": "MongoDBUser",
          "plural": "mongodbusers",
          "version": "v1"
        },
        {
          "group": "mongodb.com",
          "kind": "MongoDBOpsManager",
          "plural": "opsmanagers",
          "version": "v1"
        },
        {
          "group": "mongodb.com",
          "kind": "MongoDB",
          "plural": "mongodb",
          "version": "v1"
        },
        {
          "group": "mongodb.com",
          "kind": "MongoDBMulti",
          "plural": "mongodbmulti",
          "version": "v1"
        }
      ],
      "provider": "MongoDB, Inc",
      "related_images": [
        {
          "digest": "sha256:fe45e0de1221ea740602a3c55d6e45c2f8af46f26963b8f82ffd608079335026",
          "image": "registry.connect.redhat.com/mongodb/enterprise-operator@sha256:fe45e0de1221ea740602a3c55d6e45c2f8af46f26963b8f82ffd608079335026",
          "name": "mongodb-enterprise-operator"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.8",
      "version": "1.16.2",
      "version_original": "1.16.2"
    },
    {
      "_id": "62bb39b543b2350294b255e8",
      "alm_examples": [
        {
          "api_version": "nvidia.com/v1",
          "kind": "ClusterPolicy",
          "metadata": {
            "name": "gpu-cluster-policy"
          },
          "spec": {
            "daemonsets": {},
            "dcgm": {
              "enabled": true
            },
            "dcgmExporter": {
              "config": {
                "name": ""
              }
            },
            "devicePlugin": {},
            "driver": {
              "certConfig": {
                "name": ""
              },
              "enabled": true,
              "kernelModuleConfig": {
                "name": ""
              },
              "licensingConfig": {
                "configMapName": "",
                "nlsEnabled": false
              },
              "repoConfig": {
                "configMapName": ""
              },
              "use_ocp_driver_toolkit": true,
              "virtualTopology": {
                "config": ""
              }
            },
            "gfd": {},
            "mig": {
              "strategy": "single"
            },
            "migManager": {
              "enabled": true
            },
            "nodeStatusExporter": {
              "enabled": true
            },
            "operator": {
              "defaultRuntime": "crio",
              "deployGFD": true,
              "initContainer": {}
            },
            "toolkit": {
              "enabled": true
            },
            "validator": {
              "plugin": {
                "env": [
                  {
                    "name": "WITH_WORKLOAD",
                    "value": "true"
                  }
                ]
              }
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [
          "Disconnected"
        ],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/nvidia/gpu-operator-bundle@sha256:4d1268054559b7678028a7a66e36567c55417b729101ef131dec8d989cbef1d3",
      "bundle_path_digest": "sha256:4d1268054559b7678028a7a66e36567c55417b729101ef131dec8d989cbef1d3",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "v1.11",
      "creation_date": "2022-06-28T17:26:13.709000+00:00",
      "csv_description": "Kubernetes provides access to special hardware resources such as NVIDIA GPUs, NICs, Infiniband adapters and other devices through the [device plugin framework](https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/). However, configuring and managing nodes with these hardware resources requires configuration of multiple software components such as drivers, container runtimes or other libraries which are difficult and prone to errors.\nThe NVIDIA GPU Operator uses the [operator framework](https://coreos.com/blog/introducing-operator-framework) within Kubernetes to automate the management of all NVIDIA software components needed to provision and monitor GPUs. These components include the NVIDIA drivers (to enable CUDA), Kubernetes device plugin for GPUs, the NVIDIA Container Runtime, automatic node labelling and NVIDIA DCGM exporter.\nVisit the official site of the [GPU Operator](https://github.com/NVIDIA/gpu-operator) for more information. For getting started with using the GPU Operator with OpenShift, see the instructions [here](https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/openshift/contents.html).\n",
      "csv_display_name": "NVIDIA GPU Operator",
      "csv_metadata_description": "Automate the management and monitoring of NVIDIA GPUs.",
      "csv_name": "gpu-operator-certified.v1.10.0",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": false,
      "last_update_date": "2022-11-23T08:28:44.090000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.10",
      "organization": "certified-operators",
      "package": "gpu-operator-certified",
      "provided_apis": [
        {
          "group": "nvidia.com",
          "kind": "ClusterPolicy",
          "plural": "clusterpolicies",
          "version": "v1"
        }
      ],
      "provider": "NVIDIA Corporation",
      "related_images": [
        {
          "digest": "sha256:e2da1a42a885482a6b40b397397e79c037a10e71abd08e4658eb0b9fc2a761be",
          "image": "nvcr.io/nvidia/gpu-operator@sha256:e2da1a42a885482a6b40b397397e79c037a10e71abd08e4658eb0b9fc2a761be",
          "name": "gpu-operator-image"
        },
        {
          "digest": "sha256:67c48a8370c34f68b46eb57339e87d01c04198368a0423553fab7565c862c4df",
          "image": "nvcr.io/nvidia/k8s/dcgm-exporter@sha256:67c48a8370c34f68b46eb57339e87d01c04198368a0423553fab7565c862c4df",
          "name": "dcgm-exporter-image"
        },
        {
          "digest": "sha256:ac13256c295126b854a07421c45de0721e7a57974b38b3e66eb1fdc3a4e29f2e",
          "image": "nvcr.io/nvidia/cloud-native/dcgm@sha256:ac13256c295126b854a07421c45de0721e7a57974b38b3e66eb1fdc3a4e29f2e",
          "name": "dcgm-image"
        },
        {
          "digest": "sha256:9abb063e6b74085728fead5ed5bbe3bd7b3f57de60e87f6b964bba2d46f1bc67",
          "image": "nvcr.io/nvidia/k8s/container-toolkit@sha256:9abb063e6b74085728fead5ed5bbe3bd7b3f57de60e87f6b964bba2d46f1bc67",
          "name": "container-toolkit-image"
        },
        {
          "digest": "sha256:d46393d6bd5be020c78e1d45669d2bb3ac8681df13369ddbbbf90740e354c0cf",
          "image": "nvcr.io/nvidia/driver@sha256:d46393d6bd5be020c78e1d45669d2bb3ac8681df13369ddbbbf90740e354c0cf",
          "name": "driver-image"
        },
        {
          "digest": "sha256:aa95c16106280e36a5e32d2fe4c66e8b70f5a114860c6f4ed5b1a4085c63601b",
          "image": "nvcr.io/nvidia/k8s-device-plugin@sha256:aa95c16106280e36a5e32d2fe4c66e8b70f5a114860c6f4ed5b1a4085c63601b",
          "name": "device-plugin-image"
        },
        {
          "digest": "sha256:c2e3d0fe41a0d227dbb70caec03c780cc76317515e5ab3875f31d50c63f41c66",
          "image": "nvcr.io/nvidia/gpu-feature-discovery@sha256:c2e3d0fe41a0d227dbb70caec03c780cc76317515e5ab3875f31d50c63f41c66",
          "name": "gpu-feature-discovery-image"
        },
        {
          "digest": "sha256:e8f06ce53415e8572f633b2169289d973993cb1e49d4dcc686ac134194d88f33",
          "image": "nvcr.io/nvidia/cloud-native/k8s-mig-manager@sha256:e8f06ce53415e8572f633b2169289d973993cb1e49d4dcc686ac134194d88f33",
          "name": "mig-manager-image"
        },
        {
          "digest": "sha256:c7609c27c68ae6520ff21862665d10e1dd0dc99fe0615968310478632187341b",
          "image": "docker.io/nvidia/cuda@sha256:c7609c27c68ae6520ff21862665d10e1dd0dc99fe0615968310478632187341b",
          "name": "init-container-image"
        },
        {
          "digest": "sha256:6819f4c9136e82e44ec34b03ba71c72cbbeebe1db49c596a8bc245fea3bee596",
          "image": "nvcr.io/nvidia/cloud-native/gpu-operator-validator@sha256:6819f4c9136e82e44ec34b03ba71c72cbbeebe1db49c596a8bc245fea3bee596",
          "name": "gpu-operator-validator-image"
        },
        {
          "digest": "sha256:5b16056257acc51b517d9cdb1da3218693cefc214af93789e6e214fd2b4cacf1",
          "image": "nvcr.io/nvidia/cloud-native/k8s-driver-manager@sha256:5b16056257acc51b517d9cdb1da3218693cefc214af93789e6e214fd2b4cacf1",
          "name": "k8s-driver-manager-image"
        },
        {
          "digest": "sha256:e2da1a42a885482a6b40b397397e79c037a10e71abd08e4658eb0b9fc2a761be",
          "image": "nvcr.io/nvidia/gpu-operator@sha256:e2da1a42a885482a6b40b397397e79c037a10e71abd08e4658eb0b9fc2a761be",
          "name": "gpu-operator-e2da1a42a885482a6b40b397397e79c037a10e71abd08e4658eb0b9fc2a761be-annotation"
        },
        {
          "digest": "sha256:e2da1a42a885482a6b40b397397e79c037a10e71abd08e4658eb0b9fc2a761be",
          "image": "nvcr.io/nvidia/gpu-operator@sha256:e2da1a42a885482a6b40b397397e79c037a10e71abd08e4658eb0b9fc2a761be",
          "name": "gpu-operator"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.10",
      "version": "1.10.0",
      "version_original": "1.10.0"
    },
    {
      "_id": "62bb39b943b2350294b255ed",
      "alm_examples": [
        {
          "api_version": "nvidia.com/v1",
          "kind": "ClusterPolicy",
          "metadata": {
            "name": "gpu-cluster-policy"
          },
          "spec": {
            "daemonsets": {},
            "dcgm": {
              "enabled": true
            },
            "dcgmExporter": {
              "config": {
                "name": ""
              }
            },
            "devicePlugin": {},
            "driver": {
              "certConfig": {
                "name": ""
              },
              "enabled": true,
              "kernelModuleConfig": {
                "name": ""
              },
              "licensingConfig": {
                "configMapName": "",
                "nlsEnabled": false
              },
              "repoConfig": {
                "configMapName": ""
              },
              "use_ocp_driver_toolkit": true,
              "virtualTopology": {
                "config": ""
              }
            },
            "gfd": {},
            "mig": {
              "strategy": "single"
            },
            "migManager": {
              "enabled": true
            },
            "nodeStatusExporter": {
              "enabled": true
            },
            "operator": {
              "defaultRuntime": "crio",
              "deployGFD": true,
              "initContainer": {}
            },
            "toolkit": {
              "enabled": true
            },
            "validator": {
              "plugin": {
                "env": [
                  {
                    "name": "WITH_WORKLOAD",
                    "value": "true"
                  }
                ]
              }
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [
          "Disconnected"
        ],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/nvidia/gpu-operator-bundle@sha256:60b1d25ebb3aada94985e359bb059d4491bf0148ac75d51b68a461670659e429",
      "bundle_path_digest": "sha256:60b1d25ebb3aada94985e359bb059d4491bf0148ac75d51b68a461670659e429",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "v1.11",
      "creation_date": "2022-06-28T17:26:17.224000+00:00",
      "csv_description": "Kubernetes provides access to special hardware resources such as NVIDIA GPUs, NICs, Infiniband adapters and other devices through the [device plugin framework](https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/). However, configuring and managing nodes with these hardware resources requires configuration of multiple software components such as drivers, container runtimes or other libraries which are difficult and prone to errors.\nThe NVIDIA GPU Operator uses the [operator framework](https://coreos.com/blog/introducing-operator-framework) within Kubernetes to automate the management of all NVIDIA software components needed to provision and monitor GPUs. These components include the NVIDIA drivers (to enable CUDA), Kubernetes device plugin for GPUs, the NVIDIA Container Runtime, automatic node labelling and NVIDIA DCGM exporter.\nVisit the official site of the [GPU Operator](https://github.com/NVIDIA/gpu-operator) for more information. For getting started with using the GPU Operator with OpenShift, see the instructions [here](https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/openshift/contents.html).\n",
      "csv_display_name": "NVIDIA GPU Operator",
      "csv_metadata_description": "Automate the management and monitoring of NVIDIA GPUs.",
      "csv_name": "gpu-operator-certified.v1.10.1",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": false,
      "last_update_date": "2022-11-23T08:29:04.730000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.10",
      "organization": "certified-operators",
      "package": "gpu-operator-certified",
      "provided_apis": [
        {
          "group": "nvidia.com",
          "kind": "ClusterPolicy",
          "plural": "clusterpolicies",
          "version": "v1"
        }
      ],
      "provider": "NVIDIA Corporation",
      "related_images": [
        {
          "digest": "sha256:c7f9074c1a7f58947c807f23f2eece3a8b04e11175127919156f8e864821d45a",
          "image": "nvcr.io/nvidia/gpu-operator@sha256:c7f9074c1a7f58947c807f23f2eece3a8b04e11175127919156f8e864821d45a",
          "name": "gpu-operator-image"
        },
        {
          "digest": "sha256:67c48a8370c34f68b46eb57339e87d01c04198368a0423553fab7565c862c4df",
          "image": "nvcr.io/nvidia/k8s/dcgm-exporter@sha256:67c48a8370c34f68b46eb57339e87d01c04198368a0423553fab7565c862c4df",
          "name": "dcgm-exporter-image"
        },
        {
          "digest": "sha256:ac13256c295126b854a07421c45de0721e7a57974b38b3e66eb1fdc3a4e29f2e",
          "image": "nvcr.io/nvidia/cloud-native/dcgm@sha256:ac13256c295126b854a07421c45de0721e7a57974b38b3e66eb1fdc3a4e29f2e",
          "name": "dcgm-image"
        },
        {
          "digest": "sha256:9abb063e6b74085728fead5ed5bbe3bd7b3f57de60e87f6b964bba2d46f1bc67",
          "image": "nvcr.io/nvidia/k8s/container-toolkit@sha256:9abb063e6b74085728fead5ed5bbe3bd7b3f57de60e87f6b964bba2d46f1bc67",
          "name": "container-toolkit-image"
        },
        {
          "digest": "sha256:f99b66855e92451a59a722355dd52bf97eb985cc8a647172273de8178aeaa93c",
          "image": "nvcr.io/nvidia/driver@sha256:f99b66855e92451a59a722355dd52bf97eb985cc8a647172273de8178aeaa93c",
          "name": "driver-image"
        },
        {
          "digest": "sha256:aa95c16106280e36a5e32d2fe4c66e8b70f5a114860c6f4ed5b1a4085c63601b",
          "image": "nvcr.io/nvidia/k8s-device-plugin@sha256:aa95c16106280e36a5e32d2fe4c66e8b70f5a114860c6f4ed5b1a4085c63601b",
          "name": "device-plugin-image"
        },
        {
          "digest": "sha256:c2e3d0fe41a0d227dbb70caec03c780cc76317515e5ab3875f31d50c63f41c66",
          "image": "nvcr.io/nvidia/gpu-feature-discovery@sha256:c2e3d0fe41a0d227dbb70caec03c780cc76317515e5ab3875f31d50c63f41c66",
          "name": "gpu-feature-discovery-image"
        },
        {
          "digest": "sha256:e8f06ce53415e8572f633b2169289d973993cb1e49d4dcc686ac134194d88f33",
          "image": "nvcr.io/nvidia/cloud-native/k8s-mig-manager@sha256:e8f06ce53415e8572f633b2169289d973993cb1e49d4dcc686ac134194d88f33",
          "name": "mig-manager-image"
        },
        {
          "digest": "sha256:7f0f7dc8c993a86865d4d9f9b0e36d113ea15603d5d5279d2879fa2bc21c8236",
          "image": "nvcr.io/nvidia/cuda@sha256:7f0f7dc8c993a86865d4d9f9b0e36d113ea15603d5d5279d2879fa2bc21c8236",
          "name": "init-container-image"
        },
        {
          "digest": "sha256:24d804e8f005d7aeca8343aa13e5f92295d8642a4c47cb24e3ac86a22543bc37",
          "image": "nvcr.io/nvidia/cloud-native/gpu-operator-validator@sha256:24d804e8f005d7aeca8343aa13e5f92295d8642a4c47cb24e3ac86a22543bc37",
          "name": "gpu-operator-validator-image"
        },
        {
          "digest": "sha256:5b16056257acc51b517d9cdb1da3218693cefc214af93789e6e214fd2b4cacf1",
          "image": "nvcr.io/nvidia/cloud-native/k8s-driver-manager@sha256:5b16056257acc51b517d9cdb1da3218693cefc214af93789e6e214fd2b4cacf1",
          "name": "k8s-driver-manager-image"
        },
        {
          "digest": "sha256:c7f9074c1a7f58947c807f23f2eece3a8b04e11175127919156f8e864821d45a",
          "image": "nvcr.io/nvidia/gpu-operator@sha256:c7f9074c1a7f58947c807f23f2eece3a8b04e11175127919156f8e864821d45a",
          "name": "gpu-operator-c7f9074c1a7f58947c807f23f2eece3a8b04e11175127919156f8e864821d45a-annotation"
        },
        {
          "digest": "sha256:c7f9074c1a7f58947c807f23f2eece3a8b04e11175127919156f8e864821d45a",
          "image": "nvcr.io/nvidia/gpu-operator@sha256:c7f9074c1a7f58947c807f23f2eece3a8b04e11175127919156f8e864821d45a",
          "name": "gpu-operator"
        }
      ],
      "replaces": null,
      "skip_range": ">=v1.9.0 <v1.10.1",
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.10",
      "version": "1.10.1",
      "version_original": "1.10.1"
    },
    {
      "_id": "62bb39bd43b2350294b255f0",
      "alm_examples": [
        {
          "api_version": "nvidia.com/v1",
          "kind": "ClusterPolicy",
          "metadata": {
            "name": "gpu-cluster-policy"
          },
          "spec": {
            "daemonsets": {},
            "dcgm": {
              "enabled": true
            },
            "dcgmExporter": {
              "config": {
                "name": ""
              }
            },
            "devicePlugin": {
              "config": {
                "default": "",
                "name": ""
              }
            },
            "driver": {
              "certConfig": {
                "name": ""
              },
              "enabled": true,
              "kernelModuleConfig": {
                "name": ""
              },
              "licensingConfig": {
                "configMapName": "",
                "nlsEnabled": false
              },
              "repoConfig": {
                "configMapName": ""
              },
              "virtualTopology": {
                "config": ""
              }
            },
            "gfd": {},
            "mig": {
              "strategy": "single"
            },
            "migManager": {
              "enabled": true
            },
            "nodeStatusExporter": {
              "enabled": true
            },
            "operator": {
              "defaultRuntime": "crio",
              "initContainer": {},
              "use_ocp_driver_toolkit": true
            },
            "sandboxDevicePlugin": {
              "enabled": true
            },
            "sandboxWorkloads": {
              "defaultWorkload": "container",
              "enabled": false
            },
            "toolkit": {
              "enabled": true
            },
            "validator": {
              "plugin": {
                "env": [
                  {
                    "name": "WITH_WORKLOAD",
                    "value": "true"
                  }
                ]
              }
            },
            "vfioManager": {
              "enabled": true
            },
            "vgpuDeviceManager": {
              "config": {
                "default": "default",
                "name": "vgpu-devices-config"
              },
              "enabled": true
            },
            "vgpuManager": {
              "enabled": true
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [
          "Disconnected"
        ],
        "valid_subscription": []
      },
      "architectures": [
        "amd64",
        "arm64"
      ],
      "bundle_path": "registry.connect.redhat.com/nvidia/gpu-operator-bundle@sha256:501dd759f32adb585848f511f6dfa7bc9372d5e905abea9b082dd50cbe232216",
      "bundle_path_digest": "sha256:501dd759f32adb585848f511f6dfa7bc9372d5e905abea9b082dd50cbe232216",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "stable",
      "creation_date": "2022-06-28T17:26:21.053000+00:00",
      "csv_description": "Kubernetes provides access to special hardware resources such as NVIDIA GPUs, NICs, Infiniband adapters and other devices through the [device plugin framework](https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/). However, configuring and managing nodes with these hardware resources requires configuration of multiple software components such as drivers, container runtimes or other libraries which are difficult and prone to errors.\nThe NVIDIA GPU Operator uses the [operator framework](https://cloud.redhat.com/blog/introducing-the-operator-framework) within Kubernetes to automate the management of all NVIDIA software components needed to provision and monitor GPUs. These components include the NVIDIA drivers (to enable CUDA), Kubernetes device plugin for GPUs, the NVIDIA Container Runtime, automatic node labelling and NVIDIA DCGM exporter.\nVisit the official site of the [GPU Operator](https://github.com/NVIDIA/gpu-operator) for more information. For getting started with using the GPU Operator with OpenShift, see the instructions [here](https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/openshift/contents.html).\n",
      "csv_display_name": "NVIDIA GPU Operator",
      "csv_metadata_description": "Automate the management and monitoring of NVIDIA GPUs.",
      "csv_name": "gpu-operator-certified.v1.11.0",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": false,
      "last_update_date": "2022-11-23T08:29:17.215000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.10",
      "organization": "certified-operators",
      "package": "gpu-operator-certified",
      "provided_apis": [
        {
          "group": "nvidia.com",
          "kind": "ClusterPolicy",
          "plural": "clusterpolicies",
          "version": "v1"
        }
      ],
      "provider": "NVIDIA Corporation",
      "related_images": [
        {
          "digest": "sha256:7e3699c82bf02ba640d49775ce1b1fc96a5eca02d5ea453ea0ed1571d73779a5",
          "image": "nvcr.io/nvidia/gpu-operator@sha256:7e3699c82bf02ba640d49775ce1b1fc96a5eca02d5ea453ea0ed1571d73779a5",
          "name": "gpu-operator-image"
        },
        {
          "digest": "sha256:b90a519e9f30fbc013e1806bc081268724e9e80b4035c67c167ab3b43a37704a",
          "image": "nvcr.io/nvidia/k8s/dcgm-exporter@sha256:b90a519e9f30fbc013e1806bc081268724e9e80b4035c67c167ab3b43a37704a",
          "name": "dcgm-exporter-image"
        },
        {
          "digest": "sha256:8490d555131a57a11311561667ea9ec8cf554a399d2492a6f9e6677c01ec4b9a",
          "image": "nvcr.io/nvidia/cloud-native/dcgm@sha256:8490d555131a57a11311561667ea9ec8cf554a399d2492a6f9e6677c01ec4b9a",
          "name": "dcgm-image"
        },
        {
          "digest": "sha256:2d6e86db0e2b0db64c00566938e1851e25d4bb1019b1e7b696e2b199cba33397",
          "image": "nvcr.io/nvidia/k8s/container-toolkit@sha256:2d6e86db0e2b0db64c00566938e1851e25d4bb1019b1e7b696e2b199cba33397",
          "name": "container-toolkit-image"
        },
        {
          "digest": "sha256:8c9fabc1cd773e9526eb7e8526084f065f6cca9d0fd8dc2a672a0bb717f1cc60",
          "image": "nvcr.io/nvidia/driver@sha256:8c9fabc1cd773e9526eb7e8526084f065f6cca9d0fd8dc2a672a0bb717f1cc60",
          "name": "driver-image"
        },
        {
          "digest": "sha256:6e2902d73c54cf57bec94edf166d206cfeb0b948b5aab70800ccbe025064b3a1",
          "image": "nvcr.io/nvidia/driver@sha256:6e2902d73c54cf57bec94edf166d206cfeb0b948b5aab70800ccbe025064b3a1",
          "name": "driver-image-510"
        },
        {
          "digest": "sha256:cf8ce04907d0e8decf90a8dccfa6134af9c39509d2f8a21c355ed71223acacdb",
          "image": "nvcr.io/nvidia/driver@sha256:cf8ce04907d0e8decf90a8dccfa6134af9c39509d2f8a21c355ed71223acacdb",
          "name": "driver-image-470"
        },
        {
          "digest": "sha256:b8799e7397fae5a27446737443091eed05b918816f53c400a48be71125ee22e8",
          "image": "nvcr.io/nvidia/driver@sha256:b8799e7397fae5a27446737443091eed05b918816f53c400a48be71125ee22e8",
          "name": "driver-image-450"
        },
        {
          "digest": "sha256:7a0ccbcbe8d379bfc441e06504eb45d0d433a37e2a9e6dde83d13347296ebe81",
          "image": "nvcr.io/nvidia/k8s-device-plugin@sha256:7a0ccbcbe8d379bfc441e06504eb45d0d433a37e2a9e6dde83d13347296ebe81",
          "name": "device-plugin-image"
        },
        {
          "digest": "sha256:39127d04c9faff3f22068dcf463ed48d02be684cd18e3f14db843bddbdd783a7",
          "image": "nvcr.io/nvidia/gpu-feature-discovery@sha256:39127d04c9faff3f22068dcf463ed48d02be684cd18e3f14db843bddbdd783a7",
          "name": "gpu-feature-discovery-image"
        },
        {
          "digest": "sha256:3804e2b03c44d25dd410e3882b7fb6d1fde41f55110d7702af47136b042a63ff",
          "image": "nvcr.io/nvidia/cloud-native/k8s-mig-manager@sha256:3804e2b03c44d25dd410e3882b7fb6d1fde41f55110d7702af47136b042a63ff",
          "name": "mig-manager-image"
        },
        {
          "digest": "sha256:4400b56f81095b24fc9dde93ad7ad7d0ca6a7e9736c4c7f1d4e2bcbf0afbe45c",
          "image": "nvcr.io/nvidia/cuda@sha256:4400b56f81095b24fc9dde93ad7ad7d0ca6a7e9736c4c7f1d4e2bcbf0afbe45c",
          "name": "init-container-image"
        },
        {
          "digest": "sha256:d83aee1c7a9c3aa39e3947f0bd206e7955390943b8e3f2e56bdb96bc4ef99d69",
          "image": "nvcr.io/nvidia/cloud-native/gpu-operator-validator@sha256:d83aee1c7a9c3aa39e3947f0bd206e7955390943b8e3f2e56bdb96bc4ef99d69",
          "name": "gpu-operator-validator-image"
        },
        {
          "digest": "sha256:2cb6f2b43e153e67780b1b350ae171c800e15b3f1ae5ad6375b66de530d701fd",
          "image": "nvcr.io/nvidia/cloud-native/k8s-driver-manager@sha256:2cb6f2b43e153e67780b1b350ae171c800e15b3f1ae5ad6375b66de530d701fd",
          "name": "k8s-driver-manager-image"
        },
        {
          "digest": "sha256:4400b56f81095b24fc9dde93ad7ad7d0ca6a7e9736c4c7f1d4e2bcbf0afbe45c",
          "image": "nvcr.io/nvidia/cuda@sha256:4400b56f81095b24fc9dde93ad7ad7d0ca6a7e9736c4c7f1d4e2bcbf0afbe45c",
          "name": "vfio-manager-image"
        },
        {
          "digest": "sha256:f48e8b430248a9cadf716a9211bb0050d4a663d1f1386ea107e72917f82e5b5f",
          "image": "nvcr.io/nvidia/kubevirt-gpu-device-plugin@sha256:f48e8b430248a9cadf716a9211bb0050d4a663d1f1386ea107e72917f82e5b5f",
          "name": "sandbox-device-plugin-image"
        },
        {
          "digest": "sha256:16ee5b1a8d9217f761c40a06c7ae598cbc893fbc35d99ff9b84fa886ea6b9b37",
          "image": "nvcr.io/nvidia/cloud-native/vgpu-device-manager@sha256:16ee5b1a8d9217f761c40a06c7ae598cbc893fbc35d99ff9b84fa886ea6b9b37",
          "name": "vgpu-device-manager-image"
        },
        {
          "digest": "sha256:7e3699c82bf02ba640d49775ce1b1fc96a5eca02d5ea453ea0ed1571d73779a5",
          "image": "nvcr.io/nvidia/gpu-operator@sha256:7e3699c82bf02ba640d49775ce1b1fc96a5eca02d5ea453ea0ed1571d73779a5",
          "name": "gpu-operator-7e3699c82bf02ba640d49775ce1b1fc96a5eca02d5ea453ea0ed1571d73779a5-annotation"
        },
        {
          "digest": "sha256:7e3699c82bf02ba640d49775ce1b1fc96a5eca02d5ea453ea0ed1571d73779a5",
          "image": "nvcr.io/nvidia/gpu-operator@sha256:7e3699c82bf02ba640d49775ce1b1fc96a5eca02d5ea453ea0ed1571d73779a5",
          "name": "gpu-operator"
        }
      ],
      "replaces": null,
      "skip_range": ">=v1.9.0 <v1.11.0",
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.10",
      "version": "1.11.0",
      "version_original": "1.11.0"
    },
    {
      "_id": "62bb39c0a516f70197136cd3",
      "alm_examples": [
        {
          "api_version": "nvidia.com/v1",
          "kind": "ClusterPolicy",
          "metadata": {
            "name": "gpu-cluster-policy"
          },
          "spec": {
            "daemonsets": {},
            "dcgm": {
              "enabled": true
            },
            "dcgmExporter": {
              "config": {
                "name": ""
              }
            },
            "devicePlugin": {
              "config": {
                "default": "",
                "name": ""
              }
            },
            "driver": {
              "certConfig": {
                "name": ""
              },
              "enabled": true,
              "kernelModuleConfig": {
                "name": ""
              },
              "licensingConfig": {
                "configMapName": "",
                "nlsEnabled": false
              },
              "repoConfig": {
                "configMapName": ""
              },
              "virtualTopology": {
                "config": ""
              }
            },
            "gfd": {},
            "mig": {
              "strategy": "single"
            },
            "migManager": {
              "enabled": true
            },
            "nodeStatusExporter": {
              "enabled": true
            },
            "operator": {
              "defaultRuntime": "crio",
              "initContainer": {},
              "use_ocp_driver_toolkit": true
            },
            "sandboxDevicePlugin": {
              "enabled": true
            },
            "sandboxWorkloads": {
              "defaultWorkload": "container",
              "enabled": false
            },
            "toolkit": {
              "enabled": true
            },
            "validator": {
              "plugin": {
                "env": [
                  {
                    "name": "WITH_WORKLOAD",
                    "value": "true"
                  }
                ]
              }
            },
            "vfioManager": {
              "enabled": true
            },
            "vgpuDeviceManager": {
              "config": {
                "default": "default",
                "name": "vgpu-devices-config"
              },
              "enabled": true
            },
            "vgpuManager": {
              "enabled": true
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [
          "Disconnected"
        ],
        "valid_subscription": []
      },
      "architectures": [
        "amd64",
        "arm64"
      ],
      "bundle_path": "registry.connect.redhat.com/nvidia/gpu-operator-bundle@sha256:501dd759f32adb585848f511f6dfa7bc9372d5e905abea9b082dd50cbe232216",
      "bundle_path_digest": "sha256:501dd759f32adb585848f511f6dfa7bc9372d5e905abea9b082dd50cbe232216",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "v1.11",
      "creation_date": "2022-06-28T17:26:24.697000+00:00",
      "csv_description": "Kubernetes provides access to special hardware resources such as NVIDIA GPUs, NICs, Infiniband adapters and other devices through the [device plugin framework](https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/). However, configuring and managing nodes with these hardware resources requires configuration of multiple software components such as drivers, container runtimes or other libraries which are difficult and prone to errors.\nThe NVIDIA GPU Operator uses the [operator framework](https://cloud.redhat.com/blog/introducing-the-operator-framework) within Kubernetes to automate the management of all NVIDIA software components needed to provision and monitor GPUs. These components include the NVIDIA drivers (to enable CUDA), Kubernetes device plugin for GPUs, the NVIDIA Container Runtime, automatic node labelling and NVIDIA DCGM exporter.\nVisit the official site of the [GPU Operator](https://github.com/NVIDIA/gpu-operator) for more information. For getting started with using the GPU Operator with OpenShift, see the instructions [here](https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/openshift/contents.html).\n",
      "csv_display_name": "NVIDIA GPU Operator",
      "csv_metadata_description": "Automate the management and monitoring of NVIDIA GPUs.",
      "csv_name": "gpu-operator-certified.v1.11.0",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": false,
      "last_update_date": "2022-11-23T08:29:22.548000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.10",
      "organization": "certified-operators",
      "package": "gpu-operator-certified",
      "provided_apis": [
        {
          "group": "nvidia.com",
          "kind": "ClusterPolicy",
          "plural": "clusterpolicies",
          "version": "v1"
        }
      ],
      "provider": "NVIDIA Corporation",
      "related_images": [
        {
          "digest": "sha256:7e3699c82bf02ba640d49775ce1b1fc96a5eca02d5ea453ea0ed1571d73779a5",
          "image": "nvcr.io/nvidia/gpu-operator@sha256:7e3699c82bf02ba640d49775ce1b1fc96a5eca02d5ea453ea0ed1571d73779a5",
          "name": "gpu-operator-image"
        },
        {
          "digest": "sha256:b90a519e9f30fbc013e1806bc081268724e9e80b4035c67c167ab3b43a37704a",
          "image": "nvcr.io/nvidia/k8s/dcgm-exporter@sha256:b90a519e9f30fbc013e1806bc081268724e9e80b4035c67c167ab3b43a37704a",
          "name": "dcgm-exporter-image"
        },
        {
          "digest": "sha256:8490d555131a57a11311561667ea9ec8cf554a399d2492a6f9e6677c01ec4b9a",
          "image": "nvcr.io/nvidia/cloud-native/dcgm@sha256:8490d555131a57a11311561667ea9ec8cf554a399d2492a6f9e6677c01ec4b9a",
          "name": "dcgm-image"
        },
        {
          "digest": "sha256:2d6e86db0e2b0db64c00566938e1851e25d4bb1019b1e7b696e2b199cba33397",
          "image": "nvcr.io/nvidia/k8s/container-toolkit@sha256:2d6e86db0e2b0db64c00566938e1851e25d4bb1019b1e7b696e2b199cba33397",
          "name": "container-toolkit-image"
        },
        {
          "digest": "sha256:8c9fabc1cd773e9526eb7e8526084f065f6cca9d0fd8dc2a672a0bb717f1cc60",
          "image": "nvcr.io/nvidia/driver@sha256:8c9fabc1cd773e9526eb7e8526084f065f6cca9d0fd8dc2a672a0bb717f1cc60",
          "name": "driver-image"
        },
        {
          "digest": "sha256:6e2902d73c54cf57bec94edf166d206cfeb0b948b5aab70800ccbe025064b3a1",
          "image": "nvcr.io/nvidia/driver@sha256:6e2902d73c54cf57bec94edf166d206cfeb0b948b5aab70800ccbe025064b3a1",
          "name": "driver-image-510"
        },
        {
          "digest": "sha256:cf8ce04907d0e8decf90a8dccfa6134af9c39509d2f8a21c355ed71223acacdb",
          "image": "nvcr.io/nvidia/driver@sha256:cf8ce04907d0e8decf90a8dccfa6134af9c39509d2f8a21c355ed71223acacdb",
          "name": "driver-image-470"
        },
        {
          "digest": "sha256:b8799e7397fae5a27446737443091eed05b918816f53c400a48be71125ee22e8",
          "image": "nvcr.io/nvidia/driver@sha256:b8799e7397fae5a27446737443091eed05b918816f53c400a48be71125ee22e8",
          "name": "driver-image-450"
        },
        {
          "digest": "sha256:7a0ccbcbe8d379bfc441e06504eb45d0d433a37e2a9e6dde83d13347296ebe81",
          "image": "nvcr.io/nvidia/k8s-device-plugin@sha256:7a0ccbcbe8d379bfc441e06504eb45d0d433a37e2a9e6dde83d13347296ebe81",
          "name": "device-plugin-image"
        },
        {
          "digest": "sha256:39127d04c9faff3f22068dcf463ed48d02be684cd18e3f14db843bddbdd783a7",
          "image": "nvcr.io/nvidia/gpu-feature-discovery@sha256:39127d04c9faff3f22068dcf463ed48d02be684cd18e3f14db843bddbdd783a7",
          "name": "gpu-feature-discovery-image"
        },
        {
          "digest": "sha256:3804e2b03c44d25dd410e3882b7fb6d1fde41f55110d7702af47136b042a63ff",
          "image": "nvcr.io/nvidia/cloud-native/k8s-mig-manager@sha256:3804e2b03c44d25dd410e3882b7fb6d1fde41f55110d7702af47136b042a63ff",
          "name": "mig-manager-image"
        },
        {
          "digest": "sha256:4400b56f81095b24fc9dde93ad7ad7d0ca6a7e9736c4c7f1d4e2bcbf0afbe45c",
          "image": "nvcr.io/nvidia/cuda@sha256:4400b56f81095b24fc9dde93ad7ad7d0ca6a7e9736c4c7f1d4e2bcbf0afbe45c",
          "name": "init-container-image"
        },
        {
          "digest": "sha256:d83aee1c7a9c3aa39e3947f0bd206e7955390943b8e3f2e56bdb96bc4ef99d69",
          "image": "nvcr.io/nvidia/cloud-native/gpu-operator-validator@sha256:d83aee1c7a9c3aa39e3947f0bd206e7955390943b8e3f2e56bdb96bc4ef99d69",
          "name": "gpu-operator-validator-image"
        },
        {
          "digest": "sha256:2cb6f2b43e153e67780b1b350ae171c800e15b3f1ae5ad6375b66de530d701fd",
          "image": "nvcr.io/nvidia/cloud-native/k8s-driver-manager@sha256:2cb6f2b43e153e67780b1b350ae171c800e15b3f1ae5ad6375b66de530d701fd",
          "name": "k8s-driver-manager-image"
        },
        {
          "digest": "sha256:4400b56f81095b24fc9dde93ad7ad7d0ca6a7e9736c4c7f1d4e2bcbf0afbe45c",
          "image": "nvcr.io/nvidia/cuda@sha256:4400b56f81095b24fc9dde93ad7ad7d0ca6a7e9736c4c7f1d4e2bcbf0afbe45c",
          "name": "vfio-manager-image"
        },
        {
          "digest": "sha256:f48e8b430248a9cadf716a9211bb0050d4a663d1f1386ea107e72917f82e5b5f",
          "image": "nvcr.io/nvidia/kubevirt-gpu-device-plugin@sha256:f48e8b430248a9cadf716a9211bb0050d4a663d1f1386ea107e72917f82e5b5f",
          "name": "sandbox-device-plugin-image"
        },
        {
          "digest": "sha256:16ee5b1a8d9217f761c40a06c7ae598cbc893fbc35d99ff9b84fa886ea6b9b37",
          "image": "nvcr.io/nvidia/cloud-native/vgpu-device-manager@sha256:16ee5b1a8d9217f761c40a06c7ae598cbc893fbc35d99ff9b84fa886ea6b9b37",
          "name": "vgpu-device-manager-image"
        },
        {
          "digest": "sha256:7e3699c82bf02ba640d49775ce1b1fc96a5eca02d5ea453ea0ed1571d73779a5",
          "image": "nvcr.io/nvidia/gpu-operator@sha256:7e3699c82bf02ba640d49775ce1b1fc96a5eca02d5ea453ea0ed1571d73779a5",
          "name": "gpu-operator-7e3699c82bf02ba640d49775ce1b1fc96a5eca02d5ea453ea0ed1571d73779a5-annotation"
        },
        {
          "digest": "sha256:7e3699c82bf02ba640d49775ce1b1fc96a5eca02d5ea453ea0ed1571d73779a5",
          "image": "nvcr.io/nvidia/gpu-operator@sha256:7e3699c82bf02ba640d49775ce1b1fc96a5eca02d5ea453ea0ed1571d73779a5",
          "name": "gpu-operator"
        }
      ],
      "replaces": null,
      "skip_range": ">=v1.9.0 <v1.11.0",
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.10",
      "version": "1.11.0",
      "version_original": "1.11.0"
    },
    {
      "_id": "62bb41e5d8bdcafd21732874",
      "alm_examples": [
        {
          "api_version": "nvidia.com/v1",
          "kind": "ClusterPolicy",
          "metadata": {
            "name": "gpu-cluster-policy"
          },
          "spec": {
            "daemonsets": {},
            "dcgm": {
              "enabled": true
            },
            "dcgmExporter": {
              "config": {
                "name": ""
              }
            },
            "devicePlugin": {},
            "driver": {
              "certConfig": {
                "name": ""
              },
              "enabled": true,
              "kernelModuleConfig": {
                "name": ""
              },
              "licensingConfig": {
                "configMapName": "",
                "nlsEnabled": false
              },
              "repoConfig": {
                "configMapName": ""
              },
              "use_ocp_driver_toolkit": true,
              "virtualTopology": {
                "config": ""
              }
            },
            "gfd": {},
            "mig": {
              "strategy": "single"
            },
            "migManager": {
              "enabled": true
            },
            "nodeStatusExporter": {
              "enabled": true
            },
            "operator": {
              "defaultRuntime": "crio",
              "deployGFD": true,
              "initContainer": {}
            },
            "toolkit": {
              "enabled": true
            },
            "validator": {
              "plugin": {
                "env": [
                  {
                    "name": "WITH_WORKLOAD",
                    "value": "true"
                  }
                ]
              }
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [
          "Disconnected"
        ],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/nvidia/gpu-operator-bundle@sha256:4d1268054559b7678028a7a66e36567c55417b729101ef131dec8d989cbef1d3",
      "bundle_path_digest": "sha256:4d1268054559b7678028a7a66e36567c55417b729101ef131dec8d989cbef1d3",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "v1.11",
      "creation_date": "2022-06-28T18:01:09.610000+00:00",
      "csv_description": "Kubernetes provides access to special hardware resources such as NVIDIA GPUs, NICs, Infiniband adapters and other devices through the [device plugin framework](https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/). However, configuring and managing nodes with these hardware resources requires configuration of multiple software components such as drivers, container runtimes or other libraries which are difficult and prone to errors.\nThe NVIDIA GPU Operator uses the [operator framework](https://coreos.com/blog/introducing-operator-framework) within Kubernetes to automate the management of all NVIDIA software components needed to provision and monitor GPUs. These components include the NVIDIA drivers (to enable CUDA), Kubernetes device plugin for GPUs, the NVIDIA Container Runtime, automatic node labelling and NVIDIA DCGM exporter.\nVisit the official site of the [GPU Operator](https://github.com/NVIDIA/gpu-operator) for more information. For getting started with using the GPU Operator with OpenShift, see the instructions [here](https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/openshift/contents.html).\n",
      "csv_display_name": "NVIDIA GPU Operator",
      "csv_metadata_description": "Automate the management and monitoring of NVIDIA GPUs.",
      "csv_name": "gpu-operator-certified.v1.10.0",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": false,
      "last_update_date": "2022-11-23T07:13:12.549000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.9",
      "organization": "certified-operators",
      "package": "gpu-operator-certified",
      "provided_apis": [
        {
          "group": "nvidia.com",
          "kind": "ClusterPolicy",
          "plural": "clusterpolicies",
          "version": "v1"
        }
      ],
      "provider": "NVIDIA Corporation",
      "related_images": [
        {
          "digest": "sha256:e2da1a42a885482a6b40b397397e79c037a10e71abd08e4658eb0b9fc2a761be",
          "image": "nvcr.io/nvidia/gpu-operator@sha256:e2da1a42a885482a6b40b397397e79c037a10e71abd08e4658eb0b9fc2a761be",
          "name": "gpu-operator-image"
        },
        {
          "digest": "sha256:67c48a8370c34f68b46eb57339e87d01c04198368a0423553fab7565c862c4df",
          "image": "nvcr.io/nvidia/k8s/dcgm-exporter@sha256:67c48a8370c34f68b46eb57339e87d01c04198368a0423553fab7565c862c4df",
          "name": "dcgm-exporter-image"
        },
        {
          "digest": "sha256:ac13256c295126b854a07421c45de0721e7a57974b38b3e66eb1fdc3a4e29f2e",
          "image": "nvcr.io/nvidia/cloud-native/dcgm@sha256:ac13256c295126b854a07421c45de0721e7a57974b38b3e66eb1fdc3a4e29f2e",
          "name": "dcgm-image"
        },
        {
          "digest": "sha256:9abb063e6b74085728fead5ed5bbe3bd7b3f57de60e87f6b964bba2d46f1bc67",
          "image": "nvcr.io/nvidia/k8s/container-toolkit@sha256:9abb063e6b74085728fead5ed5bbe3bd7b3f57de60e87f6b964bba2d46f1bc67",
          "name": "container-toolkit-image"
        },
        {
          "digest": "sha256:d46393d6bd5be020c78e1d45669d2bb3ac8681df13369ddbbbf90740e354c0cf",
          "image": "nvcr.io/nvidia/driver@sha256:d46393d6bd5be020c78e1d45669d2bb3ac8681df13369ddbbbf90740e354c0cf",
          "name": "driver-image"
        },
        {
          "digest": "sha256:aa95c16106280e36a5e32d2fe4c66e8b70f5a114860c6f4ed5b1a4085c63601b",
          "image": "nvcr.io/nvidia/k8s-device-plugin@sha256:aa95c16106280e36a5e32d2fe4c66e8b70f5a114860c6f4ed5b1a4085c63601b",
          "name": "device-plugin-image"
        },
        {
          "digest": "sha256:c2e3d0fe41a0d227dbb70caec03c780cc76317515e5ab3875f31d50c63f41c66",
          "image": "nvcr.io/nvidia/gpu-feature-discovery@sha256:c2e3d0fe41a0d227dbb70caec03c780cc76317515e5ab3875f31d50c63f41c66",
          "name": "gpu-feature-discovery-image"
        },
        {
          "digest": "sha256:e8f06ce53415e8572f633b2169289d973993cb1e49d4dcc686ac134194d88f33",
          "image": "nvcr.io/nvidia/cloud-native/k8s-mig-manager@sha256:e8f06ce53415e8572f633b2169289d973993cb1e49d4dcc686ac134194d88f33",
          "name": "mig-manager-image"
        },
        {
          "digest": "sha256:c7609c27c68ae6520ff21862665d10e1dd0dc99fe0615968310478632187341b",
          "image": "docker.io/nvidia/cuda@sha256:c7609c27c68ae6520ff21862665d10e1dd0dc99fe0615968310478632187341b",
          "name": "init-container-image"
        },
        {
          "digest": "sha256:6819f4c9136e82e44ec34b03ba71c72cbbeebe1db49c596a8bc245fea3bee596",
          "image": "nvcr.io/nvidia/cloud-native/gpu-operator-validator@sha256:6819f4c9136e82e44ec34b03ba71c72cbbeebe1db49c596a8bc245fea3bee596",
          "name": "gpu-operator-validator-image"
        },
        {
          "digest": "sha256:5b16056257acc51b517d9cdb1da3218693cefc214af93789e6e214fd2b4cacf1",
          "image": "nvcr.io/nvidia/cloud-native/k8s-driver-manager@sha256:5b16056257acc51b517d9cdb1da3218693cefc214af93789e6e214fd2b4cacf1",
          "name": "k8s-driver-manager-image"
        },
        {
          "digest": "sha256:e2da1a42a885482a6b40b397397e79c037a10e71abd08e4658eb0b9fc2a761be",
          "image": "nvcr.io/nvidia/gpu-operator@sha256:e2da1a42a885482a6b40b397397e79c037a10e71abd08e4658eb0b9fc2a761be",
          "name": "gpu-operator-e2da1a42a885482a6b40b397397e79c037a10e71abd08e4658eb0b9fc2a761be-annotation"
        },
        {
          "digest": "sha256:e2da1a42a885482a6b40b397397e79c037a10e71abd08e4658eb0b9fc2a761be",
          "image": "nvcr.io/nvidia/gpu-operator@sha256:e2da1a42a885482a6b40b397397e79c037a10e71abd08e4658eb0b9fc2a761be",
          "name": "gpu-operator"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.9",
      "version": "1.10.0",
      "version_original": "1.10.0"
    },
    {
      "_id": "62bb41ecd8bdcafd2173287b",
      "alm_examples": [
        {
          "api_version": "nvidia.com/v1",
          "kind": "ClusterPolicy",
          "metadata": {
            "name": "gpu-cluster-policy"
          },
          "spec": {
            "daemonsets": {},
            "dcgm": {
              "enabled": true
            },
            "dcgmExporter": {
              "config": {
                "name": ""
              }
            },
            "devicePlugin": {},
            "driver": {
              "certConfig": {
                "name": ""
              },
              "enabled": true,
              "kernelModuleConfig": {
                "name": ""
              },
              "licensingConfig": {
                "configMapName": "",
                "nlsEnabled": false
              },
              "repoConfig": {
                "configMapName": ""
              },
              "use_ocp_driver_toolkit": true,
              "virtualTopology": {
                "config": ""
              }
            },
            "gfd": {},
            "mig": {
              "strategy": "single"
            },
            "migManager": {
              "enabled": true
            },
            "nodeStatusExporter": {
              "enabled": true
            },
            "operator": {
              "defaultRuntime": "crio",
              "deployGFD": true,
              "initContainer": {}
            },
            "toolkit": {
              "enabled": true
            },
            "validator": {
              "plugin": {
                "env": [
                  {
                    "name": "WITH_WORKLOAD",
                    "value": "true"
                  }
                ]
              }
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [
          "Disconnected"
        ],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/nvidia/gpu-operator-bundle@sha256:60b1d25ebb3aada94985e359bb059d4491bf0148ac75d51b68a461670659e429",
      "bundle_path_digest": "sha256:60b1d25ebb3aada94985e359bb059d4491bf0148ac75d51b68a461670659e429",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "v1.11",
      "creation_date": "2022-06-28T18:01:16.462000+00:00",
      "csv_description": "Kubernetes provides access to special hardware resources such as NVIDIA GPUs, NICs, Infiniband adapters and other devices through the [device plugin framework](https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/). However, configuring and managing nodes with these hardware resources requires configuration of multiple software components such as drivers, container runtimes or other libraries which are difficult and prone to errors.\nThe NVIDIA GPU Operator uses the [operator framework](https://coreos.com/blog/introducing-operator-framework) within Kubernetes to automate the management of all NVIDIA software components needed to provision and monitor GPUs. These components include the NVIDIA drivers (to enable CUDA), Kubernetes device plugin for GPUs, the NVIDIA Container Runtime, automatic node labelling and NVIDIA DCGM exporter.\nVisit the official site of the [GPU Operator](https://github.com/NVIDIA/gpu-operator) for more information. For getting started with using the GPU Operator with OpenShift, see the instructions [here](https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/openshift/contents.html).\n",
      "csv_display_name": "NVIDIA GPU Operator",
      "csv_metadata_description": "Automate the management and monitoring of NVIDIA GPUs.",
      "csv_name": "gpu-operator-certified.v1.10.1",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": false,
      "last_update_date": "2022-11-23T07:13:30.761000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.9",
      "organization": "certified-operators",
      "package": "gpu-operator-certified",
      "provided_apis": [
        {
          "group": "nvidia.com",
          "kind": "ClusterPolicy",
          "plural": "clusterpolicies",
          "version": "v1"
        }
      ],
      "provider": "NVIDIA Corporation",
      "related_images": [
        {
          "digest": "sha256:c7f9074c1a7f58947c807f23f2eece3a8b04e11175127919156f8e864821d45a",
          "image": "nvcr.io/nvidia/gpu-operator@sha256:c7f9074c1a7f58947c807f23f2eece3a8b04e11175127919156f8e864821d45a",
          "name": "gpu-operator-image"
        },
        {
          "digest": "sha256:67c48a8370c34f68b46eb57339e87d01c04198368a0423553fab7565c862c4df",
          "image": "nvcr.io/nvidia/k8s/dcgm-exporter@sha256:67c48a8370c34f68b46eb57339e87d01c04198368a0423553fab7565c862c4df",
          "name": "dcgm-exporter-image"
        },
        {
          "digest": "sha256:ac13256c295126b854a07421c45de0721e7a57974b38b3e66eb1fdc3a4e29f2e",
          "image": "nvcr.io/nvidia/cloud-native/dcgm@sha256:ac13256c295126b854a07421c45de0721e7a57974b38b3e66eb1fdc3a4e29f2e",
          "name": "dcgm-image"
        },
        {
          "digest": "sha256:9abb063e6b74085728fead5ed5bbe3bd7b3f57de60e87f6b964bba2d46f1bc67",
          "image": "nvcr.io/nvidia/k8s/container-toolkit@sha256:9abb063e6b74085728fead5ed5bbe3bd7b3f57de60e87f6b964bba2d46f1bc67",
          "name": "container-toolkit-image"
        },
        {
          "digest": "sha256:f99b66855e92451a59a722355dd52bf97eb985cc8a647172273de8178aeaa93c",
          "image": "nvcr.io/nvidia/driver@sha256:f99b66855e92451a59a722355dd52bf97eb985cc8a647172273de8178aeaa93c",
          "name": "driver-image"
        },
        {
          "digest": "sha256:aa95c16106280e36a5e32d2fe4c66e8b70f5a114860c6f4ed5b1a4085c63601b",
          "image": "nvcr.io/nvidia/k8s-device-plugin@sha256:aa95c16106280e36a5e32d2fe4c66e8b70f5a114860c6f4ed5b1a4085c63601b",
          "name": "device-plugin-image"
        },
        {
          "digest": "sha256:c2e3d0fe41a0d227dbb70caec03c780cc76317515e5ab3875f31d50c63f41c66",
          "image": "nvcr.io/nvidia/gpu-feature-discovery@sha256:c2e3d0fe41a0d227dbb70caec03c780cc76317515e5ab3875f31d50c63f41c66",
          "name": "gpu-feature-discovery-image"
        },
        {
          "digest": "sha256:e8f06ce53415e8572f633b2169289d973993cb1e49d4dcc686ac134194d88f33",
          "image": "nvcr.io/nvidia/cloud-native/k8s-mig-manager@sha256:e8f06ce53415e8572f633b2169289d973993cb1e49d4dcc686ac134194d88f33",
          "name": "mig-manager-image"
        },
        {
          "digest": "sha256:7f0f7dc8c993a86865d4d9f9b0e36d113ea15603d5d5279d2879fa2bc21c8236",
          "image": "nvcr.io/nvidia/cuda@sha256:7f0f7dc8c993a86865d4d9f9b0e36d113ea15603d5d5279d2879fa2bc21c8236",
          "name": "init-container-image"
        },
        {
          "digest": "sha256:24d804e8f005d7aeca8343aa13e5f92295d8642a4c47cb24e3ac86a22543bc37",
          "image": "nvcr.io/nvidia/cloud-native/gpu-operator-validator@sha256:24d804e8f005d7aeca8343aa13e5f92295d8642a4c47cb24e3ac86a22543bc37",
          "name": "gpu-operator-validator-image"
        },
        {
          "digest": "sha256:5b16056257acc51b517d9cdb1da3218693cefc214af93789e6e214fd2b4cacf1",
          "image": "nvcr.io/nvidia/cloud-native/k8s-driver-manager@sha256:5b16056257acc51b517d9cdb1da3218693cefc214af93789e6e214fd2b4cacf1",
          "name": "k8s-driver-manager-image"
        },
        {
          "digest": "sha256:c7f9074c1a7f58947c807f23f2eece3a8b04e11175127919156f8e864821d45a",
          "image": "nvcr.io/nvidia/gpu-operator@sha256:c7f9074c1a7f58947c807f23f2eece3a8b04e11175127919156f8e864821d45a",
          "name": "gpu-operator-c7f9074c1a7f58947c807f23f2eece3a8b04e11175127919156f8e864821d45a-annotation"
        },
        {
          "digest": "sha256:c7f9074c1a7f58947c807f23f2eece3a8b04e11175127919156f8e864821d45a",
          "image": "nvcr.io/nvidia/gpu-operator@sha256:c7f9074c1a7f58947c807f23f2eece3a8b04e11175127919156f8e864821d45a",
          "name": "gpu-operator"
        }
      ],
      "replaces": null,
      "skip_range": ">=v1.9.0 <v1.10.1",
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.9",
      "version": "1.10.1",
      "version_original": "1.10.1"
    },
    {
      "_id": "62bb41f3a516f70197136fa2",
      "alm_examples": [
        {
          "api_version": "nvidia.com/v1",
          "kind": "ClusterPolicy",
          "metadata": {
            "name": "gpu-cluster-policy"
          },
          "spec": {
            "daemonsets": {},
            "dcgm": {
              "enabled": true
            },
            "dcgmExporter": {
              "config": {
                "name": ""
              }
            },
            "devicePlugin": {
              "config": {
                "default": "",
                "name": ""
              }
            },
            "driver": {
              "certConfig": {
                "name": ""
              },
              "enabled": true,
              "kernelModuleConfig": {
                "name": ""
              },
              "licensingConfig": {
                "configMapName": "",
                "nlsEnabled": false
              },
              "repoConfig": {
                "configMapName": ""
              },
              "virtualTopology": {
                "config": ""
              }
            },
            "gfd": {},
            "mig": {
              "strategy": "single"
            },
            "migManager": {
              "enabled": true
            },
            "nodeStatusExporter": {
              "enabled": true
            },
            "operator": {
              "defaultRuntime": "crio",
              "initContainer": {},
              "use_ocp_driver_toolkit": true
            },
            "sandboxDevicePlugin": {
              "enabled": true
            },
            "sandboxWorkloads": {
              "defaultWorkload": "container",
              "enabled": false
            },
            "toolkit": {
              "enabled": true
            },
            "validator": {
              "plugin": {
                "env": [
                  {
                    "name": "WITH_WORKLOAD",
                    "value": "true"
                  }
                ]
              }
            },
            "vfioManager": {
              "enabled": true
            },
            "vgpuDeviceManager": {
              "config": {
                "default": "default",
                "name": "vgpu-devices-config"
              },
              "enabled": true
            },
            "vgpuManager": {
              "enabled": true
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [
          "Disconnected"
        ],
        "valid_subscription": []
      },
      "architectures": [
        "amd64",
        "arm64"
      ],
      "bundle_path": "registry.connect.redhat.com/nvidia/gpu-operator-bundle@sha256:501dd759f32adb585848f511f6dfa7bc9372d5e905abea9b082dd50cbe232216",
      "bundle_path_digest": "sha256:501dd759f32adb585848f511f6dfa7bc9372d5e905abea9b082dd50cbe232216",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "stable",
      "creation_date": "2022-06-28T18:01:23.412000+00:00",
      "csv_description": "Kubernetes provides access to special hardware resources such as NVIDIA GPUs, NICs, Infiniband adapters and other devices through the [device plugin framework](https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/). However, configuring and managing nodes with these hardware resources requires configuration of multiple software components such as drivers, container runtimes or other libraries which are difficult and prone to errors.\nThe NVIDIA GPU Operator uses the [operator framework](https://cloud.redhat.com/blog/introducing-the-operator-framework) within Kubernetes to automate the management of all NVIDIA software components needed to provision and monitor GPUs. These components include the NVIDIA drivers (to enable CUDA), Kubernetes device plugin for GPUs, the NVIDIA Container Runtime, automatic node labelling and NVIDIA DCGM exporter.\nVisit the official site of the [GPU Operator](https://github.com/NVIDIA/gpu-operator) for more information. For getting started with using the GPU Operator with OpenShift, see the instructions [here](https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/openshift/contents.html).\n",
      "csv_display_name": "NVIDIA GPU Operator",
      "csv_metadata_description": "Automate the management and monitoring of NVIDIA GPUs.",
      "csv_name": "gpu-operator-certified.v1.11.0",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": false,
      "last_update_date": "2022-11-23T07:13:41.056000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.9",
      "organization": "certified-operators",
      "package": "gpu-operator-certified",
      "provided_apis": [
        {
          "group": "nvidia.com",
          "kind": "ClusterPolicy",
          "plural": "clusterpolicies",
          "version": "v1"
        }
      ],
      "provider": "NVIDIA Corporation",
      "related_images": [
        {
          "digest": "sha256:7e3699c82bf02ba640d49775ce1b1fc96a5eca02d5ea453ea0ed1571d73779a5",
          "image": "nvcr.io/nvidia/gpu-operator@sha256:7e3699c82bf02ba640d49775ce1b1fc96a5eca02d5ea453ea0ed1571d73779a5",
          "name": "gpu-operator-image"
        },
        {
          "digest": "sha256:b90a519e9f30fbc013e1806bc081268724e9e80b4035c67c167ab3b43a37704a",
          "image": "nvcr.io/nvidia/k8s/dcgm-exporter@sha256:b90a519e9f30fbc013e1806bc081268724e9e80b4035c67c167ab3b43a37704a",
          "name": "dcgm-exporter-image"
        },
        {
          "digest": "sha256:8490d555131a57a11311561667ea9ec8cf554a399d2492a6f9e6677c01ec4b9a",
          "image": "nvcr.io/nvidia/cloud-native/dcgm@sha256:8490d555131a57a11311561667ea9ec8cf554a399d2492a6f9e6677c01ec4b9a",
          "name": "dcgm-image"
        },
        {
          "digest": "sha256:2d6e86db0e2b0db64c00566938e1851e25d4bb1019b1e7b696e2b199cba33397",
          "image": "nvcr.io/nvidia/k8s/container-toolkit@sha256:2d6e86db0e2b0db64c00566938e1851e25d4bb1019b1e7b696e2b199cba33397",
          "name": "container-toolkit-image"
        },
        {
          "digest": "sha256:8c9fabc1cd773e9526eb7e8526084f065f6cca9d0fd8dc2a672a0bb717f1cc60",
          "image": "nvcr.io/nvidia/driver@sha256:8c9fabc1cd773e9526eb7e8526084f065f6cca9d0fd8dc2a672a0bb717f1cc60",
          "name": "driver-image"
        },
        {
          "digest": "sha256:6e2902d73c54cf57bec94edf166d206cfeb0b948b5aab70800ccbe025064b3a1",
          "image": "nvcr.io/nvidia/driver@sha256:6e2902d73c54cf57bec94edf166d206cfeb0b948b5aab70800ccbe025064b3a1",
          "name": "driver-image-510"
        },
        {
          "digest": "sha256:cf8ce04907d0e8decf90a8dccfa6134af9c39509d2f8a21c355ed71223acacdb",
          "image": "nvcr.io/nvidia/driver@sha256:cf8ce04907d0e8decf90a8dccfa6134af9c39509d2f8a21c355ed71223acacdb",
          "name": "driver-image-470"
        },
        {
          "digest": "sha256:b8799e7397fae5a27446737443091eed05b918816f53c400a48be71125ee22e8",
          "image": "nvcr.io/nvidia/driver@sha256:b8799e7397fae5a27446737443091eed05b918816f53c400a48be71125ee22e8",
          "name": "driver-image-450"
        },
        {
          "digest": "sha256:7a0ccbcbe8d379bfc441e06504eb45d0d433a37e2a9e6dde83d13347296ebe81",
          "image": "nvcr.io/nvidia/k8s-device-plugin@sha256:7a0ccbcbe8d379bfc441e06504eb45d0d433a37e2a9e6dde83d13347296ebe81",
          "name": "device-plugin-image"
        },
        {
          "digest": "sha256:39127d04c9faff3f22068dcf463ed48d02be684cd18e3f14db843bddbdd783a7",
          "image": "nvcr.io/nvidia/gpu-feature-discovery@sha256:39127d04c9faff3f22068dcf463ed48d02be684cd18e3f14db843bddbdd783a7",
          "name": "gpu-feature-discovery-image"
        },
        {
          "digest": "sha256:3804e2b03c44d25dd410e3882b7fb6d1fde41f55110d7702af47136b042a63ff",
          "image": "nvcr.io/nvidia/cloud-native/k8s-mig-manager@sha256:3804e2b03c44d25dd410e3882b7fb6d1fde41f55110d7702af47136b042a63ff",
          "name": "mig-manager-image"
        },
        {
          "digest": "sha256:4400b56f81095b24fc9dde93ad7ad7d0ca6a7e9736c4c7f1d4e2bcbf0afbe45c",
          "image": "nvcr.io/nvidia/cuda@sha256:4400b56f81095b24fc9dde93ad7ad7d0ca6a7e9736c4c7f1d4e2bcbf0afbe45c",
          "name": "init-container-image"
        },
        {
          "digest": "sha256:d83aee1c7a9c3aa39e3947f0bd206e7955390943b8e3f2e56bdb96bc4ef99d69",
          "image": "nvcr.io/nvidia/cloud-native/gpu-operator-validator@sha256:d83aee1c7a9c3aa39e3947f0bd206e7955390943b8e3f2e56bdb96bc4ef99d69",
          "name": "gpu-operator-validator-image"
        },
        {
          "digest": "sha256:2cb6f2b43e153e67780b1b350ae171c800e15b3f1ae5ad6375b66de530d701fd",
          "image": "nvcr.io/nvidia/cloud-native/k8s-driver-manager@sha256:2cb6f2b43e153e67780b1b350ae171c800e15b3f1ae5ad6375b66de530d701fd",
          "name": "k8s-driver-manager-image"
        },
        {
          "digest": "sha256:4400b56f81095b24fc9dde93ad7ad7d0ca6a7e9736c4c7f1d4e2bcbf0afbe45c",
          "image": "nvcr.io/nvidia/cuda@sha256:4400b56f81095b24fc9dde93ad7ad7d0ca6a7e9736c4c7f1d4e2bcbf0afbe45c",
          "name": "vfio-manager-image"
        },
        {
          "digest": "sha256:f48e8b430248a9cadf716a9211bb0050d4a663d1f1386ea107e72917f82e5b5f",
          "image": "nvcr.io/nvidia/kubevirt-gpu-device-plugin@sha256:f48e8b430248a9cadf716a9211bb0050d4a663d1f1386ea107e72917f82e5b5f",
          "name": "sandbox-device-plugin-image"
        },
        {
          "digest": "sha256:16ee5b1a8d9217f761c40a06c7ae598cbc893fbc35d99ff9b84fa886ea6b9b37",
          "image": "nvcr.io/nvidia/cloud-native/vgpu-device-manager@sha256:16ee5b1a8d9217f761c40a06c7ae598cbc893fbc35d99ff9b84fa886ea6b9b37",
          "name": "vgpu-device-manager-image"
        },
        {
          "digest": "sha256:7e3699c82bf02ba640d49775ce1b1fc96a5eca02d5ea453ea0ed1571d73779a5",
          "image": "nvcr.io/nvidia/gpu-operator@sha256:7e3699c82bf02ba640d49775ce1b1fc96a5eca02d5ea453ea0ed1571d73779a5",
          "name": "gpu-operator-7e3699c82bf02ba640d49775ce1b1fc96a5eca02d5ea453ea0ed1571d73779a5-annotation"
        },
        {
          "digest": "sha256:7e3699c82bf02ba640d49775ce1b1fc96a5eca02d5ea453ea0ed1571d73779a5",
          "image": "nvcr.io/nvidia/gpu-operator@sha256:7e3699c82bf02ba640d49775ce1b1fc96a5eca02d5ea453ea0ed1571d73779a5",
          "name": "gpu-operator"
        }
      ],
      "replaces": null,
      "skip_range": ">=v1.9.0 <v1.11.0",
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.9",
      "version": "1.11.0",
      "version_original": "1.11.0"
    },
    {
      "_id": "62bb41faa516f70197136fa9",
      "alm_examples": [
        {
          "api_version": "nvidia.com/v1",
          "kind": "ClusterPolicy",
          "metadata": {
            "name": "gpu-cluster-policy"
          },
          "spec": {
            "daemonsets": {},
            "dcgm": {
              "enabled": true
            },
            "dcgmExporter": {
              "config": {
                "name": ""
              }
            },
            "devicePlugin": {
              "config": {
                "default": "",
                "name": ""
              }
            },
            "driver": {
              "certConfig": {
                "name": ""
              },
              "enabled": true,
              "kernelModuleConfig": {
                "name": ""
              },
              "licensingConfig": {
                "configMapName": "",
                "nlsEnabled": false
              },
              "repoConfig": {
                "configMapName": ""
              },
              "virtualTopology": {
                "config": ""
              }
            },
            "gfd": {},
            "mig": {
              "strategy": "single"
            },
            "migManager": {
              "enabled": true
            },
            "nodeStatusExporter": {
              "enabled": true
            },
            "operator": {
              "defaultRuntime": "crio",
              "initContainer": {},
              "use_ocp_driver_toolkit": true
            },
            "sandboxDevicePlugin": {
              "enabled": true
            },
            "sandboxWorkloads": {
              "defaultWorkload": "container",
              "enabled": false
            },
            "toolkit": {
              "enabled": true
            },
            "validator": {
              "plugin": {
                "env": [
                  {
                    "name": "WITH_WORKLOAD",
                    "value": "true"
                  }
                ]
              }
            },
            "vfioManager": {
              "enabled": true
            },
            "vgpuDeviceManager": {
              "config": {
                "default": "default",
                "name": "vgpu-devices-config"
              },
              "enabled": true
            },
            "vgpuManager": {
              "enabled": true
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [
          "Disconnected"
        ],
        "valid_subscription": []
      },
      "architectures": [
        "amd64",
        "arm64"
      ],
      "bundle_path": "registry.connect.redhat.com/nvidia/gpu-operator-bundle@sha256:501dd759f32adb585848f511f6dfa7bc9372d5e905abea9b082dd50cbe232216",
      "bundle_path_digest": "sha256:501dd759f32adb585848f511f6dfa7bc9372d5e905abea9b082dd50cbe232216",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "v1.11",
      "creation_date": "2022-06-28T18:01:30.312000+00:00",
      "csv_description": "Kubernetes provides access to special hardware resources such as NVIDIA GPUs, NICs, Infiniband adapters and other devices through the [device plugin framework](https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/). However, configuring and managing nodes with these hardware resources requires configuration of multiple software components such as drivers, container runtimes or other libraries which are difficult and prone to errors.\nThe NVIDIA GPU Operator uses the [operator framework](https://cloud.redhat.com/blog/introducing-the-operator-framework) within Kubernetes to automate the management of all NVIDIA software components needed to provision and monitor GPUs. These components include the NVIDIA drivers (to enable CUDA), Kubernetes device plugin for GPUs, the NVIDIA Container Runtime, automatic node labelling and NVIDIA DCGM exporter.\nVisit the official site of the [GPU Operator](https://github.com/NVIDIA/gpu-operator) for more information. For getting started with using the GPU Operator with OpenShift, see the instructions [here](https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/openshift/contents.html).\n",
      "csv_display_name": "NVIDIA GPU Operator",
      "csv_metadata_description": "Automate the management and monitoring of NVIDIA GPUs.",
      "csv_name": "gpu-operator-certified.v1.11.0",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": false,
      "last_update_date": "2022-11-23T07:13:45.937000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.9",
      "organization": "certified-operators",
      "package": "gpu-operator-certified",
      "provided_apis": [
        {
          "group": "nvidia.com",
          "kind": "ClusterPolicy",
          "plural": "clusterpolicies",
          "version": "v1"
        }
      ],
      "provider": "NVIDIA Corporation",
      "related_images": [
        {
          "digest": "sha256:7e3699c82bf02ba640d49775ce1b1fc96a5eca02d5ea453ea0ed1571d73779a5",
          "image": "nvcr.io/nvidia/gpu-operator@sha256:7e3699c82bf02ba640d49775ce1b1fc96a5eca02d5ea453ea0ed1571d73779a5",
          "name": "gpu-operator-image"
        },
        {
          "digest": "sha256:b90a519e9f30fbc013e1806bc081268724e9e80b4035c67c167ab3b43a37704a",
          "image": "nvcr.io/nvidia/k8s/dcgm-exporter@sha256:b90a519e9f30fbc013e1806bc081268724e9e80b4035c67c167ab3b43a37704a",
          "name": "dcgm-exporter-image"
        },
        {
          "digest": "sha256:8490d555131a57a11311561667ea9ec8cf554a399d2492a6f9e6677c01ec4b9a",
          "image": "nvcr.io/nvidia/cloud-native/dcgm@sha256:8490d555131a57a11311561667ea9ec8cf554a399d2492a6f9e6677c01ec4b9a",
          "name": "dcgm-image"
        },
        {
          "digest": "sha256:2d6e86db0e2b0db64c00566938e1851e25d4bb1019b1e7b696e2b199cba33397",
          "image": "nvcr.io/nvidia/k8s/container-toolkit@sha256:2d6e86db0e2b0db64c00566938e1851e25d4bb1019b1e7b696e2b199cba33397",
          "name": "container-toolkit-image"
        },
        {
          "digest": "sha256:8c9fabc1cd773e9526eb7e8526084f065f6cca9d0fd8dc2a672a0bb717f1cc60",
          "image": "nvcr.io/nvidia/driver@sha256:8c9fabc1cd773e9526eb7e8526084f065f6cca9d0fd8dc2a672a0bb717f1cc60",
          "name": "driver-image"
        },
        {
          "digest": "sha256:6e2902d73c54cf57bec94edf166d206cfeb0b948b5aab70800ccbe025064b3a1",
          "image": "nvcr.io/nvidia/driver@sha256:6e2902d73c54cf57bec94edf166d206cfeb0b948b5aab70800ccbe025064b3a1",
          "name": "driver-image-510"
        },
        {
          "digest": "sha256:cf8ce04907d0e8decf90a8dccfa6134af9c39509d2f8a21c355ed71223acacdb",
          "image": "nvcr.io/nvidia/driver@sha256:cf8ce04907d0e8decf90a8dccfa6134af9c39509d2f8a21c355ed71223acacdb",
          "name": "driver-image-470"
        },
        {
          "digest": "sha256:b8799e7397fae5a27446737443091eed05b918816f53c400a48be71125ee22e8",
          "image": "nvcr.io/nvidia/driver@sha256:b8799e7397fae5a27446737443091eed05b918816f53c400a48be71125ee22e8",
          "name": "driver-image-450"
        },
        {
          "digest": "sha256:7a0ccbcbe8d379bfc441e06504eb45d0d433a37e2a9e6dde83d13347296ebe81",
          "image": "nvcr.io/nvidia/k8s-device-plugin@sha256:7a0ccbcbe8d379bfc441e06504eb45d0d433a37e2a9e6dde83d13347296ebe81",
          "name": "device-plugin-image"
        },
        {
          "digest": "sha256:39127d04c9faff3f22068dcf463ed48d02be684cd18e3f14db843bddbdd783a7",
          "image": "nvcr.io/nvidia/gpu-feature-discovery@sha256:39127d04c9faff3f22068dcf463ed48d02be684cd18e3f14db843bddbdd783a7",
          "name": "gpu-feature-discovery-image"
        },
        {
          "digest": "sha256:3804e2b03c44d25dd410e3882b7fb6d1fde41f55110d7702af47136b042a63ff",
          "image": "nvcr.io/nvidia/cloud-native/k8s-mig-manager@sha256:3804e2b03c44d25dd410e3882b7fb6d1fde41f55110d7702af47136b042a63ff",
          "name": "mig-manager-image"
        },
        {
          "digest": "sha256:4400b56f81095b24fc9dde93ad7ad7d0ca6a7e9736c4c7f1d4e2bcbf0afbe45c",
          "image": "nvcr.io/nvidia/cuda@sha256:4400b56f81095b24fc9dde93ad7ad7d0ca6a7e9736c4c7f1d4e2bcbf0afbe45c",
          "name": "init-container-image"
        },
        {
          "digest": "sha256:d83aee1c7a9c3aa39e3947f0bd206e7955390943b8e3f2e56bdb96bc4ef99d69",
          "image": "nvcr.io/nvidia/cloud-native/gpu-operator-validator@sha256:d83aee1c7a9c3aa39e3947f0bd206e7955390943b8e3f2e56bdb96bc4ef99d69",
          "name": "gpu-operator-validator-image"
        },
        {
          "digest": "sha256:2cb6f2b43e153e67780b1b350ae171c800e15b3f1ae5ad6375b66de530d701fd",
          "image": "nvcr.io/nvidia/cloud-native/k8s-driver-manager@sha256:2cb6f2b43e153e67780b1b350ae171c800e15b3f1ae5ad6375b66de530d701fd",
          "name": "k8s-driver-manager-image"
        },
        {
          "digest": "sha256:4400b56f81095b24fc9dde93ad7ad7d0ca6a7e9736c4c7f1d4e2bcbf0afbe45c",
          "image": "nvcr.io/nvidia/cuda@sha256:4400b56f81095b24fc9dde93ad7ad7d0ca6a7e9736c4c7f1d4e2bcbf0afbe45c",
          "name": "vfio-manager-image"
        },
        {
          "digest": "sha256:f48e8b430248a9cadf716a9211bb0050d4a663d1f1386ea107e72917f82e5b5f",
          "image": "nvcr.io/nvidia/kubevirt-gpu-device-plugin@sha256:f48e8b430248a9cadf716a9211bb0050d4a663d1f1386ea107e72917f82e5b5f",
          "name": "sandbox-device-plugin-image"
        },
        {
          "digest": "sha256:16ee5b1a8d9217f761c40a06c7ae598cbc893fbc35d99ff9b84fa886ea6b9b37",
          "image": "nvcr.io/nvidia/cloud-native/vgpu-device-manager@sha256:16ee5b1a8d9217f761c40a06c7ae598cbc893fbc35d99ff9b84fa886ea6b9b37",
          "name": "vgpu-device-manager-image"
        },
        {
          "digest": "sha256:7e3699c82bf02ba640d49775ce1b1fc96a5eca02d5ea453ea0ed1571d73779a5",
          "image": "nvcr.io/nvidia/gpu-operator@sha256:7e3699c82bf02ba640d49775ce1b1fc96a5eca02d5ea453ea0ed1571d73779a5",
          "name": "gpu-operator-7e3699c82bf02ba640d49775ce1b1fc96a5eca02d5ea453ea0ed1571d73779a5-annotation"
        },
        {
          "digest": "sha256:7e3699c82bf02ba640d49775ce1b1fc96a5eca02d5ea453ea0ed1571d73779a5",
          "image": "nvcr.io/nvidia/gpu-operator@sha256:7e3699c82bf02ba640d49775ce1b1fc96a5eca02d5ea453ea0ed1571d73779a5",
          "name": "gpu-operator"
        }
      ],
      "replaces": null,
      "skip_range": ">=v1.9.0 <v1.11.0",
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.9",
      "version": "1.11.0",
      "version_original": "1.11.0"
    },
    {
      "_id": "62bc6842b490e03bc2bcb1ff",
      "alm_examples": [
        {
          "api_version": "bas.ibm.com/v1",
          "kind": "AirgappedDeployment",
          "metadata": {
            "name": "airgappeddeployment"
          },
          "spec": {
            "airgapped": {
              "backup_deletion_frequency": "@daily",
              "backup_retention_period": 7
            },
            "allowed_domains": "*",
            "db_archive": {
              "frequency": "@monthly",
              "persistent_storage": {
                "storage_class": "ibmc-file-bronze-gid",
                "storage_size": "10G"
              },
              "retention_age": 6
            },
            "env_type": "lite",
            "event_scheduler_frequency": "@hourly",
            "image_pull_secret": "bas-images-pull-secret",
            "kafka": {
              "storage_class": "ibmc-block-bronze",
              "storage_size": "5G",
              "zookeeper_storage_class": "ibmc-block-bronze",
              "zookeeper_storage_size": "5G"
            },
            "postgres": {
              "storage_class": "ibmc-block-bronze",
              "storage_size": "10G"
            }
          }
        },
        {
          "api_version": "bas.ibm.com/v1",
          "kind": "AnalyticsProxy",
          "metadata": {
            "name": "analyticsproxy"
          },
          "spec": {
            "airgapped": {
              "backup_deletion_frequency": "@daily",
              "backup_retention_period": 7,
              "enabled": false
            },
            "allowed_domains": "*",
            "db_archive": {
              "frequency": "@monthly",
              "persistent_storage": {
                "storage_class": "ibmc-file-bronze-gid",
                "storage_size": "10G"
              },
              "retention_age": 6
            },
            "env_type": "lite",
            "event_scheduler_frequency": "@hourly",
            "ibmproxyurl": "https://iaps.ibm.com",
            "image_pull_secret": "bas-images-pull-secret",
            "kafka": {
              "storage_class": "ibmc-block-bronze",
              "storage_size": "5G",
              "zookeeper_storage_class": "ibmc-block-bronze",
              "zookeeper_storage_size": "5G"
            },
            "postgres": {
              "storage_class": "ibmc-block-bronze",
              "storage_size": "10G"
            }
          }
        },
        {
          "api_version": "bas.ibm.com/v1",
          "kind": "DeleteCluster",
          "metadata": {
            "name": "deletecluster"
          },
          "spec": {
            "image_pull_secret": "bas-images-pull-secret"
          }
        },
        {
          "api_version": "bas.ibm.com/v1",
          "kind": "FullDeployment",
          "metadata": {
            "name": "fulldeployment"
          },
          "spec": {
            "airgapped": {
              "backup_deletion_frequency": "@daily",
              "backup_retention_period": 7,
              "enabled": false
            },
            "allowed_domains": "*",
            "db_archive": {
              "frequency": "@monthly",
              "persistent_storage": {
                "storage_class": "ibmc-file-bronze-gid",
                "storage_size": "10G"
              },
              "retention_age": 6
            },
            "env_type": "lite",
            "event_scheduler_frequency": "@hourly",
            "ibmproxyurl": "https://iaps.ibm.com",
            "image_pull_secret": "bas-images-pull-secret",
            "kafka": {
              "storage_class": "ibmc-block-bronze",
              "storage_size": "5G",
              "zookeeper_storage_class": "ibmc-block-bronze",
              "zookeeper_storage_size": "5G"
            },
            "postgres": {
              "storage_class": "ibmc-block-bronze",
              "storage_size": "10G"
            },
            "prometheus_metrics": [],
            "prometheus_scheduler_frequency": "@daily"
          }
        },
        {
          "api_version": "bas.ibm.com/v1",
          "kind": "GenerateKey",
          "metadata": {
            "name": "bas-api-key"
          },
          "spec": {
            "image_pull_secret": "bas-images-pull-secret"
          }
        },
        {
          "api_version": "bas.ibm.com/v1",
          "kind": "StoreForwardMetric",
          "metadata": {
            "name": "storeforwardmetric"
          },
          "spec": {
            "db_archive": {
              "frequency": "@monthly",
              "persistent_storage": {
                "storage_class": "ibmc-file-bronze-gid",
                "storage_size": "10G"
              },
              "retention_age": 6
            },
            "env_type": "lite",
            "event_scheduler_frequency": "@hourly",
            "ibmproxyurl": "https://iaps.ibm.com",
            "image_pull_secret": "bas-images-pull-secret",
            "postgres": {
              "storage_class": "ibmc-block-bronze",
              "storage_size": "10G"
            },
            "prometheus_metrics": [],
            "prometheus_scheduler_frequency": "@daily"
          }
        },
        {
          "api_version": "bas.ibm.com/v1",
          "kind": "Dashboard",
          "metadata": {
            "name": "dashboard"
          },
          "spec": {
            "enable_test_api": true,
            "image_pull_secret": "bas-images-pull-secret"
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/ibm-edge/behavior-analytics-services-operator-bundle@sha256:0170ac47368d23fbd6c625eac1d6b3d6f6d78c75fa216882e895150ffbe763e8",
      "bundle_path_digest": "sha256:0170ac47368d23fbd6c625eac1d6b3d6f6d78c75fa216882e895150ffbe763e8",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "alpha",
      "creation_date": "2022-06-29T14:57:06.411000+00:00",
      "csv_description": "Behavior Analytics Services Operator is a service that collects, transforms and transmits product usage data.\nThe Operator supports deployments using-\nA. Dashboard\nB. Openshift portal or CLI\n\n**A. Dashboard**\n----\nWhen the Operator is installed, the Dashboard is installed too. Goto the Dashboard Custom Resource to get the URL to the Dashboard.\n\n\n**B. Openshift portal or CLI**\n----\n----\nFor manual installation using Openshift portal or CLI, follow the steps below\n\n\n**Pre-requisites**\n----\nStep 1: Create a new Project\n```sh\noc new-project <project-name>\n```\n\nStep 2: Create a secret named \"database-credentials\" for PostgreSQL DB\n```sh\n oc create secret generic database-credentials --from-literal=db_username=<DB Username> --from-literal=db_password=<DB Password> -n <project-name>\n```\n\nStep 3: [Optional] If the images require a secret to pull the container images, create a secret of type dockerconfigjson. Pass this secret name in the CR spec image\\_pull\\_secret\n```sh\noc create secret generic bas-images-pull-secret --from-file=.dockerconfigjson=<path/to/.docker/config.json>  --type=kubernetes.io/dockerconfigjson\n```\n\n**Installing the Behavior Analytics Services**\n----\n\nSelect any one of the deployments to setup the Behavior Analytics Services\n1. Full Deployment\n2. Analytics Proxy Deployment\n3. Store and Forward metrics Deployment\n4. Airgapped Analytics Proxy Deployment\n\n| Spec                                                      | Description                                                      | Default Value          |\n| --------------------------------------------------------- | ------------------------------------------------------------ | ---------------------- |\n| `ibmproxyurl`                                               | URL of IBM Proxy Service                                     | `https://iaps.ibm.com`   |\n| `postgres.storage_class`                                    | Storage class of type ReadWriteOnce                          | Required user input        |\n| `postgres.size`                                             | Size (in G) of the storage to be attached to Database        | `10G`                    |\n| `kafka.storage_class`                                       | Storage class of type ReadWriteOnce                          | Required user input        |\n| `kafka.storage_size`                                        | Size (in G) of the storage to be attached to Kafka  | `5G`                     |\n| `kafka.zookeeper_storage_class`                                 | Storage class of type ReadWriteOnce                          | Required user input        |\n| `kafka.zookeeper_storage_size`                                 | Size (in G) of the storage to be attached to Zookeeper | `5G`                     |\n| `airgapped.enabled`                                         | Set value to \"true\" if airgapped setup is to be enabled otherwise keep the default value \"false\" | `false`                  |\n| `airgapped.backup_retention_period`   | Number of days to keep the backup files in the storage       | `7`                     |\n| `airgapped.backup_deletion_frequency` | Frequency of job to delete files from the storage. It accepts values in Cron format (https://en.wikipedia.org/wiki/Cron) | `@daily`                 |\n| `event_scheduler_frequency`                               | Frequency at which events will be forwarded to proxy in Cron format. In case if internet connection available on the cluster only during a certain time frame, specify time in UTC equivalent Cron format. | `@hourly`                |\n| `prometheus_scheduler_frequency`                          | Frequency in Cronjob format to pull metrics from Prometheus  | `@daily`                |\n| `image_pull_secret`                                       | Secret to pull container images from registry                | `bas-images-pull-secret` |\n| `db_archive.frequency`                                    | Frequency in Cronjob format to run archive job                                                           | `@monthly`             |\n| `db_archive.retention_age`                                | Number of months to retain the data in Database              | `6`                    |\n| `db_archive.persistent_storage.storage_class`             |Storage class of type ReadWriteMany         | Required user input    |\n| `db_archive.persistent_storage.storage_size`              | Size (in G) of the storage to be attached for saving airgapped files | `10G`                  |\n| `prometheus_metrics`                                      | Array of metrics information to be send to destination at desired frequency | []\n| `env_type`              | Type of environment. Can be **prod** (HA) or **lite** (non HA)   | `lite`                  |\n| `allowed_domains`                                         | Comma separated values of domains allowed by BAS. By default all domains are allowed | '*'\n\n\n**Generating the API Key**\n----\nOnce the deployment is complete, to use the Behavior Analytics Services Endpoints, generate an API key using the CR GenerateKey. This API key to be used to authenticate Behavior Analytics Services Endpoints.\n\nNote- Key can be retrieved from the secret with same name used while creating Generate Key CR's instance.\nOnce you get key from the secret, decode it using below command:\n```sh\necho -n <secret key> | base64 -d\n```\n\n**Deleting the API Key**\n----\nTo revoke the Key, delete the corresponding GenerateKey instance.\n\n**Using Behavior Analytics Services Endpoints**\n---\nFor all BAS APIs, use the Generated API Key for Authentication. Pass it as the Header- X-API-KEY and the key while using the APIs.\n",
      "csv_display_name": "Behavior Analytics Services",
      "csv_metadata_description": "Behavior Analytics Services is a service that collects, transforms and transmits product usage data.",
      "csv_name": "behavior-analytics-services-operator-certified.v1.1.6",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-11-23T07:38:41.868000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.8",
      "organization": "certified-operators",
      "package": "behavior-analytics-services-operator-certified",
      "provided_apis": [
        {
          "group": "bas.ibm.com",
          "kind": "AirgappedDeployment",
          "plural": "airgappeddeployments",
          "version": "v1"
        },
        {
          "group": "bas.ibm.com",
          "kind": "AnalyticsProxy",
          "plural": "analyticsproxies",
          "version": "v1"
        },
        {
          "group": "bas.ibm.com",
          "kind": "Dashboard",
          "plural": "dashboards",
          "version": "v1"
        },
        {
          "group": "bas.ibm.com",
          "kind": "DeleteCluster",
          "plural": "deleteclusters",
          "version": "v1"
        },
        {
          "group": "bas.ibm.com",
          "kind": "FullDeployment",
          "plural": "fulldeployments",
          "version": "v1"
        },
        {
          "group": "bas.ibm.com",
          "kind": "GenerateKey",
          "plural": "generatekeys",
          "version": "v1"
        },
        {
          "group": "bas.ibm.com",
          "kind": "StoreForwardMetric",
          "plural": "storeforwardmetrics",
          "version": "v1"
        }
      ],
      "provider": "IBM Digital Growth and Commerce",
      "related_images": [
        {
          "digest": "sha256:16e0f6de2eb27bff32beb5ea285503fef9d035065763f32f9404ec4ecf79a339",
          "image": "registry.connect.redhat.com/ibm-edge/behavior-analytics-services-operator@sha256:16e0f6de2eb27bff32beb5ea285503fef9d035065763f32f9404ec4ecf79a339",
          "name": "behavior-analytics-services-operator"
        },
        {
          "digest": "sha256:958d913b98a4a0ded8ea52d1579b79c1cf64a46989db3df4432930f741a765f2",
          "image": "registry.redhat.io/openshift4/ose-cli@sha256:958d913b98a4a0ded8ea52d1579b79c1cf64a46989db3df4432930f741a765f2",
          "name": "dashboardinstaller"
        },
        {
          "digest": "sha256:e16ff252bcd87e077b29fabfcd30a86a8f913055b6c3ddfc2c277343e419d459",
          "image": "registry.connect.redhat.com/ibm-edge/event-api@sha256:e16ff252bcd87e077b29fabfcd30a86a8f913055b6c3ddfc2c277343e419d459",
          "name": "event_api"
        },
        {
          "digest": "sha256:049c9a25970685c02e26a073fc1df0f12f5a032cdee4cffd5930c43abf2f7196",
          "image": "registry.connect.redhat.com/ibm-edge/store-api@sha256:049c9a25970685c02e26a073fc1df0f12f5a032cdee4cffd5930c43abf2f7196",
          "name": "store_api"
        },
        {
          "digest": "sha256:3c4659be4db05521de9f3094a2da93d75644e2834d9fcbd94b8a4fc89b92ef47",
          "image": "registry.connect.redhat.com/ibm-edge/event-reader@sha256:3c4659be4db05521de9f3094a2da93d75644e2834d9fcbd94b8a4fc89b92ef47",
          "name": "reader"
        },
        {
          "digest": "sha256:f4f94c828485ca93eacf456a3d1ad6504043420a3a8716a124bc49639246a97a",
          "image": "registry.connect.redhat.com/ibm-edge/event-scheduler@sha256:f4f94c828485ca93eacf456a3d1ad6504043420a3a8716a124bc49639246a97a",
          "name": "event_scheduler"
        },
        {
          "digest": "sha256:26c5ad9fa67063e0c7066c0e7b7dddf1b18059184a1abc9fc8e1ba25a7679f48",
          "image": "registry.connect.redhat.com/ibm-edge/prometheus-scheduler@sha256:26c5ad9fa67063e0c7066c0e7b7dddf1b18059184a1abc9fc8e1ba25a7679f48",
          "name": "prometheus_scheduler"
        },
        {
          "digest": "sha256:ab4584bba734856f40a9a46889408f79ab6d19a7a737038e66504eed88c7a0a1",
          "image": "registry.connect.redhat.com/ibm-edge/pgo-client@sha256:ab4584bba734856f40a9a46889408f79ab6d19a7a737038e66504eed88c7a0a1",
          "name": "pgo_client"
        },
        {
          "digest": "sha256:e40afed2bfcfc7f1d81dcd8ed3f2ad95152d08dd16310fc4043300d192b1c980",
          "image": "registry.redhat.io/openshift4/ose-grafana@sha256:e40afed2bfcfc7f1d81dcd8ed3f2ad95152d08dd16310fc4043300d192b1c980",
          "name": "grafana"
        },
        {
          "digest": "sha256:98addfbaa182d18bcce234925d6bbf79203432f601c18ae74dca5eadb02a53ad",
          "image": "registry.redhat.io/openshift3/ose-kube-state-metrics@sha256:98addfbaa182d18bcce234925d6bbf79203432f601c18ae74dca5eadb02a53ad",
          "name": "ksm"
        },
        {
          "digest": "sha256:f23231c5ac515680b8eba1abe458218378c3dcb5f4d54244b65d3559f5cb6d1a",
          "image": "registry.connect.redhat.com/ibm-edge/growth-stack-base@sha256:f23231c5ac515680b8eba1abe458218378c3dcb5f4d54244b65d3559f5cb6d1a",
          "name": "init_container"
        },
        {
          "digest": "sha256:1f89aefa39dbc96a47ae451d64f200a0013637cdbe048d1b9ef56c99976bb8c0",
          "image": "registry.access.redhat.com/ubi8/ubi@sha256:1f89aefa39dbc96a47ae451d64f200a0013637cdbe048d1b9ef56c99976bb8c0",
          "name": "ubi"
        },
        {
          "digest": "sha256:a6f6188b2fc30d351f69004c043fa38cafadee23bca356f921a273cce103d121",
          "image": "registry.connect.redhat.com/ibm-edge/airgap-download-ui@sha256:a6f6188b2fc30d351f69004c043fa38cafadee23bca356f921a273cce103d121",
          "name": "download_ui"
        },
        {
          "digest": "sha256:6b93afc6bc4a624032714cf08389cb82be60deb579ea0333efab5e365935f236",
          "image": "registry.connect.redhat.com/ibm-edge/bas-operator-dashboard@sha256:6b93afc6bc4a624032714cf08389cb82be60deb579ea0333efab5e365935f236",
          "name": "dashboard_ui"
        },
        {
          "digest": "sha256:9a5ee95f8e99a63a4ad0e8b01683ac03c75337bbbe3d504d199a97f9921eb0c1",
          "image": "registry.redhat.io/openshift4/ose-oauth-proxy@sha256:9a5ee95f8e99a63a4ad0e8b01683ac03c75337bbbe3d504d199a97f9921eb0c1",
          "name": "oauth_proxy"
        },
        {
          "digest": "sha256:479dcc101de2e98612e433736a503aa849a573418036a308c81373327527372c",
          "image": "registry.connect.redhat.com/crunchydata/pgo-deployer@sha256:479dcc101de2e98612e433736a503aa849a573418036a308c81373327527372c",
          "name": "pgo_deployer"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.8",
      "version": "1.1.6",
      "version_original": "1.1.6"
    },
    {
      "_id": "62bc6946655931047b68784d",
      "alm_examples": [
        {
          "api_version": "bas.ibm.com/v1",
          "kind": "AirgappedDeployment",
          "metadata": {
            "name": "airgappeddeployment"
          },
          "spec": {
            "airgapped": {
              "backup_deletion_frequency": "@daily",
              "backup_retention_period": 7
            },
            "allowed_domains": "*",
            "db_archive": {
              "frequency": "@monthly",
              "persistent_storage": {
                "storage_class": "ibmc-file-bronze-gid",
                "storage_size": "10G"
              },
              "retention_age": 6
            },
            "env_type": "lite",
            "event_scheduler_frequency": "@hourly",
            "image_pull_secret": "bas-images-pull-secret",
            "kafka": {
              "storage_class": "ibmc-block-bronze",
              "storage_size": "5G",
              "zookeeper_storage_class": "ibmc-block-bronze",
              "zookeeper_storage_size": "5G"
            },
            "postgres": {
              "storage_class": "ibmc-block-bronze",
              "storage_size": "10G"
            }
          }
        },
        {
          "api_version": "bas.ibm.com/v1",
          "kind": "AnalyticsProxy",
          "metadata": {
            "name": "analyticsproxy"
          },
          "spec": {
            "airgapped": {
              "backup_deletion_frequency": "@daily",
              "backup_retention_period": 7,
              "enabled": false
            },
            "allowed_domains": "*",
            "db_archive": {
              "frequency": "@monthly",
              "persistent_storage": {
                "storage_class": "ibmc-file-bronze-gid",
                "storage_size": "10G"
              },
              "retention_age": 6
            },
            "env_type": "lite",
            "event_scheduler_frequency": "@hourly",
            "ibmproxyurl": "https://iaps.ibm.com",
            "image_pull_secret": "bas-images-pull-secret",
            "kafka": {
              "storage_class": "ibmc-block-bronze",
              "storage_size": "5G",
              "zookeeper_storage_class": "ibmc-block-bronze",
              "zookeeper_storage_size": "5G"
            },
            "postgres": {
              "storage_class": "ibmc-block-bronze",
              "storage_size": "10G"
            }
          }
        },
        {
          "api_version": "bas.ibm.com/v1",
          "kind": "DeleteCluster",
          "metadata": {
            "name": "deletecluster"
          },
          "spec": {
            "image_pull_secret": "bas-images-pull-secret"
          }
        },
        {
          "api_version": "bas.ibm.com/v1",
          "kind": "FullDeployment",
          "metadata": {
            "name": "fulldeployment"
          },
          "spec": {
            "airgapped": {
              "backup_deletion_frequency": "@daily",
              "backup_retention_period": 7,
              "enabled": false
            },
            "allowed_domains": "*",
            "db_archive": {
              "frequency": "@monthly",
              "persistent_storage": {
                "storage_class": "ibmc-file-bronze-gid",
                "storage_size": "10G"
              },
              "retention_age": 6
            },
            "env_type": "lite",
            "event_scheduler_frequency": "@hourly",
            "ibmproxyurl": "https://iaps.ibm.com",
            "image_pull_secret": "bas-images-pull-secret",
            "kafka": {
              "storage_class": "ibmc-block-bronze",
              "storage_size": "5G",
              "zookeeper_storage_class": "ibmc-block-bronze",
              "zookeeper_storage_size": "5G"
            },
            "postgres": {
              "storage_class": "ibmc-block-bronze",
              "storage_size": "10G"
            },
            "prometheus_metrics": [],
            "prometheus_scheduler_frequency": "@daily"
          }
        },
        {
          "api_version": "bas.ibm.com/v1",
          "kind": "GenerateKey",
          "metadata": {
            "name": "bas-api-key"
          },
          "spec": {
            "image_pull_secret": "bas-images-pull-secret"
          }
        },
        {
          "api_version": "bas.ibm.com/v1",
          "kind": "StoreForwardMetric",
          "metadata": {
            "name": "storeforwardmetric"
          },
          "spec": {
            "db_archive": {
              "frequency": "@monthly",
              "persistent_storage": {
                "storage_class": "ibmc-file-bronze-gid",
                "storage_size": "10G"
              },
              "retention_age": 6
            },
            "env_type": "lite",
            "event_scheduler_frequency": "@hourly",
            "ibmproxyurl": "https://iaps.ibm.com",
            "image_pull_secret": "bas-images-pull-secret",
            "postgres": {
              "storage_class": "ibmc-block-bronze",
              "storage_size": "10G"
            },
            "prometheus_metrics": [],
            "prometheus_scheduler_frequency": "@daily"
          }
        },
        {
          "api_version": "bas.ibm.com/v1",
          "kind": "Dashboard",
          "metadata": {
            "name": "dashboard"
          },
          "spec": {
            "enable_test_api": true,
            "image_pull_secret": "bas-images-pull-secret"
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/ibm-edge/behavior-analytics-services-operator-bundle@sha256:0170ac47368d23fbd6c625eac1d6b3d6f6d78c75fa216882e895150ffbe763e8",
      "bundle_path_digest": "sha256:0170ac47368d23fbd6c625eac1d6b3d6f6d78c75fa216882e895150ffbe763e8",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "alpha",
      "creation_date": "2022-06-29T15:01:26.843000+00:00",
      "csv_description": "Behavior Analytics Services Operator is a service that collects, transforms and transmits product usage data.\nThe Operator supports deployments using-\nA. Dashboard\nB. Openshift portal or CLI\n\n**A. Dashboard**\n----\nWhen the Operator is installed, the Dashboard is installed too. Goto the Dashboard Custom Resource to get the URL to the Dashboard.\n\n\n**B. Openshift portal or CLI**\n----\n----\nFor manual installation using Openshift portal or CLI, follow the steps below\n\n\n**Pre-requisites**\n----\nStep 1: Create a new Project\n```sh\noc new-project <project-name>\n```\n\nStep 2: Create a secret named \"database-credentials\" for PostgreSQL DB\n```sh\n oc create secret generic database-credentials --from-literal=db_username=<DB Username> --from-literal=db_password=<DB Password> -n <project-name>\n```\n\nStep 3: [Optional] If the images require a secret to pull the container images, create a secret of type dockerconfigjson. Pass this secret name in the CR spec image\\_pull\\_secret\n```sh\noc create secret generic bas-images-pull-secret --from-file=.dockerconfigjson=<path/to/.docker/config.json>  --type=kubernetes.io/dockerconfigjson\n```\n\n**Installing the Behavior Analytics Services**\n----\n\nSelect any one of the deployments to setup the Behavior Analytics Services\n1. Full Deployment\n2. Analytics Proxy Deployment\n3. Store and Forward metrics Deployment\n4. Airgapped Analytics Proxy Deployment\n\n| Spec                                                      | Description                                                      | Default Value          |\n| --------------------------------------------------------- | ------------------------------------------------------------ | ---------------------- |\n| `ibmproxyurl`                                               | URL of IBM Proxy Service                                     | `https://iaps.ibm.com`   |\n| `postgres.storage_class`                                    | Storage class of type ReadWriteOnce                          | Required user input        |\n| `postgres.size`                                             | Size (in G) of the storage to be attached to Database        | `10G`                    |\n| `kafka.storage_class`                                       | Storage class of type ReadWriteOnce                          | Required user input        |\n| `kafka.storage_size`                                        | Size (in G) of the storage to be attached to Kafka  | `5G`                     |\n| `kafka.zookeeper_storage_class`                                 | Storage class of type ReadWriteOnce                          | Required user input        |\n| `kafka.zookeeper_storage_size`                                 | Size (in G) of the storage to be attached to Zookeeper | `5G`                     |\n| `airgapped.enabled`                                         | Set value to \"true\" if airgapped setup is to be enabled otherwise keep the default value \"false\" | `false`                  |\n| `airgapped.backup_retention_period`   | Number of days to keep the backup files in the storage       | `7`                     |\n| `airgapped.backup_deletion_frequency` | Frequency of job to delete files from the storage. It accepts values in Cron format (https://en.wikipedia.org/wiki/Cron) | `@daily`                 |\n| `event_scheduler_frequency`                               | Frequency at which events will be forwarded to proxy in Cron format. In case if internet connection available on the cluster only during a certain time frame, specify time in UTC equivalent Cron format. | `@hourly`                |\n| `prometheus_scheduler_frequency`                          | Frequency in Cronjob format to pull metrics from Prometheus  | `@daily`                |\n| `image_pull_secret`                                       | Secret to pull container images from registry                | `bas-images-pull-secret` |\n| `db_archive.frequency`                                    | Frequency in Cronjob format to run archive job                                                           | `@monthly`             |\n| `db_archive.retention_age`                                | Number of months to retain the data in Database              | `6`                    |\n| `db_archive.persistent_storage.storage_class`             |Storage class of type ReadWriteMany         | Required user input    |\n| `db_archive.persistent_storage.storage_size`              | Size (in G) of the storage to be attached for saving airgapped files | `10G`                  |\n| `prometheus_metrics`                                      | Array of metrics information to be send to destination at desired frequency | []\n| `env_type`              | Type of environment. Can be **prod** (HA) or **lite** (non HA)   | `lite`                  |\n| `allowed_domains`                                         | Comma separated values of domains allowed by BAS. By default all domains are allowed | '*'\n\n\n**Generating the API Key**\n----\nOnce the deployment is complete, to use the Behavior Analytics Services Endpoints, generate an API key using the CR GenerateKey. This API key to be used to authenticate Behavior Analytics Services Endpoints.\n\nNote- Key can be retrieved from the secret with same name used while creating Generate Key CR's instance.\nOnce you get key from the secret, decode it using below command:\n```sh\necho -n <secret key> | base64 -d\n```\n\n**Deleting the API Key**\n----\nTo revoke the Key, delete the corresponding GenerateKey instance.\n\n**Using Behavior Analytics Services Endpoints**\n---\nFor all BAS APIs, use the Generated API Key for Authentication. Pass it as the Header- X-API-KEY and the key while using the APIs.\n",
      "csv_display_name": "Behavior Analytics Services",
      "csv_metadata_description": "Behavior Analytics Services is a service that collects, transforms and transmits product usage data.",
      "csv_name": "behavior-analytics-services-operator-certified.v1.1.6",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-11-23T07:38:12.088000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.6",
      "organization": "certified-operators",
      "package": "behavior-analytics-services-operator-certified",
      "provided_apis": [
        {
          "group": "bas.ibm.com",
          "kind": "DeleteCluster",
          "plural": "deleteclusters",
          "version": "v1"
        },
        {
          "group": "bas.ibm.com",
          "kind": "FullDeployment",
          "plural": "fulldeployments",
          "version": "v1"
        },
        {
          "group": "bas.ibm.com",
          "kind": "GenerateKey",
          "plural": "generatekeys",
          "version": "v1"
        },
        {
          "group": "bas.ibm.com",
          "kind": "StoreForwardMetric",
          "plural": "storeforwardmetrics",
          "version": "v1"
        },
        {
          "group": "bas.ibm.com",
          "kind": "AirgappedDeployment",
          "plural": "airgappeddeployments",
          "version": "v1"
        },
        {
          "group": "bas.ibm.com",
          "kind": "AnalyticsProxy",
          "plural": "analyticsproxies",
          "version": "v1"
        },
        {
          "group": "bas.ibm.com",
          "kind": "Dashboard",
          "plural": "dashboards",
          "version": "v1"
        }
      ],
      "provider": "IBM Digital Growth and Commerce",
      "related_images": [
        {
          "digest": "sha256:16e0f6de2eb27bff32beb5ea285503fef9d035065763f32f9404ec4ecf79a339",
          "image": "registry.connect.redhat.com/ibm-edge/behavior-analytics-services-operator@sha256:16e0f6de2eb27bff32beb5ea285503fef9d035065763f32f9404ec4ecf79a339",
          "name": "behavior-analytics-services-operator"
        },
        {
          "digest": "sha256:958d913b98a4a0ded8ea52d1579b79c1cf64a46989db3df4432930f741a765f2",
          "image": "registry.redhat.io/openshift4/ose-cli@sha256:958d913b98a4a0ded8ea52d1579b79c1cf64a46989db3df4432930f741a765f2",
          "name": "dashboardinstaller"
        },
        {
          "digest": "sha256:e16ff252bcd87e077b29fabfcd30a86a8f913055b6c3ddfc2c277343e419d459",
          "image": "registry.connect.redhat.com/ibm-edge/event-api@sha256:e16ff252bcd87e077b29fabfcd30a86a8f913055b6c3ddfc2c277343e419d459",
          "name": "event_api"
        },
        {
          "digest": "sha256:049c9a25970685c02e26a073fc1df0f12f5a032cdee4cffd5930c43abf2f7196",
          "image": "registry.connect.redhat.com/ibm-edge/store-api@sha256:049c9a25970685c02e26a073fc1df0f12f5a032cdee4cffd5930c43abf2f7196",
          "name": "store_api"
        },
        {
          "digest": "sha256:3c4659be4db05521de9f3094a2da93d75644e2834d9fcbd94b8a4fc89b92ef47",
          "image": "registry.connect.redhat.com/ibm-edge/event-reader@sha256:3c4659be4db05521de9f3094a2da93d75644e2834d9fcbd94b8a4fc89b92ef47",
          "name": "reader"
        },
        {
          "digest": "sha256:f4f94c828485ca93eacf456a3d1ad6504043420a3a8716a124bc49639246a97a",
          "image": "registry.connect.redhat.com/ibm-edge/event-scheduler@sha256:f4f94c828485ca93eacf456a3d1ad6504043420a3a8716a124bc49639246a97a",
          "name": "event_scheduler"
        },
        {
          "digest": "sha256:26c5ad9fa67063e0c7066c0e7b7dddf1b18059184a1abc9fc8e1ba25a7679f48",
          "image": "registry.connect.redhat.com/ibm-edge/prometheus-scheduler@sha256:26c5ad9fa67063e0c7066c0e7b7dddf1b18059184a1abc9fc8e1ba25a7679f48",
          "name": "prometheus_scheduler"
        },
        {
          "digest": "sha256:ab4584bba734856f40a9a46889408f79ab6d19a7a737038e66504eed88c7a0a1",
          "image": "registry.connect.redhat.com/ibm-edge/pgo-client@sha256:ab4584bba734856f40a9a46889408f79ab6d19a7a737038e66504eed88c7a0a1",
          "name": "pgo_client"
        },
        {
          "digest": "sha256:e40afed2bfcfc7f1d81dcd8ed3f2ad95152d08dd16310fc4043300d192b1c980",
          "image": "registry.redhat.io/openshift4/ose-grafana@sha256:e40afed2bfcfc7f1d81dcd8ed3f2ad95152d08dd16310fc4043300d192b1c980",
          "name": "grafana"
        },
        {
          "digest": "sha256:98addfbaa182d18bcce234925d6bbf79203432f601c18ae74dca5eadb02a53ad",
          "image": "registry.redhat.io/openshift3/ose-kube-state-metrics@sha256:98addfbaa182d18bcce234925d6bbf79203432f601c18ae74dca5eadb02a53ad",
          "name": "ksm"
        },
        {
          "digest": "sha256:f23231c5ac515680b8eba1abe458218378c3dcb5f4d54244b65d3559f5cb6d1a",
          "image": "registry.connect.redhat.com/ibm-edge/growth-stack-base@sha256:f23231c5ac515680b8eba1abe458218378c3dcb5f4d54244b65d3559f5cb6d1a",
          "name": "init_container"
        },
        {
          "digest": "sha256:1f89aefa39dbc96a47ae451d64f200a0013637cdbe048d1b9ef56c99976bb8c0",
          "image": "registry.access.redhat.com/ubi8/ubi@sha256:1f89aefa39dbc96a47ae451d64f200a0013637cdbe048d1b9ef56c99976bb8c0",
          "name": "ubi"
        },
        {
          "digest": "sha256:a6f6188b2fc30d351f69004c043fa38cafadee23bca356f921a273cce103d121",
          "image": "registry.connect.redhat.com/ibm-edge/airgap-download-ui@sha256:a6f6188b2fc30d351f69004c043fa38cafadee23bca356f921a273cce103d121",
          "name": "download_ui"
        },
        {
          "digest": "sha256:6b93afc6bc4a624032714cf08389cb82be60deb579ea0333efab5e365935f236",
          "image": "registry.connect.redhat.com/ibm-edge/bas-operator-dashboard@sha256:6b93afc6bc4a624032714cf08389cb82be60deb579ea0333efab5e365935f236",
          "name": "dashboard_ui"
        },
        {
          "digest": "sha256:9a5ee95f8e99a63a4ad0e8b01683ac03c75337bbbe3d504d199a97f9921eb0c1",
          "image": "registry.redhat.io/openshift4/ose-oauth-proxy@sha256:9a5ee95f8e99a63a4ad0e8b01683ac03c75337bbbe3d504d199a97f9921eb0c1",
          "name": "oauth_proxy"
        },
        {
          "digest": "sha256:479dcc101de2e98612e433736a503aa849a573418036a308c81373327527372c",
          "image": "registry.connect.redhat.com/crunchydata/pgo-deployer@sha256:479dcc101de2e98612e433736a503aa849a573418036a308c81373327527372c",
          "name": "pgo_deployer"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.6",
      "version": "1.1.6",
      "version_original": "1.1.6"
    },
    {
      "_id": "62bc6a140ad353df5dffb69b",
      "alm_examples": [
        {
          "api_version": "bas.ibm.com/v1",
          "kind": "AirgappedDeployment",
          "metadata": {
            "name": "airgappeddeployment"
          },
          "spec": {
            "airgapped": {
              "backup_deletion_frequency": "@daily",
              "backup_retention_period": 7
            },
            "allowed_domains": "*",
            "db_archive": {
              "frequency": "@monthly",
              "persistent_storage": {
                "storage_class": "ibmc-file-bronze-gid",
                "storage_size": "10G"
              },
              "retention_age": 6
            },
            "env_type": "lite",
            "event_scheduler_frequency": "@hourly",
            "image_pull_secret": "bas-images-pull-secret",
            "kafka": {
              "storage_class": "ibmc-block-bronze",
              "storage_size": "5G",
              "zookeeper_storage_class": "ibmc-block-bronze",
              "zookeeper_storage_size": "5G"
            },
            "postgres": {
              "storage_class": "ibmc-block-bronze",
              "storage_size": "10G"
            }
          }
        },
        {
          "api_version": "bas.ibm.com/v1",
          "kind": "AnalyticsProxy",
          "metadata": {
            "name": "analyticsproxy"
          },
          "spec": {
            "airgapped": {
              "backup_deletion_frequency": "@daily",
              "backup_retention_period": 7,
              "enabled": false
            },
            "allowed_domains": "*",
            "db_archive": {
              "frequency": "@monthly",
              "persistent_storage": {
                "storage_class": "ibmc-file-bronze-gid",
                "storage_size": "10G"
              },
              "retention_age": 6
            },
            "env_type": "lite",
            "event_scheduler_frequency": "@hourly",
            "ibmproxyurl": "https://iaps.ibm.com",
            "image_pull_secret": "bas-images-pull-secret",
            "kafka": {
              "storage_class": "ibmc-block-bronze",
              "storage_size": "5G",
              "zookeeper_storage_class": "ibmc-block-bronze",
              "zookeeper_storage_size": "5G"
            },
            "postgres": {
              "storage_class": "ibmc-block-bronze",
              "storage_size": "10G"
            }
          }
        },
        {
          "api_version": "bas.ibm.com/v1",
          "kind": "DeleteCluster",
          "metadata": {
            "name": "deletecluster"
          },
          "spec": {
            "image_pull_secret": "bas-images-pull-secret"
          }
        },
        {
          "api_version": "bas.ibm.com/v1",
          "kind": "FullDeployment",
          "metadata": {
            "name": "fulldeployment"
          },
          "spec": {
            "airgapped": {
              "backup_deletion_frequency": "@daily",
              "backup_retention_period": 7,
              "enabled": false
            },
            "allowed_domains": "*",
            "db_archive": {
              "frequency": "@monthly",
              "persistent_storage": {
                "storage_class": "ibmc-file-bronze-gid",
                "storage_size": "10G"
              },
              "retention_age": 6
            },
            "env_type": "lite",
            "event_scheduler_frequency": "@hourly",
            "ibmproxyurl": "https://iaps.ibm.com",
            "image_pull_secret": "bas-images-pull-secret",
            "kafka": {
              "storage_class": "ibmc-block-bronze",
              "storage_size": "5G",
              "zookeeper_storage_class": "ibmc-block-bronze",
              "zookeeper_storage_size": "5G"
            },
            "postgres": {
              "storage_class": "ibmc-block-bronze",
              "storage_size": "10G"
            },
            "prometheus_metrics": [],
            "prometheus_scheduler_frequency": "@daily"
          }
        },
        {
          "api_version": "bas.ibm.com/v1",
          "kind": "GenerateKey",
          "metadata": {
            "name": "bas-api-key"
          },
          "spec": {
            "image_pull_secret": "bas-images-pull-secret"
          }
        },
        {
          "api_version": "bas.ibm.com/v1",
          "kind": "StoreForwardMetric",
          "metadata": {
            "name": "storeforwardmetric"
          },
          "spec": {
            "db_archive": {
              "frequency": "@monthly",
              "persistent_storage": {
                "storage_class": "ibmc-file-bronze-gid",
                "storage_size": "10G"
              },
              "retention_age": 6
            },
            "env_type": "lite",
            "event_scheduler_frequency": "@hourly",
            "ibmproxyurl": "https://iaps.ibm.com",
            "image_pull_secret": "bas-images-pull-secret",
            "postgres": {
              "storage_class": "ibmc-block-bronze",
              "storage_size": "10G"
            },
            "prometheus_metrics": [],
            "prometheus_scheduler_frequency": "@daily"
          }
        },
        {
          "api_version": "bas.ibm.com/v1",
          "kind": "Dashboard",
          "metadata": {
            "name": "dashboard"
          },
          "spec": {
            "enable_test_api": true,
            "image_pull_secret": "bas-images-pull-secret"
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/ibm-edge/behavior-analytics-services-operator-bundle@sha256:0170ac47368d23fbd6c625eac1d6b3d6f6d78c75fa216882e895150ffbe763e8",
      "bundle_path_digest": "sha256:0170ac47368d23fbd6c625eac1d6b3d6f6d78c75fa216882e895150ffbe763e8",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "alpha",
      "creation_date": "2022-06-29T15:04:52.893000+00:00",
      "csv_description": "Behavior Analytics Services Operator is a service that collects, transforms and transmits product usage data.\nThe Operator supports deployments using-\nA. Dashboard\nB. Openshift portal or CLI\n\n**A. Dashboard**\n----\nWhen the Operator is installed, the Dashboard is installed too. Goto the Dashboard Custom Resource to get the URL to the Dashboard.\n\n\n**B. Openshift portal or CLI**\n----\n----\nFor manual installation using Openshift portal or CLI, follow the steps below\n\n\n**Pre-requisites**\n----\nStep 1: Create a new Project\n```sh\noc new-project <project-name>\n```\n\nStep 2: Create a secret named \"database-credentials\" for PostgreSQL DB\n```sh\n oc create secret generic database-credentials --from-literal=db_username=<DB Username> --from-literal=db_password=<DB Password> -n <project-name>\n```\n\nStep 3: [Optional] If the images require a secret to pull the container images, create a secret of type dockerconfigjson. Pass this secret name in the CR spec image\\_pull\\_secret\n```sh\noc create secret generic bas-images-pull-secret --from-file=.dockerconfigjson=<path/to/.docker/config.json>  --type=kubernetes.io/dockerconfigjson\n```\n\n**Installing the Behavior Analytics Services**\n----\n\nSelect any one of the deployments to setup the Behavior Analytics Services\n1. Full Deployment\n2. Analytics Proxy Deployment\n3. Store and Forward metrics Deployment\n4. Airgapped Analytics Proxy Deployment\n\n| Spec                                                      | Description                                                      | Default Value          |\n| --------------------------------------------------------- | ------------------------------------------------------------ | ---------------------- |\n| `ibmproxyurl`                                               | URL of IBM Proxy Service                                     | `https://iaps.ibm.com`   |\n| `postgres.storage_class`                                    | Storage class of type ReadWriteOnce                          | Required user input        |\n| `postgres.size`                                             | Size (in G) of the storage to be attached to Database        | `10G`                    |\n| `kafka.storage_class`                                       | Storage class of type ReadWriteOnce                          | Required user input        |\n| `kafka.storage_size`                                        | Size (in G) of the storage to be attached to Kafka  | `5G`                     |\n| `kafka.zookeeper_storage_class`                                 | Storage class of type ReadWriteOnce                          | Required user input        |\n| `kafka.zookeeper_storage_size`                                 | Size (in G) of the storage to be attached to Zookeeper | `5G`                     |\n| `airgapped.enabled`                                         | Set value to \"true\" if airgapped setup is to be enabled otherwise keep the default value \"false\" | `false`                  |\n| `airgapped.backup_retention_period`   | Number of days to keep the backup files in the storage       | `7`                     |\n| `airgapped.backup_deletion_frequency` | Frequency of job to delete files from the storage. It accepts values in Cron format (https://en.wikipedia.org/wiki/Cron) | `@daily`                 |\n| `event_scheduler_frequency`                               | Frequency at which events will be forwarded to proxy in Cron format. In case if internet connection available on the cluster only during a certain time frame, specify time in UTC equivalent Cron format. | `@hourly`                |\n| `prometheus_scheduler_frequency`                          | Frequency in Cronjob format to pull metrics from Prometheus  | `@daily`                |\n| `image_pull_secret`                                       | Secret to pull container images from registry                | `bas-images-pull-secret` |\n| `db_archive.frequency`                                    | Frequency in Cronjob format to run archive job                                                           | `@monthly`             |\n| `db_archive.retention_age`                                | Number of months to retain the data in Database              | `6`                    |\n| `db_archive.persistent_storage.storage_class`             |Storage class of type ReadWriteMany         | Required user input    |\n| `db_archive.persistent_storage.storage_size`              | Size (in G) of the storage to be attached for saving airgapped files | `10G`                  |\n| `prometheus_metrics`                                      | Array of metrics information to be send to destination at desired frequency | []\n| `env_type`              | Type of environment. Can be **prod** (HA) or **lite** (non HA)   | `lite`                  |\n| `allowed_domains`                                         | Comma separated values of domains allowed by BAS. By default all domains are allowed | '*'\n\n\n**Generating the API Key**\n----\nOnce the deployment is complete, to use the Behavior Analytics Services Endpoints, generate an API key using the CR GenerateKey. This API key to be used to authenticate Behavior Analytics Services Endpoints.\n\nNote- Key can be retrieved from the secret with same name used while creating Generate Key CR's instance.\nOnce you get key from the secret, decode it using below command:\n```sh\necho -n <secret key> | base64 -d\n```\n\n**Deleting the API Key**\n----\nTo revoke the Key, delete the corresponding GenerateKey instance.\n\n**Using Behavior Analytics Services Endpoints**\n---\nFor all BAS APIs, use the Generated API Key for Authentication. Pass it as the Header- X-API-KEY and the key while using the APIs.\n",
      "csv_display_name": "Behavior Analytics Services",
      "csv_metadata_description": "Behavior Analytics Services is a service that collects, transforms and transmits product usage data.",
      "csv_name": "behavior-analytics-services-operator-certified.v1.1.6",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-11-23T07:18:15.974000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.7",
      "organization": "certified-operators",
      "package": "behavior-analytics-services-operator-certified",
      "provided_apis": [
        {
          "group": "bas.ibm.com",
          "kind": "StoreForwardMetric",
          "plural": "storeforwardmetrics",
          "version": "v1"
        },
        {
          "group": "bas.ibm.com",
          "kind": "AirgappedDeployment",
          "plural": "airgappeddeployments",
          "version": "v1"
        },
        {
          "group": "bas.ibm.com",
          "kind": "AnalyticsProxy",
          "plural": "analyticsproxies",
          "version": "v1"
        },
        {
          "group": "bas.ibm.com",
          "kind": "Dashboard",
          "plural": "dashboards",
          "version": "v1"
        },
        {
          "group": "bas.ibm.com",
          "kind": "DeleteCluster",
          "plural": "deleteclusters",
          "version": "v1"
        },
        {
          "group": "bas.ibm.com",
          "kind": "FullDeployment",
          "plural": "fulldeployments",
          "version": "v1"
        },
        {
          "group": "bas.ibm.com",
          "kind": "GenerateKey",
          "plural": "generatekeys",
          "version": "v1"
        }
      ],
      "provider": "IBM Digital Growth and Commerce",
      "related_images": [
        {
          "digest": "sha256:16e0f6de2eb27bff32beb5ea285503fef9d035065763f32f9404ec4ecf79a339",
          "image": "registry.connect.redhat.com/ibm-edge/behavior-analytics-services-operator@sha256:16e0f6de2eb27bff32beb5ea285503fef9d035065763f32f9404ec4ecf79a339",
          "name": "behavior-analytics-services-operator"
        },
        {
          "digest": "sha256:958d913b98a4a0ded8ea52d1579b79c1cf64a46989db3df4432930f741a765f2",
          "image": "registry.redhat.io/openshift4/ose-cli@sha256:958d913b98a4a0ded8ea52d1579b79c1cf64a46989db3df4432930f741a765f2",
          "name": "dashboardinstaller"
        },
        {
          "digest": "sha256:e16ff252bcd87e077b29fabfcd30a86a8f913055b6c3ddfc2c277343e419d459",
          "image": "registry.connect.redhat.com/ibm-edge/event-api@sha256:e16ff252bcd87e077b29fabfcd30a86a8f913055b6c3ddfc2c277343e419d459",
          "name": "event_api"
        },
        {
          "digest": "sha256:049c9a25970685c02e26a073fc1df0f12f5a032cdee4cffd5930c43abf2f7196",
          "image": "registry.connect.redhat.com/ibm-edge/store-api@sha256:049c9a25970685c02e26a073fc1df0f12f5a032cdee4cffd5930c43abf2f7196",
          "name": "store_api"
        },
        {
          "digest": "sha256:3c4659be4db05521de9f3094a2da93d75644e2834d9fcbd94b8a4fc89b92ef47",
          "image": "registry.connect.redhat.com/ibm-edge/event-reader@sha256:3c4659be4db05521de9f3094a2da93d75644e2834d9fcbd94b8a4fc89b92ef47",
          "name": "reader"
        },
        {
          "digest": "sha256:f4f94c828485ca93eacf456a3d1ad6504043420a3a8716a124bc49639246a97a",
          "image": "registry.connect.redhat.com/ibm-edge/event-scheduler@sha256:f4f94c828485ca93eacf456a3d1ad6504043420a3a8716a124bc49639246a97a",
          "name": "event_scheduler"
        },
        {
          "digest": "sha256:26c5ad9fa67063e0c7066c0e7b7dddf1b18059184a1abc9fc8e1ba25a7679f48",
          "image": "registry.connect.redhat.com/ibm-edge/prometheus-scheduler@sha256:26c5ad9fa67063e0c7066c0e7b7dddf1b18059184a1abc9fc8e1ba25a7679f48",
          "name": "prometheus_scheduler"
        },
        {
          "digest": "sha256:ab4584bba734856f40a9a46889408f79ab6d19a7a737038e66504eed88c7a0a1",
          "image": "registry.connect.redhat.com/ibm-edge/pgo-client@sha256:ab4584bba734856f40a9a46889408f79ab6d19a7a737038e66504eed88c7a0a1",
          "name": "pgo_client"
        },
        {
          "digest": "sha256:e40afed2bfcfc7f1d81dcd8ed3f2ad95152d08dd16310fc4043300d192b1c980",
          "image": "registry.redhat.io/openshift4/ose-grafana@sha256:e40afed2bfcfc7f1d81dcd8ed3f2ad95152d08dd16310fc4043300d192b1c980",
          "name": "grafana"
        },
        {
          "digest": "sha256:98addfbaa182d18bcce234925d6bbf79203432f601c18ae74dca5eadb02a53ad",
          "image": "registry.redhat.io/openshift3/ose-kube-state-metrics@sha256:98addfbaa182d18bcce234925d6bbf79203432f601c18ae74dca5eadb02a53ad",
          "name": "ksm"
        },
        {
          "digest": "sha256:f23231c5ac515680b8eba1abe458218378c3dcb5f4d54244b65d3559f5cb6d1a",
          "image": "registry.connect.redhat.com/ibm-edge/growth-stack-base@sha256:f23231c5ac515680b8eba1abe458218378c3dcb5f4d54244b65d3559f5cb6d1a",
          "name": "init_container"
        },
        {
          "digest": "sha256:1f89aefa39dbc96a47ae451d64f200a0013637cdbe048d1b9ef56c99976bb8c0",
          "image": "registry.access.redhat.com/ubi8/ubi@sha256:1f89aefa39dbc96a47ae451d64f200a0013637cdbe048d1b9ef56c99976bb8c0",
          "name": "ubi"
        },
        {
          "digest": "sha256:a6f6188b2fc30d351f69004c043fa38cafadee23bca356f921a273cce103d121",
          "image": "registry.connect.redhat.com/ibm-edge/airgap-download-ui@sha256:a6f6188b2fc30d351f69004c043fa38cafadee23bca356f921a273cce103d121",
          "name": "download_ui"
        },
        {
          "digest": "sha256:6b93afc6bc4a624032714cf08389cb82be60deb579ea0333efab5e365935f236",
          "image": "registry.connect.redhat.com/ibm-edge/bas-operator-dashboard@sha256:6b93afc6bc4a624032714cf08389cb82be60deb579ea0333efab5e365935f236",
          "name": "dashboard_ui"
        },
        {
          "digest": "sha256:9a5ee95f8e99a63a4ad0e8b01683ac03c75337bbbe3d504d199a97f9921eb0c1",
          "image": "registry.redhat.io/openshift4/ose-oauth-proxy@sha256:9a5ee95f8e99a63a4ad0e8b01683ac03c75337bbbe3d504d199a97f9921eb0c1",
          "name": "oauth_proxy"
        },
        {
          "digest": "sha256:479dcc101de2e98612e433736a503aa849a573418036a308c81373327527372c",
          "image": "registry.connect.redhat.com/crunchydata/pgo-deployer@sha256:479dcc101de2e98612e433736a503aa849a573418036a308c81373327527372c",
          "name": "pgo_deployer"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.7",
      "version": "1.1.6",
      "version_original": "1.1.6"
    },
    {
      "_id": "62bd86b509a31e274258400a",
      "alm_examples": [
        {
          "api_version": "infrastructure.asset.orchestrator.com/v1",
          "kind": "Generatekey",
          "metadata": {
            "name": "modelbuilder-token"
          },
          "spec": {}
        },
        {
          "api_version": "infrastructure.asset.orchestrator.com/v1",
          "kind": "Mb-broker-service",
          "metadata": {
            "name": "mb-broker-service-sample"
          },
          "spec": {
            "cloud_type": "IBM",
            "env_type": "prod",
            "external_db": {
              "enabled": false
            },
            "postgres": {
              "backup_frequency": "@daily",
              "backup_type": "incremental",
              "enable_pg_tls": true,
              "storage_class": "",
              "storage_size": "20G"
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/ibm/iao-bundle@sha256:31fd95c5232105d3354515102f099b9b662a8659f08da5d67d4e147d4f934fc7",
      "bundle_path_digest": "sha256:31fd95c5232105d3354515102f099b9b662a8659f08da5d67d4e147d4f934fc7",
      "capabilities": [
        "Seamless Upgrades"
      ],
      "channel_name": "stable",
      "creation_date": "2022-06-30T11:19:17.205000+00:00",
      "csv_description": "Infrastructure Asset Orchestrator provides users access to cloud resources and services. Leverages the dynamic nature of the cloud platforms to provide significant cost savings and efficiencies. Using the Infrastructure Asset Orchestrator user can provision, configure, utilize and deprovision variety of services on the IBM Cloud Catalog.",
      "csv_display_name": "Infrastructure Asset Orchestrator",
      "csv_metadata_description": "",
      "csv_name": "infrastructure-asset-orchestrator.v1.0.2",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-11-23T07:11:21.018000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.11",
      "organization": "certified-operators",
      "package": "infrastructure-asset-orchestrator-certified",
      "provided_apis": [
        {
          "group": "infrastructure.asset.orchestrator.com",
          "kind": "Generatekey",
          "version": "v1"
        },
        {
          "group": "infrastructure.asset.orchestrator.com",
          "kind": "Mb-broker-service",
          "version": "v1"
        }
      ],
      "provider": "IBM Edge",
      "related_images": [
        {
          "digest": "sha256:5cc395f22d86641e175339deb134af6c74de92b81470a65a5c736c8005d645fd",
          "image": "registry.connect.redhat.com/ibm/iao-operator@sha256:5cc395f22d86641e175339deb134af6c74de92b81470a65a5c736c8005d645fd",
          "name": "iao-operator-5cc395f22d86641e175339deb134af6c74de92b81470a65a5c736c8005d645fd-annotation"
        },
        {
          "digest": "sha256:5cc395f22d86641e175339deb134af6c74de92b81470a65a5c736c8005d645fd",
          "image": "registry.connect.redhat.com/ibm/iao-operator@sha256:5cc395f22d86641e175339deb134af6c74de92b81470a65a5c736c8005d645fd",
          "name": "infrastructure-asset-orchestrator"
        },
        {
          "digest": "sha256:609c19d1718947ee814b829334c37e1d720acf5fb642b0712b8ab1b14d57f53f",
          "image": "registry.redhat.io/openshift4/ose-kube-rbac-proxy@sha256:609c19d1718947ee814b829334c37e1d720acf5fb642b0712b8ab1b14d57f53f",
          "name": "kube-rbac-proxy"
        },
        {
          "digest": "sha256:145928929fd0c2b7d9ed5fb374d7cc0e6b48269847e810c5472580a1538ad44c",
          "image": "registry.connect.redhat.com/ibm/iao-orchestrator-service@sha256:145928929fd0c2b7d9ed5fb374d7cc0e6b48269847e810c5472580a1538ad44c",
          "name": "orchestrator"
        },
        {
          "digest": "sha256:6531b4196c51246958027453c2d9c7dbcdb4a1f55c826d1da238d256cdbc8a10",
          "image": "registry.connect.redhat.com/ibm/iao-mb-broker-service@sha256:6531b4196c51246958027453c2d9c7dbcdb4a1f55c826d1da238d256cdbc8a10",
          "name": "mb_broker"
        },
        {
          "digest": "sha256:0ec55c7f25c24f786978e045e8a67facb965a08dd519d06c82de05dd15e0f4a1",
          "image": "registry.connect.redhat.com/ibm/iao-util@sha256:0ec55c7f25c24f786978e045e8a67facb965a08dd519d06c82de05dd15e0f4a1",
          "name": "mb_util"
        }
      ],
      "replaces": null,
      "skip_range": "<1.0.2",
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.11",
      "version": "1.0.2",
      "version_original": "1.0.2"
    },
    {
      "_id": "62bd86f9655931047b68a5be",
      "alm_examples": [
        {
          "api_version": "infrastructure.asset.orchestrator.com/v1",
          "kind": "Generatekey",
          "metadata": {
            "name": "modelbuilder-token"
          },
          "spec": {}
        },
        {
          "api_version": "infrastructure.asset.orchestrator.com/v1",
          "kind": "Mb-broker-service",
          "metadata": {
            "name": "mb-broker-service-sample"
          },
          "spec": {
            "cloud_type": "IBM",
            "env_type": "prod",
            "external_db": {
              "enabled": false
            },
            "postgres": {
              "backup_frequency": "@daily",
              "backup_type": "incremental",
              "enable_pg_tls": true,
              "storage_class": "",
              "storage_size": "20G"
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/ibm/iao-bundle@sha256:31fd95c5232105d3354515102f099b9b662a8659f08da5d67d4e147d4f934fc7",
      "bundle_path_digest": "sha256:31fd95c5232105d3354515102f099b9b662a8659f08da5d67d4e147d4f934fc7",
      "capabilities": [
        "Seamless Upgrades"
      ],
      "channel_name": "stable",
      "creation_date": "2022-06-30T11:20:25.741000+00:00",
      "csv_description": "Infrastructure Asset Orchestrator provides users access to cloud resources and services. Leverages the dynamic nature of the cloud platforms to provide significant cost savings and efficiencies. Using the Infrastructure Asset Orchestrator user can provision, configure, utilize and deprovision variety of services on the IBM Cloud Catalog.",
      "csv_display_name": "Infrastructure Asset Orchestrator",
      "csv_metadata_description": "",
      "csv_name": "infrastructure-asset-orchestrator.v1.0.2",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-11-23T08:33:02.892000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.10",
      "organization": "certified-operators",
      "package": "infrastructure-asset-orchestrator-certified",
      "provided_apis": [
        {
          "group": "infrastructure.asset.orchestrator.com",
          "kind": "Generatekey",
          "plural": "generatekeys",
          "version": "v1"
        },
        {
          "group": "infrastructure.asset.orchestrator.com",
          "kind": "Mb-broker-service",
          "plural": "mb-broker-services",
          "version": "v1"
        }
      ],
      "provider": "IBM Edge",
      "related_images": [
        {
          "digest": "sha256:5cc395f22d86641e175339deb134af6c74de92b81470a65a5c736c8005d645fd",
          "image": "registry.connect.redhat.com/ibm/iao-operator@sha256:5cc395f22d86641e175339deb134af6c74de92b81470a65a5c736c8005d645fd",
          "name": "iao-operator-5cc395f22d86641e175339deb134af6c74de92b81470a65a5c736c8005d645fd-annotation"
        },
        {
          "digest": "sha256:5cc395f22d86641e175339deb134af6c74de92b81470a65a5c736c8005d645fd",
          "image": "registry.connect.redhat.com/ibm/iao-operator@sha256:5cc395f22d86641e175339deb134af6c74de92b81470a65a5c736c8005d645fd",
          "name": "infrastructure-asset-orchestrator"
        },
        {
          "digest": "sha256:609c19d1718947ee814b829334c37e1d720acf5fb642b0712b8ab1b14d57f53f",
          "image": "registry.redhat.io/openshift4/ose-kube-rbac-proxy@sha256:609c19d1718947ee814b829334c37e1d720acf5fb642b0712b8ab1b14d57f53f",
          "name": "kube-rbac-proxy"
        },
        {
          "digest": "sha256:145928929fd0c2b7d9ed5fb374d7cc0e6b48269847e810c5472580a1538ad44c",
          "image": "registry.connect.redhat.com/ibm/iao-orchestrator-service@sha256:145928929fd0c2b7d9ed5fb374d7cc0e6b48269847e810c5472580a1538ad44c",
          "name": "orchestrator"
        },
        {
          "digest": "sha256:6531b4196c51246958027453c2d9c7dbcdb4a1f55c826d1da238d256cdbc8a10",
          "image": "registry.connect.redhat.com/ibm/iao-mb-broker-service@sha256:6531b4196c51246958027453c2d9c7dbcdb4a1f55c826d1da238d256cdbc8a10",
          "name": "mb_broker"
        },
        {
          "digest": "sha256:0ec55c7f25c24f786978e045e8a67facb965a08dd519d06c82de05dd15e0f4a1",
          "image": "registry.connect.redhat.com/ibm/iao-util@sha256:0ec55c7f25c24f786978e045e8a67facb965a08dd519d06c82de05dd15e0f4a1",
          "name": "mb_util"
        }
      ],
      "replaces": null,
      "skip_range": "<1.0.2",
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.10",
      "version": "1.0.2",
      "version_original": "1.0.2"
    }
  ]
}
