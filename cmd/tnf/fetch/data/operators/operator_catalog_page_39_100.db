{
  "data": [
    {
      "_id": "637ce0185f23340823b25b3b",
      "alm_examples": [
        {
          "api_version": "ca.broadcom.com1/v1alpha1",
          "kind": "UniversalMonitoringAgentsTeam",
          "metadata": {
            "name": "uma-monitor-team"
          },
          "spec": {
            "adminNamespaceName": "broadcom-aiops",
            "agentManager": {
              "credential": "",
              "tenantID": "",
              "url": "localhost:5001"
            },
            "clusterName": "DevelopmentCluster",
            "globalDeployment": {
              "nodeAffinity": {},
              "nodeSelector": {},
              "tolerations": [
                {
                  "effect": "NoSchedule",
                  "key": "node-role.kubernetes.io/master",
                  "operator": "",
                  "value": ""
                }
              ]
            },
            "monitor": {
              "application": {
                "jmx": {
                  "enabled": true
                },
                "opentracing": {
                  "correlation": {
                    "enabled": false
                  },
                  "enabled": true,
                  "grpc": {
                    "hostport": ""
                  }
                },
                "zipkinTracing": {
                  "enabled": true,
                  "zipkinServer": {}
                }
              },
              "clusterPerformance": {
                "dataReporter": {
                  "limits": {
                    "cpu": "2",
                    "memory": "2048Mi"
                  },
                  "requests": {
                    "cpu": "200m",
                    "memory": "800Mi"
                  },
                  "vertex": {
                    "refreshInterval": 360,
                    "ttl": 720
                  }
                }
              },
              "container": {
                "dockerstats": {
                  "enabled": true
                }
              },
              "events": {
                "enabled": false
              },
              "kafka": {
                "consumergroups": {
                  "filter": "",
                  "metrics": true
                },
                "debug": false,
                "enabled": false,
                "jmx": {
                  "MBeans": {
                    "exclude": "",
                    "include": ""
                  },
                  "modules": "kafka,jvm,memory"
                },
                "limits": {
                  "cpu": "2",
                  "memory": "1024Mi"
                },
                "requests": {
                  "cpu": "200m",
                  "memory": "300Mi"
                },
                "ssl": {
                  "client": {
                    "enabled": false,
                    "endpointIdentificationEnabled": false,
                    "keystoreKey": {
                      "value": "",
                      "valueFrom": {
                        "secretKeyRef": {
                          "key": "",
                          "name": ""
                        }
                      }
                    },
                    "keystoreKeyPassword": {
                      "value": ""
                    },
                    "keystorePassword": {
                      "value": ""
                    },
                    "truststoreKey": {
                      "value": "",
                      "valueFrom": {
                        "secretKeyRef": {
                          "key": "",
                          "name": ""
                        }
                      }
                    },
                    "truststorePassword": {
                      "value": ""
                    }
                  },
                  "jmx": {
                    "enabled": false,
                    "keystoreKey": {
                      "value": "",
                      "valueFrom": {
                        "secretKeyRef": {
                          "key": "",
                          "name": ""
                        }
                      }
                    },
                    "keystorePassword": {
                      "value": ""
                    },
                    "truststoreKey": {
                      "value": "",
                      "valueFrom": {
                        "secretKeyRef": {
                          "key": "",
                          "name": ""
                        }
                      }
                    },
                    "truststorePassword": {
                      "value": ""
                    }
                  }
                }
              },
              "node": {
                "enabled": true,
                "limits": {
                  "cpu": "2",
                  "memory": "1024Mi"
                },
                "requests": {
                  "cpu": "200m",
                  "memory": "300Mi"
                }
              }
            },
            "namespaces": "",
            "role": "team",
            "type": "Openshift"
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/ca/uma-team-operator-bundle@sha256:7fd74845057b1e1b15239afcac12e02b77d33be96d50565786aabfbcbf260589",
      "bundle_path_digest": "sha256:7fd74845057b1e1b15239afcac12e02b77d33be96d50565786aabfbcbf260589",
      "capabilities": [
        "Seamless Upgrades"
      ],
      "channel_name": "alpha",
      "creation_date": "2022-11-22T14:43:36.257000+00:00",
      "csv_description": "The **DX APM Universal Monitoring Agent Team For Multi-tenancy** installed by Application Administrator. Each application team that uses a DX APM tenant installs the UMA Team capability, specifying the namespaces the team owns or wants to monitor. UMA Team acts as a client, using a REST API to pull the namespace performance data from the UMA Admin deployment(DX APM Universal Monitoring Agent Team For Multi-tenancy). The UMA Team capability reports the application, pod, and node performance metrics only for the specified namespaces. This capability limits your application teams to seeing performance metrics only for their applications running in different environments on the tenant. Example environments include development, verification, pre-production, and production.\n\n**Prerequisite:**\n   1. The **DX APM Universal Monitoring Agent Team For Multi-tenancy** operator needs to be installed by the Cluster Administrator before UMA Team is installed.\n   2. UMA Team acts as a client, using a REST API to pull the namespace performance data from the UMA Admin deployment. So any network policy that restricts the communication to the Services running in the UMA Admin namespace, needs to be updated to allow the traffic from UMA team.\n\nFor more information: https://techdocs.broadcom.com/us/en/ca-enterprise-software/it-operations-management/dx-apm-agents/SaaS/Universal-Monitoring-Agent/Install-the-Universal-Monitoring-Agent/Deploy-UMA-in-a-Multi-tenant-Environment.html\n\n**Usage**:\n```\n apiVersion: ca.broadcom.com1/v1alpha1\n kind: UniversalMonitoringAgentsTeam\n metadata:\n   name: uma-monitor-team\n spec:\n   agentManager:\n     url: localhost:5001\n     credential: \"\"\n     tenantID: \"\"\n\n   # Name of the cluster.\n   clusterName: DevelopmentCluster\n   # Cluster type, either Kubernetes or OpenShift.\n   type: Kubernetes\n   role: team\n   namespaces: \"\"\n   adminNamespaceName: broadcom-aiops\n\n   monitor:\n     application:\n       jmx:\n         enabled: true\n       opentracing:\n         enabled: true\n         grpc:\n           hostport: \"\"\n         correlation:\n           enabled: false\n       zipkinTracing:\n         enabled: true\n         zipkinServer:\n           hostport:\n     events:\n       enabled: false\n     container:\n       dockerstats:\n         enabled: true\n     clusterPerformance:\n       dataReporter:\n         vertex:\n           ttl: 720\n           refreshInterval: 360\n         # The below properties define the resources for 'broadcom-monitor' deployment that does cluster and container monitoring.\n         limits:\n           cpu: \"2\"\n           # Assign memory resources in 'Mi'(mebibytes) units only.\n           memory: \"2048Mi\"\n         requests:\n           cpu: \"200m\"\n           # Assign memory resources in 'Mi'(mebibytes) units only.\n           memory: \"800Mi\"\n     kafka:\n       enabled: false\n       consumergroups:\n         metrics: true\n         filter: \"\"\n       jmx:\n         modules: \"kafka,jvm,memory\"\n         MBeans:\n           # A semi-colon separated list of managed bean object names. JMX metrics will be automatically reported for matching beans.\n           include: \"\"\n           # A semi-colon separated list of managed bean object names. Matching beans will be ignored for Kafka JMX metrics reporting. This property overrides \"include\" configuration.\n           exclude: \"\"\n       debug: false\n       ssl:\n         client:\n           enabled: false\n           truststoreKey:\n             value: \"\"\n             valueFrom:\n               secretKeyRef:\n                 name: \"\"\n                 key: \"\"\n           truststorePassword:\n             value: \"\"\n           keystoreKey:\n             value: \"\"\n             valueFrom:\n               secretKeyRef:\n                 name: \"\"\n                 key: \"\"\n           keystorePassword:\n             value: \"\"\n           keystoreKeyPassword:\n             value: \"\"\n           endpointIdentificationEnabled: false\n         jmx:\n           enabled: false\n           truststoreKey:\n             value: \"\"\n             valueFrom:\n               secretKeyRef:\n                 name: \"\"\n                 key: \"\"\n           truststorePassword:\n             value: \"\"\n           keystoreKey:\n             value: \"\"\n             valueFrom:\n               secretKeyRef:\n                 name: \"\"\n                 key: \"\"\n           keystorePassword:\n             value: \"\"\n       # The below properties define the resources for 'apm-kafka-monitor' deployment\n       limits:\n         cpu: \"2\"\n         # Assign memory resources in 'Mi'(mebibytes) units only.\n         memory: \"1024Mi\"\n       requests:\n         cpu: \"200m\"\n         # Assign memory resources in 'Mi'(mebibytes) units only.\n         memory: \"300Mi\"\n     node:\n       enabled: true\n       # The below properties define the resources for 'broadcom-monitor-host' deployment that does host monitoring.\n       limits:\n         cpu: \"2\"\n         # Assign memory resources in 'Mi'(mebibytes) units only.\n         memory: \"1024Mi\"\n       requests:\n         cpu: \"200m\"\n         # Assign memory resources in 'Mi'(mebibytes) units only.\n         memory: \"300Mi\"\n\n   globalDeployment:\n     # Node affinity rules for UMA Pods\n     # Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity\n     nodeAffinity: {}\n\n     # Node labels for UMA Pods\n     # Ref: https://kubernetes.io/docs/user-guide/node-selection/\n     nodeSelector: {}\n\n     # Additional tolerations for UMA pods\n     # https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/\n     # Default toleration used by UMA\n     tolerations:\n       - effect: \"NoSchedule\"\n         key: \"node-role.kubernetes.io/master\"\n         operator: \"\"\n         value: \"\"\n\n```\n",
      "csv_display_name": "DX APM Universal Monitoring Agent Team For Multi-tenancy",
      "csv_metadata_description": "DX APM Universal Monitoring Agent Team For Multi-tenancy acts as single deployment that automatically discovers and monitors cloud and container infrastructures and containerized application processes given namespaces.",
      "csv_name": "uma-team-operator.v2022.11.2-11",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": true,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": false,
      "last_update_date": "2022-11-23T07:07:42.151000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.11",
      "organization": "certified-operators",
      "package": "uma-team-operator",
      "provided_apis": [
        {
          "group": "ca.broadcom.com1",
          "kind": "UniversalMonitoringAgentsTeam",
          "version": "v1alpha1"
        }
      ],
      "provider": "Broadcom, Inc.",
      "related_images": [
        {
          "digest": "sha256:152985e9cd88b827000dd95e7dae47d6bbdd583d13b6f4dab21ef24e585da706",
          "image": "registry.connect.redhat.com/ca/uma-operator@sha256:152985e9cd88b827000dd95e7dae47d6bbdd583d13b6f4dab21ef24e585da706",
          "name": "uma-operator-152985e9cd88b827000dd95e7dae47d6bbdd583d13b6f4dab21ef24e585da706-annotation"
        },
        {
          "digest": "sha256:152985e9cd88b827000dd95e7dae47d6bbdd583d13b6f4dab21ef24e585da706",
          "image": "registry.connect.redhat.com/ca/uma-operator@sha256:152985e9cd88b827000dd95e7dae47d6bbdd583d13b6f4dab21ef24e585da706",
          "name": "uma-operator"
        },
        {
          "digest": "sha256:6d6e7d9f0c662c0b24b7a3d0ac1d8526305ad17144c8e23b462965bce8e4d45e",
          "image": "registry.connect.redhat.com/ca/universalmonitoragent@sha256:6d6e7d9f0c662c0b24b7a3d0ac1d8526305ad17144c8e23b462965bce8e4d45e",
          "name": "uma"
        }
      ],
      "replaces": null,
      "skip_range": ">=2022.1.0 <2022.11.2",
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.11",
      "version": "2022.11.2-11",
      "version_original": "2022.11.2-11"
    },
    {
      "_id": "637ce01a788768d1d6f2357a",
      "alm_examples": [
        {
          "api_version": "ca.broadcom.com1/v1alpha1",
          "kind": "UniversalMonitoringAgentsTeam",
          "metadata": {
            "name": "uma-monitor-team"
          },
          "spec": {
            "adminNamespaceName": "broadcom-aiops",
            "agentManager": {
              "credential": "",
              "tenantID": "",
              "url": "localhost:5001"
            },
            "clusterName": "DevelopmentCluster",
            "globalDeployment": {
              "nodeAffinity": {},
              "nodeSelector": {},
              "tolerations": [
                {
                  "effect": "NoSchedule",
                  "key": "node-role.kubernetes.io/master",
                  "operator": "",
                  "value": ""
                }
              ]
            },
            "monitor": {
              "application": {
                "jmx": {
                  "enabled": true
                },
                "opentracing": {
                  "correlation": {
                    "enabled": false
                  },
                  "enabled": true,
                  "grpc": {
                    "hostport": ""
                  }
                },
                "zipkinTracing": {
                  "enabled": true,
                  "zipkinServer": {}
                }
              },
              "clusterPerformance": {
                "dataReporter": {
                  "limits": {
                    "cpu": "2",
                    "memory": "2048Mi"
                  },
                  "requests": {
                    "cpu": "200m",
                    "memory": "800Mi"
                  },
                  "vertex": {
                    "refreshInterval": 360,
                    "ttl": 720
                  }
                }
              },
              "container": {
                "dockerstats": {
                  "enabled": true
                }
              },
              "events": {
                "enabled": false
              },
              "kafka": {
                "consumergroups": {
                  "filter": "",
                  "metrics": true
                },
                "debug": false,
                "enabled": false,
                "jmx": {
                  "MBeans": {
                    "exclude": "",
                    "include": ""
                  },
                  "modules": "kafka,jvm,memory"
                },
                "limits": {
                  "cpu": "2",
                  "memory": "1024Mi"
                },
                "requests": {
                  "cpu": "200m",
                  "memory": "300Mi"
                },
                "ssl": {
                  "client": {
                    "enabled": false,
                    "endpointIdentificationEnabled": false,
                    "keystoreKey": {
                      "value": "",
                      "valueFrom": {
                        "secretKeyRef": {
                          "key": "",
                          "name": ""
                        }
                      }
                    },
                    "keystoreKeyPassword": {
                      "value": ""
                    },
                    "keystorePassword": {
                      "value": ""
                    },
                    "truststoreKey": {
                      "value": "",
                      "valueFrom": {
                        "secretKeyRef": {
                          "key": "",
                          "name": ""
                        }
                      }
                    },
                    "truststorePassword": {
                      "value": ""
                    }
                  },
                  "jmx": {
                    "enabled": false,
                    "keystoreKey": {
                      "value": "",
                      "valueFrom": {
                        "secretKeyRef": {
                          "key": "",
                          "name": ""
                        }
                      }
                    },
                    "keystorePassword": {
                      "value": ""
                    },
                    "truststoreKey": {
                      "value": "",
                      "valueFrom": {
                        "secretKeyRef": {
                          "key": "",
                          "name": ""
                        }
                      }
                    },
                    "truststorePassword": {
                      "value": ""
                    }
                  }
                }
              },
              "node": {
                "enabled": true,
                "limits": {
                  "cpu": "2",
                  "memory": "1024Mi"
                },
                "requests": {
                  "cpu": "200m",
                  "memory": "300Mi"
                }
              }
            },
            "namespaces": "",
            "role": "team",
            "type": "Openshift"
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/ca/uma-team-operator-bundle@sha256:7fd74845057b1e1b15239afcac12e02b77d33be96d50565786aabfbcbf260589",
      "bundle_path_digest": "sha256:7fd74845057b1e1b15239afcac12e02b77d33be96d50565786aabfbcbf260589",
      "capabilities": [
        "Seamless Upgrades"
      ],
      "channel_name": "beta",
      "creation_date": "2022-11-22T14:43:38.101000+00:00",
      "csv_description": "The **DX APM Universal Monitoring Agent Team For Multi-tenancy** installed by Application Administrator. Each application team that uses a DX APM tenant installs the UMA Team capability, specifying the namespaces the team owns or wants to monitor. UMA Team acts as a client, using a REST API to pull the namespace performance data from the UMA Admin deployment(DX APM Universal Monitoring Agent Team For Multi-tenancy). The UMA Team capability reports the application, pod, and node performance metrics only for the specified namespaces. This capability limits your application teams to seeing performance metrics only for their applications running in different environments on the tenant. Example environments include development, verification, pre-production, and production.\n\n**Prerequisite:**\n   1. The **DX APM Universal Monitoring Agent Team For Multi-tenancy** operator needs to be installed by the Cluster Administrator before UMA Team is installed.\n   2. UMA Team acts as a client, using a REST API to pull the namespace performance data from the UMA Admin deployment. So any network policy that restricts the communication to the Services running in the UMA Admin namespace, needs to be updated to allow the traffic from UMA team.\n\nFor more information: https://techdocs.broadcom.com/us/en/ca-enterprise-software/it-operations-management/dx-apm-agents/SaaS/Universal-Monitoring-Agent/Install-the-Universal-Monitoring-Agent/Deploy-UMA-in-a-Multi-tenant-Environment.html\n\n**Usage**:\n```\n apiVersion: ca.broadcom.com1/v1alpha1\n kind: UniversalMonitoringAgentsTeam\n metadata:\n   name: uma-monitor-team\n spec:\n   agentManager:\n     url: localhost:5001\n     credential: \"\"\n     tenantID: \"\"\n\n   # Name of the cluster.\n   clusterName: DevelopmentCluster\n   # Cluster type, either Kubernetes or OpenShift.\n   type: Kubernetes\n   role: team\n   namespaces: \"\"\n   adminNamespaceName: broadcom-aiops\n\n   monitor:\n     application:\n       jmx:\n         enabled: true\n       opentracing:\n         enabled: true\n         grpc:\n           hostport: \"\"\n         correlation:\n           enabled: false\n       zipkinTracing:\n         enabled: true\n         zipkinServer:\n           hostport:\n     events:\n       enabled: false\n     container:\n       dockerstats:\n         enabled: true\n     clusterPerformance:\n       dataReporter:\n         vertex:\n           ttl: 720\n           refreshInterval: 360\n         # The below properties define the resources for 'broadcom-monitor' deployment that does cluster and container monitoring.\n         limits:\n           cpu: \"2\"\n           # Assign memory resources in 'Mi'(mebibytes) units only.\n           memory: \"2048Mi\"\n         requests:\n           cpu: \"200m\"\n           # Assign memory resources in 'Mi'(mebibytes) units only.\n           memory: \"800Mi\"\n     kafka:\n       enabled: false\n       consumergroups:\n         metrics: true\n         filter: \"\"\n       jmx:\n         modules: \"kafka,jvm,memory\"\n         MBeans:\n           # A semi-colon separated list of managed bean object names. JMX metrics will be automatically reported for matching beans.\n           include: \"\"\n           # A semi-colon separated list of managed bean object names. Matching beans will be ignored for Kafka JMX metrics reporting. This property overrides \"include\" configuration.\n           exclude: \"\"\n       debug: false\n       ssl:\n         client:\n           enabled: false\n           truststoreKey:\n             value: \"\"\n             valueFrom:\n               secretKeyRef:\n                 name: \"\"\n                 key: \"\"\n           truststorePassword:\n             value: \"\"\n           keystoreKey:\n             value: \"\"\n             valueFrom:\n               secretKeyRef:\n                 name: \"\"\n                 key: \"\"\n           keystorePassword:\n             value: \"\"\n           keystoreKeyPassword:\n             value: \"\"\n           endpointIdentificationEnabled: false\n         jmx:\n           enabled: false\n           truststoreKey:\n             value: \"\"\n             valueFrom:\n               secretKeyRef:\n                 name: \"\"\n                 key: \"\"\n           truststorePassword:\n             value: \"\"\n           keystoreKey:\n             value: \"\"\n             valueFrom:\n               secretKeyRef:\n                 name: \"\"\n                 key: \"\"\n           keystorePassword:\n             value: \"\"\n       # The below properties define the resources for 'apm-kafka-monitor' deployment\n       limits:\n         cpu: \"2\"\n         # Assign memory resources in 'Mi'(mebibytes) units only.\n         memory: \"1024Mi\"\n       requests:\n         cpu: \"200m\"\n         # Assign memory resources in 'Mi'(mebibytes) units only.\n         memory: \"300Mi\"\n     node:\n       enabled: true\n       # The below properties define the resources for 'broadcom-monitor-host' deployment that does host monitoring.\n       limits:\n         cpu: \"2\"\n         # Assign memory resources in 'Mi'(mebibytes) units only.\n         memory: \"1024Mi\"\n       requests:\n         cpu: \"200m\"\n         # Assign memory resources in 'Mi'(mebibytes) units only.\n         memory: \"300Mi\"\n\n   globalDeployment:\n     # Node affinity rules for UMA Pods\n     # Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity\n     nodeAffinity: {}\n\n     # Node labels for UMA Pods\n     # Ref: https://kubernetes.io/docs/user-guide/node-selection/\n     nodeSelector: {}\n\n     # Additional tolerations for UMA pods\n     # https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/\n     # Default toleration used by UMA\n     tolerations:\n       - effect: \"NoSchedule\"\n         key: \"node-role.kubernetes.io/master\"\n         operator: \"\"\n         value: \"\"\n\n```\n",
      "csv_display_name": "DX APM Universal Monitoring Agent Team For Multi-tenancy",
      "csv_metadata_description": "DX APM Universal Monitoring Agent Team For Multi-tenancy acts as single deployment that automatically discovers and monitors cloud and container infrastructures and containerized application processes given namespaces.",
      "csv_name": "uma-team-operator.v2022.11.2-11",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": true,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": false,
      "last_update_date": "2022-11-23T07:07:52.204000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.11",
      "organization": "certified-operators",
      "package": "uma-team-operator",
      "provided_apis": [
        {
          "group": "ca.broadcom.com1",
          "kind": "UniversalMonitoringAgentsTeam",
          "version": "v1alpha1"
        }
      ],
      "provider": "Broadcom, Inc.",
      "related_images": [
        {
          "digest": "sha256:152985e9cd88b827000dd95e7dae47d6bbdd583d13b6f4dab21ef24e585da706",
          "image": "registry.connect.redhat.com/ca/uma-operator@sha256:152985e9cd88b827000dd95e7dae47d6bbdd583d13b6f4dab21ef24e585da706",
          "name": "uma-operator-152985e9cd88b827000dd95e7dae47d6bbdd583d13b6f4dab21ef24e585da706-annotation"
        },
        {
          "digest": "sha256:152985e9cd88b827000dd95e7dae47d6bbdd583d13b6f4dab21ef24e585da706",
          "image": "registry.connect.redhat.com/ca/uma-operator@sha256:152985e9cd88b827000dd95e7dae47d6bbdd583d13b6f4dab21ef24e585da706",
          "name": "uma-operator"
        },
        {
          "digest": "sha256:6d6e7d9f0c662c0b24b7a3d0ac1d8526305ad17144c8e23b462965bce8e4d45e",
          "image": "registry.connect.redhat.com/ca/universalmonitoragent@sha256:6d6e7d9f0c662c0b24b7a3d0ac1d8526305ad17144c8e23b462965bce8e4d45e",
          "name": "uma"
        }
      ],
      "replaces": null,
      "skip_range": ">=2022.1.0 <2022.11.2",
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.11",
      "version": "2022.11.2-11",
      "version_original": "2022.11.2-11"
    },
    {
      "_id": "637ce16240d971f5448a3f1a",
      "alm_examples": [
        {
          "api_version": "ca.broadcom.com1/v1alpha1",
          "kind": "UniversalMonitoringAgentsTeam",
          "metadata": {
            "name": "uma-monitor-team"
          },
          "spec": {
            "adminNamespaceName": "broadcom-aiops",
            "agentManager": {
              "credential": "",
              "tenantID": "",
              "url": "localhost:5001"
            },
            "clusterName": "DevelopmentCluster",
            "globalDeployment": {
              "nodeAffinity": {},
              "nodeSelector": {},
              "tolerations": [
                {
                  "effect": "NoSchedule",
                  "key": "node-role.kubernetes.io/master",
                  "operator": "",
                  "value": ""
                }
              ]
            },
            "monitor": {
              "application": {
                "jmx": {
                  "enabled": true
                },
                "opentracing": {
                  "correlation": {
                    "enabled": false
                  },
                  "enabled": true,
                  "grpc": {
                    "hostport": ""
                  }
                },
                "zipkinTracing": {
                  "enabled": true,
                  "zipkinServer": {}
                }
              },
              "clusterPerformance": {
                "dataReporter": {
                  "limits": {
                    "cpu": "2",
                    "memory": "2048Mi"
                  },
                  "requests": {
                    "cpu": "200m",
                    "memory": "800Mi"
                  },
                  "vertex": {
                    "refreshInterval": 360,
                    "ttl": 720
                  }
                }
              },
              "container": {
                "dockerstats": {
                  "enabled": true
                }
              },
              "events": {
                "enabled": false
              },
              "kafka": {
                "consumergroups": {
                  "filter": "",
                  "metrics": true
                },
                "debug": false,
                "enabled": false,
                "jmx": {
                  "MBeans": {
                    "exclude": "",
                    "include": ""
                  },
                  "modules": "kafka,jvm,memory"
                },
                "limits": {
                  "cpu": "2",
                  "memory": "1024Mi"
                },
                "requests": {
                  "cpu": "200m",
                  "memory": "300Mi"
                },
                "ssl": {
                  "client": {
                    "enabled": false,
                    "endpointIdentificationEnabled": false,
                    "keystoreKey": {
                      "value": "",
                      "valueFrom": {
                        "secretKeyRef": {
                          "key": "",
                          "name": ""
                        }
                      }
                    },
                    "keystoreKeyPassword": {
                      "value": ""
                    },
                    "keystorePassword": {
                      "value": ""
                    },
                    "truststoreKey": {
                      "value": "",
                      "valueFrom": {
                        "secretKeyRef": {
                          "key": "",
                          "name": ""
                        }
                      }
                    },
                    "truststorePassword": {
                      "value": ""
                    }
                  },
                  "jmx": {
                    "enabled": false,
                    "keystoreKey": {
                      "value": "",
                      "valueFrom": {
                        "secretKeyRef": {
                          "key": "",
                          "name": ""
                        }
                      }
                    },
                    "keystorePassword": {
                      "value": ""
                    },
                    "truststoreKey": {
                      "value": "",
                      "valueFrom": {
                        "secretKeyRef": {
                          "key": "",
                          "name": ""
                        }
                      }
                    },
                    "truststorePassword": {
                      "value": ""
                    }
                  }
                }
              },
              "node": {
                "enabled": true,
                "limits": {
                  "cpu": "2",
                  "memory": "1024Mi"
                },
                "requests": {
                  "cpu": "200m",
                  "memory": "300Mi"
                }
              }
            },
            "namespaces": "",
            "role": "team",
            "type": "Openshift"
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/ca/uma-team-operator-bundle@sha256:7fd74845057b1e1b15239afcac12e02b77d33be96d50565786aabfbcbf260589",
      "bundle_path_digest": "sha256:7fd74845057b1e1b15239afcac12e02b77d33be96d50565786aabfbcbf260589",
      "capabilities": [
        "Seamless Upgrades"
      ],
      "channel_name": "beta",
      "creation_date": "2022-11-22T14:49:06.141000+00:00",
      "csv_description": "The **DX APM Universal Monitoring Agent Team For Multi-tenancy** installed by Application Administrator. Each application team that uses a DX APM tenant installs the UMA Team capability, specifying the namespaces the team owns or wants to monitor. UMA Team acts as a client, using a REST API to pull the namespace performance data from the UMA Admin deployment(DX APM Universal Monitoring Agent Team For Multi-tenancy). The UMA Team capability reports the application, pod, and node performance metrics only for the specified namespaces. This capability limits your application teams to seeing performance metrics only for their applications running in different environments on the tenant. Example environments include development, verification, pre-production, and production.\n\n**Prerequisite:**\n   1. The **DX APM Universal Monitoring Agent Team For Multi-tenancy** operator needs to be installed by the Cluster Administrator before UMA Team is installed.\n   2. UMA Team acts as a client, using a REST API to pull the namespace performance data from the UMA Admin deployment. So any network policy that restricts the communication to the Services running in the UMA Admin namespace, needs to be updated to allow the traffic from UMA team.\n\nFor more information: https://techdocs.broadcom.com/us/en/ca-enterprise-software/it-operations-management/dx-apm-agents/SaaS/Universal-Monitoring-Agent/Install-the-Universal-Monitoring-Agent/Deploy-UMA-in-a-Multi-tenant-Environment.html\n\n**Usage**:\n```\n apiVersion: ca.broadcom.com1/v1alpha1\n kind: UniversalMonitoringAgentsTeam\n metadata:\n   name: uma-monitor-team\n spec:\n   agentManager:\n     url: localhost:5001\n     credential: \"\"\n     tenantID: \"\"\n\n   # Name of the cluster.\n   clusterName: DevelopmentCluster\n   # Cluster type, either Kubernetes or OpenShift.\n   type: Kubernetes\n   role: team\n   namespaces: \"\"\n   adminNamespaceName: broadcom-aiops\n\n   monitor:\n     application:\n       jmx:\n         enabled: true\n       opentracing:\n         enabled: true\n         grpc:\n           hostport: \"\"\n         correlation:\n           enabled: false\n       zipkinTracing:\n         enabled: true\n         zipkinServer:\n           hostport:\n     events:\n       enabled: false\n     container:\n       dockerstats:\n         enabled: true\n     clusterPerformance:\n       dataReporter:\n         vertex:\n           ttl: 720\n           refreshInterval: 360\n         # The below properties define the resources for 'broadcom-monitor' deployment that does cluster and container monitoring.\n         limits:\n           cpu: \"2\"\n           # Assign memory resources in 'Mi'(mebibytes) units only.\n           memory: \"2048Mi\"\n         requests:\n           cpu: \"200m\"\n           # Assign memory resources in 'Mi'(mebibytes) units only.\n           memory: \"800Mi\"\n     kafka:\n       enabled: false\n       consumergroups:\n         metrics: true\n         filter: \"\"\n       jmx:\n         modules: \"kafka,jvm,memory\"\n         MBeans:\n           # A semi-colon separated list of managed bean object names. JMX metrics will be automatically reported for matching beans.\n           include: \"\"\n           # A semi-colon separated list of managed bean object names. Matching beans will be ignored for Kafka JMX metrics reporting. This property overrides \"include\" configuration.\n           exclude: \"\"\n       debug: false\n       ssl:\n         client:\n           enabled: false\n           truststoreKey:\n             value: \"\"\n             valueFrom:\n               secretKeyRef:\n                 name: \"\"\n                 key: \"\"\n           truststorePassword:\n             value: \"\"\n           keystoreKey:\n             value: \"\"\n             valueFrom:\n               secretKeyRef:\n                 name: \"\"\n                 key: \"\"\n           keystorePassword:\n             value: \"\"\n           keystoreKeyPassword:\n             value: \"\"\n           endpointIdentificationEnabled: false\n         jmx:\n           enabled: false\n           truststoreKey:\n             value: \"\"\n             valueFrom:\n               secretKeyRef:\n                 name: \"\"\n                 key: \"\"\n           truststorePassword:\n             value: \"\"\n           keystoreKey:\n             value: \"\"\n             valueFrom:\n               secretKeyRef:\n                 name: \"\"\n                 key: \"\"\n           keystorePassword:\n             value: \"\"\n       # The below properties define the resources for 'apm-kafka-monitor' deployment\n       limits:\n         cpu: \"2\"\n         # Assign memory resources in 'Mi'(mebibytes) units only.\n         memory: \"1024Mi\"\n       requests:\n         cpu: \"200m\"\n         # Assign memory resources in 'Mi'(mebibytes) units only.\n         memory: \"300Mi\"\n     node:\n       enabled: true\n       # The below properties define the resources for 'broadcom-monitor-host' deployment that does host monitoring.\n       limits:\n         cpu: \"2\"\n         # Assign memory resources in 'Mi'(mebibytes) units only.\n         memory: \"1024Mi\"\n       requests:\n         cpu: \"200m\"\n         # Assign memory resources in 'Mi'(mebibytes) units only.\n         memory: \"300Mi\"\n\n   globalDeployment:\n     # Node affinity rules for UMA Pods\n     # Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity\n     nodeAffinity: {}\n\n     # Node labels for UMA Pods\n     # Ref: https://kubernetes.io/docs/user-guide/node-selection/\n     nodeSelector: {}\n\n     # Additional tolerations for UMA pods\n     # https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/\n     # Default toleration used by UMA\n     tolerations:\n       - effect: \"NoSchedule\"\n         key: \"node-role.kubernetes.io/master\"\n         operator: \"\"\n         value: \"\"\n\n```\n",
      "csv_display_name": "DX APM Universal Monitoring Agent Team For Multi-tenancy",
      "csv_metadata_description": "DX APM Universal Monitoring Agent Team For Multi-tenancy acts as single deployment that automatically discovers and monitors cloud and container infrastructures and containerized application processes given namespaces.",
      "csv_name": "uma-team-operator.v2022.11.2-11",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": true,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": false,
      "last_update_date": "2022-11-23T07:46:22.439000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.8",
      "organization": "certified-operators",
      "package": "uma-team-operator",
      "provided_apis": [
        {
          "group": "ca.broadcom.com1",
          "kind": "UniversalMonitoringAgentsTeam",
          "plural": "universalmonitoringagentsteams",
          "version": "v1alpha1"
        }
      ],
      "provider": "Broadcom, Inc.",
      "related_images": [
        {
          "digest": "sha256:152985e9cd88b827000dd95e7dae47d6bbdd583d13b6f4dab21ef24e585da706",
          "image": "registry.connect.redhat.com/ca/uma-operator@sha256:152985e9cd88b827000dd95e7dae47d6bbdd583d13b6f4dab21ef24e585da706",
          "name": "uma-operator-152985e9cd88b827000dd95e7dae47d6bbdd583d13b6f4dab21ef24e585da706-annotation"
        },
        {
          "digest": "sha256:152985e9cd88b827000dd95e7dae47d6bbdd583d13b6f4dab21ef24e585da706",
          "image": "registry.connect.redhat.com/ca/uma-operator@sha256:152985e9cd88b827000dd95e7dae47d6bbdd583d13b6f4dab21ef24e585da706",
          "name": "uma-operator"
        },
        {
          "digest": "sha256:6d6e7d9f0c662c0b24b7a3d0ac1d8526305ad17144c8e23b462965bce8e4d45e",
          "image": "registry.connect.redhat.com/ca/universalmonitoragent@sha256:6d6e7d9f0c662c0b24b7a3d0ac1d8526305ad17144c8e23b462965bce8e4d45e",
          "name": "uma"
        }
      ],
      "replaces": null,
      "skip_range": ">=2022.1.0 <2022.11.2",
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.8",
      "version": "2022.11.2-11",
      "version_original": "2022.11.2-11"
    },
    {
      "_id": "637ce16374e9f4cddfbef1a1",
      "alm_examples": [
        {
          "api_version": "ca.broadcom.com1/v1alpha1",
          "kind": "UniversalMonitoringAgentsTeam",
          "metadata": {
            "name": "uma-monitor-team"
          },
          "spec": {
            "adminNamespaceName": "broadcom-aiops",
            "agentManager": {
              "credential": "",
              "tenantID": "",
              "url": "localhost:5001"
            },
            "clusterName": "DevelopmentCluster",
            "globalDeployment": {
              "nodeAffinity": {},
              "nodeSelector": {},
              "tolerations": [
                {
                  "effect": "NoSchedule",
                  "key": "node-role.kubernetes.io/master",
                  "operator": "",
                  "value": ""
                }
              ]
            },
            "monitor": {
              "application": {
                "jmx": {
                  "enabled": true
                },
                "opentracing": {
                  "correlation": {
                    "enabled": false
                  },
                  "enabled": true,
                  "grpc": {
                    "hostport": ""
                  }
                },
                "zipkinTracing": {
                  "enabled": true,
                  "zipkinServer": {}
                }
              },
              "clusterPerformance": {
                "dataReporter": {
                  "limits": {
                    "cpu": "2",
                    "memory": "2048Mi"
                  },
                  "requests": {
                    "cpu": "200m",
                    "memory": "800Mi"
                  },
                  "vertex": {
                    "refreshInterval": 360,
                    "ttl": 720
                  }
                }
              },
              "container": {
                "dockerstats": {
                  "enabled": true
                }
              },
              "events": {
                "enabled": false
              },
              "kafka": {
                "consumergroups": {
                  "filter": "",
                  "metrics": true
                },
                "debug": false,
                "enabled": false,
                "jmx": {
                  "MBeans": {
                    "exclude": "",
                    "include": ""
                  },
                  "modules": "kafka,jvm,memory"
                },
                "limits": {
                  "cpu": "2",
                  "memory": "1024Mi"
                },
                "requests": {
                  "cpu": "200m",
                  "memory": "300Mi"
                },
                "ssl": {
                  "client": {
                    "enabled": false,
                    "endpointIdentificationEnabled": false,
                    "keystoreKey": {
                      "value": "",
                      "valueFrom": {
                        "secretKeyRef": {
                          "key": "",
                          "name": ""
                        }
                      }
                    },
                    "keystoreKeyPassword": {
                      "value": ""
                    },
                    "keystorePassword": {
                      "value": ""
                    },
                    "truststoreKey": {
                      "value": "",
                      "valueFrom": {
                        "secretKeyRef": {
                          "key": "",
                          "name": ""
                        }
                      }
                    },
                    "truststorePassword": {
                      "value": ""
                    }
                  },
                  "jmx": {
                    "enabled": false,
                    "keystoreKey": {
                      "value": "",
                      "valueFrom": {
                        "secretKeyRef": {
                          "key": "",
                          "name": ""
                        }
                      }
                    },
                    "keystorePassword": {
                      "value": ""
                    },
                    "truststoreKey": {
                      "value": "",
                      "valueFrom": {
                        "secretKeyRef": {
                          "key": "",
                          "name": ""
                        }
                      }
                    },
                    "truststorePassword": {
                      "value": ""
                    }
                  }
                }
              },
              "node": {
                "enabled": true,
                "limits": {
                  "cpu": "2",
                  "memory": "1024Mi"
                },
                "requests": {
                  "cpu": "200m",
                  "memory": "300Mi"
                }
              }
            },
            "namespaces": "",
            "role": "team",
            "type": "Openshift"
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/ca/uma-team-operator-bundle@sha256:7fd74845057b1e1b15239afcac12e02b77d33be96d50565786aabfbcbf260589",
      "bundle_path_digest": "sha256:7fd74845057b1e1b15239afcac12e02b77d33be96d50565786aabfbcbf260589",
      "capabilities": [
        "Seamless Upgrades"
      ],
      "channel_name": "stable",
      "creation_date": "2022-11-22T14:49:07.597000+00:00",
      "csv_description": "The **DX APM Universal Monitoring Agent Team For Multi-tenancy** installed by Application Administrator. Each application team that uses a DX APM tenant installs the UMA Team capability, specifying the namespaces the team owns or wants to monitor. UMA Team acts as a client, using a REST API to pull the namespace performance data from the UMA Admin deployment(DX APM Universal Monitoring Agent Team For Multi-tenancy). The UMA Team capability reports the application, pod, and node performance metrics only for the specified namespaces. This capability limits your application teams to seeing performance metrics only for their applications running in different environments on the tenant. Example environments include development, verification, pre-production, and production.\n\n**Prerequisite:**\n   1. The **DX APM Universal Monitoring Agent Team For Multi-tenancy** operator needs to be installed by the Cluster Administrator before UMA Team is installed.\n   2. UMA Team acts as a client, using a REST API to pull the namespace performance data from the UMA Admin deployment. So any network policy that restricts the communication to the Services running in the UMA Admin namespace, needs to be updated to allow the traffic from UMA team.\n\nFor more information: https://techdocs.broadcom.com/us/en/ca-enterprise-software/it-operations-management/dx-apm-agents/SaaS/Universal-Monitoring-Agent/Install-the-Universal-Monitoring-Agent/Deploy-UMA-in-a-Multi-tenant-Environment.html\n\n**Usage**:\n```\n apiVersion: ca.broadcom.com1/v1alpha1\n kind: UniversalMonitoringAgentsTeam\n metadata:\n   name: uma-monitor-team\n spec:\n   agentManager:\n     url: localhost:5001\n     credential: \"\"\n     tenantID: \"\"\n\n   # Name of the cluster.\n   clusterName: DevelopmentCluster\n   # Cluster type, either Kubernetes or OpenShift.\n   type: Kubernetes\n   role: team\n   namespaces: \"\"\n   adminNamespaceName: broadcom-aiops\n\n   monitor:\n     application:\n       jmx:\n         enabled: true\n       opentracing:\n         enabled: true\n         grpc:\n           hostport: \"\"\n         correlation:\n           enabled: false\n       zipkinTracing:\n         enabled: true\n         zipkinServer:\n           hostport:\n     events:\n       enabled: false\n     container:\n       dockerstats:\n         enabled: true\n     clusterPerformance:\n       dataReporter:\n         vertex:\n           ttl: 720\n           refreshInterval: 360\n         # The below properties define the resources for 'broadcom-monitor' deployment that does cluster and container monitoring.\n         limits:\n           cpu: \"2\"\n           # Assign memory resources in 'Mi'(mebibytes) units only.\n           memory: \"2048Mi\"\n         requests:\n           cpu: \"200m\"\n           # Assign memory resources in 'Mi'(mebibytes) units only.\n           memory: \"800Mi\"\n     kafka:\n       enabled: false\n       consumergroups:\n         metrics: true\n         filter: \"\"\n       jmx:\n         modules: \"kafka,jvm,memory\"\n         MBeans:\n           # A semi-colon separated list of managed bean object names. JMX metrics will be automatically reported for matching beans.\n           include: \"\"\n           # A semi-colon separated list of managed bean object names. Matching beans will be ignored for Kafka JMX metrics reporting. This property overrides \"include\" configuration.\n           exclude: \"\"\n       debug: false\n       ssl:\n         client:\n           enabled: false\n           truststoreKey:\n             value: \"\"\n             valueFrom:\n               secretKeyRef:\n                 name: \"\"\n                 key: \"\"\n           truststorePassword:\n             value: \"\"\n           keystoreKey:\n             value: \"\"\n             valueFrom:\n               secretKeyRef:\n                 name: \"\"\n                 key: \"\"\n           keystorePassword:\n             value: \"\"\n           keystoreKeyPassword:\n             value: \"\"\n           endpointIdentificationEnabled: false\n         jmx:\n           enabled: false\n           truststoreKey:\n             value: \"\"\n             valueFrom:\n               secretKeyRef:\n                 name: \"\"\n                 key: \"\"\n           truststorePassword:\n             value: \"\"\n           keystoreKey:\n             value: \"\"\n             valueFrom:\n               secretKeyRef:\n                 name: \"\"\n                 key: \"\"\n           keystorePassword:\n             value: \"\"\n       # The below properties define the resources for 'apm-kafka-monitor' deployment\n       limits:\n         cpu: \"2\"\n         # Assign memory resources in 'Mi'(mebibytes) units only.\n         memory: \"1024Mi\"\n       requests:\n         cpu: \"200m\"\n         # Assign memory resources in 'Mi'(mebibytes) units only.\n         memory: \"300Mi\"\n     node:\n       enabled: true\n       # The below properties define the resources for 'broadcom-monitor-host' deployment that does host monitoring.\n       limits:\n         cpu: \"2\"\n         # Assign memory resources in 'Mi'(mebibytes) units only.\n         memory: \"1024Mi\"\n       requests:\n         cpu: \"200m\"\n         # Assign memory resources in 'Mi'(mebibytes) units only.\n         memory: \"300Mi\"\n\n   globalDeployment:\n     # Node affinity rules for UMA Pods\n     # Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity\n     nodeAffinity: {}\n\n     # Node labels for UMA Pods\n     # Ref: https://kubernetes.io/docs/user-guide/node-selection/\n     nodeSelector: {}\n\n     # Additional tolerations for UMA pods\n     # https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/\n     # Default toleration used by UMA\n     tolerations:\n       - effect: \"NoSchedule\"\n         key: \"node-role.kubernetes.io/master\"\n         operator: \"\"\n         value: \"\"\n\n```\n",
      "csv_display_name": "DX APM Universal Monitoring Agent Team For Multi-tenancy",
      "csv_metadata_description": "DX APM Universal Monitoring Agent Team For Multi-tenancy acts as single deployment that automatically discovers and monitors cloud and container infrastructures and containerized application processes given namespaces.",
      "csv_name": "uma-team-operator.v2022.11.2-11",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": true,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-11-23T07:37:32.336000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.8",
      "organization": "certified-operators",
      "package": "uma-team-operator",
      "provided_apis": [
        {
          "group": "ca.broadcom.com1",
          "kind": "UniversalMonitoringAgentsTeam",
          "plural": "universalmonitoringagentsteams",
          "version": "v1alpha1"
        }
      ],
      "provider": "Broadcom, Inc.",
      "related_images": [
        {
          "digest": "sha256:152985e9cd88b827000dd95e7dae47d6bbdd583d13b6f4dab21ef24e585da706",
          "image": "registry.connect.redhat.com/ca/uma-operator@sha256:152985e9cd88b827000dd95e7dae47d6bbdd583d13b6f4dab21ef24e585da706",
          "name": "uma-operator-152985e9cd88b827000dd95e7dae47d6bbdd583d13b6f4dab21ef24e585da706-annotation"
        },
        {
          "digest": "sha256:152985e9cd88b827000dd95e7dae47d6bbdd583d13b6f4dab21ef24e585da706",
          "image": "registry.connect.redhat.com/ca/uma-operator@sha256:152985e9cd88b827000dd95e7dae47d6bbdd583d13b6f4dab21ef24e585da706",
          "name": "uma-operator"
        },
        {
          "digest": "sha256:6d6e7d9f0c662c0b24b7a3d0ac1d8526305ad17144c8e23b462965bce8e4d45e",
          "image": "registry.connect.redhat.com/ca/universalmonitoragent@sha256:6d6e7d9f0c662c0b24b7a3d0ac1d8526305ad17144c8e23b462965bce8e4d45e",
          "name": "uma"
        }
      ],
      "replaces": null,
      "skip_range": ">=2022.1.0 <2022.11.2",
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.8",
      "version": "2022.11.2-11",
      "version_original": "2022.11.2-11"
    },
    {
      "_id": "637ce164884420e0c62a24d0",
      "alm_examples": [
        {
          "api_version": "ca.broadcom.com1/v1alpha1",
          "kind": "UniversalMonitoringAgentsTeam",
          "metadata": {
            "name": "uma-monitor-team"
          },
          "spec": {
            "adminNamespaceName": "broadcom-aiops",
            "agentManager": {
              "credential": "",
              "tenantID": "",
              "url": "localhost:5001"
            },
            "clusterName": "DevelopmentCluster",
            "globalDeployment": {
              "nodeAffinity": {},
              "nodeSelector": {},
              "tolerations": [
                {
                  "effect": "NoSchedule",
                  "key": "node-role.kubernetes.io/master",
                  "operator": "",
                  "value": ""
                }
              ]
            },
            "monitor": {
              "application": {
                "jmx": {
                  "enabled": true
                },
                "opentracing": {
                  "correlation": {
                    "enabled": false
                  },
                  "enabled": true,
                  "grpc": {
                    "hostport": ""
                  }
                },
                "zipkinTracing": {
                  "enabled": true,
                  "zipkinServer": {}
                }
              },
              "clusterPerformance": {
                "dataReporter": {
                  "limits": {
                    "cpu": "2",
                    "memory": "2048Mi"
                  },
                  "requests": {
                    "cpu": "200m",
                    "memory": "800Mi"
                  },
                  "vertex": {
                    "refreshInterval": 360,
                    "ttl": 720
                  }
                }
              },
              "container": {
                "dockerstats": {
                  "enabled": true
                }
              },
              "events": {
                "enabled": false
              },
              "kafka": {
                "consumergroups": {
                  "filter": "",
                  "metrics": true
                },
                "debug": false,
                "enabled": false,
                "jmx": {
                  "MBeans": {
                    "exclude": "",
                    "include": ""
                  },
                  "modules": "kafka,jvm,memory"
                },
                "limits": {
                  "cpu": "2",
                  "memory": "1024Mi"
                },
                "requests": {
                  "cpu": "200m",
                  "memory": "300Mi"
                },
                "ssl": {
                  "client": {
                    "enabled": false,
                    "endpointIdentificationEnabled": false,
                    "keystoreKey": {
                      "value": "",
                      "valueFrom": {
                        "secretKeyRef": {
                          "key": "",
                          "name": ""
                        }
                      }
                    },
                    "keystoreKeyPassword": {
                      "value": ""
                    },
                    "keystorePassword": {
                      "value": ""
                    },
                    "truststoreKey": {
                      "value": "",
                      "valueFrom": {
                        "secretKeyRef": {
                          "key": "",
                          "name": ""
                        }
                      }
                    },
                    "truststorePassword": {
                      "value": ""
                    }
                  },
                  "jmx": {
                    "enabled": false,
                    "keystoreKey": {
                      "value": "",
                      "valueFrom": {
                        "secretKeyRef": {
                          "key": "",
                          "name": ""
                        }
                      }
                    },
                    "keystorePassword": {
                      "value": ""
                    },
                    "truststoreKey": {
                      "value": "",
                      "valueFrom": {
                        "secretKeyRef": {
                          "key": "",
                          "name": ""
                        }
                      }
                    },
                    "truststorePassword": {
                      "value": ""
                    }
                  }
                }
              },
              "node": {
                "enabled": true,
                "limits": {
                  "cpu": "2",
                  "memory": "1024Mi"
                },
                "requests": {
                  "cpu": "200m",
                  "memory": "300Mi"
                }
              }
            },
            "namespaces": "",
            "role": "team",
            "type": "Openshift"
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/ca/uma-team-operator-bundle@sha256:7fd74845057b1e1b15239afcac12e02b77d33be96d50565786aabfbcbf260589",
      "bundle_path_digest": "sha256:7fd74845057b1e1b15239afcac12e02b77d33be96d50565786aabfbcbf260589",
      "capabilities": [
        "Seamless Upgrades"
      ],
      "channel_name": "alpha",
      "creation_date": "2022-11-22T14:49:08.938000+00:00",
      "csv_description": "The **DX APM Universal Monitoring Agent Team For Multi-tenancy** installed by Application Administrator. Each application team that uses a DX APM tenant installs the UMA Team capability, specifying the namespaces the team owns or wants to monitor. UMA Team acts as a client, using a REST API to pull the namespace performance data from the UMA Admin deployment(DX APM Universal Monitoring Agent Team For Multi-tenancy). The UMA Team capability reports the application, pod, and node performance metrics only for the specified namespaces. This capability limits your application teams to seeing performance metrics only for their applications running in different environments on the tenant. Example environments include development, verification, pre-production, and production.\n\n**Prerequisite:**\n   1. The **DX APM Universal Monitoring Agent Team For Multi-tenancy** operator needs to be installed by the Cluster Administrator before UMA Team is installed.\n   2. UMA Team acts as a client, using a REST API to pull the namespace performance data from the UMA Admin deployment. So any network policy that restricts the communication to the Services running in the UMA Admin namespace, needs to be updated to allow the traffic from UMA team.\n\nFor more information: https://techdocs.broadcom.com/us/en/ca-enterprise-software/it-operations-management/dx-apm-agents/SaaS/Universal-Monitoring-Agent/Install-the-Universal-Monitoring-Agent/Deploy-UMA-in-a-Multi-tenant-Environment.html\n\n**Usage**:\n```\n apiVersion: ca.broadcom.com1/v1alpha1\n kind: UniversalMonitoringAgentsTeam\n metadata:\n   name: uma-monitor-team\n spec:\n   agentManager:\n     url: localhost:5001\n     credential: \"\"\n     tenantID: \"\"\n\n   # Name of the cluster.\n   clusterName: DevelopmentCluster\n   # Cluster type, either Kubernetes or OpenShift.\n   type: Kubernetes\n   role: team\n   namespaces: \"\"\n   adminNamespaceName: broadcom-aiops\n\n   monitor:\n     application:\n       jmx:\n         enabled: true\n       opentracing:\n         enabled: true\n         grpc:\n           hostport: \"\"\n         correlation:\n           enabled: false\n       zipkinTracing:\n         enabled: true\n         zipkinServer:\n           hostport:\n     events:\n       enabled: false\n     container:\n       dockerstats:\n         enabled: true\n     clusterPerformance:\n       dataReporter:\n         vertex:\n           ttl: 720\n           refreshInterval: 360\n         # The below properties define the resources for 'broadcom-monitor' deployment that does cluster and container monitoring.\n         limits:\n           cpu: \"2\"\n           # Assign memory resources in 'Mi'(mebibytes) units only.\n           memory: \"2048Mi\"\n         requests:\n           cpu: \"200m\"\n           # Assign memory resources in 'Mi'(mebibytes) units only.\n           memory: \"800Mi\"\n     kafka:\n       enabled: false\n       consumergroups:\n         metrics: true\n         filter: \"\"\n       jmx:\n         modules: \"kafka,jvm,memory\"\n         MBeans:\n           # A semi-colon separated list of managed bean object names. JMX metrics will be automatically reported for matching beans.\n           include: \"\"\n           # A semi-colon separated list of managed bean object names. Matching beans will be ignored for Kafka JMX metrics reporting. This property overrides \"include\" configuration.\n           exclude: \"\"\n       debug: false\n       ssl:\n         client:\n           enabled: false\n           truststoreKey:\n             value: \"\"\n             valueFrom:\n               secretKeyRef:\n                 name: \"\"\n                 key: \"\"\n           truststorePassword:\n             value: \"\"\n           keystoreKey:\n             value: \"\"\n             valueFrom:\n               secretKeyRef:\n                 name: \"\"\n                 key: \"\"\n           keystorePassword:\n             value: \"\"\n           keystoreKeyPassword:\n             value: \"\"\n           endpointIdentificationEnabled: false\n         jmx:\n           enabled: false\n           truststoreKey:\n             value: \"\"\n             valueFrom:\n               secretKeyRef:\n                 name: \"\"\n                 key: \"\"\n           truststorePassword:\n             value: \"\"\n           keystoreKey:\n             value: \"\"\n             valueFrom:\n               secretKeyRef:\n                 name: \"\"\n                 key: \"\"\n           keystorePassword:\n             value: \"\"\n       # The below properties define the resources for 'apm-kafka-monitor' deployment\n       limits:\n         cpu: \"2\"\n         # Assign memory resources in 'Mi'(mebibytes) units only.\n         memory: \"1024Mi\"\n       requests:\n         cpu: \"200m\"\n         # Assign memory resources in 'Mi'(mebibytes) units only.\n         memory: \"300Mi\"\n     node:\n       enabled: true\n       # The below properties define the resources for 'broadcom-monitor-host' deployment that does host monitoring.\n       limits:\n         cpu: \"2\"\n         # Assign memory resources in 'Mi'(mebibytes) units only.\n         memory: \"1024Mi\"\n       requests:\n         cpu: \"200m\"\n         # Assign memory resources in 'Mi'(mebibytes) units only.\n         memory: \"300Mi\"\n\n   globalDeployment:\n     # Node affinity rules for UMA Pods\n     # Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity\n     nodeAffinity: {}\n\n     # Node labels for UMA Pods\n     # Ref: https://kubernetes.io/docs/user-guide/node-selection/\n     nodeSelector: {}\n\n     # Additional tolerations for UMA pods\n     # https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/\n     # Default toleration used by UMA\n     tolerations:\n       - effect: \"NoSchedule\"\n         key: \"node-role.kubernetes.io/master\"\n         operator: \"\"\n         value: \"\"\n\n```\n",
      "csv_display_name": "DX APM Universal Monitoring Agent Team For Multi-tenancy",
      "csv_metadata_description": "DX APM Universal Monitoring Agent Team For Multi-tenancy acts as single deployment that automatically discovers and monitors cloud and container infrastructures and containerized application processes given namespaces.",
      "csv_name": "uma-team-operator.v2022.11.2-11",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": true,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": false,
      "last_update_date": "2022-11-23T07:46:09.072000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.8",
      "organization": "certified-operators",
      "package": "uma-team-operator",
      "provided_apis": [
        {
          "group": "ca.broadcom.com1",
          "kind": "UniversalMonitoringAgentsTeam",
          "plural": "universalmonitoringagentsteams",
          "version": "v1alpha1"
        }
      ],
      "provider": "Broadcom, Inc.",
      "related_images": [
        {
          "digest": "sha256:152985e9cd88b827000dd95e7dae47d6bbdd583d13b6f4dab21ef24e585da706",
          "image": "registry.connect.redhat.com/ca/uma-operator@sha256:152985e9cd88b827000dd95e7dae47d6bbdd583d13b6f4dab21ef24e585da706",
          "name": "uma-operator-152985e9cd88b827000dd95e7dae47d6bbdd583d13b6f4dab21ef24e585da706-annotation"
        },
        {
          "digest": "sha256:152985e9cd88b827000dd95e7dae47d6bbdd583d13b6f4dab21ef24e585da706",
          "image": "registry.connect.redhat.com/ca/uma-operator@sha256:152985e9cd88b827000dd95e7dae47d6bbdd583d13b6f4dab21ef24e585da706",
          "name": "uma-operator"
        },
        {
          "digest": "sha256:6d6e7d9f0c662c0b24b7a3d0ac1d8526305ad17144c8e23b462965bce8e4d45e",
          "image": "registry.connect.redhat.com/ca/universalmonitoragent@sha256:6d6e7d9f0c662c0b24b7a3d0ac1d8526305ad17144c8e23b462965bce8e4d45e",
          "name": "uma"
        }
      ],
      "replaces": null,
      "skip_range": ">=2022.1.0 <2022.11.2",
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.8",
      "version": "2022.11.2-11",
      "version_original": "2022.11.2-11"
    },
    {
      "_id": "637ce2b85f23340823b262a3",
      "alm_examples": [
        {
          "api_version": "ca.broadcom.com1/v1alpha1",
          "kind": "UniversalMonitoringAgentsTeam",
          "metadata": {
            "name": "uma-monitor-team"
          },
          "spec": {
            "adminNamespaceName": "broadcom-aiops",
            "agentManager": {
              "credential": "",
              "tenantID": "",
              "url": "localhost:5001"
            },
            "clusterName": "DevelopmentCluster",
            "globalDeployment": {
              "nodeAffinity": {},
              "nodeSelector": {},
              "tolerations": [
                {
                  "effect": "NoSchedule",
                  "key": "node-role.kubernetes.io/master",
                  "operator": "",
                  "value": ""
                }
              ]
            },
            "monitor": {
              "application": {
                "jmx": {
                  "enabled": true
                },
                "opentracing": {
                  "correlation": {
                    "enabled": false
                  },
                  "enabled": true,
                  "grpc": {
                    "hostport": ""
                  }
                },
                "zipkinTracing": {
                  "enabled": true,
                  "zipkinServer": {}
                }
              },
              "clusterPerformance": {
                "dataReporter": {
                  "limits": {
                    "cpu": "2",
                    "memory": "2048Mi"
                  },
                  "requests": {
                    "cpu": "200m",
                    "memory": "800Mi"
                  },
                  "vertex": {
                    "refreshInterval": 360,
                    "ttl": 720
                  }
                }
              },
              "container": {
                "dockerstats": {
                  "enabled": true
                }
              },
              "events": {
                "enabled": false
              },
              "kafka": {
                "consumergroups": {
                  "filter": "",
                  "metrics": true
                },
                "debug": false,
                "enabled": false,
                "jmx": {
                  "MBeans": {
                    "exclude": "",
                    "include": ""
                  },
                  "modules": "kafka,jvm,memory"
                },
                "limits": {
                  "cpu": "2",
                  "memory": "1024Mi"
                },
                "requests": {
                  "cpu": "200m",
                  "memory": "300Mi"
                },
                "ssl": {
                  "client": {
                    "enabled": false,
                    "endpointIdentificationEnabled": false,
                    "keystoreKey": {
                      "value": "",
                      "valueFrom": {
                        "secretKeyRef": {
                          "key": "",
                          "name": ""
                        }
                      }
                    },
                    "keystoreKeyPassword": {
                      "value": ""
                    },
                    "keystorePassword": {
                      "value": ""
                    },
                    "truststoreKey": {
                      "value": "",
                      "valueFrom": {
                        "secretKeyRef": {
                          "key": "",
                          "name": ""
                        }
                      }
                    },
                    "truststorePassword": {
                      "value": ""
                    }
                  },
                  "jmx": {
                    "enabled": false,
                    "keystoreKey": {
                      "value": "",
                      "valueFrom": {
                        "secretKeyRef": {
                          "key": "",
                          "name": ""
                        }
                      }
                    },
                    "keystorePassword": {
                      "value": ""
                    },
                    "truststoreKey": {
                      "value": "",
                      "valueFrom": {
                        "secretKeyRef": {
                          "key": "",
                          "name": ""
                        }
                      }
                    },
                    "truststorePassword": {
                      "value": ""
                    }
                  }
                }
              },
              "node": {
                "enabled": true,
                "limits": {
                  "cpu": "2",
                  "memory": "1024Mi"
                },
                "requests": {
                  "cpu": "200m",
                  "memory": "300Mi"
                }
              }
            },
            "namespaces": "",
            "role": "team",
            "type": "Openshift"
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/ca/uma-team-operator-bundle@sha256:7fd74845057b1e1b15239afcac12e02b77d33be96d50565786aabfbcbf260589",
      "bundle_path_digest": "sha256:7fd74845057b1e1b15239afcac12e02b77d33be96d50565786aabfbcbf260589",
      "capabilities": [
        "Seamless Upgrades"
      ],
      "channel_name": "alpha",
      "creation_date": "2022-11-22T14:54:48.906000+00:00",
      "csv_description": "The **DX APM Universal Monitoring Agent Team For Multi-tenancy** installed by Application Administrator. Each application team that uses a DX APM tenant installs the UMA Team capability, specifying the namespaces the team owns or wants to monitor. UMA Team acts as a client, using a REST API to pull the namespace performance data from the UMA Admin deployment(DX APM Universal Monitoring Agent Team For Multi-tenancy). The UMA Team capability reports the application, pod, and node performance metrics only for the specified namespaces. This capability limits your application teams to seeing performance metrics only for their applications running in different environments on the tenant. Example environments include development, verification, pre-production, and production.\n\n**Prerequisite:**\n   1. The **DX APM Universal Monitoring Agent Team For Multi-tenancy** operator needs to be installed by the Cluster Administrator before UMA Team is installed.\n   2. UMA Team acts as a client, using a REST API to pull the namespace performance data from the UMA Admin deployment. So any network policy that restricts the communication to the Services running in the UMA Admin namespace, needs to be updated to allow the traffic from UMA team.\n\nFor more information: https://techdocs.broadcom.com/us/en/ca-enterprise-software/it-operations-management/dx-apm-agents/SaaS/Universal-Monitoring-Agent/Install-the-Universal-Monitoring-Agent/Deploy-UMA-in-a-Multi-tenant-Environment.html\n\n**Usage**:\n```\n apiVersion: ca.broadcom.com1/v1alpha1\n kind: UniversalMonitoringAgentsTeam\n metadata:\n   name: uma-monitor-team\n spec:\n   agentManager:\n     url: localhost:5001\n     credential: \"\"\n     tenantID: \"\"\n\n   # Name of the cluster.\n   clusterName: DevelopmentCluster\n   # Cluster type, either Kubernetes or OpenShift.\n   type: Kubernetes\n   role: team\n   namespaces: \"\"\n   adminNamespaceName: broadcom-aiops\n\n   monitor:\n     application:\n       jmx:\n         enabled: true\n       opentracing:\n         enabled: true\n         grpc:\n           hostport: \"\"\n         correlation:\n           enabled: false\n       zipkinTracing:\n         enabled: true\n         zipkinServer:\n           hostport:\n     events:\n       enabled: false\n     container:\n       dockerstats:\n         enabled: true\n     clusterPerformance:\n       dataReporter:\n         vertex:\n           ttl: 720\n           refreshInterval: 360\n         # The below properties define the resources for 'broadcom-monitor' deployment that does cluster and container monitoring.\n         limits:\n           cpu: \"2\"\n           # Assign memory resources in 'Mi'(mebibytes) units only.\n           memory: \"2048Mi\"\n         requests:\n           cpu: \"200m\"\n           # Assign memory resources in 'Mi'(mebibytes) units only.\n           memory: \"800Mi\"\n     kafka:\n       enabled: false\n       consumergroups:\n         metrics: true\n         filter: \"\"\n       jmx:\n         modules: \"kafka,jvm,memory\"\n         MBeans:\n           # A semi-colon separated list of managed bean object names. JMX metrics will be automatically reported for matching beans.\n           include: \"\"\n           # A semi-colon separated list of managed bean object names. Matching beans will be ignored for Kafka JMX metrics reporting. This property overrides \"include\" configuration.\n           exclude: \"\"\n       debug: false\n       ssl:\n         client:\n           enabled: false\n           truststoreKey:\n             value: \"\"\n             valueFrom:\n               secretKeyRef:\n                 name: \"\"\n                 key: \"\"\n           truststorePassword:\n             value: \"\"\n           keystoreKey:\n             value: \"\"\n             valueFrom:\n               secretKeyRef:\n                 name: \"\"\n                 key: \"\"\n           keystorePassword:\n             value: \"\"\n           keystoreKeyPassword:\n             value: \"\"\n           endpointIdentificationEnabled: false\n         jmx:\n           enabled: false\n           truststoreKey:\n             value: \"\"\n             valueFrom:\n               secretKeyRef:\n                 name: \"\"\n                 key: \"\"\n           truststorePassword:\n             value: \"\"\n           keystoreKey:\n             value: \"\"\n             valueFrom:\n               secretKeyRef:\n                 name: \"\"\n                 key: \"\"\n           keystorePassword:\n             value: \"\"\n       # The below properties define the resources for 'apm-kafka-monitor' deployment\n       limits:\n         cpu: \"2\"\n         # Assign memory resources in 'Mi'(mebibytes) units only.\n         memory: \"1024Mi\"\n       requests:\n         cpu: \"200m\"\n         # Assign memory resources in 'Mi'(mebibytes) units only.\n         memory: \"300Mi\"\n     node:\n       enabled: true\n       # The below properties define the resources for 'broadcom-monitor-host' deployment that does host monitoring.\n       limits:\n         cpu: \"2\"\n         # Assign memory resources in 'Mi'(mebibytes) units only.\n         memory: \"1024Mi\"\n       requests:\n         cpu: \"200m\"\n         # Assign memory resources in 'Mi'(mebibytes) units only.\n         memory: \"300Mi\"\n\n   globalDeployment:\n     # Node affinity rules for UMA Pods\n     # Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity\n     nodeAffinity: {}\n\n     # Node labels for UMA Pods\n     # Ref: https://kubernetes.io/docs/user-guide/node-selection/\n     nodeSelector: {}\n\n     # Additional tolerations for UMA pods\n     # https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/\n     # Default toleration used by UMA\n     tolerations:\n       - effect: \"NoSchedule\"\n         key: \"node-role.kubernetes.io/master\"\n         operator: \"\"\n         value: \"\"\n\n```\n",
      "csv_display_name": "DX APM Universal Monitoring Agent Team For Multi-tenancy",
      "csv_metadata_description": "DX APM Universal Monitoring Agent Team For Multi-tenancy acts as single deployment that automatically discovers and monitors cloud and container infrastructures and containerized application processes given namespaces.",
      "csv_name": "uma-team-operator.v2022.11.2-11",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": true,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": false,
      "last_update_date": "2022-11-23T07:30:39.679000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.9",
      "organization": "certified-operators",
      "package": "uma-team-operator",
      "provided_apis": [
        {
          "group": "ca.broadcom.com1",
          "kind": "UniversalMonitoringAgentsTeam",
          "plural": "universalmonitoringagentsteams",
          "version": "v1alpha1"
        }
      ],
      "provider": "Broadcom, Inc.",
      "related_images": [
        {
          "digest": "sha256:152985e9cd88b827000dd95e7dae47d6bbdd583d13b6f4dab21ef24e585da706",
          "image": "registry.connect.redhat.com/ca/uma-operator@sha256:152985e9cd88b827000dd95e7dae47d6bbdd583d13b6f4dab21ef24e585da706",
          "name": "uma-operator-152985e9cd88b827000dd95e7dae47d6bbdd583d13b6f4dab21ef24e585da706-annotation"
        },
        {
          "digest": "sha256:152985e9cd88b827000dd95e7dae47d6bbdd583d13b6f4dab21ef24e585da706",
          "image": "registry.connect.redhat.com/ca/uma-operator@sha256:152985e9cd88b827000dd95e7dae47d6bbdd583d13b6f4dab21ef24e585da706",
          "name": "uma-operator"
        },
        {
          "digest": "sha256:6d6e7d9f0c662c0b24b7a3d0ac1d8526305ad17144c8e23b462965bce8e4d45e",
          "image": "registry.connect.redhat.com/ca/universalmonitoragent@sha256:6d6e7d9f0c662c0b24b7a3d0ac1d8526305ad17144c8e23b462965bce8e4d45e",
          "name": "uma"
        }
      ],
      "replaces": null,
      "skip_range": ">=2022.1.0 <2022.11.2",
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.9",
      "version": "2022.11.2-11",
      "version_original": "2022.11.2-11"
    },
    {
      "_id": "637ce2bb0ee97ebabe36b90a",
      "alm_examples": [
        {
          "api_version": "ca.broadcom.com1/v1alpha1",
          "kind": "UniversalMonitoringAgentsTeam",
          "metadata": {
            "name": "uma-monitor-team"
          },
          "spec": {
            "adminNamespaceName": "broadcom-aiops",
            "agentManager": {
              "credential": "",
              "tenantID": "",
              "url": "localhost:5001"
            },
            "clusterName": "DevelopmentCluster",
            "globalDeployment": {
              "nodeAffinity": {},
              "nodeSelector": {},
              "tolerations": [
                {
                  "effect": "NoSchedule",
                  "key": "node-role.kubernetes.io/master",
                  "operator": "",
                  "value": ""
                }
              ]
            },
            "monitor": {
              "application": {
                "jmx": {
                  "enabled": true
                },
                "opentracing": {
                  "correlation": {
                    "enabled": false
                  },
                  "enabled": true,
                  "grpc": {
                    "hostport": ""
                  }
                },
                "zipkinTracing": {
                  "enabled": true,
                  "zipkinServer": {}
                }
              },
              "clusterPerformance": {
                "dataReporter": {
                  "limits": {
                    "cpu": "2",
                    "memory": "2048Mi"
                  },
                  "requests": {
                    "cpu": "200m",
                    "memory": "800Mi"
                  },
                  "vertex": {
                    "refreshInterval": 360,
                    "ttl": 720
                  }
                }
              },
              "container": {
                "dockerstats": {
                  "enabled": true
                }
              },
              "events": {
                "enabled": false
              },
              "kafka": {
                "consumergroups": {
                  "filter": "",
                  "metrics": true
                },
                "debug": false,
                "enabled": false,
                "jmx": {
                  "MBeans": {
                    "exclude": "",
                    "include": ""
                  },
                  "modules": "kafka,jvm,memory"
                },
                "limits": {
                  "cpu": "2",
                  "memory": "1024Mi"
                },
                "requests": {
                  "cpu": "200m",
                  "memory": "300Mi"
                },
                "ssl": {
                  "client": {
                    "enabled": false,
                    "endpointIdentificationEnabled": false,
                    "keystoreKey": {
                      "value": "",
                      "valueFrom": {
                        "secretKeyRef": {
                          "key": "",
                          "name": ""
                        }
                      }
                    },
                    "keystoreKeyPassword": {
                      "value": ""
                    },
                    "keystorePassword": {
                      "value": ""
                    },
                    "truststoreKey": {
                      "value": "",
                      "valueFrom": {
                        "secretKeyRef": {
                          "key": "",
                          "name": ""
                        }
                      }
                    },
                    "truststorePassword": {
                      "value": ""
                    }
                  },
                  "jmx": {
                    "enabled": false,
                    "keystoreKey": {
                      "value": "",
                      "valueFrom": {
                        "secretKeyRef": {
                          "key": "",
                          "name": ""
                        }
                      }
                    },
                    "keystorePassword": {
                      "value": ""
                    },
                    "truststoreKey": {
                      "value": "",
                      "valueFrom": {
                        "secretKeyRef": {
                          "key": "",
                          "name": ""
                        }
                      }
                    },
                    "truststorePassword": {
                      "value": ""
                    }
                  }
                }
              },
              "node": {
                "enabled": true,
                "limits": {
                  "cpu": "2",
                  "memory": "1024Mi"
                },
                "requests": {
                  "cpu": "200m",
                  "memory": "300Mi"
                }
              }
            },
            "namespaces": "",
            "role": "team",
            "type": "Openshift"
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/ca/uma-team-operator-bundle@sha256:7fd74845057b1e1b15239afcac12e02b77d33be96d50565786aabfbcbf260589",
      "bundle_path_digest": "sha256:7fd74845057b1e1b15239afcac12e02b77d33be96d50565786aabfbcbf260589",
      "capabilities": [
        "Seamless Upgrades"
      ],
      "channel_name": "beta",
      "creation_date": "2022-11-22T14:54:51.247000+00:00",
      "csv_description": "The **DX APM Universal Monitoring Agent Team For Multi-tenancy** installed by Application Administrator. Each application team that uses a DX APM tenant installs the UMA Team capability, specifying the namespaces the team owns or wants to monitor. UMA Team acts as a client, using a REST API to pull the namespace performance data from the UMA Admin deployment(DX APM Universal Monitoring Agent Team For Multi-tenancy). The UMA Team capability reports the application, pod, and node performance metrics only for the specified namespaces. This capability limits your application teams to seeing performance metrics only for their applications running in different environments on the tenant. Example environments include development, verification, pre-production, and production.\n\n**Prerequisite:**\n   1. The **DX APM Universal Monitoring Agent Team For Multi-tenancy** operator needs to be installed by the Cluster Administrator before UMA Team is installed.\n   2. UMA Team acts as a client, using a REST API to pull the namespace performance data from the UMA Admin deployment. So any network policy that restricts the communication to the Services running in the UMA Admin namespace, needs to be updated to allow the traffic from UMA team.\n\nFor more information: https://techdocs.broadcom.com/us/en/ca-enterprise-software/it-operations-management/dx-apm-agents/SaaS/Universal-Monitoring-Agent/Install-the-Universal-Monitoring-Agent/Deploy-UMA-in-a-Multi-tenant-Environment.html\n\n**Usage**:\n```\n apiVersion: ca.broadcom.com1/v1alpha1\n kind: UniversalMonitoringAgentsTeam\n metadata:\n   name: uma-monitor-team\n spec:\n   agentManager:\n     url: localhost:5001\n     credential: \"\"\n     tenantID: \"\"\n\n   # Name of the cluster.\n   clusterName: DevelopmentCluster\n   # Cluster type, either Kubernetes or OpenShift.\n   type: Kubernetes\n   role: team\n   namespaces: \"\"\n   adminNamespaceName: broadcom-aiops\n\n   monitor:\n     application:\n       jmx:\n         enabled: true\n       opentracing:\n         enabled: true\n         grpc:\n           hostport: \"\"\n         correlation:\n           enabled: false\n       zipkinTracing:\n         enabled: true\n         zipkinServer:\n           hostport:\n     events:\n       enabled: false\n     container:\n       dockerstats:\n         enabled: true\n     clusterPerformance:\n       dataReporter:\n         vertex:\n           ttl: 720\n           refreshInterval: 360\n         # The below properties define the resources for 'broadcom-monitor' deployment that does cluster and container monitoring.\n         limits:\n           cpu: \"2\"\n           # Assign memory resources in 'Mi'(mebibytes) units only.\n           memory: \"2048Mi\"\n         requests:\n           cpu: \"200m\"\n           # Assign memory resources in 'Mi'(mebibytes) units only.\n           memory: \"800Mi\"\n     kafka:\n       enabled: false\n       consumergroups:\n         metrics: true\n         filter: \"\"\n       jmx:\n         modules: \"kafka,jvm,memory\"\n         MBeans:\n           # A semi-colon separated list of managed bean object names. JMX metrics will be automatically reported for matching beans.\n           include: \"\"\n           # A semi-colon separated list of managed bean object names. Matching beans will be ignored for Kafka JMX metrics reporting. This property overrides \"include\" configuration.\n           exclude: \"\"\n       debug: false\n       ssl:\n         client:\n           enabled: false\n           truststoreKey:\n             value: \"\"\n             valueFrom:\n               secretKeyRef:\n                 name: \"\"\n                 key: \"\"\n           truststorePassword:\n             value: \"\"\n           keystoreKey:\n             value: \"\"\n             valueFrom:\n               secretKeyRef:\n                 name: \"\"\n                 key: \"\"\n           keystorePassword:\n             value: \"\"\n           keystoreKeyPassword:\n             value: \"\"\n           endpointIdentificationEnabled: false\n         jmx:\n           enabled: false\n           truststoreKey:\n             value: \"\"\n             valueFrom:\n               secretKeyRef:\n                 name: \"\"\n                 key: \"\"\n           truststorePassword:\n             value: \"\"\n           keystoreKey:\n             value: \"\"\n             valueFrom:\n               secretKeyRef:\n                 name: \"\"\n                 key: \"\"\n           keystorePassword:\n             value: \"\"\n       # The below properties define the resources for 'apm-kafka-monitor' deployment\n       limits:\n         cpu: \"2\"\n         # Assign memory resources in 'Mi'(mebibytes) units only.\n         memory: \"1024Mi\"\n       requests:\n         cpu: \"200m\"\n         # Assign memory resources in 'Mi'(mebibytes) units only.\n         memory: \"300Mi\"\n     node:\n       enabled: true\n       # The below properties define the resources for 'broadcom-monitor-host' deployment that does host monitoring.\n       limits:\n         cpu: \"2\"\n         # Assign memory resources in 'Mi'(mebibytes) units only.\n         memory: \"1024Mi\"\n       requests:\n         cpu: \"200m\"\n         # Assign memory resources in 'Mi'(mebibytes) units only.\n         memory: \"300Mi\"\n\n   globalDeployment:\n     # Node affinity rules for UMA Pods\n     # Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity\n     nodeAffinity: {}\n\n     # Node labels for UMA Pods\n     # Ref: https://kubernetes.io/docs/user-guide/node-selection/\n     nodeSelector: {}\n\n     # Additional tolerations for UMA pods\n     # https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/\n     # Default toleration used by UMA\n     tolerations:\n       - effect: \"NoSchedule\"\n         key: \"node-role.kubernetes.io/master\"\n         operator: \"\"\n         value: \"\"\n\n```\n",
      "csv_display_name": "DX APM Universal Monitoring Agent Team For Multi-tenancy",
      "csv_metadata_description": "DX APM Universal Monitoring Agent Team For Multi-tenancy acts as single deployment that automatically discovers and monitors cloud and container infrastructures and containerized application processes given namespaces.",
      "csv_name": "uma-team-operator.v2022.11.2-11",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": true,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": false,
      "last_update_date": "2022-11-23T07:30:41.444000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.9",
      "organization": "certified-operators",
      "package": "uma-team-operator",
      "provided_apis": [
        {
          "group": "ca.broadcom.com1",
          "kind": "UniversalMonitoringAgentsTeam",
          "plural": "universalmonitoringagentsteams",
          "version": "v1alpha1"
        }
      ],
      "provider": "Broadcom, Inc.",
      "related_images": [
        {
          "digest": "sha256:152985e9cd88b827000dd95e7dae47d6bbdd583d13b6f4dab21ef24e585da706",
          "image": "registry.connect.redhat.com/ca/uma-operator@sha256:152985e9cd88b827000dd95e7dae47d6bbdd583d13b6f4dab21ef24e585da706",
          "name": "uma-operator-152985e9cd88b827000dd95e7dae47d6bbdd583d13b6f4dab21ef24e585da706-annotation"
        },
        {
          "digest": "sha256:152985e9cd88b827000dd95e7dae47d6bbdd583d13b6f4dab21ef24e585da706",
          "image": "registry.connect.redhat.com/ca/uma-operator@sha256:152985e9cd88b827000dd95e7dae47d6bbdd583d13b6f4dab21ef24e585da706",
          "name": "uma-operator"
        },
        {
          "digest": "sha256:6d6e7d9f0c662c0b24b7a3d0ac1d8526305ad17144c8e23b462965bce8e4d45e",
          "image": "registry.connect.redhat.com/ca/universalmonitoragent@sha256:6d6e7d9f0c662c0b24b7a3d0ac1d8526305ad17144c8e23b462965bce8e4d45e",
          "name": "uma"
        }
      ],
      "replaces": null,
      "skip_range": ">=2022.1.0 <2022.11.2",
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.9",
      "version": "2022.11.2-11",
      "version_original": "2022.11.2-11"
    },
    {
      "_id": "637ce2bc74e9f4cddfbef597",
      "alm_examples": [
        {
          "api_version": "ca.broadcom.com1/v1alpha1",
          "kind": "UniversalMonitoringAgentsTeam",
          "metadata": {
            "name": "uma-monitor-team"
          },
          "spec": {
            "adminNamespaceName": "broadcom-aiops",
            "agentManager": {
              "credential": "",
              "tenantID": "",
              "url": "localhost:5001"
            },
            "clusterName": "DevelopmentCluster",
            "globalDeployment": {
              "nodeAffinity": {},
              "nodeSelector": {},
              "tolerations": [
                {
                  "effect": "NoSchedule",
                  "key": "node-role.kubernetes.io/master",
                  "operator": "",
                  "value": ""
                }
              ]
            },
            "monitor": {
              "application": {
                "jmx": {
                  "enabled": true
                },
                "opentracing": {
                  "correlation": {
                    "enabled": false
                  },
                  "enabled": true,
                  "grpc": {
                    "hostport": ""
                  }
                },
                "zipkinTracing": {
                  "enabled": true,
                  "zipkinServer": {}
                }
              },
              "clusterPerformance": {
                "dataReporter": {
                  "limits": {
                    "cpu": "2",
                    "memory": "2048Mi"
                  },
                  "requests": {
                    "cpu": "200m",
                    "memory": "800Mi"
                  },
                  "vertex": {
                    "refreshInterval": 360,
                    "ttl": 720
                  }
                }
              },
              "container": {
                "dockerstats": {
                  "enabled": true
                }
              },
              "events": {
                "enabled": false
              },
              "kafka": {
                "consumergroups": {
                  "filter": "",
                  "metrics": true
                },
                "debug": false,
                "enabled": false,
                "jmx": {
                  "MBeans": {
                    "exclude": "",
                    "include": ""
                  },
                  "modules": "kafka,jvm,memory"
                },
                "limits": {
                  "cpu": "2",
                  "memory": "1024Mi"
                },
                "requests": {
                  "cpu": "200m",
                  "memory": "300Mi"
                },
                "ssl": {
                  "client": {
                    "enabled": false,
                    "endpointIdentificationEnabled": false,
                    "keystoreKey": {
                      "value": "",
                      "valueFrom": {
                        "secretKeyRef": {
                          "key": "",
                          "name": ""
                        }
                      }
                    },
                    "keystoreKeyPassword": {
                      "value": ""
                    },
                    "keystorePassword": {
                      "value": ""
                    },
                    "truststoreKey": {
                      "value": "",
                      "valueFrom": {
                        "secretKeyRef": {
                          "key": "",
                          "name": ""
                        }
                      }
                    },
                    "truststorePassword": {
                      "value": ""
                    }
                  },
                  "jmx": {
                    "enabled": false,
                    "keystoreKey": {
                      "value": "",
                      "valueFrom": {
                        "secretKeyRef": {
                          "key": "",
                          "name": ""
                        }
                      }
                    },
                    "keystorePassword": {
                      "value": ""
                    },
                    "truststoreKey": {
                      "value": "",
                      "valueFrom": {
                        "secretKeyRef": {
                          "key": "",
                          "name": ""
                        }
                      }
                    },
                    "truststorePassword": {
                      "value": ""
                    }
                  }
                }
              },
              "node": {
                "enabled": true,
                "limits": {
                  "cpu": "2",
                  "memory": "1024Mi"
                },
                "requests": {
                  "cpu": "200m",
                  "memory": "300Mi"
                }
              }
            },
            "namespaces": "",
            "role": "team",
            "type": "Openshift"
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/ca/uma-team-operator-bundle@sha256:7fd74845057b1e1b15239afcac12e02b77d33be96d50565786aabfbcbf260589",
      "bundle_path_digest": "sha256:7fd74845057b1e1b15239afcac12e02b77d33be96d50565786aabfbcbf260589",
      "capabilities": [
        "Seamless Upgrades"
      ],
      "channel_name": "stable",
      "creation_date": "2022-11-22T14:54:52.918000+00:00",
      "csv_description": "The **DX APM Universal Monitoring Agent Team For Multi-tenancy** installed by Application Administrator. Each application team that uses a DX APM tenant installs the UMA Team capability, specifying the namespaces the team owns or wants to monitor. UMA Team acts as a client, using a REST API to pull the namespace performance data from the UMA Admin deployment(DX APM Universal Monitoring Agent Team For Multi-tenancy). The UMA Team capability reports the application, pod, and node performance metrics only for the specified namespaces. This capability limits your application teams to seeing performance metrics only for their applications running in different environments on the tenant. Example environments include development, verification, pre-production, and production.\n\n**Prerequisite:**\n   1. The **DX APM Universal Monitoring Agent Team For Multi-tenancy** operator needs to be installed by the Cluster Administrator before UMA Team is installed.\n   2. UMA Team acts as a client, using a REST API to pull the namespace performance data from the UMA Admin deployment. So any network policy that restricts the communication to the Services running in the UMA Admin namespace, needs to be updated to allow the traffic from UMA team.\n\nFor more information: https://techdocs.broadcom.com/us/en/ca-enterprise-software/it-operations-management/dx-apm-agents/SaaS/Universal-Monitoring-Agent/Install-the-Universal-Monitoring-Agent/Deploy-UMA-in-a-Multi-tenant-Environment.html\n\n**Usage**:\n```\n apiVersion: ca.broadcom.com1/v1alpha1\n kind: UniversalMonitoringAgentsTeam\n metadata:\n   name: uma-monitor-team\n spec:\n   agentManager:\n     url: localhost:5001\n     credential: \"\"\n     tenantID: \"\"\n\n   # Name of the cluster.\n   clusterName: DevelopmentCluster\n   # Cluster type, either Kubernetes or OpenShift.\n   type: Kubernetes\n   role: team\n   namespaces: \"\"\n   adminNamespaceName: broadcom-aiops\n\n   monitor:\n     application:\n       jmx:\n         enabled: true\n       opentracing:\n         enabled: true\n         grpc:\n           hostport: \"\"\n         correlation:\n           enabled: false\n       zipkinTracing:\n         enabled: true\n         zipkinServer:\n           hostport:\n     events:\n       enabled: false\n     container:\n       dockerstats:\n         enabled: true\n     clusterPerformance:\n       dataReporter:\n         vertex:\n           ttl: 720\n           refreshInterval: 360\n         # The below properties define the resources for 'broadcom-monitor' deployment that does cluster and container monitoring.\n         limits:\n           cpu: \"2\"\n           # Assign memory resources in 'Mi'(mebibytes) units only.\n           memory: \"2048Mi\"\n         requests:\n           cpu: \"200m\"\n           # Assign memory resources in 'Mi'(mebibytes) units only.\n           memory: \"800Mi\"\n     kafka:\n       enabled: false\n       consumergroups:\n         metrics: true\n         filter: \"\"\n       jmx:\n         modules: \"kafka,jvm,memory\"\n         MBeans:\n           # A semi-colon separated list of managed bean object names. JMX metrics will be automatically reported for matching beans.\n           include: \"\"\n           # A semi-colon separated list of managed bean object names. Matching beans will be ignored for Kafka JMX metrics reporting. This property overrides \"include\" configuration.\n           exclude: \"\"\n       debug: false\n       ssl:\n         client:\n           enabled: false\n           truststoreKey:\n             value: \"\"\n             valueFrom:\n               secretKeyRef:\n                 name: \"\"\n                 key: \"\"\n           truststorePassword:\n             value: \"\"\n           keystoreKey:\n             value: \"\"\n             valueFrom:\n               secretKeyRef:\n                 name: \"\"\n                 key: \"\"\n           keystorePassword:\n             value: \"\"\n           keystoreKeyPassword:\n             value: \"\"\n           endpointIdentificationEnabled: false\n         jmx:\n           enabled: false\n           truststoreKey:\n             value: \"\"\n             valueFrom:\n               secretKeyRef:\n                 name: \"\"\n                 key: \"\"\n           truststorePassword:\n             value: \"\"\n           keystoreKey:\n             value: \"\"\n             valueFrom:\n               secretKeyRef:\n                 name: \"\"\n                 key: \"\"\n           keystorePassword:\n             value: \"\"\n       # The below properties define the resources for 'apm-kafka-monitor' deployment\n       limits:\n         cpu: \"2\"\n         # Assign memory resources in 'Mi'(mebibytes) units only.\n         memory: \"1024Mi\"\n       requests:\n         cpu: \"200m\"\n         # Assign memory resources in 'Mi'(mebibytes) units only.\n         memory: \"300Mi\"\n     node:\n       enabled: true\n       # The below properties define the resources for 'broadcom-monitor-host' deployment that does host monitoring.\n       limits:\n         cpu: \"2\"\n         # Assign memory resources in 'Mi'(mebibytes) units only.\n         memory: \"1024Mi\"\n       requests:\n         cpu: \"200m\"\n         # Assign memory resources in 'Mi'(mebibytes) units only.\n         memory: \"300Mi\"\n\n   globalDeployment:\n     # Node affinity rules for UMA Pods\n     # Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity\n     nodeAffinity: {}\n\n     # Node labels for UMA Pods\n     # Ref: https://kubernetes.io/docs/user-guide/node-selection/\n     nodeSelector: {}\n\n     # Additional tolerations for UMA pods\n     # https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/\n     # Default toleration used by UMA\n     tolerations:\n       - effect: \"NoSchedule\"\n         key: \"node-role.kubernetes.io/master\"\n         operator: \"\"\n         value: \"\"\n\n```\n",
      "csv_display_name": "DX APM Universal Monitoring Agent Team For Multi-tenancy",
      "csv_metadata_description": "DX APM Universal Monitoring Agent Team For Multi-tenancy acts as single deployment that automatically discovers and monitors cloud and container infrastructures and containerized application processes given namespaces.",
      "csv_name": "uma-team-operator.v2022.11.2-11",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": true,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-11-23T07:30:45.966000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.9",
      "organization": "certified-operators",
      "package": "uma-team-operator",
      "provided_apis": [
        {
          "group": "ca.broadcom.com1",
          "kind": "UniversalMonitoringAgentsTeam",
          "plural": "universalmonitoringagentsteams",
          "version": "v1alpha1"
        }
      ],
      "provider": "Broadcom, Inc.",
      "related_images": [
        {
          "digest": "sha256:152985e9cd88b827000dd95e7dae47d6bbdd583d13b6f4dab21ef24e585da706",
          "image": "registry.connect.redhat.com/ca/uma-operator@sha256:152985e9cd88b827000dd95e7dae47d6bbdd583d13b6f4dab21ef24e585da706",
          "name": "uma-operator-152985e9cd88b827000dd95e7dae47d6bbdd583d13b6f4dab21ef24e585da706-annotation"
        },
        {
          "digest": "sha256:152985e9cd88b827000dd95e7dae47d6bbdd583d13b6f4dab21ef24e585da706",
          "image": "registry.connect.redhat.com/ca/uma-operator@sha256:152985e9cd88b827000dd95e7dae47d6bbdd583d13b6f4dab21ef24e585da706",
          "name": "uma-operator"
        },
        {
          "digest": "sha256:6d6e7d9f0c662c0b24b7a3d0ac1d8526305ad17144c8e23b462965bce8e4d45e",
          "image": "registry.connect.redhat.com/ca/universalmonitoragent@sha256:6d6e7d9f0c662c0b24b7a3d0ac1d8526305ad17144c8e23b462965bce8e4d45e",
          "name": "uma"
        }
      ],
      "replaces": null,
      "skip_range": ">=2022.1.0 <2022.11.2",
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.9",
      "version": "2022.11.2-11",
      "version_original": "2022.11.2-11"
    },
    {
      "_id": "637ce3595556118ee8dfda5d",
      "alm_examples": [
        {
          "api_version": "ca.broadcom.com1/v1alpha1",
          "kind": "UniversalMonitoringAgentsTeam",
          "metadata": {
            "name": "uma-monitor-team"
          },
          "spec": {
            "adminNamespaceName": "broadcom-aiops",
            "agentManager": {
              "credential": "",
              "tenantID": "",
              "url": "localhost:5001"
            },
            "clusterName": "DevelopmentCluster",
            "globalDeployment": {
              "nodeAffinity": {},
              "nodeSelector": {},
              "tolerations": [
                {
                  "effect": "NoSchedule",
                  "key": "node-role.kubernetes.io/master",
                  "operator": "",
                  "value": ""
                }
              ]
            },
            "monitor": {
              "application": {
                "jmx": {
                  "enabled": true
                },
                "opentracing": {
                  "correlation": {
                    "enabled": false
                  },
                  "enabled": true,
                  "grpc": {
                    "hostport": ""
                  }
                },
                "zipkinTracing": {
                  "enabled": true,
                  "zipkinServer": {}
                }
              },
              "clusterPerformance": {
                "dataReporter": {
                  "limits": {
                    "cpu": "2",
                    "memory": "2048Mi"
                  },
                  "requests": {
                    "cpu": "200m",
                    "memory": "800Mi"
                  },
                  "vertex": {
                    "refreshInterval": 360,
                    "ttl": 720
                  }
                }
              },
              "container": {
                "dockerstats": {
                  "enabled": true
                }
              },
              "events": {
                "enabled": false
              },
              "kafka": {
                "consumergroups": {
                  "filter": "",
                  "metrics": true
                },
                "debug": false,
                "enabled": false,
                "jmx": {
                  "MBeans": {
                    "exclude": "",
                    "include": ""
                  },
                  "modules": "kafka,jvm,memory"
                },
                "limits": {
                  "cpu": "2",
                  "memory": "1024Mi"
                },
                "requests": {
                  "cpu": "200m",
                  "memory": "300Mi"
                },
                "ssl": {
                  "client": {
                    "enabled": false,
                    "endpointIdentificationEnabled": false,
                    "keystoreKey": {
                      "value": "",
                      "valueFrom": {
                        "secretKeyRef": {
                          "key": "",
                          "name": ""
                        }
                      }
                    },
                    "keystoreKeyPassword": {
                      "value": ""
                    },
                    "keystorePassword": {
                      "value": ""
                    },
                    "truststoreKey": {
                      "value": "",
                      "valueFrom": {
                        "secretKeyRef": {
                          "key": "",
                          "name": ""
                        }
                      }
                    },
                    "truststorePassword": {
                      "value": ""
                    }
                  },
                  "jmx": {
                    "enabled": false,
                    "keystoreKey": {
                      "value": "",
                      "valueFrom": {
                        "secretKeyRef": {
                          "key": "",
                          "name": ""
                        }
                      }
                    },
                    "keystorePassword": {
                      "value": ""
                    },
                    "truststoreKey": {
                      "value": "",
                      "valueFrom": {
                        "secretKeyRef": {
                          "key": "",
                          "name": ""
                        }
                      }
                    },
                    "truststorePassword": {
                      "value": ""
                    }
                  }
                }
              },
              "node": {
                "enabled": true,
                "limits": {
                  "cpu": "2",
                  "memory": "1024Mi"
                },
                "requests": {
                  "cpu": "200m",
                  "memory": "300Mi"
                }
              }
            },
            "namespaces": "",
            "role": "team",
            "type": "Openshift"
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/ca/uma-team-operator-bundle@sha256:7fd74845057b1e1b15239afcac12e02b77d33be96d50565786aabfbcbf260589",
      "bundle_path_digest": "sha256:7fd74845057b1e1b15239afcac12e02b77d33be96d50565786aabfbcbf260589",
      "capabilities": [
        "Seamless Upgrades"
      ],
      "channel_name": "alpha",
      "creation_date": "2022-11-22T14:57:29.715000+00:00",
      "csv_description": "The **DX APM Universal Monitoring Agent Team For Multi-tenancy** installed by Application Administrator. Each application team that uses a DX APM tenant installs the UMA Team capability, specifying the namespaces the team owns or wants to monitor. UMA Team acts as a client, using a REST API to pull the namespace performance data from the UMA Admin deployment(DX APM Universal Monitoring Agent Team For Multi-tenancy). The UMA Team capability reports the application, pod, and node performance metrics only for the specified namespaces. This capability limits your application teams to seeing performance metrics only for their applications running in different environments on the tenant. Example environments include development, verification, pre-production, and production.\n\n**Prerequisite:**\n   1. The **DX APM Universal Monitoring Agent Team For Multi-tenancy** operator needs to be installed by the Cluster Administrator before UMA Team is installed.\n   2. UMA Team acts as a client, using a REST API to pull the namespace performance data from the UMA Admin deployment. So any network policy that restricts the communication to the Services running in the UMA Admin namespace, needs to be updated to allow the traffic from UMA team.\n\nFor more information: https://techdocs.broadcom.com/us/en/ca-enterprise-software/it-operations-management/dx-apm-agents/SaaS/Universal-Monitoring-Agent/Install-the-Universal-Monitoring-Agent/Deploy-UMA-in-a-Multi-tenant-Environment.html\n\n**Usage**:\n```\n apiVersion: ca.broadcom.com1/v1alpha1\n kind: UniversalMonitoringAgentsTeam\n metadata:\n   name: uma-monitor-team\n spec:\n   agentManager:\n     url: localhost:5001\n     credential: \"\"\n     tenantID: \"\"\n\n   # Name of the cluster.\n   clusterName: DevelopmentCluster\n   # Cluster type, either Kubernetes or OpenShift.\n   type: Kubernetes\n   role: team\n   namespaces: \"\"\n   adminNamespaceName: broadcom-aiops\n\n   monitor:\n     application:\n       jmx:\n         enabled: true\n       opentracing:\n         enabled: true\n         grpc:\n           hostport: \"\"\n         correlation:\n           enabled: false\n       zipkinTracing:\n         enabled: true\n         zipkinServer:\n           hostport:\n     events:\n       enabled: false\n     container:\n       dockerstats:\n         enabled: true\n     clusterPerformance:\n       dataReporter:\n         vertex:\n           ttl: 720\n           refreshInterval: 360\n         # The below properties define the resources for 'broadcom-monitor' deployment that does cluster and container monitoring.\n         limits:\n           cpu: \"2\"\n           # Assign memory resources in 'Mi'(mebibytes) units only.\n           memory: \"2048Mi\"\n         requests:\n           cpu: \"200m\"\n           # Assign memory resources in 'Mi'(mebibytes) units only.\n           memory: \"800Mi\"\n     kafka:\n       enabled: false\n       consumergroups:\n         metrics: true\n         filter: \"\"\n       jmx:\n         modules: \"kafka,jvm,memory\"\n         MBeans:\n           # A semi-colon separated list of managed bean object names. JMX metrics will be automatically reported for matching beans.\n           include: \"\"\n           # A semi-colon separated list of managed bean object names. Matching beans will be ignored for Kafka JMX metrics reporting. This property overrides \"include\" configuration.\n           exclude: \"\"\n       debug: false\n       ssl:\n         client:\n           enabled: false\n           truststoreKey:\n             value: \"\"\n             valueFrom:\n               secretKeyRef:\n                 name: \"\"\n                 key: \"\"\n           truststorePassword:\n             value: \"\"\n           keystoreKey:\n             value: \"\"\n             valueFrom:\n               secretKeyRef:\n                 name: \"\"\n                 key: \"\"\n           keystorePassword:\n             value: \"\"\n           keystoreKeyPassword:\n             value: \"\"\n           endpointIdentificationEnabled: false\n         jmx:\n           enabled: false\n           truststoreKey:\n             value: \"\"\n             valueFrom:\n               secretKeyRef:\n                 name: \"\"\n                 key: \"\"\n           truststorePassword:\n             value: \"\"\n           keystoreKey:\n             value: \"\"\n             valueFrom:\n               secretKeyRef:\n                 name: \"\"\n                 key: \"\"\n           keystorePassword:\n             value: \"\"\n       # The below properties define the resources for 'apm-kafka-monitor' deployment\n       limits:\n         cpu: \"2\"\n         # Assign memory resources in 'Mi'(mebibytes) units only.\n         memory: \"1024Mi\"\n       requests:\n         cpu: \"200m\"\n         # Assign memory resources in 'Mi'(mebibytes) units only.\n         memory: \"300Mi\"\n     node:\n       enabled: true\n       # The below properties define the resources for 'broadcom-monitor-host' deployment that does host monitoring.\n       limits:\n         cpu: \"2\"\n         # Assign memory resources in 'Mi'(mebibytes) units only.\n         memory: \"1024Mi\"\n       requests:\n         cpu: \"200m\"\n         # Assign memory resources in 'Mi'(mebibytes) units only.\n         memory: \"300Mi\"\n\n   globalDeployment:\n     # Node affinity rules for UMA Pods\n     # Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity\n     nodeAffinity: {}\n\n     # Node labels for UMA Pods\n     # Ref: https://kubernetes.io/docs/user-guide/node-selection/\n     nodeSelector: {}\n\n     # Additional tolerations for UMA pods\n     # https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/\n     # Default toleration used by UMA\n     tolerations:\n       - effect: \"NoSchedule\"\n         key: \"node-role.kubernetes.io/master\"\n         operator: \"\"\n         value: \"\"\n\n```\n",
      "csv_display_name": "DX APM Universal Monitoring Agent Team For Multi-tenancy",
      "csv_metadata_description": "DX APM Universal Monitoring Agent Team For Multi-tenancy acts as single deployment that automatically discovers and monitors cloud and container infrastructures and containerized application processes given namespaces.",
      "csv_name": "uma-team-operator.v2022.11.2-11",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": true,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": false,
      "last_update_date": "2022-11-23T06:39:32.862000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.10",
      "organization": "certified-operators",
      "package": "uma-team-operator",
      "provided_apis": [
        {
          "group": "ca.broadcom.com1",
          "kind": "UniversalMonitoringAgentsTeam",
          "plural": "universalmonitoringagentsteams",
          "version": "v1alpha1"
        }
      ],
      "provider": "Broadcom, Inc.",
      "related_images": [
        {
          "digest": "sha256:152985e9cd88b827000dd95e7dae47d6bbdd583d13b6f4dab21ef24e585da706",
          "image": "registry.connect.redhat.com/ca/uma-operator@sha256:152985e9cd88b827000dd95e7dae47d6bbdd583d13b6f4dab21ef24e585da706",
          "name": "uma-operator-152985e9cd88b827000dd95e7dae47d6bbdd583d13b6f4dab21ef24e585da706-annotation"
        },
        {
          "digest": "sha256:152985e9cd88b827000dd95e7dae47d6bbdd583d13b6f4dab21ef24e585da706",
          "image": "registry.connect.redhat.com/ca/uma-operator@sha256:152985e9cd88b827000dd95e7dae47d6bbdd583d13b6f4dab21ef24e585da706",
          "name": "uma-operator"
        },
        {
          "digest": "sha256:6d6e7d9f0c662c0b24b7a3d0ac1d8526305ad17144c8e23b462965bce8e4d45e",
          "image": "registry.connect.redhat.com/ca/universalmonitoragent@sha256:6d6e7d9f0c662c0b24b7a3d0ac1d8526305ad17144c8e23b462965bce8e4d45e",
          "name": "uma"
        }
      ],
      "replaces": null,
      "skip_range": ">=2022.1.0 <2022.11.2",
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.10",
      "version": "2022.11.2-11",
      "version_original": "2022.11.2-11"
    },
    {
      "_id": "637ce35ac96e3e7d20bb3909",
      "alm_examples": [
        {
          "api_version": "ca.broadcom.com1/v1alpha1",
          "kind": "UniversalMonitoringAgentsTeam",
          "metadata": {
            "name": "uma-monitor-team"
          },
          "spec": {
            "adminNamespaceName": "broadcom-aiops",
            "agentManager": {
              "credential": "",
              "tenantID": "",
              "url": "localhost:5001"
            },
            "clusterName": "DevelopmentCluster",
            "globalDeployment": {
              "nodeAffinity": {},
              "nodeSelector": {},
              "tolerations": [
                {
                  "effect": "NoSchedule",
                  "key": "node-role.kubernetes.io/master",
                  "operator": "",
                  "value": ""
                }
              ]
            },
            "monitor": {
              "application": {
                "jmx": {
                  "enabled": true
                },
                "opentracing": {
                  "correlation": {
                    "enabled": false
                  },
                  "enabled": true,
                  "grpc": {
                    "hostport": ""
                  }
                },
                "zipkinTracing": {
                  "enabled": true,
                  "zipkinServer": {}
                }
              },
              "clusterPerformance": {
                "dataReporter": {
                  "limits": {
                    "cpu": "2",
                    "memory": "2048Mi"
                  },
                  "requests": {
                    "cpu": "200m",
                    "memory": "800Mi"
                  },
                  "vertex": {
                    "refreshInterval": 360,
                    "ttl": 720
                  }
                }
              },
              "container": {
                "dockerstats": {
                  "enabled": true
                }
              },
              "events": {
                "enabled": false
              },
              "kafka": {
                "consumergroups": {
                  "filter": "",
                  "metrics": true
                },
                "debug": false,
                "enabled": false,
                "jmx": {
                  "MBeans": {
                    "exclude": "",
                    "include": ""
                  },
                  "modules": "kafka,jvm,memory"
                },
                "limits": {
                  "cpu": "2",
                  "memory": "1024Mi"
                },
                "requests": {
                  "cpu": "200m",
                  "memory": "300Mi"
                },
                "ssl": {
                  "client": {
                    "enabled": false,
                    "endpointIdentificationEnabled": false,
                    "keystoreKey": {
                      "value": "",
                      "valueFrom": {
                        "secretKeyRef": {
                          "key": "",
                          "name": ""
                        }
                      }
                    },
                    "keystoreKeyPassword": {
                      "value": ""
                    },
                    "keystorePassword": {
                      "value": ""
                    },
                    "truststoreKey": {
                      "value": "",
                      "valueFrom": {
                        "secretKeyRef": {
                          "key": "",
                          "name": ""
                        }
                      }
                    },
                    "truststorePassword": {
                      "value": ""
                    }
                  },
                  "jmx": {
                    "enabled": false,
                    "keystoreKey": {
                      "value": "",
                      "valueFrom": {
                        "secretKeyRef": {
                          "key": "",
                          "name": ""
                        }
                      }
                    },
                    "keystorePassword": {
                      "value": ""
                    },
                    "truststoreKey": {
                      "value": "",
                      "valueFrom": {
                        "secretKeyRef": {
                          "key": "",
                          "name": ""
                        }
                      }
                    },
                    "truststorePassword": {
                      "value": ""
                    }
                  }
                }
              },
              "node": {
                "enabled": true,
                "limits": {
                  "cpu": "2",
                  "memory": "1024Mi"
                },
                "requests": {
                  "cpu": "200m",
                  "memory": "300Mi"
                }
              }
            },
            "namespaces": "",
            "role": "team",
            "type": "Openshift"
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/ca/uma-team-operator-bundle@sha256:7fd74845057b1e1b15239afcac12e02b77d33be96d50565786aabfbcbf260589",
      "bundle_path_digest": "sha256:7fd74845057b1e1b15239afcac12e02b77d33be96d50565786aabfbcbf260589",
      "capabilities": [
        "Seamless Upgrades"
      ],
      "channel_name": "beta",
      "creation_date": "2022-11-22T14:57:30.566000+00:00",
      "csv_description": "The **DX APM Universal Monitoring Agent Team For Multi-tenancy** installed by Application Administrator. Each application team that uses a DX APM tenant installs the UMA Team capability, specifying the namespaces the team owns or wants to monitor. UMA Team acts as a client, using a REST API to pull the namespace performance data from the UMA Admin deployment(DX APM Universal Monitoring Agent Team For Multi-tenancy). The UMA Team capability reports the application, pod, and node performance metrics only for the specified namespaces. This capability limits your application teams to seeing performance metrics only for their applications running in different environments on the tenant. Example environments include development, verification, pre-production, and production.\n\n**Prerequisite:**\n   1. The **DX APM Universal Monitoring Agent Team For Multi-tenancy** operator needs to be installed by the Cluster Administrator before UMA Team is installed.\n   2. UMA Team acts as a client, using a REST API to pull the namespace performance data from the UMA Admin deployment. So any network policy that restricts the communication to the Services running in the UMA Admin namespace, needs to be updated to allow the traffic from UMA team.\n\nFor more information: https://techdocs.broadcom.com/us/en/ca-enterprise-software/it-operations-management/dx-apm-agents/SaaS/Universal-Monitoring-Agent/Install-the-Universal-Monitoring-Agent/Deploy-UMA-in-a-Multi-tenant-Environment.html\n\n**Usage**:\n```\n apiVersion: ca.broadcom.com1/v1alpha1\n kind: UniversalMonitoringAgentsTeam\n metadata:\n   name: uma-monitor-team\n spec:\n   agentManager:\n     url: localhost:5001\n     credential: \"\"\n     tenantID: \"\"\n\n   # Name of the cluster.\n   clusterName: DevelopmentCluster\n   # Cluster type, either Kubernetes or OpenShift.\n   type: Kubernetes\n   role: team\n   namespaces: \"\"\n   adminNamespaceName: broadcom-aiops\n\n   monitor:\n     application:\n       jmx:\n         enabled: true\n       opentracing:\n         enabled: true\n         grpc:\n           hostport: \"\"\n         correlation:\n           enabled: false\n       zipkinTracing:\n         enabled: true\n         zipkinServer:\n           hostport:\n     events:\n       enabled: false\n     container:\n       dockerstats:\n         enabled: true\n     clusterPerformance:\n       dataReporter:\n         vertex:\n           ttl: 720\n           refreshInterval: 360\n         # The below properties define the resources for 'broadcom-monitor' deployment that does cluster and container monitoring.\n         limits:\n           cpu: \"2\"\n           # Assign memory resources in 'Mi'(mebibytes) units only.\n           memory: \"2048Mi\"\n         requests:\n           cpu: \"200m\"\n           # Assign memory resources in 'Mi'(mebibytes) units only.\n           memory: \"800Mi\"\n     kafka:\n       enabled: false\n       consumergroups:\n         metrics: true\n         filter: \"\"\n       jmx:\n         modules: \"kafka,jvm,memory\"\n         MBeans:\n           # A semi-colon separated list of managed bean object names. JMX metrics will be automatically reported for matching beans.\n           include: \"\"\n           # A semi-colon separated list of managed bean object names. Matching beans will be ignored for Kafka JMX metrics reporting. This property overrides \"include\" configuration.\n           exclude: \"\"\n       debug: false\n       ssl:\n         client:\n           enabled: false\n           truststoreKey:\n             value: \"\"\n             valueFrom:\n               secretKeyRef:\n                 name: \"\"\n                 key: \"\"\n           truststorePassword:\n             value: \"\"\n           keystoreKey:\n             value: \"\"\n             valueFrom:\n               secretKeyRef:\n                 name: \"\"\n                 key: \"\"\n           keystorePassword:\n             value: \"\"\n           keystoreKeyPassword:\n             value: \"\"\n           endpointIdentificationEnabled: false\n         jmx:\n           enabled: false\n           truststoreKey:\n             value: \"\"\n             valueFrom:\n               secretKeyRef:\n                 name: \"\"\n                 key: \"\"\n           truststorePassword:\n             value: \"\"\n           keystoreKey:\n             value: \"\"\n             valueFrom:\n               secretKeyRef:\n                 name: \"\"\n                 key: \"\"\n           keystorePassword:\n             value: \"\"\n       # The below properties define the resources for 'apm-kafka-monitor' deployment\n       limits:\n         cpu: \"2\"\n         # Assign memory resources in 'Mi'(mebibytes) units only.\n         memory: \"1024Mi\"\n       requests:\n         cpu: \"200m\"\n         # Assign memory resources in 'Mi'(mebibytes) units only.\n         memory: \"300Mi\"\n     node:\n       enabled: true\n       # The below properties define the resources for 'broadcom-monitor-host' deployment that does host monitoring.\n       limits:\n         cpu: \"2\"\n         # Assign memory resources in 'Mi'(mebibytes) units only.\n         memory: \"1024Mi\"\n       requests:\n         cpu: \"200m\"\n         # Assign memory resources in 'Mi'(mebibytes) units only.\n         memory: \"300Mi\"\n\n   globalDeployment:\n     # Node affinity rules for UMA Pods\n     # Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity\n     nodeAffinity: {}\n\n     # Node labels for UMA Pods\n     # Ref: https://kubernetes.io/docs/user-guide/node-selection/\n     nodeSelector: {}\n\n     # Additional tolerations for UMA pods\n     # https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/\n     # Default toleration used by UMA\n     tolerations:\n       - effect: \"NoSchedule\"\n         key: \"node-role.kubernetes.io/master\"\n         operator: \"\"\n         value: \"\"\n\n```\n",
      "csv_display_name": "DX APM Universal Monitoring Agent Team For Multi-tenancy",
      "csv_metadata_description": "DX APM Universal Monitoring Agent Team For Multi-tenancy acts as single deployment that automatically discovers and monitors cloud and container infrastructures and containerized application processes given namespaces.",
      "csv_name": "uma-team-operator.v2022.11.2-11",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": true,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": false,
      "last_update_date": "2022-11-23T06:39:39.629000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.10",
      "organization": "certified-operators",
      "package": "uma-team-operator",
      "provided_apis": [
        {
          "group": "ca.broadcom.com1",
          "kind": "UniversalMonitoringAgentsTeam",
          "plural": "universalmonitoringagentsteams",
          "version": "v1alpha1"
        }
      ],
      "provider": "Broadcom, Inc.",
      "related_images": [
        {
          "digest": "sha256:152985e9cd88b827000dd95e7dae47d6bbdd583d13b6f4dab21ef24e585da706",
          "image": "registry.connect.redhat.com/ca/uma-operator@sha256:152985e9cd88b827000dd95e7dae47d6bbdd583d13b6f4dab21ef24e585da706",
          "name": "uma-operator-152985e9cd88b827000dd95e7dae47d6bbdd583d13b6f4dab21ef24e585da706-annotation"
        },
        {
          "digest": "sha256:152985e9cd88b827000dd95e7dae47d6bbdd583d13b6f4dab21ef24e585da706",
          "image": "registry.connect.redhat.com/ca/uma-operator@sha256:152985e9cd88b827000dd95e7dae47d6bbdd583d13b6f4dab21ef24e585da706",
          "name": "uma-operator"
        },
        {
          "digest": "sha256:6d6e7d9f0c662c0b24b7a3d0ac1d8526305ad17144c8e23b462965bce8e4d45e",
          "image": "registry.connect.redhat.com/ca/universalmonitoragent@sha256:6d6e7d9f0c662c0b24b7a3d0ac1d8526305ad17144c8e23b462965bce8e4d45e",
          "name": "uma"
        }
      ],
      "replaces": null,
      "skip_range": ">=2022.1.0 <2022.11.2",
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.10",
      "version": "2022.11.2-11",
      "version_original": "2022.11.2-11"
    },
    {
      "_id": "637ce35b0ee97ebabe36bb3b",
      "alm_examples": [
        {
          "api_version": "ca.broadcom.com1/v1alpha1",
          "kind": "UniversalMonitoringAgentsTeam",
          "metadata": {
            "name": "uma-monitor-team"
          },
          "spec": {
            "adminNamespaceName": "broadcom-aiops",
            "agentManager": {
              "credential": "",
              "tenantID": "",
              "url": "localhost:5001"
            },
            "clusterName": "DevelopmentCluster",
            "globalDeployment": {
              "nodeAffinity": {},
              "nodeSelector": {},
              "tolerations": [
                {
                  "effect": "NoSchedule",
                  "key": "node-role.kubernetes.io/master",
                  "operator": "",
                  "value": ""
                }
              ]
            },
            "monitor": {
              "application": {
                "jmx": {
                  "enabled": true
                },
                "opentracing": {
                  "correlation": {
                    "enabled": false
                  },
                  "enabled": true,
                  "grpc": {
                    "hostport": ""
                  }
                },
                "zipkinTracing": {
                  "enabled": true,
                  "zipkinServer": {}
                }
              },
              "clusterPerformance": {
                "dataReporter": {
                  "limits": {
                    "cpu": "2",
                    "memory": "2048Mi"
                  },
                  "requests": {
                    "cpu": "200m",
                    "memory": "800Mi"
                  },
                  "vertex": {
                    "refreshInterval": 360,
                    "ttl": 720
                  }
                }
              },
              "container": {
                "dockerstats": {
                  "enabled": true
                }
              },
              "events": {
                "enabled": false
              },
              "kafka": {
                "consumergroups": {
                  "filter": "",
                  "metrics": true
                },
                "debug": false,
                "enabled": false,
                "jmx": {
                  "MBeans": {
                    "exclude": "",
                    "include": ""
                  },
                  "modules": "kafka,jvm,memory"
                },
                "limits": {
                  "cpu": "2",
                  "memory": "1024Mi"
                },
                "requests": {
                  "cpu": "200m",
                  "memory": "300Mi"
                },
                "ssl": {
                  "client": {
                    "enabled": false,
                    "endpointIdentificationEnabled": false,
                    "keystoreKey": {
                      "value": "",
                      "valueFrom": {
                        "secretKeyRef": {
                          "key": "",
                          "name": ""
                        }
                      }
                    },
                    "keystoreKeyPassword": {
                      "value": ""
                    },
                    "keystorePassword": {
                      "value": ""
                    },
                    "truststoreKey": {
                      "value": "",
                      "valueFrom": {
                        "secretKeyRef": {
                          "key": "",
                          "name": ""
                        }
                      }
                    },
                    "truststorePassword": {
                      "value": ""
                    }
                  },
                  "jmx": {
                    "enabled": false,
                    "keystoreKey": {
                      "value": "",
                      "valueFrom": {
                        "secretKeyRef": {
                          "key": "",
                          "name": ""
                        }
                      }
                    },
                    "keystorePassword": {
                      "value": ""
                    },
                    "truststoreKey": {
                      "value": "",
                      "valueFrom": {
                        "secretKeyRef": {
                          "key": "",
                          "name": ""
                        }
                      }
                    },
                    "truststorePassword": {
                      "value": ""
                    }
                  }
                }
              },
              "node": {
                "enabled": true,
                "limits": {
                  "cpu": "2",
                  "memory": "1024Mi"
                },
                "requests": {
                  "cpu": "200m",
                  "memory": "300Mi"
                }
              }
            },
            "namespaces": "",
            "role": "team",
            "type": "Openshift"
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/ca/uma-team-operator-bundle@sha256:7fd74845057b1e1b15239afcac12e02b77d33be96d50565786aabfbcbf260589",
      "bundle_path_digest": "sha256:7fd74845057b1e1b15239afcac12e02b77d33be96d50565786aabfbcbf260589",
      "capabilities": [
        "Seamless Upgrades"
      ],
      "channel_name": "stable",
      "creation_date": "2022-11-22T14:57:31.429000+00:00",
      "csv_description": "The **DX APM Universal Monitoring Agent Team For Multi-tenancy** installed by Application Administrator. Each application team that uses a DX APM tenant installs the UMA Team capability, specifying the namespaces the team owns or wants to monitor. UMA Team acts as a client, using a REST API to pull the namespace performance data from the UMA Admin deployment(DX APM Universal Monitoring Agent Team For Multi-tenancy). The UMA Team capability reports the application, pod, and node performance metrics only for the specified namespaces. This capability limits your application teams to seeing performance metrics only for their applications running in different environments on the tenant. Example environments include development, verification, pre-production, and production.\n\n**Prerequisite:**\n   1. The **DX APM Universal Monitoring Agent Team For Multi-tenancy** operator needs to be installed by the Cluster Administrator before UMA Team is installed.\n   2. UMA Team acts as a client, using a REST API to pull the namespace performance data from the UMA Admin deployment. So any network policy that restricts the communication to the Services running in the UMA Admin namespace, needs to be updated to allow the traffic from UMA team.\n\nFor more information: https://techdocs.broadcom.com/us/en/ca-enterprise-software/it-operations-management/dx-apm-agents/SaaS/Universal-Monitoring-Agent/Install-the-Universal-Monitoring-Agent/Deploy-UMA-in-a-Multi-tenant-Environment.html\n\n**Usage**:\n```\n apiVersion: ca.broadcom.com1/v1alpha1\n kind: UniversalMonitoringAgentsTeam\n metadata:\n   name: uma-monitor-team\n spec:\n   agentManager:\n     url: localhost:5001\n     credential: \"\"\n     tenantID: \"\"\n\n   # Name of the cluster.\n   clusterName: DevelopmentCluster\n   # Cluster type, either Kubernetes or OpenShift.\n   type: Kubernetes\n   role: team\n   namespaces: \"\"\n   adminNamespaceName: broadcom-aiops\n\n   monitor:\n     application:\n       jmx:\n         enabled: true\n       opentracing:\n         enabled: true\n         grpc:\n           hostport: \"\"\n         correlation:\n           enabled: false\n       zipkinTracing:\n         enabled: true\n         zipkinServer:\n           hostport:\n     events:\n       enabled: false\n     container:\n       dockerstats:\n         enabled: true\n     clusterPerformance:\n       dataReporter:\n         vertex:\n           ttl: 720\n           refreshInterval: 360\n         # The below properties define the resources for 'broadcom-monitor' deployment that does cluster and container monitoring.\n         limits:\n           cpu: \"2\"\n           # Assign memory resources in 'Mi'(mebibytes) units only.\n           memory: \"2048Mi\"\n         requests:\n           cpu: \"200m\"\n           # Assign memory resources in 'Mi'(mebibytes) units only.\n           memory: \"800Mi\"\n     kafka:\n       enabled: false\n       consumergroups:\n         metrics: true\n         filter: \"\"\n       jmx:\n         modules: \"kafka,jvm,memory\"\n         MBeans:\n           # A semi-colon separated list of managed bean object names. JMX metrics will be automatically reported for matching beans.\n           include: \"\"\n           # A semi-colon separated list of managed bean object names. Matching beans will be ignored for Kafka JMX metrics reporting. This property overrides \"include\" configuration.\n           exclude: \"\"\n       debug: false\n       ssl:\n         client:\n           enabled: false\n           truststoreKey:\n             value: \"\"\n             valueFrom:\n               secretKeyRef:\n                 name: \"\"\n                 key: \"\"\n           truststorePassword:\n             value: \"\"\n           keystoreKey:\n             value: \"\"\n             valueFrom:\n               secretKeyRef:\n                 name: \"\"\n                 key: \"\"\n           keystorePassword:\n             value: \"\"\n           keystoreKeyPassword:\n             value: \"\"\n           endpointIdentificationEnabled: false\n         jmx:\n           enabled: false\n           truststoreKey:\n             value: \"\"\n             valueFrom:\n               secretKeyRef:\n                 name: \"\"\n                 key: \"\"\n           truststorePassword:\n             value: \"\"\n           keystoreKey:\n             value: \"\"\n             valueFrom:\n               secretKeyRef:\n                 name: \"\"\n                 key: \"\"\n           keystorePassword:\n             value: \"\"\n       # The below properties define the resources for 'apm-kafka-monitor' deployment\n       limits:\n         cpu: \"2\"\n         # Assign memory resources in 'Mi'(mebibytes) units only.\n         memory: \"1024Mi\"\n       requests:\n         cpu: \"200m\"\n         # Assign memory resources in 'Mi'(mebibytes) units only.\n         memory: \"300Mi\"\n     node:\n       enabled: true\n       # The below properties define the resources for 'broadcom-monitor-host' deployment that does host monitoring.\n       limits:\n         cpu: \"2\"\n         # Assign memory resources in 'Mi'(mebibytes) units only.\n         memory: \"1024Mi\"\n       requests:\n         cpu: \"200m\"\n         # Assign memory resources in 'Mi'(mebibytes) units only.\n         memory: \"300Mi\"\n\n   globalDeployment:\n     # Node affinity rules for UMA Pods\n     # Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity\n     nodeAffinity: {}\n\n     # Node labels for UMA Pods\n     # Ref: https://kubernetes.io/docs/user-guide/node-selection/\n     nodeSelector: {}\n\n     # Additional tolerations for UMA pods\n     # https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/\n     # Default toleration used by UMA\n     tolerations:\n       - effect: \"NoSchedule\"\n         key: \"node-role.kubernetes.io/master\"\n         operator: \"\"\n         value: \"\"\n\n```\n",
      "csv_display_name": "DX APM Universal Monitoring Agent Team For Multi-tenancy",
      "csv_metadata_description": "DX APM Universal Monitoring Agent Team For Multi-tenancy acts as single deployment that automatically discovers and monitors cloud and container infrastructures and containerized application processes given namespaces.",
      "csv_name": "uma-team-operator.v2022.11.2-11",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": true,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-11-23T06:39:48.154000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.10",
      "organization": "certified-operators",
      "package": "uma-team-operator",
      "provided_apis": [
        {
          "group": "ca.broadcom.com1",
          "kind": "UniversalMonitoringAgentsTeam",
          "plural": "universalmonitoringagentsteams",
          "version": "v1alpha1"
        }
      ],
      "provider": "Broadcom, Inc.",
      "related_images": [
        {
          "digest": "sha256:152985e9cd88b827000dd95e7dae47d6bbdd583d13b6f4dab21ef24e585da706",
          "image": "registry.connect.redhat.com/ca/uma-operator@sha256:152985e9cd88b827000dd95e7dae47d6bbdd583d13b6f4dab21ef24e585da706",
          "name": "uma-operator-152985e9cd88b827000dd95e7dae47d6bbdd583d13b6f4dab21ef24e585da706-annotation"
        },
        {
          "digest": "sha256:152985e9cd88b827000dd95e7dae47d6bbdd583d13b6f4dab21ef24e585da706",
          "image": "registry.connect.redhat.com/ca/uma-operator@sha256:152985e9cd88b827000dd95e7dae47d6bbdd583d13b6f4dab21ef24e585da706",
          "name": "uma-operator"
        },
        {
          "digest": "sha256:6d6e7d9f0c662c0b24b7a3d0ac1d8526305ad17144c8e23b462965bce8e4d45e",
          "image": "registry.connect.redhat.com/ca/universalmonitoragent@sha256:6d6e7d9f0c662c0b24b7a3d0ac1d8526305ad17144c8e23b462965bce8e4d45e",
          "name": "uma"
        }
      ],
      "replaces": null,
      "skip_range": ">=2022.1.0 <2022.11.2",
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.10",
      "version": "2022.11.2-11",
      "version_original": "2022.11.2-11"
    },
    {
      "_id": "637d07e474e9f4cddfbf69d6",
      "alm_examples": [
        {
          "api_version": "couchdb.databases.cloud.ibm.com/v1",
          "kind": "CouchDBCluster",
          "metadata": {
            "name": "example-couchdbcluster"
          },
          "spec": {
            "cpu": "1",
            "disk": "1Gi",
            "environment": {
              "adminPassword": "changeme",
              "search": true
            },
            "memory": "1Gi",
            "size": 3,
            "storageClass": ""
          }
        },
        {
          "api_version": "couchdb.databases.cloud.ibm.com/v1",
          "kind": "FormationLock",
          "metadata": {
            "name": "example-formationlock"
          },
          "spec": {}
        },
        {
          "api_version": "couchdb.databases.cloud.ibm.com/v1",
          "kind": "Recipe",
          "metadata": {
            "name": "example-recipe"
          },
          "spec": {}
        },
        {
          "api_version": "couchdb.databases.cloud.ibm.com/v1",
          "kind": "RecipeTemplate",
          "metadata": {
            "name": "example-recipetemplate"
          },
          "spec": {}
        },
        {
          "api_version": "couchdb.databases.cloud.ibm.com/v1",
          "kind": "Backup",
          "metadata": {
            "name": "example-backup"
          },
          "spec": {}
        },
        {
          "api_version": "couchdb.databases.cloud.ibm.com/v1",
          "kind": "Bucket",
          "metadata": {
            "name": "example-bucket"
          },
          "spec": {}
        },
        {
          "api_version": "couchdb.databases.cloud.ibm.com/v1",
          "kind": "Formation",
          "metadata": {
            "name": "example-formation"
          },
          "spec": {}
        }
      ],
      "annotations": {
        "infrastructure_features": [
          "disconnected"
        ],
        "valid_subscription": []
      },
      "architectures": [
        "amd64"
      ],
      "bundle_path": "registry.connect.redhat.com/ibm/couchdb-operator-certified-bundle@sha256:5c28f847a3a9069bded0da0c8134254139c5f8c26799d84272b9d2eff7f58914",
      "bundle_path_digest": "sha256:5c28f847a3a9069bded0da0c8134254139c5f8c26799d84272b9d2eff7f58914",
      "capabilities": [
        "Seamless Upgrades"
      ],
      "channel_name": "v2.2",
      "creation_date": "2022-11-22T17:33:24.719000+00:00",
      "csv_description": "Apache CouchDB lets you access your data where you need it. The [Couch Replication Protocol](http://docs.couchdb.org/en/master/replication/protocol.html) is implemented in a variety of projects and products that span every imaginable computing environment from globally distributed server-clusters, over mobile phones to web browsers.\n\nStore your data safely, on your own servers, or with any leading cloud provider. Your web- and native applications love CouchDB, because it speaks JSON natively and supports binary data for all your data storage needs.\n\nThe [Couch Replication Protocol](http://docs.couchdb.org/en/master/replication/protocol.html) lets your data flow seamlessly between server clusters to mobile phones and web browsers, enabling a compelling [offline-first](http://offlinefirst.org/) user-experience while maintaining high performance and strong reliability. CouchDB comes with a developer-friendly query language, and optionally MapReduce for simple, efficient, and comprehensive data retrieval.\n\n### Operator for Apache CouchDB Features\n* Fully automated deployment and configuration of Apache CouchDB clusters.\n* Support single, multiple, or all namespace install modes.\n\n#### Security\n* TLS - TLS  is supported using Red Hat Service certificates or user-provided certificates.\n* Authentication - The parameter `require_valid_user` defaults to `true`, which means that no requests are allowed from anonymous users. Every request must be authenticated.\n* Authorization - Databases are initially accessible by Apache CouchDB admins only.\n\n#### High Availability\n* Nodes - Each database node in an Apache CouchDB cluster requires its own Kubernetes node. It's recommended that you run it with a minimum of three nodes for any production deployment.\n* Zones - The Apache CouchDB cluster database nodes are spread across available Kubernetes fault zones where available.\n* Replicas - The default configuration for each database is two shards (Q=2) and three shard copies (N=3), where each shard copy is deployed on a separate node in the cluster.\n\n### Reading and writing to Apache CouchDB\n\nThe Operator for Apache CouchDB automatically generates a Service. This can be accessed using port-forwarding e.g. `kubectl port-forward svc/my-couchdb-cluster 443:443 -u admin:mypassword`. Alternatively, configure an OpenShift Route to expose the service externally.\n\n### CouchDB 2.3.1 Update Instructions\nWhen the operator is updated to v2.0 or above any existing CouchDB 2.3.1 cluster will not be automatically upgraded to CouchDB 3. The user must manually trigger this process by removing the version statement `version: 2.3.1` in the CouchDBCluster CR for each CouchDB cluster\nThe operator will then perform a rolling update, with the new pods running CouchDB3.\nIt is advised that customers do this straightaway as CouchDB 2.3.1 is no longer supported.\n\n[Read the complete guide to using the Operator for Apache CouchDB](https://ibm.biz/BdfGQy)\n",
      "csv_display_name": "Operator for Apache CouchDB",
      "csv_metadata_description": "Apache CouchDB is a highly available NOSQL database for web and mobile applications\n",
      "csv_name": "couchdb-operator.v2.2.3",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": true,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-11-23T07:29:38.678000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.8",
      "organization": "certified-operators",
      "package": "couchdb-operator-certified",
      "provided_apis": [
        {
          "group": "couchdb.databases.cloud.ibm.com",
          "kind": "Formation",
          "plural": "formations",
          "version": "v1"
        },
        {
          "group": "couchdb.databases.cloud.ibm.com",
          "kind": "Recipe",
          "plural": "recipes",
          "version": "v1"
        },
        {
          "group": "couchdb.databases.cloud.ibm.com",
          "kind": "RecipeTemplate",
          "plural": "recipetemplates",
          "version": "v1"
        },
        {
          "group": "couchdb.databases.cloud.ibm.com",
          "kind": "Backup",
          "plural": "backups",
          "version": "v1"
        },
        {
          "group": "couchdb.databases.cloud.ibm.com",
          "kind": "Bucket",
          "plural": "buckets",
          "version": "v1"
        },
        {
          "group": "couchdb.databases.cloud.ibm.com",
          "kind": "CouchDBCluster",
          "plural": "couchdbclusters",
          "version": "v1"
        },
        {
          "group": "couchdb.databases.cloud.ibm.com",
          "kind": "FormationLock",
          "plural": "formationlocks",
          "version": "v1"
        }
      ],
      "provider": "IBM",
      "related_images": [
        {
          "digest": "sha256:1f8b0f31be95395c07436d92c89bf30eaca4f62cba805643d5562a64167f574a",
          "image": "registry.connect.redhat.com/ibm/couchdb3@sha256:1f8b0f31be95395c07436d92c89bf30eaca4f62cba805643d5562a64167f574a",
          "name": "COUCHDB3"
        },
        {
          "digest": "sha256:2115a1370e45bcdb9a59445ed78f4abc10d73ae090f07a8309941ba760b8a0bb",
          "image": "registry.connect.redhat.com/ibm/couchdb-operator-mgmt@sha256:2115a1370e45bcdb9a59445ed78f4abc10d73ae090f07a8309941ba760b8a0bb",
          "name": "COUCHDB-MGMT"
        },
        {
          "digest": "sha256:968c56be05781854ccc6068cc2f2f9f81dc7a34219210791810c2138d40a1cc3",
          "image": "registry.connect.redhat.com/ibm/couchdb-operator@sha256:968c56be05781854ccc6068cc2f2f9f81dc7a34219210791810c2138d40a1cc3",
          "name": "couchdb-operator-968c56be05781854ccc6068cc2f2f9f81dc7a34219210791810c2138d40a1cc3-annotation"
        },
        {
          "digest": "sha256:968c56be05781854ccc6068cc2f2f9f81dc7a34219210791810c2138d40a1cc3",
          "image": "registry.connect.redhat.com/ibm/couchdb-operator@sha256:968c56be05781854ccc6068cc2f2f9f81dc7a34219210791810c2138d40a1cc3",
          "name": "couchdb-operator"
        },
        {
          "digest": "sha256:1f8b0f31be95395c07436d92c89bf30eaca4f62cba805643d5562a64167f574a",
          "image": "registry.connect.redhat.com/ibm/couchdb3@sha256:1f8b0f31be95395c07436d92c89bf30eaca4f62cba805643d5562a64167f574a",
          "name": "couchdb3"
        },
        {
          "digest": "sha256:2115a1370e45bcdb9a59445ed78f4abc10d73ae090f07a8309941ba760b8a0bb",
          "image": "registry.connect.redhat.com/ibm/couchdb-operator-mgmt@sha256:2115a1370e45bcdb9a59445ed78f4abc10d73ae090f07a8309941ba760b8a0bb",
          "name": "mgmt"
        }
      ],
      "replaces": null,
      "skip_range": "<2.2.3",
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.8",
      "version": "2.2.3",
      "version_original": "2.2.3"
    },
    {
      "_id": "637d07e640d971f5448ab6cb",
      "alm_examples": [
        {
          "api_version": "couchdb.databases.cloud.ibm.com/v1",
          "kind": "CouchDBCluster",
          "metadata": {
            "name": "example-couchdbcluster"
          },
          "spec": {
            "cpu": "1",
            "disk": "1Gi",
            "environment": {
              "adminPassword": "changeme",
              "search": true
            },
            "memory": "1Gi",
            "size": 3,
            "storageClass": ""
          }
        },
        {
          "api_version": "couchdb.databases.cloud.ibm.com/v1",
          "kind": "FormationLock",
          "metadata": {
            "name": "example-formationlock"
          },
          "spec": {}
        },
        {
          "api_version": "couchdb.databases.cloud.ibm.com/v1",
          "kind": "Recipe",
          "metadata": {
            "name": "example-recipe"
          },
          "spec": {}
        },
        {
          "api_version": "couchdb.databases.cloud.ibm.com/v1",
          "kind": "RecipeTemplate",
          "metadata": {
            "name": "example-recipetemplate"
          },
          "spec": {}
        },
        {
          "api_version": "couchdb.databases.cloud.ibm.com/v1",
          "kind": "Backup",
          "metadata": {
            "name": "example-backup"
          },
          "spec": {}
        },
        {
          "api_version": "couchdb.databases.cloud.ibm.com/v1",
          "kind": "Bucket",
          "metadata": {
            "name": "example-bucket"
          },
          "spec": {}
        },
        {
          "api_version": "couchdb.databases.cloud.ibm.com/v1",
          "kind": "Formation",
          "metadata": {
            "name": "example-formation"
          },
          "spec": {}
        }
      ],
      "annotations": {
        "infrastructure_features": [
          "disconnected"
        ],
        "valid_subscription": []
      },
      "architectures": [
        "amd64"
      ],
      "bundle_path": "registry.connect.redhat.com/ibm/couchdb-operator-certified-bundle@sha256:5c28f847a3a9069bded0da0c8134254139c5f8c26799d84272b9d2eff7f58914",
      "bundle_path_digest": "sha256:5c28f847a3a9069bded0da0c8134254139c5f8c26799d84272b9d2eff7f58914",
      "capabilities": [
        "Seamless Upgrades"
      ],
      "channel_name": "stable",
      "creation_date": "2022-11-22T17:33:26.145000+00:00",
      "csv_description": "Apache CouchDB lets you access your data where you need it. The [Couch Replication Protocol](http://docs.couchdb.org/en/master/replication/protocol.html) is implemented in a variety of projects and products that span every imaginable computing environment from globally distributed server-clusters, over mobile phones to web browsers.\n\nStore your data safely, on your own servers, or with any leading cloud provider. Your web- and native applications love CouchDB, because it speaks JSON natively and supports binary data for all your data storage needs.\n\nThe [Couch Replication Protocol](http://docs.couchdb.org/en/master/replication/protocol.html) lets your data flow seamlessly between server clusters to mobile phones and web browsers, enabling a compelling [offline-first](http://offlinefirst.org/) user-experience while maintaining high performance and strong reliability. CouchDB comes with a developer-friendly query language, and optionally MapReduce for simple, efficient, and comprehensive data retrieval.\n\n### Operator for Apache CouchDB Features\n* Fully automated deployment and configuration of Apache CouchDB clusters.\n* Support single, multiple, or all namespace install modes.\n\n#### Security\n* TLS - TLS  is supported using Red Hat Service certificates or user-provided certificates.\n* Authentication - The parameter `require_valid_user` defaults to `true`, which means that no requests are allowed from anonymous users. Every request must be authenticated.\n* Authorization - Databases are initially accessible by Apache CouchDB admins only.\n\n#### High Availability\n* Nodes - Each database node in an Apache CouchDB cluster requires its own Kubernetes node. It's recommended that you run it with a minimum of three nodes for any production deployment.\n* Zones - The Apache CouchDB cluster database nodes are spread across available Kubernetes fault zones where available.\n* Replicas - The default configuration for each database is two shards (Q=2) and three shard copies (N=3), where each shard copy is deployed on a separate node in the cluster.\n\n### Reading and writing to Apache CouchDB\n\nThe Operator for Apache CouchDB automatically generates a Service. This can be accessed using port-forwarding e.g. `kubectl port-forward svc/my-couchdb-cluster 443:443 -u admin:mypassword`. Alternatively, configure an OpenShift Route to expose the service externally.\n\n### CouchDB 2.3.1 Update Instructions\nWhen the operator is updated to v2.0 or above any existing CouchDB 2.3.1 cluster will not be automatically upgraded to CouchDB 3. The user must manually trigger this process by removing the version statement `version: 2.3.1` in the CouchDBCluster CR for each CouchDB cluster\nThe operator will then perform a rolling update, with the new pods running CouchDB3.\nIt is advised that customers do this straightaway as CouchDB 2.3.1 is no longer supported.\n\n[Read the complete guide to using the Operator for Apache CouchDB](https://ibm.biz/BdfGQy)\n",
      "csv_display_name": "Operator for Apache CouchDB",
      "csv_metadata_description": "Apache CouchDB is a highly available NOSQL database for web and mobile applications\n",
      "csv_name": "couchdb-operator.v2.2.3",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": true,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": false,
      "last_update_date": "2022-11-23T07:43:30.368000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.8",
      "organization": "certified-operators",
      "package": "couchdb-operator-certified",
      "provided_apis": [
        {
          "group": "couchdb.databases.cloud.ibm.com",
          "kind": "Formation",
          "plural": "formations",
          "version": "v1"
        },
        {
          "group": "couchdb.databases.cloud.ibm.com",
          "kind": "Recipe",
          "plural": "recipes",
          "version": "v1"
        },
        {
          "group": "couchdb.databases.cloud.ibm.com",
          "kind": "RecipeTemplate",
          "plural": "recipetemplates",
          "version": "v1"
        },
        {
          "group": "couchdb.databases.cloud.ibm.com",
          "kind": "Backup",
          "plural": "backups",
          "version": "v1"
        },
        {
          "group": "couchdb.databases.cloud.ibm.com",
          "kind": "Bucket",
          "plural": "buckets",
          "version": "v1"
        },
        {
          "group": "couchdb.databases.cloud.ibm.com",
          "kind": "CouchDBCluster",
          "plural": "couchdbclusters",
          "version": "v1"
        },
        {
          "group": "couchdb.databases.cloud.ibm.com",
          "kind": "FormationLock",
          "plural": "formationlocks",
          "version": "v1"
        }
      ],
      "provider": "IBM",
      "related_images": [
        {
          "digest": "sha256:1f8b0f31be95395c07436d92c89bf30eaca4f62cba805643d5562a64167f574a",
          "image": "registry.connect.redhat.com/ibm/couchdb3@sha256:1f8b0f31be95395c07436d92c89bf30eaca4f62cba805643d5562a64167f574a",
          "name": "COUCHDB3"
        },
        {
          "digest": "sha256:2115a1370e45bcdb9a59445ed78f4abc10d73ae090f07a8309941ba760b8a0bb",
          "image": "registry.connect.redhat.com/ibm/couchdb-operator-mgmt@sha256:2115a1370e45bcdb9a59445ed78f4abc10d73ae090f07a8309941ba760b8a0bb",
          "name": "COUCHDB-MGMT"
        },
        {
          "digest": "sha256:968c56be05781854ccc6068cc2f2f9f81dc7a34219210791810c2138d40a1cc3",
          "image": "registry.connect.redhat.com/ibm/couchdb-operator@sha256:968c56be05781854ccc6068cc2f2f9f81dc7a34219210791810c2138d40a1cc3",
          "name": "couchdb-operator-968c56be05781854ccc6068cc2f2f9f81dc7a34219210791810c2138d40a1cc3-annotation"
        },
        {
          "digest": "sha256:968c56be05781854ccc6068cc2f2f9f81dc7a34219210791810c2138d40a1cc3",
          "image": "registry.connect.redhat.com/ibm/couchdb-operator@sha256:968c56be05781854ccc6068cc2f2f9f81dc7a34219210791810c2138d40a1cc3",
          "name": "couchdb-operator"
        },
        {
          "digest": "sha256:1f8b0f31be95395c07436d92c89bf30eaca4f62cba805643d5562a64167f574a",
          "image": "registry.connect.redhat.com/ibm/couchdb3@sha256:1f8b0f31be95395c07436d92c89bf30eaca4f62cba805643d5562a64167f574a",
          "name": "couchdb3"
        },
        {
          "digest": "sha256:2115a1370e45bcdb9a59445ed78f4abc10d73ae090f07a8309941ba760b8a0bb",
          "image": "registry.connect.redhat.com/ibm/couchdb-operator-mgmt@sha256:2115a1370e45bcdb9a59445ed78f4abc10d73ae090f07a8309941ba760b8a0bb",
          "name": "mgmt"
        }
      ],
      "replaces": null,
      "skip_range": "<2.2.3",
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.8",
      "version": "2.2.3",
      "version_original": "2.2.3"
    },
    {
      "_id": "637d0897c96e3e7d20bbaf15",
      "alm_examples": [
        {
          "api_version": "couchdb.databases.cloud.ibm.com/v1",
          "kind": "CouchDBCluster",
          "metadata": {
            "name": "example-couchdbcluster"
          },
          "spec": {
            "cpu": "1",
            "disk": "1Gi",
            "environment": {
              "adminPassword": "changeme",
              "search": true
            },
            "memory": "1Gi",
            "size": 3,
            "storageClass": ""
          }
        },
        {
          "api_version": "couchdb.databases.cloud.ibm.com/v1",
          "kind": "FormationLock",
          "metadata": {
            "name": "example-formationlock"
          },
          "spec": {}
        },
        {
          "api_version": "couchdb.databases.cloud.ibm.com/v1",
          "kind": "Recipe",
          "metadata": {
            "name": "example-recipe"
          },
          "spec": {}
        },
        {
          "api_version": "couchdb.databases.cloud.ibm.com/v1",
          "kind": "RecipeTemplate",
          "metadata": {
            "name": "example-recipetemplate"
          },
          "spec": {}
        },
        {
          "api_version": "couchdb.databases.cloud.ibm.com/v1",
          "kind": "Backup",
          "metadata": {
            "name": "example-backup"
          },
          "spec": {}
        },
        {
          "api_version": "couchdb.databases.cloud.ibm.com/v1",
          "kind": "Bucket",
          "metadata": {
            "name": "example-bucket"
          },
          "spec": {}
        },
        {
          "api_version": "couchdb.databases.cloud.ibm.com/v1",
          "kind": "Formation",
          "metadata": {
            "name": "example-formation"
          },
          "spec": {}
        }
      ],
      "annotations": {
        "infrastructure_features": [
          "disconnected"
        ],
        "valid_subscription": []
      },
      "architectures": [
        "amd64"
      ],
      "bundle_path": "registry.connect.redhat.com/ibm/couchdb-operator-certified-bundle@sha256:5c28f847a3a9069bded0da0c8134254139c5f8c26799d84272b9d2eff7f58914",
      "bundle_path_digest": "sha256:5c28f847a3a9069bded0da0c8134254139c5f8c26799d84272b9d2eff7f58914",
      "capabilities": [
        "Seamless Upgrades"
      ],
      "channel_name": "stable",
      "creation_date": "2022-11-22T17:36:23.122000+00:00",
      "csv_description": "Apache CouchDB lets you access your data where you need it. The [Couch Replication Protocol](http://docs.couchdb.org/en/master/replication/protocol.html) is implemented in a variety of projects and products that span every imaginable computing environment from globally distributed server-clusters, over mobile phones to web browsers.\n\nStore your data safely, on your own servers, or with any leading cloud provider. Your web- and native applications love CouchDB, because it speaks JSON natively and supports binary data for all your data storage needs.\n\nThe [Couch Replication Protocol](http://docs.couchdb.org/en/master/replication/protocol.html) lets your data flow seamlessly between server clusters to mobile phones and web browsers, enabling a compelling [offline-first](http://offlinefirst.org/) user-experience while maintaining high performance and strong reliability. CouchDB comes with a developer-friendly query language, and optionally MapReduce for simple, efficient, and comprehensive data retrieval.\n\n### Operator for Apache CouchDB Features\n* Fully automated deployment and configuration of Apache CouchDB clusters.\n* Support single, multiple, or all namespace install modes.\n\n#### Security\n* TLS - TLS  is supported using Red Hat Service certificates or user-provided certificates.\n* Authentication - The parameter `require_valid_user` defaults to `true`, which means that no requests are allowed from anonymous users. Every request must be authenticated.\n* Authorization - Databases are initially accessible by Apache CouchDB admins only.\n\n#### High Availability\n* Nodes - Each database node in an Apache CouchDB cluster requires its own Kubernetes node. It's recommended that you run it with a minimum of three nodes for any production deployment.\n* Zones - The Apache CouchDB cluster database nodes are spread across available Kubernetes fault zones where available.\n* Replicas - The default configuration for each database is two shards (Q=2) and three shard copies (N=3), where each shard copy is deployed on a separate node in the cluster.\n\n### Reading and writing to Apache CouchDB\n\nThe Operator for Apache CouchDB automatically generates a Service. This can be accessed using port-forwarding e.g. `kubectl port-forward svc/my-couchdb-cluster 443:443 -u admin:mypassword`. Alternatively, configure an OpenShift Route to expose the service externally.\n\n### CouchDB 2.3.1 Update Instructions\nWhen the operator is updated to v2.0 or above any existing CouchDB 2.3.1 cluster will not be automatically upgraded to CouchDB 3. The user must manually trigger this process by removing the version statement `version: 2.3.1` in the CouchDBCluster CR for each CouchDB cluster\nThe operator will then perform a rolling update, with the new pods running CouchDB3.\nIt is advised that customers do this straightaway as CouchDB 2.3.1 is no longer supported.\n\n[Read the complete guide to using the Operator for Apache CouchDB](https://ibm.biz/BdfGQy)\n",
      "csv_display_name": "Operator for Apache CouchDB",
      "csv_metadata_description": "Apache CouchDB is a highly available NOSQL database for web and mobile applications\n",
      "csv_name": "couchdb-operator.v2.2.3",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": true,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": false,
      "last_update_date": "2022-11-23T08:24:46.196000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.10",
      "organization": "certified-operators",
      "package": "couchdb-operator-certified",
      "provided_apis": [
        {
          "group": "couchdb.databases.cloud.ibm.com",
          "kind": "Recipe",
          "plural": "recipes",
          "version": "v1"
        },
        {
          "group": "couchdb.databases.cloud.ibm.com",
          "kind": "RecipeTemplate",
          "plural": "recipetemplates",
          "version": "v1"
        },
        {
          "group": "couchdb.databases.cloud.ibm.com",
          "kind": "Backup",
          "plural": "backups",
          "version": "v1"
        },
        {
          "group": "couchdb.databases.cloud.ibm.com",
          "kind": "Bucket",
          "plural": "buckets",
          "version": "v1"
        },
        {
          "group": "couchdb.databases.cloud.ibm.com",
          "kind": "CouchDBCluster",
          "plural": "couchdbclusters",
          "version": "v1"
        },
        {
          "group": "couchdb.databases.cloud.ibm.com",
          "kind": "FormationLock",
          "plural": "formationlocks",
          "version": "v1"
        },
        {
          "group": "couchdb.databases.cloud.ibm.com",
          "kind": "Formation",
          "plural": "formations",
          "version": "v1"
        }
      ],
      "provider": "IBM",
      "related_images": [
        {
          "digest": "sha256:1f8b0f31be95395c07436d92c89bf30eaca4f62cba805643d5562a64167f574a",
          "image": "registry.connect.redhat.com/ibm/couchdb3@sha256:1f8b0f31be95395c07436d92c89bf30eaca4f62cba805643d5562a64167f574a",
          "name": "COUCHDB3"
        },
        {
          "digest": "sha256:2115a1370e45bcdb9a59445ed78f4abc10d73ae090f07a8309941ba760b8a0bb",
          "image": "registry.connect.redhat.com/ibm/couchdb-operator-mgmt@sha256:2115a1370e45bcdb9a59445ed78f4abc10d73ae090f07a8309941ba760b8a0bb",
          "name": "COUCHDB-MGMT"
        },
        {
          "digest": "sha256:968c56be05781854ccc6068cc2f2f9f81dc7a34219210791810c2138d40a1cc3",
          "image": "registry.connect.redhat.com/ibm/couchdb-operator@sha256:968c56be05781854ccc6068cc2f2f9f81dc7a34219210791810c2138d40a1cc3",
          "name": "couchdb-operator-968c56be05781854ccc6068cc2f2f9f81dc7a34219210791810c2138d40a1cc3-annotation"
        },
        {
          "digest": "sha256:968c56be05781854ccc6068cc2f2f9f81dc7a34219210791810c2138d40a1cc3",
          "image": "registry.connect.redhat.com/ibm/couchdb-operator@sha256:968c56be05781854ccc6068cc2f2f9f81dc7a34219210791810c2138d40a1cc3",
          "name": "couchdb-operator"
        },
        {
          "digest": "sha256:1f8b0f31be95395c07436d92c89bf30eaca4f62cba805643d5562a64167f574a",
          "image": "registry.connect.redhat.com/ibm/couchdb3@sha256:1f8b0f31be95395c07436d92c89bf30eaca4f62cba805643d5562a64167f574a",
          "name": "couchdb3"
        },
        {
          "digest": "sha256:2115a1370e45bcdb9a59445ed78f4abc10d73ae090f07a8309941ba760b8a0bb",
          "image": "registry.connect.redhat.com/ibm/couchdb-operator-mgmt@sha256:2115a1370e45bcdb9a59445ed78f4abc10d73ae090f07a8309941ba760b8a0bb",
          "name": "mgmt"
        }
      ],
      "replaces": null,
      "skip_range": "<2.2.3",
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.10",
      "version": "2.2.3",
      "version_original": "2.2.3"
    },
    {
      "_id": "637d089974e9f4cddfbf6cb6",
      "alm_examples": [
        {
          "api_version": "couchdb.databases.cloud.ibm.com/v1",
          "kind": "CouchDBCluster",
          "metadata": {
            "name": "example-couchdbcluster"
          },
          "spec": {
            "cpu": "1",
            "disk": "1Gi",
            "environment": {
              "adminPassword": "changeme",
              "search": true
            },
            "memory": "1Gi",
            "size": 3,
            "storageClass": ""
          }
        },
        {
          "api_version": "couchdb.databases.cloud.ibm.com/v1",
          "kind": "FormationLock",
          "metadata": {
            "name": "example-formationlock"
          },
          "spec": {}
        },
        {
          "api_version": "couchdb.databases.cloud.ibm.com/v1",
          "kind": "Recipe",
          "metadata": {
            "name": "example-recipe"
          },
          "spec": {}
        },
        {
          "api_version": "couchdb.databases.cloud.ibm.com/v1",
          "kind": "RecipeTemplate",
          "metadata": {
            "name": "example-recipetemplate"
          },
          "spec": {}
        },
        {
          "api_version": "couchdb.databases.cloud.ibm.com/v1",
          "kind": "Backup",
          "metadata": {
            "name": "example-backup"
          },
          "spec": {}
        },
        {
          "api_version": "couchdb.databases.cloud.ibm.com/v1",
          "kind": "Bucket",
          "metadata": {
            "name": "example-bucket"
          },
          "spec": {}
        },
        {
          "api_version": "couchdb.databases.cloud.ibm.com/v1",
          "kind": "Formation",
          "metadata": {
            "name": "example-formation"
          },
          "spec": {}
        }
      ],
      "annotations": {
        "infrastructure_features": [
          "disconnected"
        ],
        "valid_subscription": []
      },
      "architectures": [
        "amd64"
      ],
      "bundle_path": "registry.connect.redhat.com/ibm/couchdb-operator-certified-bundle@sha256:5c28f847a3a9069bded0da0c8134254139c5f8c26799d84272b9d2eff7f58914",
      "bundle_path_digest": "sha256:5c28f847a3a9069bded0da0c8134254139c5f8c26799d84272b9d2eff7f58914",
      "capabilities": [
        "Seamless Upgrades"
      ],
      "channel_name": "v2.2",
      "creation_date": "2022-11-22T17:36:25.127000+00:00",
      "csv_description": "Apache CouchDB lets you access your data where you need it. The [Couch Replication Protocol](http://docs.couchdb.org/en/master/replication/protocol.html) is implemented in a variety of projects and products that span every imaginable computing environment from globally distributed server-clusters, over mobile phones to web browsers.\n\nStore your data safely, on your own servers, or with any leading cloud provider. Your web- and native applications love CouchDB, because it speaks JSON natively and supports binary data for all your data storage needs.\n\nThe [Couch Replication Protocol](http://docs.couchdb.org/en/master/replication/protocol.html) lets your data flow seamlessly between server clusters to mobile phones and web browsers, enabling a compelling [offline-first](http://offlinefirst.org/) user-experience while maintaining high performance and strong reliability. CouchDB comes with a developer-friendly query language, and optionally MapReduce for simple, efficient, and comprehensive data retrieval.\n\n### Operator for Apache CouchDB Features\n* Fully automated deployment and configuration of Apache CouchDB clusters.\n* Support single, multiple, or all namespace install modes.\n\n#### Security\n* TLS - TLS  is supported using Red Hat Service certificates or user-provided certificates.\n* Authentication - The parameter `require_valid_user` defaults to `true`, which means that no requests are allowed from anonymous users. Every request must be authenticated.\n* Authorization - Databases are initially accessible by Apache CouchDB admins only.\n\n#### High Availability\n* Nodes - Each database node in an Apache CouchDB cluster requires its own Kubernetes node. It's recommended that you run it with a minimum of three nodes for any production deployment.\n* Zones - The Apache CouchDB cluster database nodes are spread across available Kubernetes fault zones where available.\n* Replicas - The default configuration for each database is two shards (Q=2) and three shard copies (N=3), where each shard copy is deployed on a separate node in the cluster.\n\n### Reading and writing to Apache CouchDB\n\nThe Operator for Apache CouchDB automatically generates a Service. This can be accessed using port-forwarding e.g. `kubectl port-forward svc/my-couchdb-cluster 443:443 -u admin:mypassword`. Alternatively, configure an OpenShift Route to expose the service externally.\n\n### CouchDB 2.3.1 Update Instructions\nWhen the operator is updated to v2.0 or above any existing CouchDB 2.3.1 cluster will not be automatically upgraded to CouchDB 3. The user must manually trigger this process by removing the version statement `version: 2.3.1` in the CouchDBCluster CR for each CouchDB cluster\nThe operator will then perform a rolling update, with the new pods running CouchDB3.\nIt is advised that customers do this straightaway as CouchDB 2.3.1 is no longer supported.\n\n[Read the complete guide to using the Operator for Apache CouchDB](https://ibm.biz/BdfGQy)\n",
      "csv_display_name": "Operator for Apache CouchDB",
      "csv_metadata_description": "Apache CouchDB is a highly available NOSQL database for web and mobile applications\n",
      "csv_name": "couchdb-operator.v2.2.3",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": true,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-11-23T08:24:52.311000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.10",
      "organization": "certified-operators",
      "package": "couchdb-operator-certified",
      "provided_apis": [
        {
          "group": "couchdb.databases.cloud.ibm.com",
          "kind": "Recipe",
          "plural": "recipes",
          "version": "v1"
        },
        {
          "group": "couchdb.databases.cloud.ibm.com",
          "kind": "RecipeTemplate",
          "plural": "recipetemplates",
          "version": "v1"
        },
        {
          "group": "couchdb.databases.cloud.ibm.com",
          "kind": "Backup",
          "plural": "backups",
          "version": "v1"
        },
        {
          "group": "couchdb.databases.cloud.ibm.com",
          "kind": "Bucket",
          "plural": "buckets",
          "version": "v1"
        },
        {
          "group": "couchdb.databases.cloud.ibm.com",
          "kind": "CouchDBCluster",
          "plural": "couchdbclusters",
          "version": "v1"
        },
        {
          "group": "couchdb.databases.cloud.ibm.com",
          "kind": "FormationLock",
          "plural": "formationlocks",
          "version": "v1"
        },
        {
          "group": "couchdb.databases.cloud.ibm.com",
          "kind": "Formation",
          "plural": "formations",
          "version": "v1"
        }
      ],
      "provider": "IBM",
      "related_images": [
        {
          "digest": "sha256:1f8b0f31be95395c07436d92c89bf30eaca4f62cba805643d5562a64167f574a",
          "image": "registry.connect.redhat.com/ibm/couchdb3@sha256:1f8b0f31be95395c07436d92c89bf30eaca4f62cba805643d5562a64167f574a",
          "name": "COUCHDB3"
        },
        {
          "digest": "sha256:2115a1370e45bcdb9a59445ed78f4abc10d73ae090f07a8309941ba760b8a0bb",
          "image": "registry.connect.redhat.com/ibm/couchdb-operator-mgmt@sha256:2115a1370e45bcdb9a59445ed78f4abc10d73ae090f07a8309941ba760b8a0bb",
          "name": "COUCHDB-MGMT"
        },
        {
          "digest": "sha256:968c56be05781854ccc6068cc2f2f9f81dc7a34219210791810c2138d40a1cc3",
          "image": "registry.connect.redhat.com/ibm/couchdb-operator@sha256:968c56be05781854ccc6068cc2f2f9f81dc7a34219210791810c2138d40a1cc3",
          "name": "couchdb-operator-968c56be05781854ccc6068cc2f2f9f81dc7a34219210791810c2138d40a1cc3-annotation"
        },
        {
          "digest": "sha256:968c56be05781854ccc6068cc2f2f9f81dc7a34219210791810c2138d40a1cc3",
          "image": "registry.connect.redhat.com/ibm/couchdb-operator@sha256:968c56be05781854ccc6068cc2f2f9f81dc7a34219210791810c2138d40a1cc3",
          "name": "couchdb-operator"
        },
        {
          "digest": "sha256:1f8b0f31be95395c07436d92c89bf30eaca4f62cba805643d5562a64167f574a",
          "image": "registry.connect.redhat.com/ibm/couchdb3@sha256:1f8b0f31be95395c07436d92c89bf30eaca4f62cba805643d5562a64167f574a",
          "name": "couchdb3"
        },
        {
          "digest": "sha256:2115a1370e45bcdb9a59445ed78f4abc10d73ae090f07a8309941ba760b8a0bb",
          "image": "registry.connect.redhat.com/ibm/couchdb-operator-mgmt@sha256:2115a1370e45bcdb9a59445ed78f4abc10d73ae090f07a8309941ba760b8a0bb",
          "name": "mgmt"
        }
      ],
      "replaces": null,
      "skip_range": "<2.2.3",
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.10",
      "version": "2.2.3",
      "version_original": "2.2.3"
    },
    {
      "_id": "637d08b25f23340823b2d9d0",
      "alm_examples": [
        {
          "api_version": "couchdb.databases.cloud.ibm.com/v1",
          "kind": "CouchDBCluster",
          "metadata": {
            "name": "example-couchdbcluster"
          },
          "spec": {
            "cpu": "1",
            "disk": "1Gi",
            "environment": {
              "adminPassword": "changeme",
              "search": true
            },
            "memory": "1Gi",
            "size": 3,
            "storageClass": ""
          }
        },
        {
          "api_version": "couchdb.databases.cloud.ibm.com/v1",
          "kind": "FormationLock",
          "metadata": {
            "name": "example-formationlock"
          },
          "spec": {}
        },
        {
          "api_version": "couchdb.databases.cloud.ibm.com/v1",
          "kind": "Recipe",
          "metadata": {
            "name": "example-recipe"
          },
          "spec": {}
        },
        {
          "api_version": "couchdb.databases.cloud.ibm.com/v1",
          "kind": "RecipeTemplate",
          "metadata": {
            "name": "example-recipetemplate"
          },
          "spec": {}
        },
        {
          "api_version": "couchdb.databases.cloud.ibm.com/v1",
          "kind": "Backup",
          "metadata": {
            "name": "example-backup"
          },
          "spec": {}
        },
        {
          "api_version": "couchdb.databases.cloud.ibm.com/v1",
          "kind": "Bucket",
          "metadata": {
            "name": "example-bucket"
          },
          "spec": {}
        },
        {
          "api_version": "couchdb.databases.cloud.ibm.com/v1",
          "kind": "Formation",
          "metadata": {
            "name": "example-formation"
          },
          "spec": {}
        }
      ],
      "annotations": {
        "infrastructure_features": [
          "disconnected"
        ],
        "valid_subscription": []
      },
      "architectures": [
        "amd64"
      ],
      "bundle_path": "registry.connect.redhat.com/ibm/couchdb-operator-certified-bundle@sha256:5c28f847a3a9069bded0da0c8134254139c5f8c26799d84272b9d2eff7f58914",
      "bundle_path_digest": "sha256:5c28f847a3a9069bded0da0c8134254139c5f8c26799d84272b9d2eff7f58914",
      "capabilities": [
        "Seamless Upgrades"
      ],
      "channel_name": "stable",
      "creation_date": "2022-11-22T17:36:50.776000+00:00",
      "csv_description": "Apache CouchDB lets you access your data where you need it. The [Couch Replication Protocol](http://docs.couchdb.org/en/master/replication/protocol.html) is implemented in a variety of projects and products that span every imaginable computing environment from globally distributed server-clusters, over mobile phones to web browsers.\n\nStore your data safely, on your own servers, or with any leading cloud provider. Your web- and native applications love CouchDB, because it speaks JSON natively and supports binary data for all your data storage needs.\n\nThe [Couch Replication Protocol](http://docs.couchdb.org/en/master/replication/protocol.html) lets your data flow seamlessly between server clusters to mobile phones and web browsers, enabling a compelling [offline-first](http://offlinefirst.org/) user-experience while maintaining high performance and strong reliability. CouchDB comes with a developer-friendly query language, and optionally MapReduce for simple, efficient, and comprehensive data retrieval.\n\n### Operator for Apache CouchDB Features\n* Fully automated deployment and configuration of Apache CouchDB clusters.\n* Support single, multiple, or all namespace install modes.\n\n#### Security\n* TLS - TLS  is supported using Red Hat Service certificates or user-provided certificates.\n* Authentication - The parameter `require_valid_user` defaults to `true`, which means that no requests are allowed from anonymous users. Every request must be authenticated.\n* Authorization - Databases are initially accessible by Apache CouchDB admins only.\n\n#### High Availability\n* Nodes - Each database node in an Apache CouchDB cluster requires its own Kubernetes node. It's recommended that you run it with a minimum of three nodes for any production deployment.\n* Zones - The Apache CouchDB cluster database nodes are spread across available Kubernetes fault zones where available.\n* Replicas - The default configuration for each database is two shards (Q=2) and three shard copies (N=3), where each shard copy is deployed on a separate node in the cluster.\n\n### Reading and writing to Apache CouchDB\n\nThe Operator for Apache CouchDB automatically generates a Service. This can be accessed using port-forwarding e.g. `kubectl port-forward svc/my-couchdb-cluster 443:443 -u admin:mypassword`. Alternatively, configure an OpenShift Route to expose the service externally.\n\n### CouchDB 2.3.1 Update Instructions\nWhen the operator is updated to v2.0 or above any existing CouchDB 2.3.1 cluster will not be automatically upgraded to CouchDB 3. The user must manually trigger this process by removing the version statement `version: 2.3.1` in the CouchDBCluster CR for each CouchDB cluster\nThe operator will then perform a rolling update, with the new pods running CouchDB3.\nIt is advised that customers do this straightaway as CouchDB 2.3.1 is no longer supported.\n\n[Read the complete guide to using the Operator for Apache CouchDB](https://ibm.biz/BdfGQy)\n",
      "csv_display_name": "Operator for Apache CouchDB",
      "csv_metadata_description": "Apache CouchDB is a highly available NOSQL database for web and mobile applications\n",
      "csv_name": "couchdb-operator.v2.2.3",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": true,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": false,
      "last_update_date": "2022-11-23T07:10:01.216000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.9",
      "organization": "certified-operators",
      "package": "couchdb-operator-certified",
      "provided_apis": [
        {
          "group": "couchdb.databases.cloud.ibm.com",
          "kind": "RecipeTemplate",
          "plural": "recipetemplates",
          "version": "v1"
        },
        {
          "group": "couchdb.databases.cloud.ibm.com",
          "kind": "Backup",
          "plural": "backups",
          "version": "v1"
        },
        {
          "group": "couchdb.databases.cloud.ibm.com",
          "kind": "Bucket",
          "plural": "buckets",
          "version": "v1"
        },
        {
          "group": "couchdb.databases.cloud.ibm.com",
          "kind": "CouchDBCluster",
          "plural": "couchdbclusters",
          "version": "v1"
        },
        {
          "group": "couchdb.databases.cloud.ibm.com",
          "kind": "FormationLock",
          "plural": "formationlocks",
          "version": "v1"
        },
        {
          "group": "couchdb.databases.cloud.ibm.com",
          "kind": "Formation",
          "plural": "formations",
          "version": "v1"
        },
        {
          "group": "couchdb.databases.cloud.ibm.com",
          "kind": "Recipe",
          "plural": "recipes",
          "version": "v1"
        }
      ],
      "provider": "IBM",
      "related_images": [
        {
          "digest": "sha256:1f8b0f31be95395c07436d92c89bf30eaca4f62cba805643d5562a64167f574a",
          "image": "registry.connect.redhat.com/ibm/couchdb3@sha256:1f8b0f31be95395c07436d92c89bf30eaca4f62cba805643d5562a64167f574a",
          "name": "COUCHDB3"
        },
        {
          "digest": "sha256:2115a1370e45bcdb9a59445ed78f4abc10d73ae090f07a8309941ba760b8a0bb",
          "image": "registry.connect.redhat.com/ibm/couchdb-operator-mgmt@sha256:2115a1370e45bcdb9a59445ed78f4abc10d73ae090f07a8309941ba760b8a0bb",
          "name": "COUCHDB-MGMT"
        },
        {
          "digest": "sha256:968c56be05781854ccc6068cc2f2f9f81dc7a34219210791810c2138d40a1cc3",
          "image": "registry.connect.redhat.com/ibm/couchdb-operator@sha256:968c56be05781854ccc6068cc2f2f9f81dc7a34219210791810c2138d40a1cc3",
          "name": "couchdb-operator-968c56be05781854ccc6068cc2f2f9f81dc7a34219210791810c2138d40a1cc3-annotation"
        },
        {
          "digest": "sha256:968c56be05781854ccc6068cc2f2f9f81dc7a34219210791810c2138d40a1cc3",
          "image": "registry.connect.redhat.com/ibm/couchdb-operator@sha256:968c56be05781854ccc6068cc2f2f9f81dc7a34219210791810c2138d40a1cc3",
          "name": "couchdb-operator"
        },
        {
          "digest": "sha256:1f8b0f31be95395c07436d92c89bf30eaca4f62cba805643d5562a64167f574a",
          "image": "registry.connect.redhat.com/ibm/couchdb3@sha256:1f8b0f31be95395c07436d92c89bf30eaca4f62cba805643d5562a64167f574a",
          "name": "couchdb3"
        },
        {
          "digest": "sha256:2115a1370e45bcdb9a59445ed78f4abc10d73ae090f07a8309941ba760b8a0bb",
          "image": "registry.connect.redhat.com/ibm/couchdb-operator-mgmt@sha256:2115a1370e45bcdb9a59445ed78f4abc10d73ae090f07a8309941ba760b8a0bb",
          "name": "mgmt"
        }
      ],
      "replaces": null,
      "skip_range": "<2.2.3",
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.9",
      "version": "2.2.3",
      "version_original": "2.2.3"
    },
    {
      "_id": "637d08b4884420e0c62a9f50",
      "alm_examples": [
        {
          "api_version": "couchdb.databases.cloud.ibm.com/v1",
          "kind": "CouchDBCluster",
          "metadata": {
            "name": "example-couchdbcluster"
          },
          "spec": {
            "cpu": "1",
            "disk": "1Gi",
            "environment": {
              "adminPassword": "changeme",
              "search": true
            },
            "memory": "1Gi",
            "size": 3,
            "storageClass": ""
          }
        },
        {
          "api_version": "couchdb.databases.cloud.ibm.com/v1",
          "kind": "FormationLock",
          "metadata": {
            "name": "example-formationlock"
          },
          "spec": {}
        },
        {
          "api_version": "couchdb.databases.cloud.ibm.com/v1",
          "kind": "Recipe",
          "metadata": {
            "name": "example-recipe"
          },
          "spec": {}
        },
        {
          "api_version": "couchdb.databases.cloud.ibm.com/v1",
          "kind": "RecipeTemplate",
          "metadata": {
            "name": "example-recipetemplate"
          },
          "spec": {}
        },
        {
          "api_version": "couchdb.databases.cloud.ibm.com/v1",
          "kind": "Backup",
          "metadata": {
            "name": "example-backup"
          },
          "spec": {}
        },
        {
          "api_version": "couchdb.databases.cloud.ibm.com/v1",
          "kind": "Bucket",
          "metadata": {
            "name": "example-bucket"
          },
          "spec": {}
        },
        {
          "api_version": "couchdb.databases.cloud.ibm.com/v1",
          "kind": "Formation",
          "metadata": {
            "name": "example-formation"
          },
          "spec": {}
        }
      ],
      "annotations": {
        "infrastructure_features": [
          "disconnected"
        ],
        "valid_subscription": []
      },
      "architectures": [
        "amd64"
      ],
      "bundle_path": "registry.connect.redhat.com/ibm/couchdb-operator-certified-bundle@sha256:5c28f847a3a9069bded0da0c8134254139c5f8c26799d84272b9d2eff7f58914",
      "bundle_path_digest": "sha256:5c28f847a3a9069bded0da0c8134254139c5f8c26799d84272b9d2eff7f58914",
      "capabilities": [
        "Seamless Upgrades"
      ],
      "channel_name": "v2.2",
      "creation_date": "2022-11-22T17:36:52.764000+00:00",
      "csv_description": "Apache CouchDB lets you access your data where you need it. The [Couch Replication Protocol](http://docs.couchdb.org/en/master/replication/protocol.html) is implemented in a variety of projects and products that span every imaginable computing environment from globally distributed server-clusters, over mobile phones to web browsers.\n\nStore your data safely, on your own servers, or with any leading cloud provider. Your web- and native applications love CouchDB, because it speaks JSON natively and supports binary data for all your data storage needs.\n\nThe [Couch Replication Protocol](http://docs.couchdb.org/en/master/replication/protocol.html) lets your data flow seamlessly between server clusters to mobile phones and web browsers, enabling a compelling [offline-first](http://offlinefirst.org/) user-experience while maintaining high performance and strong reliability. CouchDB comes with a developer-friendly query language, and optionally MapReduce for simple, efficient, and comprehensive data retrieval.\n\n### Operator for Apache CouchDB Features\n* Fully automated deployment and configuration of Apache CouchDB clusters.\n* Support single, multiple, or all namespace install modes.\n\n#### Security\n* TLS - TLS  is supported using Red Hat Service certificates or user-provided certificates.\n* Authentication - The parameter `require_valid_user` defaults to `true`, which means that no requests are allowed from anonymous users. Every request must be authenticated.\n* Authorization - Databases are initially accessible by Apache CouchDB admins only.\n\n#### High Availability\n* Nodes - Each database node in an Apache CouchDB cluster requires its own Kubernetes node. It's recommended that you run it with a minimum of three nodes for any production deployment.\n* Zones - The Apache CouchDB cluster database nodes are spread across available Kubernetes fault zones where available.\n* Replicas - The default configuration for each database is two shards (Q=2) and three shard copies (N=3), where each shard copy is deployed on a separate node in the cluster.\n\n### Reading and writing to Apache CouchDB\n\nThe Operator for Apache CouchDB automatically generates a Service. This can be accessed using port-forwarding e.g. `kubectl port-forward svc/my-couchdb-cluster 443:443 -u admin:mypassword`. Alternatively, configure an OpenShift Route to expose the service externally.\n\n### CouchDB 2.3.1 Update Instructions\nWhen the operator is updated to v2.0 or above any existing CouchDB 2.3.1 cluster will not be automatically upgraded to CouchDB 3. The user must manually trigger this process by removing the version statement `version: 2.3.1` in the CouchDBCluster CR for each CouchDB cluster\nThe operator will then perform a rolling update, with the new pods running CouchDB3.\nIt is advised that customers do this straightaway as CouchDB 2.3.1 is no longer supported.\n\n[Read the complete guide to using the Operator for Apache CouchDB](https://ibm.biz/BdfGQy)\n",
      "csv_display_name": "Operator for Apache CouchDB",
      "csv_metadata_description": "Apache CouchDB is a highly available NOSQL database for web and mobile applications\n",
      "csv_name": "couchdb-operator.v2.2.3",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": true,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-11-23T07:10:06.060000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.9",
      "organization": "certified-operators",
      "package": "couchdb-operator-certified",
      "provided_apis": [
        {
          "group": "couchdb.databases.cloud.ibm.com",
          "kind": "RecipeTemplate",
          "plural": "recipetemplates",
          "version": "v1"
        },
        {
          "group": "couchdb.databases.cloud.ibm.com",
          "kind": "Backup",
          "plural": "backups",
          "version": "v1"
        },
        {
          "group": "couchdb.databases.cloud.ibm.com",
          "kind": "Bucket",
          "plural": "buckets",
          "version": "v1"
        },
        {
          "group": "couchdb.databases.cloud.ibm.com",
          "kind": "CouchDBCluster",
          "plural": "couchdbclusters",
          "version": "v1"
        },
        {
          "group": "couchdb.databases.cloud.ibm.com",
          "kind": "FormationLock",
          "plural": "formationlocks",
          "version": "v1"
        },
        {
          "group": "couchdb.databases.cloud.ibm.com",
          "kind": "Formation",
          "plural": "formations",
          "version": "v1"
        },
        {
          "group": "couchdb.databases.cloud.ibm.com",
          "kind": "Recipe",
          "plural": "recipes",
          "version": "v1"
        }
      ],
      "provider": "IBM",
      "related_images": [
        {
          "digest": "sha256:1f8b0f31be95395c07436d92c89bf30eaca4f62cba805643d5562a64167f574a",
          "image": "registry.connect.redhat.com/ibm/couchdb3@sha256:1f8b0f31be95395c07436d92c89bf30eaca4f62cba805643d5562a64167f574a",
          "name": "COUCHDB3"
        },
        {
          "digest": "sha256:2115a1370e45bcdb9a59445ed78f4abc10d73ae090f07a8309941ba760b8a0bb",
          "image": "registry.connect.redhat.com/ibm/couchdb-operator-mgmt@sha256:2115a1370e45bcdb9a59445ed78f4abc10d73ae090f07a8309941ba760b8a0bb",
          "name": "COUCHDB-MGMT"
        },
        {
          "digest": "sha256:968c56be05781854ccc6068cc2f2f9f81dc7a34219210791810c2138d40a1cc3",
          "image": "registry.connect.redhat.com/ibm/couchdb-operator@sha256:968c56be05781854ccc6068cc2f2f9f81dc7a34219210791810c2138d40a1cc3",
          "name": "couchdb-operator-968c56be05781854ccc6068cc2f2f9f81dc7a34219210791810c2138d40a1cc3-annotation"
        },
        {
          "digest": "sha256:968c56be05781854ccc6068cc2f2f9f81dc7a34219210791810c2138d40a1cc3",
          "image": "registry.connect.redhat.com/ibm/couchdb-operator@sha256:968c56be05781854ccc6068cc2f2f9f81dc7a34219210791810c2138d40a1cc3",
          "name": "couchdb-operator"
        },
        {
          "digest": "sha256:1f8b0f31be95395c07436d92c89bf30eaca4f62cba805643d5562a64167f574a",
          "image": "registry.connect.redhat.com/ibm/couchdb3@sha256:1f8b0f31be95395c07436d92c89bf30eaca4f62cba805643d5562a64167f574a",
          "name": "couchdb3"
        },
        {
          "digest": "sha256:2115a1370e45bcdb9a59445ed78f4abc10d73ae090f07a8309941ba760b8a0bb",
          "image": "registry.connect.redhat.com/ibm/couchdb-operator-mgmt@sha256:2115a1370e45bcdb9a59445ed78f4abc10d73ae090f07a8309941ba760b8a0bb",
          "name": "mgmt"
        }
      ],
      "replaces": null,
      "skip_range": "<2.2.3",
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.9",
      "version": "2.2.3",
      "version_original": "2.2.3"
    },
    {
      "_id": "637db5f9788768d1d6f500d7",
      "alm_examples": [
        {
          "api_version": "postgresql.k8s.enterprisedb.io/v1",
          "kind": "Backup",
          "metadata": {
            "name": "backup-sample"
          },
          "spec": {
            "cluster": {
              "name": "cluster-sample"
            }
          }
        },
        {
          "api_version": "postgresql.k8s.enterprisedb.io/v1",
          "kind": "Cluster",
          "metadata": {
            "name": "cluster-sample"
          },
          "spec": {
            "instances": 3,
            "logLevel": "info",
            "primaryUpdateStrategy": "unsupervised",
            "storage": {
              "size": "1Gi"
            }
          }
        },
        {
          "api_version": "postgresql.k8s.enterprisedb.io/v1",
          "kind": "Pooler",
          "metadata": {
            "name": "pooler-sample-rw"
          },
          "spec": {
            "cluster": {
              "name": "cluster-sample"
            },
            "instances": 1,
            "pgbouncer": {
              "poolMode": "session"
            },
            "type": "rw"
          }
        },
        {
          "api_version": "postgresql.k8s.enterprisedb.io/v1",
          "kind": "ScheduledBackup",
          "metadata": {
            "name": "scheduledbackup-sample"
          },
          "spec": {
            "cluster": {
              "name": "cluster-sample"
            },
            "schedule": "0 0 0 * * *"
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [
        "amd64",
        "ppc64le",
        "s390x"
      ],
      "bundle_path": "registry.connect.redhat.com/enterprisedb/cloud-native-postgresql@sha256:e0624abf48e3f643638e93a633a4017961b454f133830f6d369b238d58a6eb8b",
      "bundle_path_digest": "sha256:e0624abf48e3f643638e93a633a4017961b454f133830f6d369b238d58a6eb8b",
      "capabilities": [
        "Auto Pilot"
      ],
      "channel_name": "null",
      "creation_date": "2022-11-23T05:56:09.871000+00:00",
      "csv_description": "EDB Postgres for Kubernetes is an operator designed, developed, and supported by EDB that covers the full \nlifecycle of a highly available Postgres database clusters with a primary/standby architecture, using native\nstreaming replication. It is based on the open source CloudNativePG operator, and provides additional value\nsuch as compatibility with Oracle using EDB Postgres Advanced Server and additional supported platforms such\nas IBM Power and OpenShift.\n\nKey features available include:\n\n* Kubernetes API integration for high availability\n* Self-healing through failover and automated recreation of replicas\n* Capacity management with scale up/down capabilities\n* Planned switchovers for scheduled maintenance\n* Read-only and read-write Kubernetes services definitions\n* Rolling updates for Postgres minor versions and operator upgrades\n* Continuous backup and point-in-time recovery\n* Connection Pooling with PgBouncer\n* Integrated metrics exporter out of the box\n* PostgreSQL replication across multiple Kubernetes clusters\n* Red Hat certified operator for OpenShift\n\nThe operator has been renamed from Cloud Native PostgreSQL. Existing users of Cloud Native PostgreSQL will not\nexperience any change, as the underlying components and resources have not changed.\n",
      "csv_display_name": "EDB Postgres for Kubernetes",
      "csv_metadata_description": "Operator to manage Postgres high availability clusters with a primary/standby architecture.",
      "csv_name": "cloud-native-postgresql.v1.15.2",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": true,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": false,
      "last_update_date": "2022-11-23T05:56:09.871000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.10",
      "organization": "certified-operators",
      "package": "cloud-native-postgresql",
      "provided_apis": [
        {
          "group": "postgresql.k8s.enterprisedb.io",
          "kind": "ScheduledBackup",
          "plural": "scheduledbackups",
          "version": "v1"
        },
        {
          "group": "postgresql.k8s.enterprisedb.io",
          "kind": "Backup",
          "plural": "backups",
          "version": "v1"
        },
        {
          "group": "postgresql.k8s.enterprisedb.io",
          "kind": "Cluster",
          "plural": "clusters",
          "version": "v1"
        },
        {
          "group": "postgresql.k8s.enterprisedb.io",
          "kind": "Pooler",
          "plural": "poolers",
          "version": "v1"
        }
      ],
      "provider": "EnterpriseDB Corporation",
      "related_images": [
        {
          "digest": "sha256:af4718fd40222fb3fc68fa1952baa69e61a35085429f10e6fcd5db46d60f0bfc",
          "image": "quay.io/enterprisedb/cloud-native-postgresql@sha256:af4718fd40222fb3fc68fa1952baa69e61a35085429f10e6fcd5db46d60f0bfc",
          "name": "cloud-native-postgresql-af4718fd40222fb3fc68fa1952baa69e61a35085429f10e6fcd5db46d60f0bfc-annotation"
        },
        {
          "digest": "sha256:af4718fd40222fb3fc68fa1952baa69e61a35085429f10e6fcd5db46d60f0bfc",
          "image": "quay.io/enterprisedb/cloud-native-postgresql@sha256:af4718fd40222fb3fc68fa1952baa69e61a35085429f10e6fcd5db46d60f0bfc",
          "name": "manager"
        }
      ],
      "replaces": null,
      "skip_range": ">=0.6.0 < 1.15.2",
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.10",
      "version": "1.15.2",
      "version_original": "1.15.2"
    },
    {
      "_id": "637db6430ee97ebabe397f62",
      "alm_examples": [
        {
          "api_version": "postgresql.k8s.enterprisedb.io/v1",
          "kind": "Backup",
          "metadata": {
            "name": "backup-sample"
          },
          "spec": {
            "cluster": {
              "name": "cluster-sample"
            }
          }
        },
        {
          "api_version": "postgresql.k8s.enterprisedb.io/v1",
          "kind": "Cluster",
          "metadata": {
            "name": "cluster-sample"
          },
          "spec": {
            "instances": 3,
            "logLevel": "info",
            "primaryUpdateStrategy": "unsupervised",
            "storage": {
              "size": "1Gi"
            }
          }
        },
        {
          "api_version": "postgresql.k8s.enterprisedb.io/v1",
          "kind": "Pooler",
          "metadata": {
            "name": "pooler-sample-rw"
          },
          "spec": {
            "cluster": {
              "name": "cluster-sample"
            },
            "instances": 1,
            "pgbouncer": {
              "poolMode": "session"
            },
            "type": "rw"
          }
        },
        {
          "api_version": "postgresql.k8s.enterprisedb.io/v1",
          "kind": "ScheduledBackup",
          "metadata": {
            "name": "scheduledbackup-sample"
          },
          "spec": {
            "cluster": {
              "name": "cluster-sample"
            },
            "schedule": "0 0 0 * * *"
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [
        "amd64",
        "ppc64le",
        "s390x"
      ],
      "bundle_path": "registry.connect.redhat.com/enterprisedb/cloud-native-postgresql@sha256:e0624abf48e3f643638e93a633a4017961b454f133830f6d369b238d58a6eb8b",
      "bundle_path_digest": "sha256:e0624abf48e3f643638e93a633a4017961b454f133830f6d369b238d58a6eb8b",
      "capabilities": [
        "Auto Pilot"
      ],
      "channel_name": "null",
      "creation_date": "2022-11-23T05:57:23.939000+00:00",
      "csv_description": "EDB Postgres for Kubernetes is an operator designed, developed, and supported by EDB that covers the full \nlifecycle of a highly available Postgres database clusters with a primary/standby architecture, using native\nstreaming replication. It is based on the open source CloudNativePG operator, and provides additional value\nsuch as compatibility with Oracle using EDB Postgres Advanced Server and additional supported platforms such\nas IBM Power and OpenShift.\n\nKey features available include:\n\n* Kubernetes API integration for high availability\n* Self-healing through failover and automated recreation of replicas\n* Capacity management with scale up/down capabilities\n* Planned switchovers for scheduled maintenance\n* Read-only and read-write Kubernetes services definitions\n* Rolling updates for Postgres minor versions and operator upgrades\n* Continuous backup and point-in-time recovery\n* Connection Pooling with PgBouncer\n* Integrated metrics exporter out of the box\n* PostgreSQL replication across multiple Kubernetes clusters\n* Red Hat certified operator for OpenShift\n\nThe operator has been renamed from Cloud Native PostgreSQL. Existing users of Cloud Native PostgreSQL will not\nexperience any change, as the underlying components and resources have not changed.\n",
      "csv_display_name": "EDB Postgres for Kubernetes",
      "csv_metadata_description": "Operator to manage Postgres high availability clusters with a primary/standby architecture.",
      "csv_name": "cloud-native-postgresql.v1.15.2",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": true,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": false,
      "last_update_date": "2022-11-23T05:57:23.939000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.9",
      "organization": "certified-operators",
      "package": "cloud-native-postgresql",
      "provided_apis": [
        {
          "group": "postgresql.k8s.enterprisedb.io",
          "kind": "Backup",
          "plural": "backups",
          "version": "v1"
        },
        {
          "group": "postgresql.k8s.enterprisedb.io",
          "kind": "Cluster",
          "plural": "clusters",
          "version": "v1"
        },
        {
          "group": "postgresql.k8s.enterprisedb.io",
          "kind": "Pooler",
          "plural": "poolers",
          "version": "v1"
        },
        {
          "group": "postgresql.k8s.enterprisedb.io",
          "kind": "ScheduledBackup",
          "plural": "scheduledbackups",
          "version": "v1"
        }
      ],
      "provider": "EnterpriseDB Corporation",
      "related_images": [
        {
          "digest": "sha256:af4718fd40222fb3fc68fa1952baa69e61a35085429f10e6fcd5db46d60f0bfc",
          "image": "quay.io/enterprisedb/cloud-native-postgresql@sha256:af4718fd40222fb3fc68fa1952baa69e61a35085429f10e6fcd5db46d60f0bfc",
          "name": "cloud-native-postgresql-af4718fd40222fb3fc68fa1952baa69e61a35085429f10e6fcd5db46d60f0bfc-annotation"
        },
        {
          "digest": "sha256:af4718fd40222fb3fc68fa1952baa69e61a35085429f10e6fcd5db46d60f0bfc",
          "image": "quay.io/enterprisedb/cloud-native-postgresql@sha256:af4718fd40222fb3fc68fa1952baa69e61a35085429f10e6fcd5db46d60f0bfc",
          "name": "manager"
        }
      ],
      "replaces": null,
      "skip_range": ">=0.6.0 < 1.15.2",
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.9",
      "version": "1.15.2",
      "version_original": "1.15.2"
    },
    {
      "_id": "637dc656884420e0c62d2a0f",
      "alm_examples": [
        {
          "api_version": "postgresql.k8s.enterprisedb.io/v1",
          "kind": "Backup",
          "metadata": {
            "name": "backup-sample"
          },
          "spec": {
            "cluster": {
              "name": "cluster-sample"
            }
          }
        },
        {
          "api_version": "postgresql.k8s.enterprisedb.io/v1",
          "kind": "Cluster",
          "metadata": {
            "name": "cluster-sample"
          },
          "spec": {
            "instances": 3,
            "logLevel": "info",
            "primaryUpdateStrategy": "unsupervised",
            "storage": {
              "size": "1Gi"
            }
          }
        },
        {
          "api_version": "postgresql.k8s.enterprisedb.io/v1",
          "kind": "Pooler",
          "metadata": {
            "name": "pooler-sample-rw"
          },
          "spec": {
            "cluster": {
              "name": "cluster-sample"
            },
            "instances": 1,
            "pgbouncer": {
              "poolMode": "session"
            },
            "type": "rw"
          }
        },
        {
          "api_version": "postgresql.k8s.enterprisedb.io/v1",
          "kind": "ScheduledBackup",
          "metadata": {
            "name": "scheduledbackup-sample"
          },
          "spec": {
            "cluster": {
              "name": "cluster-sample"
            },
            "schedule": "0 0 0 * * *"
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [
        "amd64",
        "ppc64le",
        "s390x"
      ],
      "bundle_path": "registry.connect.redhat.com/enterprisedb/cloud-native-postgresql@sha256:e0624abf48e3f643638e93a633a4017961b454f133830f6d369b238d58a6eb8b",
      "bundle_path_digest": "sha256:e0624abf48e3f643638e93a633a4017961b454f133830f6d369b238d58a6eb8b",
      "capabilities": [
        "Auto Pilot"
      ],
      "channel_name": "null",
      "creation_date": "2022-11-23T07:05:58.568000+00:00",
      "csv_description": "EDB Postgres for Kubernetes is an operator designed, developed, and supported by EDB that covers the full \nlifecycle of a highly available Postgres database clusters with a primary/standby architecture, using native\nstreaming replication. It is based on the open source CloudNativePG operator, and provides additional value\nsuch as compatibility with Oracle using EDB Postgres Advanced Server and additional supported platforms such\nas IBM Power and OpenShift.\n\nKey features available include:\n\n* Kubernetes API integration for high availability\n* Self-healing through failover and automated recreation of replicas\n* Capacity management with scale up/down capabilities\n* Planned switchovers for scheduled maintenance\n* Read-only and read-write Kubernetes services definitions\n* Rolling updates for Postgres minor versions and operator upgrades\n* Continuous backup and point-in-time recovery\n* Connection Pooling with PgBouncer\n* Integrated metrics exporter out of the box\n* PostgreSQL replication across multiple Kubernetes clusters\n* Red Hat certified operator for OpenShift\n\nThe operator has been renamed from Cloud Native PostgreSQL. Existing users of Cloud Native PostgreSQL will not\nexperience any change, as the underlying components and resources have not changed.\n",
      "csv_display_name": "EDB Postgres for Kubernetes",
      "csv_metadata_description": "Operator to manage Postgres high availability clusters with a primary/standby architecture.",
      "csv_name": "cloud-native-postgresql.v1.15.2",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": true,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": false,
      "last_update_date": "2022-11-23T07:05:58.568000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.7",
      "organization": "certified-operators",
      "package": "cloud-native-postgresql",
      "provided_apis": [
        {
          "group": "postgresql.k8s.enterprisedb.io",
          "kind": "ScheduledBackup",
          "version": "v1"
        },
        {
          "group": "postgresql.k8s.enterprisedb.io",
          "kind": "Backup",
          "version": "v1"
        },
        {
          "group": "postgresql.k8s.enterprisedb.io",
          "kind": "Cluster",
          "version": "v1"
        },
        {
          "group": "postgresql.k8s.enterprisedb.io",
          "kind": "Pooler",
          "version": "v1"
        }
      ],
      "provider": "EnterpriseDB Corporation",
      "related_images": [
        {
          "digest": "sha256:af4718fd40222fb3fc68fa1952baa69e61a35085429f10e6fcd5db46d60f0bfc",
          "image": "quay.io/enterprisedb/cloud-native-postgresql@sha256:af4718fd40222fb3fc68fa1952baa69e61a35085429f10e6fcd5db46d60f0bfc",
          "name": "cloud-native-postgresql-af4718fd40222fb3fc68fa1952baa69e61a35085429f10e6fcd5db46d60f0bfc-annotation"
        },
        {
          "digest": "sha256:af4718fd40222fb3fc68fa1952baa69e61a35085429f10e6fcd5db46d60f0bfc",
          "image": "quay.io/enterprisedb/cloud-native-postgresql@sha256:af4718fd40222fb3fc68fa1952baa69e61a35085429f10e6fcd5db46d60f0bfc",
          "name": "manager"
        }
      ],
      "replaces": null,
      "skip_range": ">=0.6.0 < 1.15.2",
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.7",
      "version": "1.15.2",
      "version_original": "1.15.2"
    },
    {
      "_id": "637dc6b50ee97ebabe39bca0",
      "alm_examples": [
        {
          "api_version": "postgresql.k8s.enterprisedb.io/v1",
          "kind": "Backup",
          "metadata": {
            "name": "backup-sample"
          },
          "spec": {
            "cluster": {
              "name": "cluster-sample"
            }
          }
        },
        {
          "api_version": "postgresql.k8s.enterprisedb.io/v1",
          "kind": "Cluster",
          "metadata": {
            "name": "cluster-sample"
          },
          "spec": {
            "instances": 3,
            "logLevel": "info",
            "primaryUpdateStrategy": "unsupervised",
            "storage": {
              "size": "1Gi"
            }
          }
        },
        {
          "api_version": "postgresql.k8s.enterprisedb.io/v1",
          "kind": "Pooler",
          "metadata": {
            "name": "pooler-sample-rw"
          },
          "spec": {
            "cluster": {
              "name": "cluster-sample"
            },
            "instances": 1,
            "pgbouncer": {
              "poolMode": "session"
            },
            "type": "rw"
          }
        },
        {
          "api_version": "postgresql.k8s.enterprisedb.io/v1",
          "kind": "ScheduledBackup",
          "metadata": {
            "name": "scheduledbackup-sample"
          },
          "spec": {
            "cluster": {
              "name": "cluster-sample"
            },
            "schedule": "0 0 0 * * *"
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [
        "amd64",
        "ppc64le",
        "s390x"
      ],
      "bundle_path": "registry.connect.redhat.com/enterprisedb/cloud-native-postgresql@sha256:e0624abf48e3f643638e93a633a4017961b454f133830f6d369b238d58a6eb8b",
      "bundle_path_digest": "sha256:e0624abf48e3f643638e93a633a4017961b454f133830f6d369b238d58a6eb8b",
      "capabilities": [
        "Auto Pilot"
      ],
      "channel_name": "null",
      "creation_date": "2022-11-23T07:07:33.722000+00:00",
      "csv_description": "EDB Postgres for Kubernetes is an operator designed, developed, and supported by EDB that covers the full \nlifecycle of a highly available Postgres database clusters with a primary/standby architecture, using native\nstreaming replication. It is based on the open source CloudNativePG operator, and provides additional value\nsuch as compatibility with Oracle using EDB Postgres Advanced Server and additional supported platforms such\nas IBM Power and OpenShift.\n\nKey features available include:\n\n* Kubernetes API integration for high availability\n* Self-healing through failover and automated recreation of replicas\n* Capacity management with scale up/down capabilities\n* Planned switchovers for scheduled maintenance\n* Read-only and read-write Kubernetes services definitions\n* Rolling updates for Postgres minor versions and operator upgrades\n* Continuous backup and point-in-time recovery\n* Connection Pooling with PgBouncer\n* Integrated metrics exporter out of the box\n* PostgreSQL replication across multiple Kubernetes clusters\n* Red Hat certified operator for OpenShift\n\nThe operator has been renamed from Cloud Native PostgreSQL. Existing users of Cloud Native PostgreSQL will not\nexperience any change, as the underlying components and resources have not changed.\n",
      "csv_display_name": "EDB Postgres for Kubernetes",
      "csv_metadata_description": "Operator to manage Postgres high availability clusters with a primary/standby architecture.",
      "csv_name": "cloud-native-postgresql.v1.15.2",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": true,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": false,
      "last_update_date": "2022-11-23T07:07:33.722000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.9",
      "organization": "certified-operators",
      "package": "cloud-native-postgresql",
      "provided_apis": [
        {
          "group": "postgresql.k8s.enterprisedb.io",
          "kind": "Backup",
          "plural": "backups",
          "version": "v1"
        },
        {
          "group": "postgresql.k8s.enterprisedb.io",
          "kind": "Cluster",
          "plural": "clusters",
          "version": "v1"
        },
        {
          "group": "postgresql.k8s.enterprisedb.io",
          "kind": "Pooler",
          "plural": "poolers",
          "version": "v1"
        },
        {
          "group": "postgresql.k8s.enterprisedb.io",
          "kind": "ScheduledBackup",
          "plural": "scheduledbackups",
          "version": "v1"
        }
      ],
      "provider": "EnterpriseDB Corporation",
      "related_images": [
        {
          "digest": "sha256:af4718fd40222fb3fc68fa1952baa69e61a35085429f10e6fcd5db46d60f0bfc",
          "image": "quay.io/enterprisedb/cloud-native-postgresql@sha256:af4718fd40222fb3fc68fa1952baa69e61a35085429f10e6fcd5db46d60f0bfc",
          "name": "cloud-native-postgresql-af4718fd40222fb3fc68fa1952baa69e61a35085429f10e6fcd5db46d60f0bfc-annotation"
        },
        {
          "digest": "sha256:af4718fd40222fb3fc68fa1952baa69e61a35085429f10e6fcd5db46d60f0bfc",
          "image": "quay.io/enterprisedb/cloud-native-postgresql@sha256:af4718fd40222fb3fc68fa1952baa69e61a35085429f10e6fcd5db46d60f0bfc",
          "name": "manager"
        }
      ],
      "replaces": null,
      "skip_range": ">=0.6.0 < 1.15.2",
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.9",
      "version": "1.15.2",
      "version_original": "1.15.2"
    },
    {
      "_id": "637dcbc940d971f5448d5be6",
      "alm_examples": [
        {
          "api_version": "postgresql.k8s.enterprisedb.io/v1",
          "kind": "Backup",
          "metadata": {
            "name": "backup-sample"
          },
          "spec": {
            "cluster": {
              "name": "cluster-sample"
            }
          }
        },
        {
          "api_version": "postgresql.k8s.enterprisedb.io/v1",
          "kind": "Cluster",
          "metadata": {
            "name": "cluster-sample"
          },
          "spec": {
            "instances": 3,
            "logLevel": "info",
            "primaryUpdateStrategy": "unsupervised",
            "storage": {
              "size": "1Gi"
            }
          }
        },
        {
          "api_version": "postgresql.k8s.enterprisedb.io/v1",
          "kind": "Pooler",
          "metadata": {
            "name": "pooler-sample-rw"
          },
          "spec": {
            "cluster": {
              "name": "cluster-sample"
            },
            "instances": 1,
            "pgbouncer": {
              "poolMode": "session"
            },
            "type": "rw"
          }
        },
        {
          "api_version": "postgresql.k8s.enterprisedb.io/v1",
          "kind": "ScheduledBackup",
          "metadata": {
            "name": "scheduledbackup-sample"
          },
          "spec": {
            "cluster": {
              "name": "cluster-sample"
            },
            "schedule": "0 0 0 * * *"
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [
        "amd64",
        "ppc64le",
        "s390x"
      ],
      "bundle_path": "registry.connect.redhat.com/enterprisedb/cloud-native-postgresql@sha256:e0624abf48e3f643638e93a633a4017961b454f133830f6d369b238d58a6eb8b",
      "bundle_path_digest": "sha256:e0624abf48e3f643638e93a633a4017961b454f133830f6d369b238d58a6eb8b",
      "capabilities": [
        "Auto Pilot"
      ],
      "channel_name": "null",
      "creation_date": "2022-11-23T07:29:13.088000+00:00",
      "csv_description": "EDB Postgres for Kubernetes is an operator designed, developed, and supported by EDB that covers the full \nlifecycle of a highly available Postgres database clusters with a primary/standby architecture, using native\nstreaming replication. It is based on the open source CloudNativePG operator, and provides additional value\nsuch as compatibility with Oracle using EDB Postgres Advanced Server and additional supported platforms such\nas IBM Power and OpenShift.\n\nKey features available include:\n\n* Kubernetes API integration for high availability\n* Self-healing through failover and automated recreation of replicas\n* Capacity management with scale up/down capabilities\n* Planned switchovers for scheduled maintenance\n* Read-only and read-write Kubernetes services definitions\n* Rolling updates for Postgres minor versions and operator upgrades\n* Continuous backup and point-in-time recovery\n* Connection Pooling with PgBouncer\n* Integrated metrics exporter out of the box\n* PostgreSQL replication across multiple Kubernetes clusters\n* Red Hat certified operator for OpenShift\n\nThe operator has been renamed from Cloud Native PostgreSQL. Existing users of Cloud Native PostgreSQL will not\nexperience any change, as the underlying components and resources have not changed.\n",
      "csv_display_name": "EDB Postgres for Kubernetes",
      "csv_metadata_description": "Operator to manage Postgres high availability clusters with a primary/standby architecture.",
      "csv_name": "cloud-native-postgresql.v1.15.2",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": true,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": false,
      "last_update_date": "2022-11-23T07:29:13.088000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.8",
      "organization": "certified-operators",
      "package": "cloud-native-postgresql",
      "provided_apis": [
        {
          "group": "postgresql.k8s.enterprisedb.io",
          "kind": "Cluster",
          "plural": "clusters",
          "version": "v1"
        },
        {
          "group": "postgresql.k8s.enterprisedb.io",
          "kind": "Pooler",
          "plural": "poolers",
          "version": "v1"
        },
        {
          "group": "postgresql.k8s.enterprisedb.io",
          "kind": "ScheduledBackup",
          "plural": "scheduledbackups",
          "version": "v1"
        },
        {
          "group": "postgresql.k8s.enterprisedb.io",
          "kind": "Backup",
          "plural": "backups",
          "version": "v1"
        }
      ],
      "provider": "EnterpriseDB Corporation",
      "related_images": [
        {
          "digest": "sha256:af4718fd40222fb3fc68fa1952baa69e61a35085429f10e6fcd5db46d60f0bfc",
          "image": "quay.io/enterprisedb/cloud-native-postgresql@sha256:af4718fd40222fb3fc68fa1952baa69e61a35085429f10e6fcd5db46d60f0bfc",
          "name": "cloud-native-postgresql-af4718fd40222fb3fc68fa1952baa69e61a35085429f10e6fcd5db46d60f0bfc-annotation"
        },
        {
          "digest": "sha256:af4718fd40222fb3fc68fa1952baa69e61a35085429f10e6fcd5db46d60f0bfc",
          "image": "quay.io/enterprisedb/cloud-native-postgresql@sha256:af4718fd40222fb3fc68fa1952baa69e61a35085429f10e6fcd5db46d60f0bfc",
          "name": "manager"
        }
      ],
      "replaces": null,
      "skip_range": ">=0.6.0 < 1.15.2",
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.8",
      "version": "1.15.2",
      "version_original": "1.15.2"
    },
    {
      "_id": "637dcd1b5556118ee8e2fc16",
      "alm_examples": [
        {
          "api_version": "postgresql.k8s.enterprisedb.io/v1",
          "kind": "Backup",
          "metadata": {
            "name": "backup-sample"
          },
          "spec": {
            "cluster": {
              "name": "cluster-sample"
            }
          }
        },
        {
          "api_version": "postgresql.k8s.enterprisedb.io/v1",
          "kind": "Cluster",
          "metadata": {
            "name": "cluster-sample"
          },
          "spec": {
            "instances": 3,
            "logLevel": "info",
            "primaryUpdateStrategy": "unsupervised",
            "storage": {
              "size": "1Gi"
            }
          }
        },
        {
          "api_version": "postgresql.k8s.enterprisedb.io/v1",
          "kind": "Pooler",
          "metadata": {
            "name": "pooler-sample-rw"
          },
          "spec": {
            "cluster": {
              "name": "cluster-sample"
            },
            "instances": 1,
            "pgbouncer": {
              "poolMode": "session"
            },
            "type": "rw"
          }
        },
        {
          "api_version": "postgresql.k8s.enterprisedb.io/v1",
          "kind": "ScheduledBackup",
          "metadata": {
            "name": "scheduledbackup-sample"
          },
          "spec": {
            "cluster": {
              "name": "cluster-sample"
            },
            "schedule": "0 0 0 * * *"
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [
        "amd64",
        "ppc64le",
        "s390x"
      ],
      "bundle_path": "registry.connect.redhat.com/enterprisedb/cloud-native-postgresql@sha256:e0624abf48e3f643638e93a633a4017961b454f133830f6d369b238d58a6eb8b",
      "bundle_path_digest": "sha256:e0624abf48e3f643638e93a633a4017961b454f133830f6d369b238d58a6eb8b",
      "capabilities": [
        "Auto Pilot"
      ],
      "channel_name": "null",
      "creation_date": "2022-11-23T07:34:51.785000+00:00",
      "csv_description": "EDB Postgres for Kubernetes is an operator designed, developed, and supported by EDB that covers the full \nlifecycle of a highly available Postgres database clusters with a primary/standby architecture, using native\nstreaming replication. It is based on the open source CloudNativePG operator, and provides additional value\nsuch as compatibility with Oracle using EDB Postgres Advanced Server and additional supported platforms such\nas IBM Power and OpenShift.\n\nKey features available include:\n\n* Kubernetes API integration for high availability\n* Self-healing through failover and automated recreation of replicas\n* Capacity management with scale up/down capabilities\n* Planned switchovers for scheduled maintenance\n* Read-only and read-write Kubernetes services definitions\n* Rolling updates for Postgres minor versions and operator upgrades\n* Continuous backup and point-in-time recovery\n* Connection Pooling with PgBouncer\n* Integrated metrics exporter out of the box\n* PostgreSQL replication across multiple Kubernetes clusters\n* Red Hat certified operator for OpenShift\n\nThe operator has been renamed from Cloud Native PostgreSQL. Existing users of Cloud Native PostgreSQL will not\nexperience any change, as the underlying components and resources have not changed.\n",
      "csv_display_name": "EDB Postgres for Kubernetes",
      "csv_metadata_description": "Operator to manage Postgres high availability clusters with a primary/standby architecture.",
      "csv_name": "cloud-native-postgresql.v1.15.2",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": true,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": false,
      "last_update_date": "2022-11-23T07:34:51.785000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.6",
      "organization": "certified-operators",
      "package": "cloud-native-postgresql",
      "provided_apis": [
        {
          "group": "postgresql.k8s.enterprisedb.io",
          "kind": "Cluster",
          "plural": "clusters",
          "version": "v1"
        },
        {
          "group": "postgresql.k8s.enterprisedb.io",
          "kind": "Pooler",
          "plural": "poolers",
          "version": "v1"
        },
        {
          "group": "postgresql.k8s.enterprisedb.io",
          "kind": "ScheduledBackup",
          "plural": "scheduledbackups",
          "version": "v1"
        },
        {
          "group": "postgresql.k8s.enterprisedb.io",
          "kind": "Backup",
          "plural": "backups",
          "version": "v1"
        }
      ],
      "provider": "EnterpriseDB Corporation",
      "related_images": [
        {
          "digest": "sha256:af4718fd40222fb3fc68fa1952baa69e61a35085429f10e6fcd5db46d60f0bfc",
          "image": "quay.io/enterprisedb/cloud-native-postgresql@sha256:af4718fd40222fb3fc68fa1952baa69e61a35085429f10e6fcd5db46d60f0bfc",
          "name": "cloud-native-postgresql-af4718fd40222fb3fc68fa1952baa69e61a35085429f10e6fcd5db46d60f0bfc-annotation"
        },
        {
          "digest": "sha256:af4718fd40222fb3fc68fa1952baa69e61a35085429f10e6fcd5db46d60f0bfc",
          "image": "quay.io/enterprisedb/cloud-native-postgresql@sha256:af4718fd40222fb3fc68fa1952baa69e61a35085429f10e6fcd5db46d60f0bfc",
          "name": "manager"
        }
      ],
      "replaces": null,
      "skip_range": ">=0.6.0 < 1.15.2",
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.6",
      "version": "1.15.2",
      "version_original": "1.15.2"
    },
    {
      "_id": "637dd85e0ee97ebabe3a0df1",
      "alm_examples": [
        {
          "api_version": "postgresql.k8s.enterprisedb.io/v1",
          "kind": "Backup",
          "metadata": {
            "name": "backup-sample"
          },
          "spec": {
            "cluster": {
              "name": "cluster-sample"
            }
          }
        },
        {
          "api_version": "postgresql.k8s.enterprisedb.io/v1",
          "kind": "Cluster",
          "metadata": {
            "name": "cluster-sample"
          },
          "spec": {
            "instances": 3,
            "logLevel": "info",
            "primaryUpdateStrategy": "unsupervised",
            "storage": {
              "size": "1Gi"
            }
          }
        },
        {
          "api_version": "postgresql.k8s.enterprisedb.io/v1",
          "kind": "Pooler",
          "metadata": {
            "name": "pooler-sample-rw"
          },
          "spec": {
            "cluster": {
              "name": "cluster-sample"
            },
            "instances": 1,
            "pgbouncer": {
              "poolMode": "session"
            },
            "type": "rw"
          }
        },
        {
          "api_version": "postgresql.k8s.enterprisedb.io/v1",
          "kind": "ScheduledBackup",
          "metadata": {
            "name": "scheduledbackup-sample"
          },
          "spec": {
            "cluster": {
              "name": "cluster-sample"
            },
            "schedule": "0 0 0 * * *"
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [
        "amd64",
        "ppc64le",
        "s390x"
      ],
      "bundle_path": "registry.connect.redhat.com/enterprisedb/cloud-native-postgresql@sha256:e0624abf48e3f643638e93a633a4017961b454f133830f6d369b238d58a6eb8b",
      "bundle_path_digest": "sha256:e0624abf48e3f643638e93a633a4017961b454f133830f6d369b238d58a6eb8b",
      "capabilities": [
        "Auto Pilot"
      ],
      "channel_name": "null",
      "creation_date": "2022-11-23T08:22:54.121000+00:00",
      "csv_description": "EDB Postgres for Kubernetes is an operator designed, developed, and supported by EDB that covers the full \nlifecycle of a highly available Postgres database clusters with a primary/standby architecture, using native\nstreaming replication. It is based on the open source CloudNativePG operator, and provides additional value\nsuch as compatibility with Oracle using EDB Postgres Advanced Server and additional supported platforms such\nas IBM Power and OpenShift.\n\nKey features available include:\n\n* Kubernetes API integration for high availability\n* Self-healing through failover and automated recreation of replicas\n* Capacity management with scale up/down capabilities\n* Planned switchovers for scheduled maintenance\n* Read-only and read-write Kubernetes services definitions\n* Rolling updates for Postgres minor versions and operator upgrades\n* Continuous backup and point-in-time recovery\n* Connection Pooling with PgBouncer\n* Integrated metrics exporter out of the box\n* PostgreSQL replication across multiple Kubernetes clusters\n* Red Hat certified operator for OpenShift\n\nThe operator has been renamed from Cloud Native PostgreSQL. Existing users of Cloud Native PostgreSQL will not\nexperience any change, as the underlying components and resources have not changed.\n",
      "csv_display_name": "EDB Postgres for Kubernetes",
      "csv_metadata_description": "Operator to manage Postgres high availability clusters with a primary/standby architecture.",
      "csv_name": "cloud-native-postgresql.v1.15.2",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": true,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": false,
      "last_update_date": "2022-11-23T08:22:54.121000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.10",
      "organization": "certified-operators",
      "package": "cloud-native-postgresql",
      "provided_apis": [
        {
          "group": "postgresql.k8s.enterprisedb.io",
          "kind": "ScheduledBackup",
          "plural": "scheduledbackups",
          "version": "v1"
        },
        {
          "group": "postgresql.k8s.enterprisedb.io",
          "kind": "Backup",
          "plural": "backups",
          "version": "v1"
        },
        {
          "group": "postgresql.k8s.enterprisedb.io",
          "kind": "Cluster",
          "plural": "clusters",
          "version": "v1"
        },
        {
          "group": "postgresql.k8s.enterprisedb.io",
          "kind": "Pooler",
          "plural": "poolers",
          "version": "v1"
        }
      ],
      "provider": "EnterpriseDB Corporation",
      "related_images": [
        {
          "digest": "sha256:af4718fd40222fb3fc68fa1952baa69e61a35085429f10e6fcd5db46d60f0bfc",
          "image": "quay.io/enterprisedb/cloud-native-postgresql@sha256:af4718fd40222fb3fc68fa1952baa69e61a35085429f10e6fcd5db46d60f0bfc",
          "name": "cloud-native-postgresql-af4718fd40222fb3fc68fa1952baa69e61a35085429f10e6fcd5db46d60f0bfc-annotation"
        },
        {
          "digest": "sha256:af4718fd40222fb3fc68fa1952baa69e61a35085429f10e6fcd5db46d60f0bfc",
          "image": "quay.io/enterprisedb/cloud-native-postgresql@sha256:af4718fd40222fb3fc68fa1952baa69e61a35085429f10e6fcd5db46d60f0bfc",
          "name": "manager"
        }
      ],
      "replaces": null,
      "skip_range": ">=0.6.0 < 1.15.2",
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.10",
      "version": "1.15.2",
      "version_original": "1.15.2"
    },
    {
      "_id": "637ec7566c513637ee4f20ff",
      "alm_examples": [
        {
          "api_version": "hazelcast.com/v1alpha1",
          "kind": "CronHotBackup",
          "metadata": {
            "name": "cronhotbackup-sample"
          },
          "spec": {
            "hotBackupTemplate": {
              "metadata": {
                "labels": {
                  "cron-hotbackup": "true"
                }
              },
              "spec": {
                "hazelcastResourceName": "hazelcast"
              }
            },
            "schedule": "*/30 * * * *"
          }
        },
        {
          "api_version": "hazelcast.com/v1alpha1",
          "kind": "Hazelcast",
          "metadata": {
            "name": "hazelcast"
          },
          "spec": {
            "clusterSize": 3,
            "licenseKeySecret": "hazelcast-license-key",
            "repository": "docker.io/hazelcast/hazelcast-enterprise",
            "version": "5.1.4"
          }
        },
        {
          "api_version": "hazelcast.com/v1alpha1",
          "kind": "ManagementCenter",
          "metadata": {
            "name": "managementcenter"
          },
          "spec": {
            "externalConnectivity": {
              "type": "LoadBalancer"
            },
            "hazelcastClusters": [
              {
                "address": "hazelcast",
                "name": "dev"
              }
            ],
            "licenseKeySecret": "hazelcast-license-key",
            "persistence": {
              "enabled": true,
              "size": "10Gi"
            },
            "repository": "hazelcast/management-center",
            "version": "5.1.4"
          }
        },
        {
          "api_version": "hazelcast.com/v1alpha1",
          "kind": "Map",
          "metadata": {
            "name": "map"
          },
          "spec": {
            "hazelcastResourceName": "hazelcast"
          }
        },
        {
          "api_version": "hazelcast.com/v1alpha1",
          "kind": "MultiMap",
          "metadata": {
            "name": "multimap-sample"
          },
          "spec": {
            "backupCount": 3,
            "binary": true,
            "collectionType": "LIST",
            "hazelcastResourceName": "hazelcast",
            "name": "multi-map"
          }
        },
        {
          "api_version": "hazelcast.com/v1alpha1",
          "kind": "ReplicatedMap",
          "metadata": {
            "name": "replicatedmap-sample"
          },
          "spec": {
            "asyncFillup": true,
            "hazelcastResourceName": "hazelcast",
            "inMemoryFormat": "OBJECT",
            "name": "replicated-map"
          }
        },
        {
          "api_version": "hazelcast.com/v1alpha1",
          "kind": "Topic",
          "metadata": {
            "name": "topic-sample2"
          },
          "spec": {
            "globalOrderingEnabled": true,
            "hazelcastResourceName": "hazelcast"
          }
        },
        {
          "api_version": "hazelcast.com/v1alpha1",
          "kind": "WanReplication",
          "metadata": {
            "name": "wanreplication-sample"
          },
          "spec": {
            "endpoints": "35.192.33.252",
            "resources": [
              {
                "kind": "Map",
                "name": "map-sample"
              },
              {
                "kind": "Hazelcast",
                "name": "hz-sample"
              }
            ],
            "targetClusterName": "dev"
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/hazelcast/hazelcast-platform-operator-bundle@sha256:0d6f326f88e90ddac3bbfae0451c79e9e414111f0fe1eac09d1719a4fb0aed9e",
      "bundle_path_digest": "sha256:0d6f326f88e90ddac3bbfae0451c79e9e414111f0fe1eac09d1719a4fb0aed9e",
      "capabilities": [
        "Seamless Upgrades"
      ],
      "channel_name": "alpha",
      "creation_date": "2022-11-24T01:22:30.528000+00:00",
      "csv_description": "# Hazelcast Platform Operator #\n\nEasily deploy Hazelcast clusters and Management Center into Kubernetes environments and manage their lifecycles.\n\n## Before You Start\n\nIf you are planning to create Hazelcast Platform Enterprise clusters, you need to [create a secret](https://docs.hazelcast.com/operator/latest/get-started#step-2-start-the-hazelcast-cluster) for the license. You can request a trial license key from [here](https://trialrequest.hazelcast.com).\n\nFor Hazelcast Platform clusters, you can simply continue.\n\n## Documentation\n\n1. [Get started](https://docs.hazelcast.com/operator/latest/get-started) with the Operator.\n2. [Connect to the cluster from outside Kubernetes](https://guides.hazelcast.org/hazelcast-platform-operator-expose-externally/main)\n  from the outside.\n3. [Restore a Cluster from Cloud Storage with Hazelcast Platform Operator](https://docs.hazelcast.com/tutorials/hazelcast-platform-operator-external-backup-restore)\n4. [Replicate Data between Two Hazelcast Clusters with Hazelcast Platform Operator](https://docs.hazelcast.com/tutorials/hazelcast-platform-operator-wan-replication)\n\n## Features\n\nHazelcast Platform Operator supports the features below:\n\n* Custom resource for Hazelcast Platform (Open Source & Enterprise) and Management Center\n* Observe status of Hazelcast and Management Center clusters\n* Scale up and down Hazelcast clusters\n* Expose Hazelcast cluster to external\n  clients ([Smart & Unisocket](https://docs.hazelcast.com/hazelcast/latest/clients/java#java-client-operation-modes))\n* Backup Hazelcast persistence data to cloud storage with the possibility of scheduling it and restoring the data accordingly\n* WAN Replication feature when you need to synchronize multiple Hazelcast clusters, which are connected by WANs\n* User Code Deployment feature, which allows you to deploy custom and domain classes from cloud storages to Hazelcast members\n* ExecutorService and EntryProcessor support\n* Support several data structures like Map, Topic, MultiMap, and ReplicatedMap, which can be created dynamically via specific Custom Resources\n* MapStore support for Map CR\n",
      "csv_display_name": "Hazelcast Platform Operator",
      "csv_metadata_description": "Install Hazelcast clusters in Kubernetes environments.",
      "csv_name": "hazelcast-platform-operator.v5.5.0",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": false,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-11-24T01:22:30.528000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.11",
      "organization": "certified-operators",
      "package": "hazelcast-platform-operator",
      "provided_apis": [
        {
          "group": "hazelcast.com",
          "kind": "CronHotBackup",
          "version": "v1alpha1"
        },
        {
          "group": "hazelcast.com",
          "kind": "Hazelcast",
          "version": "v1alpha1"
        },
        {
          "group": "hazelcast.com",
          "kind": "HotBackup",
          "version": "v1alpha1"
        },
        {
          "group": "hazelcast.com",
          "kind": "ManagementCenter",
          "version": "v1alpha1"
        },
        {
          "group": "hazelcast.com",
          "kind": "Map",
          "version": "v1alpha1"
        },
        {
          "group": "hazelcast.com",
          "kind": "MultiMap",
          "version": "v1alpha1"
        },
        {
          "group": "hazelcast.com",
          "kind": "ReplicatedMap",
          "version": "v1alpha1"
        },
        {
          "group": "hazelcast.com",
          "kind": "Topic",
          "version": "v1alpha1"
        },
        {
          "group": "hazelcast.com",
          "kind": "WanReplication",
          "version": "v1alpha1"
        }
      ],
      "provider": "Hazelcast, Inc",
      "related_images": [
        {
          "digest": "sha256:7d60383a54855351f0e9c50bec415990b6306d0bff832eaec34cfe7929c791da",
          "image": "docker.io/hazelcast/hazelcast-platform-operator@sha256:7d60383a54855351f0e9c50bec415990b6306d0bff832eaec34cfe7929c791da",
          "name": "hazelcast-platform-operator-7d60383a54855351f0e9c50bec415990b6306d0bff832eaec34cfe7929c791da-annotation"
        },
        {
          "digest": "sha256:7d60383a54855351f0e9c50bec415990b6306d0bff832eaec34cfe7929c791da",
          "image": "docker.io/hazelcast/hazelcast-platform-operator@sha256:7d60383a54855351f0e9c50bec415990b6306d0bff832eaec34cfe7929c791da",
          "name": "manager"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.11",
      "version": "5.5.0",
      "version_original": "5.5.0"
    },
    {
      "_id": "637ec7ef6c513637ee4f2464",
      "alm_examples": [
        {
          "api_version": "hazelcast.com/v1alpha1",
          "kind": "CronHotBackup",
          "metadata": {
            "name": "cronhotbackup-sample"
          },
          "spec": {
            "hotBackupTemplate": {
              "metadata": {
                "labels": {
                  "cron-hotbackup": "true"
                }
              },
              "spec": {
                "hazelcastResourceName": "hazelcast"
              }
            },
            "schedule": "*/30 * * * *"
          }
        },
        {
          "api_version": "hazelcast.com/v1alpha1",
          "kind": "Hazelcast",
          "metadata": {
            "name": "hazelcast"
          },
          "spec": {
            "clusterSize": 3,
            "licenseKeySecret": "hazelcast-license-key",
            "repository": "docker.io/hazelcast/hazelcast-enterprise",
            "version": "5.1.4"
          }
        },
        {
          "api_version": "hazelcast.com/v1alpha1",
          "kind": "ManagementCenter",
          "metadata": {
            "name": "managementcenter"
          },
          "spec": {
            "externalConnectivity": {
              "type": "LoadBalancer"
            },
            "hazelcastClusters": [
              {
                "address": "hazelcast",
                "name": "dev"
              }
            ],
            "licenseKeySecret": "hazelcast-license-key",
            "persistence": {
              "enabled": true,
              "size": "10Gi"
            },
            "repository": "hazelcast/management-center",
            "version": "5.1.4"
          }
        },
        {
          "api_version": "hazelcast.com/v1alpha1",
          "kind": "Map",
          "metadata": {
            "name": "map"
          },
          "spec": {
            "hazelcastResourceName": "hazelcast"
          }
        },
        {
          "api_version": "hazelcast.com/v1alpha1",
          "kind": "MultiMap",
          "metadata": {
            "name": "multimap-sample"
          },
          "spec": {
            "backupCount": 3,
            "binary": true,
            "collectionType": "LIST",
            "hazelcastResourceName": "hazelcast",
            "name": "multi-map"
          }
        },
        {
          "api_version": "hazelcast.com/v1alpha1",
          "kind": "ReplicatedMap",
          "metadata": {
            "name": "replicatedmap-sample"
          },
          "spec": {
            "asyncFillup": true,
            "hazelcastResourceName": "hazelcast",
            "inMemoryFormat": "OBJECT",
            "name": "replicated-map"
          }
        },
        {
          "api_version": "hazelcast.com/v1alpha1",
          "kind": "Topic",
          "metadata": {
            "name": "topic-sample2"
          },
          "spec": {
            "globalOrderingEnabled": true,
            "hazelcastResourceName": "hazelcast"
          }
        },
        {
          "api_version": "hazelcast.com/v1alpha1",
          "kind": "WanReplication",
          "metadata": {
            "name": "wanreplication-sample"
          },
          "spec": {
            "endpoints": "35.192.33.252",
            "resources": [
              {
                "kind": "Map",
                "name": "map-sample"
              },
              {
                "kind": "Hazelcast",
                "name": "hz-sample"
              }
            ],
            "targetClusterName": "dev"
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/hazelcast/hazelcast-platform-operator-bundle@sha256:0d6f326f88e90ddac3bbfae0451c79e9e414111f0fe1eac09d1719a4fb0aed9e",
      "bundle_path_digest": "sha256:0d6f326f88e90ddac3bbfae0451c79e9e414111f0fe1eac09d1719a4fb0aed9e",
      "capabilities": [
        "Seamless Upgrades"
      ],
      "channel_name": "alpha",
      "creation_date": "2022-11-24T01:25:03.698000+00:00",
      "csv_description": "# Hazelcast Platform Operator #\n\nEasily deploy Hazelcast clusters and Management Center into Kubernetes environments and manage their lifecycles.\n\n## Before You Start\n\nIf you are planning to create Hazelcast Platform Enterprise clusters, you need to [create a secret](https://docs.hazelcast.com/operator/latest/get-started#step-2-start-the-hazelcast-cluster) for the license. You can request a trial license key from [here](https://trialrequest.hazelcast.com).\n\nFor Hazelcast Platform clusters, you can simply continue.\n\n## Documentation\n\n1. [Get started](https://docs.hazelcast.com/operator/latest/get-started) with the Operator.\n2. [Connect to the cluster from outside Kubernetes](https://guides.hazelcast.org/hazelcast-platform-operator-expose-externally/main)\n  from the outside.\n3. [Restore a Cluster from Cloud Storage with Hazelcast Platform Operator](https://docs.hazelcast.com/tutorials/hazelcast-platform-operator-external-backup-restore)\n4. [Replicate Data between Two Hazelcast Clusters with Hazelcast Platform Operator](https://docs.hazelcast.com/tutorials/hazelcast-platform-operator-wan-replication)\n\n## Features\n\nHazelcast Platform Operator supports the features below:\n\n* Custom resource for Hazelcast Platform (Open Source & Enterprise) and Management Center\n* Observe status of Hazelcast and Management Center clusters\n* Scale up and down Hazelcast clusters\n* Expose Hazelcast cluster to external\n  clients ([Smart & Unisocket](https://docs.hazelcast.com/hazelcast/latest/clients/java#java-client-operation-modes))\n* Backup Hazelcast persistence data to cloud storage with the possibility of scheduling it and restoring the data accordingly\n* WAN Replication feature when you need to synchronize multiple Hazelcast clusters, which are connected by WANs\n* User Code Deployment feature, which allows you to deploy custom and domain classes from cloud storages to Hazelcast members\n* ExecutorService and EntryProcessor support\n* Support several data structures like Map, Topic, MultiMap, and ReplicatedMap, which can be created dynamically via specific Custom Resources\n* MapStore support for Map CR\n",
      "csv_display_name": "Hazelcast Platform Operator",
      "csv_metadata_description": "Install Hazelcast clusters in Kubernetes environments.",
      "csv_name": "hazelcast-platform-operator.v5.5.0",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": false,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-11-24T01:25:03.698000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.12",
      "organization": "certified-operators",
      "package": "hazelcast-platform-operator",
      "provided_apis": [
        {
          "group": "hazelcast.com",
          "kind": "CronHotBackup",
          "version": "v1alpha1"
        },
        {
          "group": "hazelcast.com",
          "kind": "Hazelcast",
          "version": "v1alpha1"
        },
        {
          "group": "hazelcast.com",
          "kind": "HotBackup",
          "version": "v1alpha1"
        },
        {
          "group": "hazelcast.com",
          "kind": "ManagementCenter",
          "version": "v1alpha1"
        },
        {
          "group": "hazelcast.com",
          "kind": "Map",
          "version": "v1alpha1"
        },
        {
          "group": "hazelcast.com",
          "kind": "MultiMap",
          "version": "v1alpha1"
        },
        {
          "group": "hazelcast.com",
          "kind": "ReplicatedMap",
          "version": "v1alpha1"
        },
        {
          "group": "hazelcast.com",
          "kind": "Topic",
          "version": "v1alpha1"
        },
        {
          "group": "hazelcast.com",
          "kind": "WanReplication",
          "version": "v1alpha1"
        }
      ],
      "provider": "Hazelcast, Inc",
      "related_images": [
        {
          "digest": "sha256:7d60383a54855351f0e9c50bec415990b6306d0bff832eaec34cfe7929c791da",
          "image": "docker.io/hazelcast/hazelcast-platform-operator@sha256:7d60383a54855351f0e9c50bec415990b6306d0bff832eaec34cfe7929c791da",
          "name": "hazelcast-platform-operator-7d60383a54855351f0e9c50bec415990b6306d0bff832eaec34cfe7929c791da-annotation"
        },
        {
          "digest": "sha256:7d60383a54855351f0e9c50bec415990b6306d0bff832eaec34cfe7929c791da",
          "image": "docker.io/hazelcast/hazelcast-platform-operator@sha256:7d60383a54855351f0e9c50bec415990b6306d0bff832eaec34cfe7929c791da",
          "name": "manager"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.12",
      "version": "5.5.0",
      "version_original": "5.5.0"
    },
    {
      "_id": "637ec8a78f4a976d0416e608",
      "alm_examples": [
        {
          "api_version": "hazelcast.com/v1alpha1",
          "kind": "CronHotBackup",
          "metadata": {
            "name": "cronhotbackup-sample"
          },
          "spec": {
            "hotBackupTemplate": {
              "metadata": {
                "labels": {
                  "cron-hotbackup": "true"
                }
              },
              "spec": {
                "hazelcastResourceName": "hazelcast"
              }
            },
            "schedule": "*/30 * * * *"
          }
        },
        {
          "api_version": "hazelcast.com/v1alpha1",
          "kind": "Hazelcast",
          "metadata": {
            "name": "hazelcast"
          },
          "spec": {
            "clusterSize": 3,
            "licenseKeySecret": "hazelcast-license-key",
            "repository": "docker.io/hazelcast/hazelcast-enterprise",
            "version": "5.1.4"
          }
        },
        {
          "api_version": "hazelcast.com/v1alpha1",
          "kind": "ManagementCenter",
          "metadata": {
            "name": "managementcenter"
          },
          "spec": {
            "externalConnectivity": {
              "type": "LoadBalancer"
            },
            "hazelcastClusters": [
              {
                "address": "hazelcast",
                "name": "dev"
              }
            ],
            "licenseKeySecret": "hazelcast-license-key",
            "persistence": {
              "enabled": true,
              "size": "10Gi"
            },
            "repository": "hazelcast/management-center",
            "version": "5.1.4"
          }
        },
        {
          "api_version": "hazelcast.com/v1alpha1",
          "kind": "Map",
          "metadata": {
            "name": "map"
          },
          "spec": {
            "hazelcastResourceName": "hazelcast"
          }
        },
        {
          "api_version": "hazelcast.com/v1alpha1",
          "kind": "MultiMap",
          "metadata": {
            "name": "multimap-sample"
          },
          "spec": {
            "backupCount": 3,
            "binary": true,
            "collectionType": "LIST",
            "hazelcastResourceName": "hazelcast",
            "name": "multi-map"
          }
        },
        {
          "api_version": "hazelcast.com/v1alpha1",
          "kind": "ReplicatedMap",
          "metadata": {
            "name": "replicatedmap-sample"
          },
          "spec": {
            "asyncFillup": true,
            "hazelcastResourceName": "hazelcast",
            "inMemoryFormat": "OBJECT",
            "name": "replicated-map"
          }
        },
        {
          "api_version": "hazelcast.com/v1alpha1",
          "kind": "Topic",
          "metadata": {
            "name": "topic-sample2"
          },
          "spec": {
            "globalOrderingEnabled": true,
            "hazelcastResourceName": "hazelcast"
          }
        },
        {
          "api_version": "hazelcast.com/v1alpha1",
          "kind": "WanReplication",
          "metadata": {
            "name": "wanreplication-sample"
          },
          "spec": {
            "endpoints": "35.192.33.252",
            "resources": [
              {
                "kind": "Map",
                "name": "map-sample"
              },
              {
                "kind": "Hazelcast",
                "name": "hz-sample"
              }
            ],
            "targetClusterName": "dev"
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/hazelcast/hazelcast-platform-operator-bundle@sha256:0d6f326f88e90ddac3bbfae0451c79e9e414111f0fe1eac09d1719a4fb0aed9e",
      "bundle_path_digest": "sha256:0d6f326f88e90ddac3bbfae0451c79e9e414111f0fe1eac09d1719a4fb0aed9e",
      "capabilities": [
        "Seamless Upgrades"
      ],
      "channel_name": "alpha",
      "creation_date": "2022-11-24T01:28:07.161000+00:00",
      "csv_description": "# Hazelcast Platform Operator #\n\nEasily deploy Hazelcast clusters and Management Center into Kubernetes environments and manage their lifecycles.\n\n## Before You Start\n\nIf you are planning to create Hazelcast Platform Enterprise clusters, you need to [create a secret](https://docs.hazelcast.com/operator/latest/get-started#step-2-start-the-hazelcast-cluster) for the license. You can request a trial license key from [here](https://trialrequest.hazelcast.com).\n\nFor Hazelcast Platform clusters, you can simply continue.\n\n## Documentation\n\n1. [Get started](https://docs.hazelcast.com/operator/latest/get-started) with the Operator.\n2. [Connect to the cluster from outside Kubernetes](https://guides.hazelcast.org/hazelcast-platform-operator-expose-externally/main)\n  from the outside.\n3. [Restore a Cluster from Cloud Storage with Hazelcast Platform Operator](https://docs.hazelcast.com/tutorials/hazelcast-platform-operator-external-backup-restore)\n4. [Replicate Data between Two Hazelcast Clusters with Hazelcast Platform Operator](https://docs.hazelcast.com/tutorials/hazelcast-platform-operator-wan-replication)\n\n## Features\n\nHazelcast Platform Operator supports the features below:\n\n* Custom resource for Hazelcast Platform (Open Source & Enterprise) and Management Center\n* Observe status of Hazelcast and Management Center clusters\n* Scale up and down Hazelcast clusters\n* Expose Hazelcast cluster to external\n  clients ([Smart & Unisocket](https://docs.hazelcast.com/hazelcast/latest/clients/java#java-client-operation-modes))\n* Backup Hazelcast persistence data to cloud storage with the possibility of scheduling it and restoring the data accordingly\n* WAN Replication feature when you need to synchronize multiple Hazelcast clusters, which are connected by WANs\n* User Code Deployment feature, which allows you to deploy custom and domain classes from cloud storages to Hazelcast members\n* ExecutorService and EntryProcessor support\n* Support several data structures like Map, Topic, MultiMap, and ReplicatedMap, which can be created dynamically via specific Custom Resources\n* MapStore support for Map CR\n",
      "csv_display_name": "Hazelcast Platform Operator",
      "csv_metadata_description": "Install Hazelcast clusters in Kubernetes environments.",
      "csv_name": "hazelcast-platform-operator.v5.5.0",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": false,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-11-24T01:28:07.161000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.10",
      "organization": "certified-operators",
      "package": "hazelcast-platform-operator",
      "provided_apis": [
        {
          "group": "hazelcast.com",
          "kind": "Hazelcast",
          "plural": "hazelcasts",
          "version": "v1alpha1"
        },
        {
          "group": "hazelcast.com",
          "kind": "MultiMap",
          "plural": "multimaps",
          "version": "v1alpha1"
        },
        {
          "group": "hazelcast.com",
          "kind": "HotBackup",
          "plural": "hotbackups",
          "version": "v1alpha1"
        },
        {
          "group": "hazelcast.com",
          "kind": "ManagementCenter",
          "plural": "managementcenters",
          "version": "v1alpha1"
        },
        {
          "group": "hazelcast.com",
          "kind": "Map",
          "plural": "maps",
          "version": "v1alpha1"
        },
        {
          "group": "hazelcast.com",
          "kind": "CronHotBackup",
          "plural": "cronhotbackups",
          "version": "v1alpha1"
        },
        {
          "group": "hazelcast.com",
          "kind": "ReplicatedMap",
          "plural": "replicatedmaps",
          "version": "v1alpha1"
        },
        {
          "group": "hazelcast.com",
          "kind": "Topic",
          "plural": "topics",
          "version": "v1alpha1"
        },
        {
          "group": "hazelcast.com",
          "kind": "WanReplication",
          "plural": "wanreplications",
          "version": "v1alpha1"
        }
      ],
      "provider": "Hazelcast, Inc",
      "related_images": [
        {
          "digest": "sha256:7d60383a54855351f0e9c50bec415990b6306d0bff832eaec34cfe7929c791da",
          "image": "docker.io/hazelcast/hazelcast-platform-operator@sha256:7d60383a54855351f0e9c50bec415990b6306d0bff832eaec34cfe7929c791da",
          "name": "hazelcast-platform-operator-7d60383a54855351f0e9c50bec415990b6306d0bff832eaec34cfe7929c791da-annotation"
        },
        {
          "digest": "sha256:7d60383a54855351f0e9c50bec415990b6306d0bff832eaec34cfe7929c791da",
          "image": "docker.io/hazelcast/hazelcast-platform-operator@sha256:7d60383a54855351f0e9c50bec415990b6306d0bff832eaec34cfe7929c791da",
          "name": "manager"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.10",
      "version": "5.5.0",
      "version_original": "5.5.0"
    },
    {
      "_id": "637ec8b8bb5a79c41a873ae5",
      "alm_examples": [
        {
          "api_version": "hazelcast.com/v1alpha1",
          "kind": "CronHotBackup",
          "metadata": {
            "name": "cronhotbackup-sample"
          },
          "spec": {
            "hotBackupTemplate": {
              "metadata": {
                "labels": {
                  "cron-hotbackup": "true"
                }
              },
              "spec": {
                "hazelcastResourceName": "hazelcast"
              }
            },
            "schedule": "*/30 * * * *"
          }
        },
        {
          "api_version": "hazelcast.com/v1alpha1",
          "kind": "Hazelcast",
          "metadata": {
            "name": "hazelcast"
          },
          "spec": {
            "clusterSize": 3,
            "licenseKeySecret": "hazelcast-license-key",
            "repository": "docker.io/hazelcast/hazelcast-enterprise",
            "version": "5.1.4"
          }
        },
        {
          "api_version": "hazelcast.com/v1alpha1",
          "kind": "ManagementCenter",
          "metadata": {
            "name": "managementcenter"
          },
          "spec": {
            "externalConnectivity": {
              "type": "LoadBalancer"
            },
            "hazelcastClusters": [
              {
                "address": "hazelcast",
                "name": "dev"
              }
            ],
            "licenseKeySecret": "hazelcast-license-key",
            "persistence": {
              "enabled": true,
              "size": "10Gi"
            },
            "repository": "hazelcast/management-center",
            "version": "5.1.4"
          }
        },
        {
          "api_version": "hazelcast.com/v1alpha1",
          "kind": "Map",
          "metadata": {
            "name": "map"
          },
          "spec": {
            "hazelcastResourceName": "hazelcast"
          }
        },
        {
          "api_version": "hazelcast.com/v1alpha1",
          "kind": "MultiMap",
          "metadata": {
            "name": "multimap-sample"
          },
          "spec": {
            "backupCount": 3,
            "binary": true,
            "collectionType": "LIST",
            "hazelcastResourceName": "hazelcast",
            "name": "multi-map"
          }
        },
        {
          "api_version": "hazelcast.com/v1alpha1",
          "kind": "ReplicatedMap",
          "metadata": {
            "name": "replicatedmap-sample"
          },
          "spec": {
            "asyncFillup": true,
            "hazelcastResourceName": "hazelcast",
            "inMemoryFormat": "OBJECT",
            "name": "replicated-map"
          }
        },
        {
          "api_version": "hazelcast.com/v1alpha1",
          "kind": "Topic",
          "metadata": {
            "name": "topic-sample2"
          },
          "spec": {
            "globalOrderingEnabled": true,
            "hazelcastResourceName": "hazelcast"
          }
        },
        {
          "api_version": "hazelcast.com/v1alpha1",
          "kind": "WanReplication",
          "metadata": {
            "name": "wanreplication-sample"
          },
          "spec": {
            "endpoints": "35.192.33.252",
            "resources": [
              {
                "kind": "Map",
                "name": "map-sample"
              },
              {
                "kind": "Hazelcast",
                "name": "hz-sample"
              }
            ],
            "targetClusterName": "dev"
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/hazelcast/hazelcast-platform-operator-bundle@sha256:0d6f326f88e90ddac3bbfae0451c79e9e414111f0fe1eac09d1719a4fb0aed9e",
      "bundle_path_digest": "sha256:0d6f326f88e90ddac3bbfae0451c79e9e414111f0fe1eac09d1719a4fb0aed9e",
      "capabilities": [
        "Seamless Upgrades"
      ],
      "channel_name": "alpha",
      "creation_date": "2022-11-24T01:28:24.804000+00:00",
      "csv_description": "# Hazelcast Platform Operator #\n\nEasily deploy Hazelcast clusters and Management Center into Kubernetes environments and manage their lifecycles.\n\n## Before You Start\n\nIf you are planning to create Hazelcast Platform Enterprise clusters, you need to [create a secret](https://docs.hazelcast.com/operator/latest/get-started#step-2-start-the-hazelcast-cluster) for the license. You can request a trial license key from [here](https://trialrequest.hazelcast.com).\n\nFor Hazelcast Platform clusters, you can simply continue.\n\n## Documentation\n\n1. [Get started](https://docs.hazelcast.com/operator/latest/get-started) with the Operator.\n2. [Connect to the cluster from outside Kubernetes](https://guides.hazelcast.org/hazelcast-platform-operator-expose-externally/main)\n  from the outside.\n3. [Restore a Cluster from Cloud Storage with Hazelcast Platform Operator](https://docs.hazelcast.com/tutorials/hazelcast-platform-operator-external-backup-restore)\n4. [Replicate Data between Two Hazelcast Clusters with Hazelcast Platform Operator](https://docs.hazelcast.com/tutorials/hazelcast-platform-operator-wan-replication)\n\n## Features\n\nHazelcast Platform Operator supports the features below:\n\n* Custom resource for Hazelcast Platform (Open Source & Enterprise) and Management Center\n* Observe status of Hazelcast and Management Center clusters\n* Scale up and down Hazelcast clusters\n* Expose Hazelcast cluster to external\n  clients ([Smart & Unisocket](https://docs.hazelcast.com/hazelcast/latest/clients/java#java-client-operation-modes))\n* Backup Hazelcast persistence data to cloud storage with the possibility of scheduling it and restoring the data accordingly\n* WAN Replication feature when you need to synchronize multiple Hazelcast clusters, which are connected by WANs\n* User Code Deployment feature, which allows you to deploy custom and domain classes from cloud storages to Hazelcast members\n* ExecutorService and EntryProcessor support\n* Support several data structures like Map, Topic, MultiMap, and ReplicatedMap, which can be created dynamically via specific Custom Resources\n* MapStore support for Map CR\n",
      "csv_display_name": "Hazelcast Platform Operator",
      "csv_metadata_description": "Install Hazelcast clusters in Kubernetes environments.",
      "csv_name": "hazelcast-platform-operator.v5.5.0",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": false,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-11-24T01:28:24.804000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.9",
      "organization": "certified-operators",
      "package": "hazelcast-platform-operator",
      "provided_apis": [
        {
          "group": "hazelcast.com",
          "kind": "WanReplication",
          "plural": "wanreplications",
          "version": "v1alpha1"
        },
        {
          "group": "hazelcast.com",
          "kind": "ManagementCenter",
          "plural": "managementcenters",
          "version": "v1alpha1"
        },
        {
          "group": "hazelcast.com",
          "kind": "MultiMap",
          "plural": "multimaps",
          "version": "v1alpha1"
        },
        {
          "group": "hazelcast.com",
          "kind": "Topic",
          "plural": "topics",
          "version": "v1alpha1"
        },
        {
          "group": "hazelcast.com",
          "kind": "HotBackup",
          "plural": "hotbackups",
          "version": "v1alpha1"
        },
        {
          "group": "hazelcast.com",
          "kind": "CronHotBackup",
          "plural": "cronhotbackups",
          "version": "v1alpha1"
        },
        {
          "group": "hazelcast.com",
          "kind": "Map",
          "plural": "maps",
          "version": "v1alpha1"
        },
        {
          "group": "hazelcast.com",
          "kind": "ReplicatedMap",
          "plural": "replicatedmaps",
          "version": "v1alpha1"
        },
        {
          "group": "hazelcast.com",
          "kind": "Hazelcast",
          "plural": "hazelcasts",
          "version": "v1alpha1"
        }
      ],
      "provider": "Hazelcast, Inc",
      "related_images": [
        {
          "digest": "sha256:7d60383a54855351f0e9c50bec415990b6306d0bff832eaec34cfe7929c791da",
          "image": "docker.io/hazelcast/hazelcast-platform-operator@sha256:7d60383a54855351f0e9c50bec415990b6306d0bff832eaec34cfe7929c791da",
          "name": "hazelcast-platform-operator-7d60383a54855351f0e9c50bec415990b6306d0bff832eaec34cfe7929c791da-annotation"
        },
        {
          "digest": "sha256:7d60383a54855351f0e9c50bec415990b6306d0bff832eaec34cfe7929c791da",
          "image": "docker.io/hazelcast/hazelcast-platform-operator@sha256:7d60383a54855351f0e9c50bec415990b6306d0bff832eaec34cfe7929c791da",
          "name": "manager"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.9",
      "version": "5.5.0",
      "version_original": "5.5.0"
    },
    {
      "_id": "637ec92720b0e7da8ea6972f",
      "alm_examples": [
        {
          "api_version": "hazelcast.com/v1alpha1",
          "kind": "CronHotBackup",
          "metadata": {
            "name": "cronhotbackup-sample"
          },
          "spec": {
            "hotBackupTemplate": {
              "metadata": {
                "labels": {
                  "cron-hotbackup": "true"
                }
              },
              "spec": {
                "hazelcastResourceName": "hazelcast"
              }
            },
            "schedule": "*/30 * * * *"
          }
        },
        {
          "api_version": "hazelcast.com/v1alpha1",
          "kind": "Hazelcast",
          "metadata": {
            "name": "hazelcast"
          },
          "spec": {
            "clusterSize": 3,
            "licenseKeySecret": "hazelcast-license-key",
            "repository": "docker.io/hazelcast/hazelcast-enterprise",
            "version": "5.1.4"
          }
        },
        {
          "api_version": "hazelcast.com/v1alpha1",
          "kind": "ManagementCenter",
          "metadata": {
            "name": "managementcenter"
          },
          "spec": {
            "externalConnectivity": {
              "type": "LoadBalancer"
            },
            "hazelcastClusters": [
              {
                "address": "hazelcast",
                "name": "dev"
              }
            ],
            "licenseKeySecret": "hazelcast-license-key",
            "persistence": {
              "enabled": true,
              "size": "10Gi"
            },
            "repository": "hazelcast/management-center",
            "version": "5.1.4"
          }
        },
        {
          "api_version": "hazelcast.com/v1alpha1",
          "kind": "Map",
          "metadata": {
            "name": "map"
          },
          "spec": {
            "hazelcastResourceName": "hazelcast"
          }
        },
        {
          "api_version": "hazelcast.com/v1alpha1",
          "kind": "MultiMap",
          "metadata": {
            "name": "multimap-sample"
          },
          "spec": {
            "backupCount": 3,
            "binary": true,
            "collectionType": "LIST",
            "hazelcastResourceName": "hazelcast",
            "name": "multi-map"
          }
        },
        {
          "api_version": "hazelcast.com/v1alpha1",
          "kind": "ReplicatedMap",
          "metadata": {
            "name": "replicatedmap-sample"
          },
          "spec": {
            "asyncFillup": true,
            "hazelcastResourceName": "hazelcast",
            "inMemoryFormat": "OBJECT",
            "name": "replicated-map"
          }
        },
        {
          "api_version": "hazelcast.com/v1alpha1",
          "kind": "Topic",
          "metadata": {
            "name": "topic-sample2"
          },
          "spec": {
            "globalOrderingEnabled": true,
            "hazelcastResourceName": "hazelcast"
          }
        },
        {
          "api_version": "hazelcast.com/v1alpha1",
          "kind": "WanReplication",
          "metadata": {
            "name": "wanreplication-sample"
          },
          "spec": {
            "endpoints": "35.192.33.252",
            "resources": [
              {
                "kind": "Map",
                "name": "map-sample"
              },
              {
                "kind": "Hazelcast",
                "name": "hz-sample"
              }
            ],
            "targetClusterName": "dev"
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/hazelcast/hazelcast-platform-operator-bundle@sha256:0d6f326f88e90ddac3bbfae0451c79e9e414111f0fe1eac09d1719a4fb0aed9e",
      "bundle_path_digest": "sha256:0d6f326f88e90ddac3bbfae0451c79e9e414111f0fe1eac09d1719a4fb0aed9e",
      "capabilities": [
        "Seamless Upgrades"
      ],
      "channel_name": "alpha",
      "creation_date": "2022-11-24T01:30:15.928000+00:00",
      "csv_description": "# Hazelcast Platform Operator #\n\nEasily deploy Hazelcast clusters and Management Center into Kubernetes environments and manage their lifecycles.\n\n## Before You Start\n\nIf you are planning to create Hazelcast Platform Enterprise clusters, you need to [create a secret](https://docs.hazelcast.com/operator/latest/get-started#step-2-start-the-hazelcast-cluster) for the license. You can request a trial license key from [here](https://trialrequest.hazelcast.com).\n\nFor Hazelcast Platform clusters, you can simply continue.\n\n## Documentation\n\n1. [Get started](https://docs.hazelcast.com/operator/latest/get-started) with the Operator.\n2. [Connect to the cluster from outside Kubernetes](https://guides.hazelcast.org/hazelcast-platform-operator-expose-externally/main)\n  from the outside.\n3. [Restore a Cluster from Cloud Storage with Hazelcast Platform Operator](https://docs.hazelcast.com/tutorials/hazelcast-platform-operator-external-backup-restore)\n4. [Replicate Data between Two Hazelcast Clusters with Hazelcast Platform Operator](https://docs.hazelcast.com/tutorials/hazelcast-platform-operator-wan-replication)\n\n## Features\n\nHazelcast Platform Operator supports the features below:\n\n* Custom resource for Hazelcast Platform (Open Source & Enterprise) and Management Center\n* Observe status of Hazelcast and Management Center clusters\n* Scale up and down Hazelcast clusters\n* Expose Hazelcast cluster to external\n  clients ([Smart & Unisocket](https://docs.hazelcast.com/hazelcast/latest/clients/java#java-client-operation-modes))\n* Backup Hazelcast persistence data to cloud storage with the possibility of scheduling it and restoring the data accordingly\n* WAN Replication feature when you need to synchronize multiple Hazelcast clusters, which are connected by WANs\n* User Code Deployment feature, which allows you to deploy custom and domain classes from cloud storages to Hazelcast members\n* ExecutorService and EntryProcessor support\n* Support several data structures like Map, Topic, MultiMap, and ReplicatedMap, which can be created dynamically via specific Custom Resources\n* MapStore support for Map CR\n",
      "csv_display_name": "Hazelcast Platform Operator",
      "csv_metadata_description": "Install Hazelcast clusters in Kubernetes environments.",
      "csv_name": "hazelcast-platform-operator.v5.5.0",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": false,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-11-24T01:30:15.928000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.8",
      "organization": "certified-operators",
      "package": "hazelcast-platform-operator",
      "provided_apis": [
        {
          "group": "hazelcast.com",
          "kind": "Hazelcast",
          "plural": "hazelcasts",
          "version": "v1alpha1"
        },
        {
          "group": "hazelcast.com",
          "kind": "ManagementCenter",
          "plural": "managementcenters",
          "version": "v1alpha1"
        },
        {
          "group": "hazelcast.com",
          "kind": "MultiMap",
          "plural": "multimaps",
          "version": "v1alpha1"
        },
        {
          "group": "hazelcast.com",
          "kind": "CronHotBackup",
          "plural": "cronhotbackups",
          "version": "v1alpha1"
        },
        {
          "group": "hazelcast.com",
          "kind": "HotBackup",
          "plural": "hotbackups",
          "version": "v1alpha1"
        },
        {
          "group": "hazelcast.com",
          "kind": "ReplicatedMap",
          "plural": "replicatedmaps",
          "version": "v1alpha1"
        },
        {
          "group": "hazelcast.com",
          "kind": "WanReplication",
          "plural": "wanreplications",
          "version": "v1alpha1"
        },
        {
          "group": "hazelcast.com",
          "kind": "Map",
          "plural": "maps",
          "version": "v1alpha1"
        },
        {
          "group": "hazelcast.com",
          "kind": "Topic",
          "plural": "topics",
          "version": "v1alpha1"
        }
      ],
      "provider": "Hazelcast, Inc",
      "related_images": [
        {
          "digest": "sha256:7d60383a54855351f0e9c50bec415990b6306d0bff832eaec34cfe7929c791da",
          "image": "docker.io/hazelcast/hazelcast-platform-operator@sha256:7d60383a54855351f0e9c50bec415990b6306d0bff832eaec34cfe7929c791da",
          "name": "hazelcast-platform-operator-7d60383a54855351f0e9c50bec415990b6306d0bff832eaec34cfe7929c791da-annotation"
        },
        {
          "digest": "sha256:7d60383a54855351f0e9c50bec415990b6306d0bff832eaec34cfe7929c791da",
          "image": "docker.io/hazelcast/hazelcast-platform-operator@sha256:7d60383a54855351f0e9c50bec415990b6306d0bff832eaec34cfe7929c791da",
          "name": "manager"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.8",
      "version": "5.5.0",
      "version_original": "5.5.0"
    },
    {
      "_id": "63808c8b20b0e7da8eaf6cad",
      "alm_examples": [
        {
          "api_version": "charts.helm.k8s.io/v1",
          "kind": "OpenshiftArtifactoryHa",
          "metadata": {
            "name": "openshiftartifactoryha"
          },
          "spec": {
            "artifactory-ha": {
              "artifactory": {
                "joinKey": "OVERRIDE",
                "masterKey": "OVERRIDE",
                "node": {
                  "replicaCount": 2,
                  "waitForPrimaryStartup": {
                    "enabled": false
                  }
                },
                "uid": "1000721030"
              },
              "database": {
                "driver": "OVERRIDE",
                "password": "OVERRIDE",
                "type": "OVERRIDE",
                "url": "OVERRIDE",
                "user": "OVERRIDE"
              },
              "databaseUpgradeReady": true,
              "nginx": {
                "gid": "1000720107",
                "http": {
                  "externalPort": 80,
                  "internalPort": 8080
                },
                "https": {
                  "externalPort": 443,
                  "internalPort": 8443
                },
                "service": {
                  "ssloffload": false
                },
                "tlsSecretName": "OVERRIDE",
                "uid": "1000720104"
              },
              "postgresql": {
                "enabled": false
              },
              "waitForDatabase": true
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/jfrog/artifactory-operator-bundle@sha256:d765be1c6edf9c12ea55064546bf3b0284c769479bdb890df580e7b2ab5cfa7c",
      "bundle_path_digest": "sha256:d765be1c6edf9c12ea55064546bf3b0284c769479bdb890df580e7b2ab5cfa7c",
      "capabilities": [
        "Seamless Upgrades"
      ],
      "channel_name": "alpha",
      "creation_date": "2022-11-25T09:36:11.814000+00:00",
      "csv_description": " ## Breaking change\nPlease update to version 1.1.10 of the operator.\nVersion 1.1.8 and 1.1.9 have issues. Please skip these versions.\nIf you are using operator version < 1.2.0 there is a issue with upgrade where artifactory version is not updated if you update operator version.As workaround you can update the Custom Resource OpenshiftArtifactoryHa which has hardcored image value.After upgrading to version 1.2.0 you can remove the below parameters from your CR so that future upgrade works without workaround. ``` artifactory-ha.artifactory.image.registry artifactory-ha.artifactory.image.repository artifactory-ha.artifactory.image.tag ```\n## Overview\nOpenshift Operator to deploy JFrog Artifactory Enterprise into your Openshift cluster.\n## Security Context Constraints\nTo deploy this helm chart you will need to be a cluster admin w/ access to the anyuid scc and add the operator service account to the anyuid scc.\n```\noc adm policy add-scc-to-user anyuid -z artifactory-ha-operator -n openshift-operators\n```\nAdd the service account for the Artifactory chart to deploy successfully:\n``` oc adm policy add-scc-to-user anyuid -z openshiftartifactoryha-artifactory-ha -n artifactory ```\n### Usage\n\nAn external DB is required. The operator will not deploy a DB but will require you to specify the configuration values to connect to it.\n\nSearch for JFrog and click JFrog Artifactory Enterprise Operator to install.\n\nGo to the Installed Operators.\n\nWait for the JFrog Artifactory Enterprise Operator to complete the installation.\n\nOpen the Operator and click on the provided API: Artifactory HA.\n\nClick Create New Instance and provide the following parameters for your DB configuration:\n\n```\nDATABASE_TYPE\nDATABASE_DRIVER\nDATABASE_URL\nDATABASE_USER\nDATABASE_PASSWORD\n```\nMaster key and Join key must be supplied. To generate a new key for each run the command below:\n\n```\n# Create a key\nexport JOIN_KEY=$(openssl rand -hex 32)\necho ${JOIN_KEY}\n```\n\nTo use TLS you will need to first create a k8s tls secret to store your .crt and .key file into.\nThen supply the value of this k8s secret into the TLS_SECRET field.\n``` oc create secret tls my_tls_secret --cert=tls.crt --key=tls.key --namespace=my_namespace ```\nClick Create for Artifactory Enterprise to deploy into OpenShift and connect to it on the external IP exposed by the load balancer.\n### Create a route\nTo expose Artifactory from Openshift we recommend you create a new route in Openshift.\nYou can either use the oc command line tool or the Openshift web console to generate a new route.\nDepending upon where you terminate TLS you will need to either specify pass through or edge.\nCommand Line (Edge):\n``` oc create route edge --service=openshiftartifactory-ha --cert=tls.crt --key=tls.key --ca-cert=ca.crt --hostname=www.example.com ```\nOr for more information visit the official Openshift documentation on routes here:\nhttps://docs.openshift.com/container-platform/4.6/networking/routes/route-configuration.html\n\n",
      "csv_display_name": "JFrog Artifactory Enterprise Operator",
      "csv_metadata_description": "JFrog Artifactory Enterprise deploys Artifactory in a high availability environment across multiple pods",
      "csv_name": "artifactory-ha-operator.v1.2.2",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-11-25T09:36:11.814000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.10",
      "organization": "certified-operators",
      "package": "openshiftartifactoryha-operator",
      "provided_apis": [
        {
          "group": "charts.helm.k8s.io",
          "kind": "OpenshiftArtifactoryHa",
          "plural": "openshiftartifactoryhas",
          "version": "v1"
        }
      ],
      "provider": "JFrog",
      "related_images": [
        {
          "digest": "sha256:0b085c2e0984b592ca816827b6df1c52294c480e8e9fb03e0acc32924faac486",
          "image": "registry.connect.redhat.com/jfrog/artifactory-pro@sha256:0b085c2e0984b592ca816827b6df1c52294c480e8e9fb03e0acc32924faac486",
          "name": "artifactory-pro"
        },
        {
          "digest": "sha256:a0ecd4a391ad678125f605dcdf4dfd06c9278c6d23db01335093b1361200c9f0",
          "image": "registry.connect.redhat.com/jfrog/artifactory-operator@sha256:a0ecd4a391ad678125f605dcdf4dfd06c9278c6d23db01335093b1361200c9f0",
          "name": "artifactory-operator"
        },
        {
          "digest": "sha256:bce2fa53c4e5b913c707183f49ab9dcde9e601b22f9fb85b98ba56f2e163c1a8",
          "image": "registry.redhat.io/rhel8/nginx-116@sha256:bce2fa53c4e5b913c707183f49ab9dcde9e601b22f9fb85b98ba56f2e163c1a8",
          "name": "nginx"
        },
        {
          "digest": "sha256:a0ecd4a391ad678125f605dcdf4dfd06c9278c6d23db01335093b1361200c9f0",
          "image": "registry.connect.redhat.com/jfrog/artifactory-operator@sha256:a0ecd4a391ad678125f605dcdf4dfd06c9278c6d23db01335093b1361200c9f0",
          "name": "artifactory-operator-a0ecd4a391ad678125f605dcdf4dfd06c9278c6d23db01335093b1361200c9f0-annotation"
        },
        {
          "digest": "sha256:a0ecd4a391ad678125f605dcdf4dfd06c9278c6d23db01335093b1361200c9f0",
          "image": "registry.connect.redhat.com/jfrog/artifactory-operator@sha256:a0ecd4a391ad678125f605dcdf4dfd06c9278c6d23db01335093b1361200c9f0",
          "name": "artifactory-ha-operator"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.10",
      "version": "1.2.2",
      "version_original": "1.2.2"
    },
    {
      "_id": "63808d5be6ccc0257e0cbb10",
      "alm_examples": [
        {
          "api_version": "charts.helm.k8s.io/v1",
          "kind": "OpenshiftArtifactoryHa",
          "metadata": {
            "name": "openshiftartifactoryha"
          },
          "spec": {
            "artifactory-ha": {
              "artifactory": {
                "joinKey": "OVERRIDE",
                "masterKey": "OVERRIDE",
                "node": {
                  "replicaCount": 2,
                  "waitForPrimaryStartup": {
                    "enabled": false
                  }
                },
                "uid": "1000721030"
              },
              "database": {
                "driver": "OVERRIDE",
                "password": "OVERRIDE",
                "type": "OVERRIDE",
                "url": "OVERRIDE",
                "user": "OVERRIDE"
              },
              "databaseUpgradeReady": true,
              "nginx": {
                "gid": "1000720107",
                "http": {
                  "externalPort": 80,
                  "internalPort": 8080
                },
                "https": {
                  "externalPort": 443,
                  "internalPort": 8443
                },
                "service": {
                  "ssloffload": false
                },
                "tlsSecretName": "OVERRIDE",
                "uid": "1000720104"
              },
              "postgresql": {
                "enabled": false
              },
              "waitForDatabase": true
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/jfrog/artifactory-operator-bundle@sha256:d765be1c6edf9c12ea55064546bf3b0284c769479bdb890df580e7b2ab5cfa7c",
      "bundle_path_digest": "sha256:d765be1c6edf9c12ea55064546bf3b0284c769479bdb890df580e7b2ab5cfa7c",
      "capabilities": [
        "Seamless Upgrades"
      ],
      "channel_name": "alpha",
      "creation_date": "2022-11-25T09:39:39.567000+00:00",
      "csv_description": " ## Breaking change\nPlease update to version 1.1.10 of the operator.\nVersion 1.1.8 and 1.1.9 have issues. Please skip these versions.\nIf you are using operator version < 1.2.0 there is a issue with upgrade where artifactory version is not updated if you update operator version.As workaround you can update the Custom Resource OpenshiftArtifactoryHa which has hardcored image value.After upgrading to version 1.2.0 you can remove the below parameters from your CR so that future upgrade works without workaround. ``` artifactory-ha.artifactory.image.registry artifactory-ha.artifactory.image.repository artifactory-ha.artifactory.image.tag ```\n## Overview\nOpenshift Operator to deploy JFrog Artifactory Enterprise into your Openshift cluster.\n## Security Context Constraints\nTo deploy this helm chart you will need to be a cluster admin w/ access to the anyuid scc and add the operator service account to the anyuid scc.\n```\noc adm policy add-scc-to-user anyuid -z artifactory-ha-operator -n openshift-operators\n```\nAdd the service account for the Artifactory chart to deploy successfully:\n``` oc adm policy add-scc-to-user anyuid -z openshiftartifactoryha-artifactory-ha -n artifactory ```\n### Usage\n\nAn external DB is required. The operator will not deploy a DB but will require you to specify the configuration values to connect to it.\n\nSearch for JFrog and click JFrog Artifactory Enterprise Operator to install.\n\nGo to the Installed Operators.\n\nWait for the JFrog Artifactory Enterprise Operator to complete the installation.\n\nOpen the Operator and click on the provided API: Artifactory HA.\n\nClick Create New Instance and provide the following parameters for your DB configuration:\n\n```\nDATABASE_TYPE\nDATABASE_DRIVER\nDATABASE_URL\nDATABASE_USER\nDATABASE_PASSWORD\n```\nMaster key and Join key must be supplied. To generate a new key for each run the command below:\n\n```\n# Create a key\nexport JOIN_KEY=$(openssl rand -hex 32)\necho ${JOIN_KEY}\n```\n\nTo use TLS you will need to first create a k8s tls secret to store your .crt and .key file into.\nThen supply the value of this k8s secret into the TLS_SECRET field.\n``` oc create secret tls my_tls_secret --cert=tls.crt --key=tls.key --namespace=my_namespace ```\nClick Create for Artifactory Enterprise to deploy into OpenShift and connect to it on the external IP exposed by the load balancer.\n### Create a route\nTo expose Artifactory from Openshift we recommend you create a new route in Openshift.\nYou can either use the oc command line tool or the Openshift web console to generate a new route.\nDepending upon where you terminate TLS you will need to either specify pass through or edge.\nCommand Line (Edge):\n``` oc create route edge --service=openshiftartifactory-ha --cert=tls.crt --key=tls.key --ca-cert=ca.crt --hostname=www.example.com ```\nOr for more information visit the official Openshift documentation on routes here:\nhttps://docs.openshift.com/container-platform/4.6/networking/routes/route-configuration.html\n\n",
      "csv_display_name": "JFrog Artifactory Enterprise Operator",
      "csv_metadata_description": "JFrog Artifactory Enterprise deploys Artifactory in a high availability environment across multiple pods",
      "csv_name": "artifactory-ha-operator.v1.2.2",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-11-25T09:39:39.567000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.12",
      "organization": "certified-operators",
      "package": "openshiftartifactoryha-operator",
      "provided_apis": [
        {
          "group": "charts.helm.k8s.io",
          "kind": "OpenshiftArtifactoryHa",
          "version": "v1"
        }
      ],
      "provider": "JFrog",
      "related_images": [
        {
          "digest": "sha256:0b085c2e0984b592ca816827b6df1c52294c480e8e9fb03e0acc32924faac486",
          "image": "registry.connect.redhat.com/jfrog/artifactory-pro@sha256:0b085c2e0984b592ca816827b6df1c52294c480e8e9fb03e0acc32924faac486",
          "name": "artifactory-pro"
        },
        {
          "digest": "sha256:a0ecd4a391ad678125f605dcdf4dfd06c9278c6d23db01335093b1361200c9f0",
          "image": "registry.connect.redhat.com/jfrog/artifactory-operator@sha256:a0ecd4a391ad678125f605dcdf4dfd06c9278c6d23db01335093b1361200c9f0",
          "name": "artifactory-operator"
        },
        {
          "digest": "sha256:bce2fa53c4e5b913c707183f49ab9dcde9e601b22f9fb85b98ba56f2e163c1a8",
          "image": "registry.redhat.io/rhel8/nginx-116@sha256:bce2fa53c4e5b913c707183f49ab9dcde9e601b22f9fb85b98ba56f2e163c1a8",
          "name": "nginx"
        },
        {
          "digest": "sha256:a0ecd4a391ad678125f605dcdf4dfd06c9278c6d23db01335093b1361200c9f0",
          "image": "registry.connect.redhat.com/jfrog/artifactory-operator@sha256:a0ecd4a391ad678125f605dcdf4dfd06c9278c6d23db01335093b1361200c9f0",
          "name": "artifactory-operator-a0ecd4a391ad678125f605dcdf4dfd06c9278c6d23db01335093b1361200c9f0-annotation"
        },
        {
          "digest": "sha256:a0ecd4a391ad678125f605dcdf4dfd06c9278c6d23db01335093b1361200c9f0",
          "image": "registry.connect.redhat.com/jfrog/artifactory-operator@sha256:a0ecd4a391ad678125f605dcdf4dfd06c9278c6d23db01335093b1361200c9f0",
          "name": "artifactory-ha-operator"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.12",
      "version": "1.2.2",
      "version_original": "1.2.2"
    },
    {
      "_id": "63808f44d1516305d2f1aa9f",
      "alm_examples": [
        {
          "api_version": "charts.helm.k8s.io/v1",
          "kind": "OpenshiftArtifactoryHa",
          "metadata": {
            "name": "openshiftartifactoryha"
          },
          "spec": {
            "artifactory-ha": {
              "artifactory": {
                "joinKey": "OVERRIDE",
                "masterKey": "OVERRIDE",
                "node": {
                  "replicaCount": 2,
                  "waitForPrimaryStartup": {
                    "enabled": false
                  }
                },
                "uid": "1000721030"
              },
              "database": {
                "driver": "OVERRIDE",
                "password": "OVERRIDE",
                "type": "OVERRIDE",
                "url": "OVERRIDE",
                "user": "OVERRIDE"
              },
              "databaseUpgradeReady": true,
              "nginx": {
                "gid": "1000720107",
                "http": {
                  "externalPort": 80,
                  "internalPort": 8080
                },
                "https": {
                  "externalPort": 443,
                  "internalPort": 8443
                },
                "service": {
                  "ssloffload": false
                },
                "tlsSecretName": "OVERRIDE",
                "uid": "1000720104"
              },
              "postgresql": {
                "enabled": false
              },
              "waitForDatabase": true
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/jfrog/artifactory-operator-bundle@sha256:d765be1c6edf9c12ea55064546bf3b0284c769479bdb890df580e7b2ab5cfa7c",
      "bundle_path_digest": "sha256:d765be1c6edf9c12ea55064546bf3b0284c769479bdb890df580e7b2ab5cfa7c",
      "capabilities": [
        "Seamless Upgrades"
      ],
      "channel_name": "alpha",
      "creation_date": "2022-11-25T09:47:48.405000+00:00",
      "csv_description": " ## Breaking change\nPlease update to version 1.1.10 of the operator.\nVersion 1.1.8 and 1.1.9 have issues. Please skip these versions.\nIf you are using operator version < 1.2.0 there is a issue with upgrade where artifactory version is not updated if you update operator version.As workaround you can update the Custom Resource OpenshiftArtifactoryHa which has hardcored image value.After upgrading to version 1.2.0 you can remove the below parameters from your CR so that future upgrade works without workaround. ``` artifactory-ha.artifactory.image.registry artifactory-ha.artifactory.image.repository artifactory-ha.artifactory.image.tag ```\n## Overview\nOpenshift Operator to deploy JFrog Artifactory Enterprise into your Openshift cluster.\n## Security Context Constraints\nTo deploy this helm chart you will need to be a cluster admin w/ access to the anyuid scc and add the operator service account to the anyuid scc.\n```\noc adm policy add-scc-to-user anyuid -z artifactory-ha-operator -n openshift-operators\n```\nAdd the service account for the Artifactory chart to deploy successfully:\n``` oc adm policy add-scc-to-user anyuid -z openshiftartifactoryha-artifactory-ha -n artifactory ```\n### Usage\n\nAn external DB is required. The operator will not deploy a DB but will require you to specify the configuration values to connect to it.\n\nSearch for JFrog and click JFrog Artifactory Enterprise Operator to install.\n\nGo to the Installed Operators.\n\nWait for the JFrog Artifactory Enterprise Operator to complete the installation.\n\nOpen the Operator and click on the provided API: Artifactory HA.\n\nClick Create New Instance and provide the following parameters for your DB configuration:\n\n```\nDATABASE_TYPE\nDATABASE_DRIVER\nDATABASE_URL\nDATABASE_USER\nDATABASE_PASSWORD\n```\nMaster key and Join key must be supplied. To generate a new key for each run the command below:\n\n```\n# Create a key\nexport JOIN_KEY=$(openssl rand -hex 32)\necho ${JOIN_KEY}\n```\n\nTo use TLS you will need to first create a k8s tls secret to store your .crt and .key file into.\nThen supply the value of this k8s secret into the TLS_SECRET field.\n``` oc create secret tls my_tls_secret --cert=tls.crt --key=tls.key --namespace=my_namespace ```\nClick Create for Artifactory Enterprise to deploy into OpenShift and connect to it on the external IP exposed by the load balancer.\n### Create a route\nTo expose Artifactory from Openshift we recommend you create a new route in Openshift.\nYou can either use the oc command line tool or the Openshift web console to generate a new route.\nDepending upon where you terminate TLS you will need to either specify pass through or edge.\nCommand Line (Edge):\n``` oc create route edge --service=openshiftartifactory-ha --cert=tls.crt --key=tls.key --ca-cert=ca.crt --hostname=www.example.com ```\nOr for more information visit the official Openshift documentation on routes here:\nhttps://docs.openshift.com/container-platform/4.6/networking/routes/route-configuration.html\n\n",
      "csv_display_name": "JFrog Artifactory Enterprise Operator",
      "csv_metadata_description": "JFrog Artifactory Enterprise deploys Artifactory in a high availability environment across multiple pods",
      "csv_name": "artifactory-ha-operator.v1.2.2",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-11-25T09:47:48.405000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.9",
      "organization": "certified-operators",
      "package": "openshiftartifactoryha-operator",
      "provided_apis": [
        {
          "group": "charts.helm.k8s.io",
          "kind": "OpenshiftArtifactoryHa",
          "plural": "openshiftartifactoryhas",
          "version": "v1"
        }
      ],
      "provider": "JFrog",
      "related_images": [
        {
          "digest": "sha256:0b085c2e0984b592ca816827b6df1c52294c480e8e9fb03e0acc32924faac486",
          "image": "registry.connect.redhat.com/jfrog/artifactory-pro@sha256:0b085c2e0984b592ca816827b6df1c52294c480e8e9fb03e0acc32924faac486",
          "name": "artifactory-pro"
        },
        {
          "digest": "sha256:a0ecd4a391ad678125f605dcdf4dfd06c9278c6d23db01335093b1361200c9f0",
          "image": "registry.connect.redhat.com/jfrog/artifactory-operator@sha256:a0ecd4a391ad678125f605dcdf4dfd06c9278c6d23db01335093b1361200c9f0",
          "name": "artifactory-operator"
        },
        {
          "digest": "sha256:bce2fa53c4e5b913c707183f49ab9dcde9e601b22f9fb85b98ba56f2e163c1a8",
          "image": "registry.redhat.io/rhel8/nginx-116@sha256:bce2fa53c4e5b913c707183f49ab9dcde9e601b22f9fb85b98ba56f2e163c1a8",
          "name": "nginx"
        },
        {
          "digest": "sha256:a0ecd4a391ad678125f605dcdf4dfd06c9278c6d23db01335093b1361200c9f0",
          "image": "registry.connect.redhat.com/jfrog/artifactory-operator@sha256:a0ecd4a391ad678125f605dcdf4dfd06c9278c6d23db01335093b1361200c9f0",
          "name": "artifactory-operator-a0ecd4a391ad678125f605dcdf4dfd06c9278c6d23db01335093b1361200c9f0-annotation"
        },
        {
          "digest": "sha256:a0ecd4a391ad678125f605dcdf4dfd06c9278c6d23db01335093b1361200c9f0",
          "image": "registry.connect.redhat.com/jfrog/artifactory-operator@sha256:a0ecd4a391ad678125f605dcdf4dfd06c9278c6d23db01335093b1361200c9f0",
          "name": "artifactory-ha-operator"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.9",
      "version": "1.2.2",
      "version_original": "1.2.2"
    },
    {
      "_id": "63808f70e6ccc0257e0cc188",
      "alm_examples": [
        {
          "api_version": "charts.helm.k8s.io/v1",
          "kind": "OpenshiftArtifactoryHa",
          "metadata": {
            "name": "openshiftartifactoryha"
          },
          "spec": {
            "artifactory-ha": {
              "artifactory": {
                "joinKey": "OVERRIDE",
                "masterKey": "OVERRIDE",
                "node": {
                  "replicaCount": 2,
                  "waitForPrimaryStartup": {
                    "enabled": false
                  }
                },
                "uid": "1000721030"
              },
              "database": {
                "driver": "OVERRIDE",
                "password": "OVERRIDE",
                "type": "OVERRIDE",
                "url": "OVERRIDE",
                "user": "OVERRIDE"
              },
              "databaseUpgradeReady": true,
              "nginx": {
                "gid": "1000720107",
                "http": {
                  "externalPort": 80,
                  "internalPort": 8080
                },
                "https": {
                  "externalPort": 443,
                  "internalPort": 8443
                },
                "service": {
                  "ssloffload": false
                },
                "tlsSecretName": "OVERRIDE",
                "uid": "1000720104"
              },
              "postgresql": {
                "enabled": false
              },
              "waitForDatabase": true
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/jfrog/artifactory-operator-bundle@sha256:d765be1c6edf9c12ea55064546bf3b0284c769479bdb890df580e7b2ab5cfa7c",
      "bundle_path_digest": "sha256:d765be1c6edf9c12ea55064546bf3b0284c769479bdb890df580e7b2ab5cfa7c",
      "capabilities": [
        "Seamless Upgrades"
      ],
      "channel_name": "alpha",
      "creation_date": "2022-11-25T09:48:32.543000+00:00",
      "csv_description": " ## Breaking change\nPlease update to version 1.1.10 of the operator.\nVersion 1.1.8 and 1.1.9 have issues. Please skip these versions.\nIf you are using operator version < 1.2.0 there is a issue with upgrade where artifactory version is not updated if you update operator version.As workaround you can update the Custom Resource OpenshiftArtifactoryHa which has hardcored image value.After upgrading to version 1.2.0 you can remove the below parameters from your CR so that future upgrade works without workaround. ``` artifactory-ha.artifactory.image.registry artifactory-ha.artifactory.image.repository artifactory-ha.artifactory.image.tag ```\n## Overview\nOpenshift Operator to deploy JFrog Artifactory Enterprise into your Openshift cluster.\n## Security Context Constraints\nTo deploy this helm chart you will need to be a cluster admin w/ access to the anyuid scc and add the operator service account to the anyuid scc.\n```\noc adm policy add-scc-to-user anyuid -z artifactory-ha-operator -n openshift-operators\n```\nAdd the service account for the Artifactory chart to deploy successfully:\n``` oc adm policy add-scc-to-user anyuid -z openshiftartifactoryha-artifactory-ha -n artifactory ```\n### Usage\n\nAn external DB is required. The operator will not deploy a DB but will require you to specify the configuration values to connect to it.\n\nSearch for JFrog and click JFrog Artifactory Enterprise Operator to install.\n\nGo to the Installed Operators.\n\nWait for the JFrog Artifactory Enterprise Operator to complete the installation.\n\nOpen the Operator and click on the provided API: Artifactory HA.\n\nClick Create New Instance and provide the following parameters for your DB configuration:\n\n```\nDATABASE_TYPE\nDATABASE_DRIVER\nDATABASE_URL\nDATABASE_USER\nDATABASE_PASSWORD\n```\nMaster key and Join key must be supplied. To generate a new key for each run the command below:\n\n```\n# Create a key\nexport JOIN_KEY=$(openssl rand -hex 32)\necho ${JOIN_KEY}\n```\n\nTo use TLS you will need to first create a k8s tls secret to store your .crt and .key file into.\nThen supply the value of this k8s secret into the TLS_SECRET field.\n``` oc create secret tls my_tls_secret --cert=tls.crt --key=tls.key --namespace=my_namespace ```\nClick Create for Artifactory Enterprise to deploy into OpenShift and connect to it on the external IP exposed by the load balancer.\n### Create a route\nTo expose Artifactory from Openshift we recommend you create a new route in Openshift.\nYou can either use the oc command line tool or the Openshift web console to generate a new route.\nDepending upon where you terminate TLS you will need to either specify pass through or edge.\nCommand Line (Edge):\n``` oc create route edge --service=openshiftartifactory-ha --cert=tls.crt --key=tls.key --ca-cert=ca.crt --hostname=www.example.com ```\nOr for more information visit the official Openshift documentation on routes here:\nhttps://docs.openshift.com/container-platform/4.6/networking/routes/route-configuration.html\n\n",
      "csv_display_name": "JFrog Artifactory Enterprise Operator",
      "csv_metadata_description": "JFrog Artifactory Enterprise deploys Artifactory in a high availability environment across multiple pods",
      "csv_name": "artifactory-ha-operator.v1.2.2",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-11-25T09:48:32.543000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.11",
      "organization": "certified-operators",
      "package": "openshiftartifactoryha-operator",
      "provided_apis": [
        {
          "group": "charts.helm.k8s.io",
          "kind": "OpenshiftArtifactoryHa",
          "version": "v1"
        }
      ],
      "provider": "JFrog",
      "related_images": [
        {
          "digest": "sha256:0b085c2e0984b592ca816827b6df1c52294c480e8e9fb03e0acc32924faac486",
          "image": "registry.connect.redhat.com/jfrog/artifactory-pro@sha256:0b085c2e0984b592ca816827b6df1c52294c480e8e9fb03e0acc32924faac486",
          "name": "artifactory-pro"
        },
        {
          "digest": "sha256:a0ecd4a391ad678125f605dcdf4dfd06c9278c6d23db01335093b1361200c9f0",
          "image": "registry.connect.redhat.com/jfrog/artifactory-operator@sha256:a0ecd4a391ad678125f605dcdf4dfd06c9278c6d23db01335093b1361200c9f0",
          "name": "artifactory-operator"
        },
        {
          "digest": "sha256:bce2fa53c4e5b913c707183f49ab9dcde9e601b22f9fb85b98ba56f2e163c1a8",
          "image": "registry.redhat.io/rhel8/nginx-116@sha256:bce2fa53c4e5b913c707183f49ab9dcde9e601b22f9fb85b98ba56f2e163c1a8",
          "name": "nginx"
        },
        {
          "digest": "sha256:a0ecd4a391ad678125f605dcdf4dfd06c9278c6d23db01335093b1361200c9f0",
          "image": "registry.connect.redhat.com/jfrog/artifactory-operator@sha256:a0ecd4a391ad678125f605dcdf4dfd06c9278c6d23db01335093b1361200c9f0",
          "name": "artifactory-operator-a0ecd4a391ad678125f605dcdf4dfd06c9278c6d23db01335093b1361200c9f0-annotation"
        },
        {
          "digest": "sha256:a0ecd4a391ad678125f605dcdf4dfd06c9278c6d23db01335093b1361200c9f0",
          "image": "registry.connect.redhat.com/jfrog/artifactory-operator@sha256:a0ecd4a391ad678125f605dcdf4dfd06c9278c6d23db01335093b1361200c9f0",
          "name": "artifactory-ha-operator"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.11",
      "version": "1.2.2",
      "version_original": "1.2.2"
    },
    {
      "_id": "63808fbde6ccc0257e0cc288",
      "alm_examples": [
        {
          "api_version": "charts.helm.k8s.io/v1",
          "kind": "OpenshiftArtifactoryHa",
          "metadata": {
            "name": "openshiftartifactoryha"
          },
          "spec": {
            "artifactory-ha": {
              "artifactory": {
                "joinKey": "OVERRIDE",
                "masterKey": "OVERRIDE",
                "node": {
                  "replicaCount": 2,
                  "waitForPrimaryStartup": {
                    "enabled": false
                  }
                },
                "uid": "1000721030"
              },
              "database": {
                "driver": "OVERRIDE",
                "password": "OVERRIDE",
                "type": "OVERRIDE",
                "url": "OVERRIDE",
                "user": "OVERRIDE"
              },
              "databaseUpgradeReady": true,
              "nginx": {
                "gid": "1000720107",
                "http": {
                  "externalPort": 80,
                  "internalPort": 8080
                },
                "https": {
                  "externalPort": 443,
                  "internalPort": 8443
                },
                "service": {
                  "ssloffload": false
                },
                "tlsSecretName": "OVERRIDE",
                "uid": "1000720104"
              },
              "postgresql": {
                "enabled": false
              },
              "waitForDatabase": true
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/jfrog/artifactory-operator-bundle@sha256:d765be1c6edf9c12ea55064546bf3b0284c769479bdb890df580e7b2ab5cfa7c",
      "bundle_path_digest": "sha256:d765be1c6edf9c12ea55064546bf3b0284c769479bdb890df580e7b2ab5cfa7c",
      "capabilities": [
        "Seamless Upgrades"
      ],
      "channel_name": "alpha",
      "creation_date": "2022-11-25T09:49:49.143000+00:00",
      "csv_description": " ## Breaking change\nPlease update to version 1.1.10 of the operator.\nVersion 1.1.8 and 1.1.9 have issues. Please skip these versions.\nIf you are using operator version < 1.2.0 there is a issue with upgrade where artifactory version is not updated if you update operator version.As workaround you can update the Custom Resource OpenshiftArtifactoryHa which has hardcored image value.After upgrading to version 1.2.0 you can remove the below parameters from your CR so that future upgrade works without workaround. ``` artifactory-ha.artifactory.image.registry artifactory-ha.artifactory.image.repository artifactory-ha.artifactory.image.tag ```\n## Overview\nOpenshift Operator to deploy JFrog Artifactory Enterprise into your Openshift cluster.\n## Security Context Constraints\nTo deploy this helm chart you will need to be a cluster admin w/ access to the anyuid scc and add the operator service account to the anyuid scc.\n```\noc adm policy add-scc-to-user anyuid -z artifactory-ha-operator -n openshift-operators\n```\nAdd the service account for the Artifactory chart to deploy successfully:\n``` oc adm policy add-scc-to-user anyuid -z openshiftartifactoryha-artifactory-ha -n artifactory ```\n### Usage\n\nAn external DB is required. The operator will not deploy a DB but will require you to specify the configuration values to connect to it.\n\nSearch for JFrog and click JFrog Artifactory Enterprise Operator to install.\n\nGo to the Installed Operators.\n\nWait for the JFrog Artifactory Enterprise Operator to complete the installation.\n\nOpen the Operator and click on the provided API: Artifactory HA.\n\nClick Create New Instance and provide the following parameters for your DB configuration:\n\n```\nDATABASE_TYPE\nDATABASE_DRIVER\nDATABASE_URL\nDATABASE_USER\nDATABASE_PASSWORD\n```\nMaster key and Join key must be supplied. To generate a new key for each run the command below:\n\n```\n# Create a key\nexport JOIN_KEY=$(openssl rand -hex 32)\necho ${JOIN_KEY}\n```\n\nTo use TLS you will need to first create a k8s tls secret to store your .crt and .key file into.\nThen supply the value of this k8s secret into the TLS_SECRET field.\n``` oc create secret tls my_tls_secret --cert=tls.crt --key=tls.key --namespace=my_namespace ```\nClick Create for Artifactory Enterprise to deploy into OpenShift and connect to it on the external IP exposed by the load balancer.\n### Create a route\nTo expose Artifactory from Openshift we recommend you create a new route in Openshift.\nYou can either use the oc command line tool or the Openshift web console to generate a new route.\nDepending upon where you terminate TLS you will need to either specify pass through or edge.\nCommand Line (Edge):\n``` oc create route edge --service=openshiftartifactory-ha --cert=tls.crt --key=tls.key --ca-cert=ca.crt --hostname=www.example.com ```\nOr for more information visit the official Openshift documentation on routes here:\nhttps://docs.openshift.com/container-platform/4.6/networking/routes/route-configuration.html\n\n",
      "csv_display_name": "JFrog Artifactory Enterprise Operator",
      "csv_metadata_description": "JFrog Artifactory Enterprise deploys Artifactory in a high availability environment across multiple pods",
      "csv_name": "artifactory-ha-operator.v1.2.2",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-11-25T09:49:49.143000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.8",
      "organization": "certified-operators",
      "package": "openshiftartifactoryha-operator",
      "provided_apis": [
        {
          "group": "charts.helm.k8s.io",
          "kind": "OpenshiftArtifactoryHa",
          "plural": "openshiftartifactoryhas",
          "version": "v1"
        }
      ],
      "provider": "JFrog",
      "related_images": [
        {
          "digest": "sha256:0b085c2e0984b592ca816827b6df1c52294c480e8e9fb03e0acc32924faac486",
          "image": "registry.connect.redhat.com/jfrog/artifactory-pro@sha256:0b085c2e0984b592ca816827b6df1c52294c480e8e9fb03e0acc32924faac486",
          "name": "artifactory-pro"
        },
        {
          "digest": "sha256:a0ecd4a391ad678125f605dcdf4dfd06c9278c6d23db01335093b1361200c9f0",
          "image": "registry.connect.redhat.com/jfrog/artifactory-operator@sha256:a0ecd4a391ad678125f605dcdf4dfd06c9278c6d23db01335093b1361200c9f0",
          "name": "artifactory-operator"
        },
        {
          "digest": "sha256:bce2fa53c4e5b913c707183f49ab9dcde9e601b22f9fb85b98ba56f2e163c1a8",
          "image": "registry.redhat.io/rhel8/nginx-116@sha256:bce2fa53c4e5b913c707183f49ab9dcde9e601b22f9fb85b98ba56f2e163c1a8",
          "name": "nginx"
        },
        {
          "digest": "sha256:a0ecd4a391ad678125f605dcdf4dfd06c9278c6d23db01335093b1361200c9f0",
          "image": "registry.connect.redhat.com/jfrog/artifactory-operator@sha256:a0ecd4a391ad678125f605dcdf4dfd06c9278c6d23db01335093b1361200c9f0",
          "name": "artifactory-operator-a0ecd4a391ad678125f605dcdf4dfd06c9278c6d23db01335093b1361200c9f0-annotation"
        },
        {
          "digest": "sha256:a0ecd4a391ad678125f605dcdf4dfd06c9278c6d23db01335093b1361200c9f0",
          "image": "registry.connect.redhat.com/jfrog/artifactory-operator@sha256:a0ecd4a391ad678125f605dcdf4dfd06c9278c6d23db01335093b1361200c9f0",
          "name": "artifactory-ha-operator"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.8",
      "version": "1.2.2",
      "version_original": "1.2.2"
    },
    {
      "_id": "6380d21a8f4a976d0420abbb",
      "alm_examples": [
        {
          "api_version": "ako.vmware.com/v1alpha1",
          "kind": "AKOConfig",
          "metadata": {
            "name": "ako-sample",
            "namespace": "avi-system"
          },
          "spec": {
            "akoSettings": {
              "apiServerPort": 8080,
              "blockedNamespaceList": [],
              "clusterName": "my-cluster",
              "cniPlugin": "openshift",
              "deleteConfig": false,
              "disableStaticRouteSync": false,
              "enableEVH": false,
              "enableEvents": true,
              "fullSyncFrequency": "1800",
              "ipFamily": "",
              "istioEnabled": false,
              "layer7Only": false,
              "logLevel": "INFO",
              "namespaceSelector": {
                "labelKey": "",
                "labelValue": ""
              },
              "servicesAPI": false,
              "vipPerNamespace": false
            },
            "controllerSettings": {
              "cloudName": "Default-Cloud",
              "controllerIP": "",
              "controllerVersion": "20.1.5",
              "serviceEngineGroupName": "Default-Group",
              "tenantName": "admin"
            },
            "imagePullPolicy": "IfNotPresent",
            "imageRepository": "projects.registry.vmware.com/ako/ako@sha256:34586548325141111088f6747d8034e5ae28fef1fb7ec6163f2a1849e3b4a648",
            "l4Settings": {
              "autoFQDN": "default",
              "defaultDomain": ""
            },
            "l7Settings": {
              "defaultIngController": true,
              "noPGForSNI": false,
              "passthroughShardSize": "SMALL",
              "serviceType": "ClusterIP",
              "shardVSSize": "LARGE"
            },
            "logFile": "avi.log",
            "mountPath": "/log",
            "networkSettings": {
              "bgpPeerLabels": [],
              "enableRHI": false,
              "nodeNetworkList": [
                {
                  "cidrs": [],
                  "networkName": ""
                }
              ],
              "nsxtT1LR": "",
              "vipNetworkList": [
                {
                  "cidr": "",
                  "networkName": "",
                  "v6cidr": ""
                }
              ]
            },
            "nodePortSelector": {
              "key": "",
              "value": ""
            },
            "rbac": {
              "pspEnable": false
            },
            "resources": {
              "limits": {
                "cpu": "350m",
                "memory": "400Mi"
              },
              "requests": {
                "cpu": "200m",
                "memory": "300Mi"
              }
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/wavefronthq/ako-operator-bundle@sha256:ba35bd528120eace63e313595c8becc90ba7b362bec8ccc37309a775863943c1",
      "bundle_path_digest": "sha256:ba35bd528120eace63e313595c8becc90ba7b362bec8ccc37309a775863943c1",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "stable",
      "creation_date": "2022-11-25T14:32:58.907000+00:00",
      "csv_description": "Operator to manage the artifacts of the AKO Controller",
      "csv_display_name": "AKO Operator",
      "csv_metadata_description": "",
      "csv_name": "ako-operator.v1.8.1",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-11-25T14:32:58.907000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.9",
      "organization": "certified-operators",
      "package": "ako-operator",
      "provided_apis": [
        {
          "group": "ako.vmware.com",
          "kind": "AKOConfig",
          "plural": "akoconfigs",
          "version": "v1alpha1"
        }
      ],
      "provider": "VMware",
      "related_images": [
        {
          "digest": "sha256:c556d055a9d6773c9a41655051f831dc83e548ee8d505a3794f1590fa6b8ce32",
          "image": "projects.registry.vmware.com/ako/ako-operator@sha256:c556d055a9d6773c9a41655051f831dc83e548ee8d505a3794f1590fa6b8ce32",
          "name": "ako-operator"
        },
        {
          "digest": "sha256:c556d055a9d6773c9a41655051f831dc83e548ee8d505a3794f1590fa6b8ce32",
          "image": "projects.registry.vmware.com/ako/ako-operator@sha256:c556d055a9d6773c9a41655051f831dc83e548ee8d505a3794f1590fa6b8ce32",
          "name": "ako-operator-c556d055a9d6773c9a41655051f831dc83e548ee8d505a3794f1590fa6b8ce32-annotation"
        },
        {
          "digest": "sha256:34586548325141111088f6747d8034e5ae28fef1fb7ec6163f2a1849e3b4a648",
          "image": "projects.registry.vmware.com/ako/ako@sha256:34586548325141111088f6747d8034e5ae28fef1fb7ec6163f2a1849e3b4a648",
          "name": "ako-34586548325141111088f6747d8034e5ae28fef1fb7ec6163f2a1849e3b4a648-annotation"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.9",
      "version": "1.8.1",
      "version_original": "1.8.1"
    },
    {
      "_id": "6380d2f2bb5a79c41a90fabc",
      "alm_examples": [
        {
          "api_version": "ako.vmware.com/v1alpha1",
          "kind": "AKOConfig",
          "metadata": {
            "name": "ako-sample",
            "namespace": "avi-system"
          },
          "spec": {
            "akoSettings": {
              "apiServerPort": 8080,
              "blockedNamespaceList": [],
              "clusterName": "my-cluster",
              "cniPlugin": "openshift",
              "deleteConfig": false,
              "disableStaticRouteSync": false,
              "enableEVH": false,
              "enableEvents": true,
              "fullSyncFrequency": "1800",
              "ipFamily": "",
              "istioEnabled": false,
              "layer7Only": false,
              "logLevel": "INFO",
              "namespaceSelector": {
                "labelKey": "",
                "labelValue": ""
              },
              "servicesAPI": false,
              "vipPerNamespace": false
            },
            "controllerSettings": {
              "cloudName": "Default-Cloud",
              "controllerIP": "",
              "controllerVersion": "20.1.5",
              "serviceEngineGroupName": "Default-Group",
              "tenantName": "admin"
            },
            "imagePullPolicy": "IfNotPresent",
            "imageRepository": "projects.registry.vmware.com/ako/ako@sha256:34586548325141111088f6747d8034e5ae28fef1fb7ec6163f2a1849e3b4a648",
            "l4Settings": {
              "autoFQDN": "default",
              "defaultDomain": ""
            },
            "l7Settings": {
              "defaultIngController": true,
              "noPGForSNI": false,
              "passthroughShardSize": "SMALL",
              "serviceType": "ClusterIP",
              "shardVSSize": "LARGE"
            },
            "logFile": "avi.log",
            "mountPath": "/log",
            "networkSettings": {
              "bgpPeerLabels": [],
              "enableRHI": false,
              "nodeNetworkList": [
                {
                  "cidrs": [],
                  "networkName": ""
                }
              ],
              "nsxtT1LR": "",
              "vipNetworkList": [
                {
                  "cidr": "",
                  "networkName": "",
                  "v6cidr": ""
                }
              ]
            },
            "nodePortSelector": {
              "key": "",
              "value": ""
            },
            "rbac": {
              "pspEnable": false
            },
            "resources": {
              "limits": {
                "cpu": "350m",
                "memory": "400Mi"
              },
              "requests": {
                "cpu": "200m",
                "memory": "300Mi"
              }
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/wavefronthq/ako-operator-bundle@sha256:ba35bd528120eace63e313595c8becc90ba7b362bec8ccc37309a775863943c1",
      "bundle_path_digest": "sha256:ba35bd528120eace63e313595c8becc90ba7b362bec8ccc37309a775863943c1",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "stable",
      "creation_date": "2022-11-25T14:36:34.480000+00:00",
      "csv_description": "Operator to manage the artifacts of the AKO Controller",
      "csv_display_name": "AKO Operator",
      "csv_metadata_description": "",
      "csv_name": "ako-operator.v1.8.1",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-11-25T14:36:34.480000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.8",
      "organization": "certified-operators",
      "package": "ako-operator",
      "provided_apis": [
        {
          "group": "ako.vmware.com",
          "kind": "AKOConfig",
          "plural": "akoconfigs",
          "version": "v1alpha1"
        }
      ],
      "provider": "VMware",
      "related_images": [
        {
          "digest": "sha256:c556d055a9d6773c9a41655051f831dc83e548ee8d505a3794f1590fa6b8ce32",
          "image": "projects.registry.vmware.com/ako/ako-operator@sha256:c556d055a9d6773c9a41655051f831dc83e548ee8d505a3794f1590fa6b8ce32",
          "name": "ako-operator"
        },
        {
          "digest": "sha256:c556d055a9d6773c9a41655051f831dc83e548ee8d505a3794f1590fa6b8ce32",
          "image": "projects.registry.vmware.com/ako/ako-operator@sha256:c556d055a9d6773c9a41655051f831dc83e548ee8d505a3794f1590fa6b8ce32",
          "name": "ako-operator-c556d055a9d6773c9a41655051f831dc83e548ee8d505a3794f1590fa6b8ce32-annotation"
        },
        {
          "digest": "sha256:34586548325141111088f6747d8034e5ae28fef1fb7ec6163f2a1849e3b4a648",
          "image": "projects.registry.vmware.com/ako/ako@sha256:34586548325141111088f6747d8034e5ae28fef1fb7ec6163f2a1849e3b4a648",
          "name": "ako-34586548325141111088f6747d8034e5ae28fef1fb7ec6163f2a1849e3b4a648-annotation"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.8",
      "version": "1.8.1",
      "version_original": "1.8.1"
    },
    {
      "_id": "6380d32587925e76f1d0af34",
      "alm_examples": [
        {
          "api_version": "ako.vmware.com/v1alpha1",
          "kind": "AKOConfig",
          "metadata": {
            "name": "ako-sample",
            "namespace": "avi-system"
          },
          "spec": {
            "akoSettings": {
              "apiServerPort": 8080,
              "blockedNamespaceList": [],
              "clusterName": "my-cluster",
              "cniPlugin": "openshift",
              "deleteConfig": false,
              "disableStaticRouteSync": false,
              "enableEVH": false,
              "enableEvents": true,
              "fullSyncFrequency": "1800",
              "ipFamily": "",
              "istioEnabled": false,
              "layer7Only": false,
              "logLevel": "INFO",
              "namespaceSelector": {
                "labelKey": "",
                "labelValue": ""
              },
              "servicesAPI": false,
              "vipPerNamespace": false
            },
            "controllerSettings": {
              "cloudName": "Default-Cloud",
              "controllerIP": "",
              "controllerVersion": "20.1.5",
              "serviceEngineGroupName": "Default-Group",
              "tenantName": "admin"
            },
            "imagePullPolicy": "IfNotPresent",
            "imageRepository": "projects.registry.vmware.com/ako/ako@sha256:34586548325141111088f6747d8034e5ae28fef1fb7ec6163f2a1849e3b4a648",
            "l4Settings": {
              "autoFQDN": "default",
              "defaultDomain": ""
            },
            "l7Settings": {
              "defaultIngController": true,
              "noPGForSNI": false,
              "passthroughShardSize": "SMALL",
              "serviceType": "ClusterIP",
              "shardVSSize": "LARGE"
            },
            "logFile": "avi.log",
            "mountPath": "/log",
            "networkSettings": {
              "bgpPeerLabels": [],
              "enableRHI": false,
              "nodeNetworkList": [
                {
                  "cidrs": [],
                  "networkName": ""
                }
              ],
              "nsxtT1LR": "",
              "vipNetworkList": [
                {
                  "cidr": "",
                  "networkName": "",
                  "v6cidr": ""
                }
              ]
            },
            "nodePortSelector": {
              "key": "",
              "value": ""
            },
            "rbac": {
              "pspEnable": false
            },
            "resources": {
              "limits": {
                "cpu": "350m",
                "memory": "400Mi"
              },
              "requests": {
                "cpu": "200m",
                "memory": "300Mi"
              }
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/wavefronthq/ako-operator-bundle@sha256:ba35bd528120eace63e313595c8becc90ba7b362bec8ccc37309a775863943c1",
      "bundle_path_digest": "sha256:ba35bd528120eace63e313595c8becc90ba7b362bec8ccc37309a775863943c1",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "stable",
      "creation_date": "2022-11-25T14:37:25.987000+00:00",
      "csv_description": "Operator to manage the artifacts of the AKO Controller",
      "csv_display_name": "AKO Operator",
      "csv_metadata_description": "",
      "csv_name": "ako-operator.v1.8.1",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-11-25T14:37:25.987000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.10",
      "organization": "certified-operators",
      "package": "ako-operator",
      "provided_apis": [
        {
          "group": "ako.vmware.com",
          "kind": "AKOConfig",
          "plural": "akoconfigs",
          "version": "v1alpha1"
        }
      ],
      "provider": "VMware",
      "related_images": [
        {
          "digest": "sha256:c556d055a9d6773c9a41655051f831dc83e548ee8d505a3794f1590fa6b8ce32",
          "image": "projects.registry.vmware.com/ako/ako-operator@sha256:c556d055a9d6773c9a41655051f831dc83e548ee8d505a3794f1590fa6b8ce32",
          "name": "ako-operator"
        },
        {
          "digest": "sha256:c556d055a9d6773c9a41655051f831dc83e548ee8d505a3794f1590fa6b8ce32",
          "image": "projects.registry.vmware.com/ako/ako-operator@sha256:c556d055a9d6773c9a41655051f831dc83e548ee8d505a3794f1590fa6b8ce32",
          "name": "ako-operator-c556d055a9d6773c9a41655051f831dc83e548ee8d505a3794f1590fa6b8ce32-annotation"
        },
        {
          "digest": "sha256:34586548325141111088f6747d8034e5ae28fef1fb7ec6163f2a1849e3b4a648",
          "image": "projects.registry.vmware.com/ako/ako@sha256:34586548325141111088f6747d8034e5ae28fef1fb7ec6163f2a1849e3b4a648",
          "name": "ako-34586548325141111088f6747d8034e5ae28fef1fb7ec6163f2a1849e3b4a648-annotation"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.10",
      "version": "1.8.1",
      "version_original": "1.8.1"
    },
    {
      "_id": "63846c758f4a976d04355079",
      "alm_examples": [
        {
          "api_version": "charts.helm.k8s.io/v1",
          "kind": "OpenshiftXray",
          "metadata": {
            "name": "openshiftxray"
          },
          "spec": {
            "xray": {
              "analysis": {
                "image": {
                  "registry": "registry.connect.redhat.com",
                  "repository": "jfrog/xray-analysis"
                },
                "name": "xray-analysis",
                "podManagementPolicy": "Parallel",
                "updateStrategy": "RollingUpdate"
              },
              "common": {
                "xrayGroupId": "1000721035",
                "xrayUserId": "1000721035"
              },
              "database": {
                "password": "OVERRIDE",
                "url": "OVERRIDE",
                "user": "OVERRIDE"
              },
              "global": {},
              "indexer": {
                "image": {
                  "registry": "registry.connect.redhat.com",
                  "repository": "jfrog/xray-indexer"
                },
                "name": "xray-indexer",
                "podManagementPolicy": "Parallel",
                "updateStrategy": "RollingUpdate"
              },
              "persist": {
                "image": {
                  "registry": "registry.connect.redhat.com",
                  "repository": "jfrog/xray-persist"
                },
                "name": "xray-persist",
                "persistence": {
                  "size": "10Gi"
                },
                "podManagementPolicy": "Parallel",
                "updateStrategy": "RollingUpdate"
              },
              "postgresql": {
                "enabled": false
              },
              "rabbitmq": {
                "auth": {
                  "erlangCookie": "XRAYRABBITMQCLUSTER",
                  "password": "xray",
                  "tls": {
                    "caCertificate": "",
                    "enabled": false,
                    "failIfNoPeerCert": true,
                    "serverCertificate": "",
                    "serverKey": "",
                    "sslOptionsVerify": "verify_peer"
                  },
                  "username": "xray"
                },
                "enabled": true,
                "image": {
                  "registry": "registry.connect.redhat.com",
                  "repository": "jfrog/xray-rabbitmq"
                },
                "podSecurityContext": {
                  "fsGroup": 1000721001,
                  "runAsUser": 1000721001
                },
                "rbac": {
                  "create": true
                },
                "replicaCount": 1
              },
              "rabbitmq-ha": {
                "enabled": false,
                "image": {
                  "repository": "registry.connect.redhat.com/jfrog/xray-rabbitmq"
                },
                "initContainer": {
                  "enabled": false
                },
                "managementPassword": "guest",
                "managementUsername": "guest",
                "rabbitmqPassword": "guest",
                "rabbitmqUsername": "guest",
                "replicaCount": 1,
                "securityContext": {
                  "fsGroup": 1000721035,
                  "runAsGroup": 1000721035,
                  "runAsUser": 1000721035
                }
              },
              "replicaCount": 1,
              "router": {
                "image": {
                  "imagePullPolicy": "IfNotPresent",
                  "registry": "registry.connect.redhat.com",
                  "repository": "jfrog/xray-router"
                },
                "name": "router"
              },
              "server": {
                "image": {
                  "registry": "registry.connect.redhat.com",
                  "repository": "jfrog/xray-server"
                },
                "name": "xray-server",
                "podManagementPolicy": "Parallel",
                "replicaCount": 1,
                "updateStrategy": "RollingUpdate"
              },
              "unifiedUpgradeAllowed": "true",
              "xray": {
                "consoleLog": false,
                "jfrogUrl": "OVERRIDE",
                "joinKey": "OVERRIDE",
                "masterKey": "OVERRIDE"
              }
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/jfrog/xray-operator-bundle@sha256:cd8fefb907e481fb2f1f12b50e347f5433d68dd4ab28e653fbecc44531f9ddcf",
      "bundle_path_digest": "sha256:cd8fefb907e481fb2f1f12b50e347f5433d68dd4ab28e653fbecc44531f9ddcf",
      "capabilities": [
        "Seamless Upgrades"
      ],
      "channel_name": "alpha",
      "creation_date": "2022-11-28T08:08:21.737000+00:00",
      "csv_description": "## Overview Openshift Operator to deploy JFrog Xray Continuous Security scanner into your Openshift cluster. NOTE: Artifactory is required for JFrog Xray to connect with and scan artifacts against.\n### Major version v2.0.4 Breaking change notification!!\nThis release has major bug fix for Rabbitmq Pod. Please update to version 2.0.4 of the operator.\n### Major version v2.0.0 Breaking change notification!! Xray is dependent upon Rabbitmq. The version of Rabbitmq that this chart uses has significantly changed from version 1.1.8 of the operator. For full details on how to migrate please visit: `https://github.com/jfrog/charts/blob/master/stable/xray/RABBITMQ_MIGRATION_NOTES.md`\nIf you are using operator version < 2.1.0 there is a issue with upgrade where xray version is not updated if you update operator version.As workaround you can update the Custom Resource OpenshiftXray which has hardcored image tag value.After upgrading to version 2.1.0 you can remove the image tags parameters from your CR so that future upgrade works without workaround.\n### Security Context Constraints To deploy this helm chart you will need to be a cluster admin w/ access to the anyuid scc and add the operator service account to the anyuid scc. ``` oc adm policy add-scc-to-user anyuid -z xray-operator ``` ``` oc adm policy add-scc-to-user anyuid -z openshiftxray ``` ``` oc adm policy add-scc-to-user anyuid -z openshiftxray-rabbitmq-ha ``` ## Usage An external DB is required. The operator will not deploy a DB but will require you to specify the configuration values to connect to it. Search for JFrog and click JFrog Xray Operator to install. Go to the Installed Operators. Wait for the JFrog Xray Operator to complete the installation. Open the Operator and click on the provided API: Xray Click Create New Instance and provide the following parameters for your DB configuration: ``` JFROG_URL DATABASE_URL DATABASE_USER DATABASE_PASSWORD ``` JFROG_URL is the external ip or DNS of your Artifactory to connect Xray to. Artifactory is required to use this operator. You can set your JFROG_URL to the service name of your Artifactory Nginx: ``` oc get svc -n my_namespace | grep nginx ``` DATABASE_URL must be a Postgresql URL in the format: `` postgres://postgres-postgresql:5432/xraydb?sslmode=disable `` DATABASE_USER and DATABASE_PASSWORD must supply a valid user on Postgresql. Click Create for Xray to deploy into OpenShift. Open Artifactory in a web browser to complete the onboarding wizard for Xray! ### Air gap environments To use Xray Operator in an air gap environment you will need to download the images as image streams into your Openshift air gap cluster manually. Use the image overrides to then specify the image stream locations that are local to your cluster. Next you will need to setup the database of index data for xray to use when scanning artifacts. Follow the link below for instructions on setup: https://www.jfrog.com/confluence/display/JFROG/Configuring+Xray#ConfiguringXray-SynchronizingtheDatabase ",
      "csv_display_name": "JFrog Xray Continuous Security Operator",
      "csv_metadata_description": "JFrog Xray Enterprise deploys Xray continuous security scanner into Openshift (Requires Jfrog Artifactory)",
      "csv_name": "xray-operator.v2.1.1",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-11-28T08:08:21.737000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.12",
      "organization": "certified-operators",
      "package": "openshiftxray-operator",
      "provided_apis": [
        {
          "group": "charts.helm.k8s.io",
          "kind": "OpenshiftXray",
          "version": "v1"
        }
      ],
      "provider": "JFrog",
      "related_images": [
        {
          "digest": "sha256:c2cec947796e5c5e1b008aca8685bab865d45cc2a128c18e46f0f822782c334e",
          "image": "registry.connect.redhat.com/jfrog/xray-server@sha256:c2cec947796e5c5e1b008aca8685bab865d45cc2a128c18e46f0f822782c334e",
          "name": "xray-server"
        },
        {
          "digest": "sha256:069f198a2b76c141cdc6d5882d6b48ede535f1f0c74f567ec260942a4819a85e",
          "image": "registry.connect.redhat.com/jfrog/xray-analysis@sha256:069f198a2b76c141cdc6d5882d6b48ede535f1f0c74f567ec260942a4819a85e",
          "name": "xray-analysis"
        },
        {
          "digest": "sha256:fc419b7b34749ad4e0d07e6da4b8dcba345322b1789fdbd6039688e69a2089e7",
          "image": "registry.connect.redhat.com/jfrog/xray-persist@sha256:fc419b7b34749ad4e0d07e6da4b8dcba345322b1789fdbd6039688e69a2089e7",
          "name": "xray-persist"
        },
        {
          "digest": "sha256:37fbf31f2ff9c56d466884e310f694276bbc3056d0822d952eeee61800a99b73",
          "image": "registry.connect.redhat.com/jfrog/xray-indexer@sha256:37fbf31f2ff9c56d466884e310f694276bbc3056d0822d952eeee61800a99b73",
          "name": "xray-indexer"
        },
        {
          "digest": "sha256:1e432b7be09ac4901fbd070d2a9d0ceab310d70d5f037811328d5389d2c4b0d6",
          "image": "registry.connect.redhat.com/jfrog/xray-router@sha256:1e432b7be09ac4901fbd070d2a9d0ceab310d70d5f037811328d5389d2c4b0d6",
          "name": "xray-router"
        },
        {
          "digest": "sha256:580e6c5a19370de5c79e8d432d4c31379857de8d4dec73713d147b0e5714ac05",
          "image": "registry.connect.redhat.com/jfrog/xray-rabbitmq@sha256:580e6c5a19370de5c79e8d432d4c31379857de8d4dec73713d147b0e5714ac05",
          "name": "xray-rabbitmq"
        },
        {
          "digest": "sha256:393fc395df12f109fc40c773224ceeffbab0bb1f9efd992b2d16d420f7090dbc",
          "image": "registry.connect.redhat.com/jfrog/xray-operator@sha256:393fc395df12f109fc40c773224ceeffbab0bb1f9efd992b2d16d420f7090dbc",
          "name": "xray-operator"
        },
        {
          "digest": "sha256:43af37b35badaca5d49ae916214abc6b795ad89b8a37dd2c6725956440f8534e",
          "image": "registry.connect.redhat.com/jfrog/init@sha256:43af37b35badaca5d49ae916214abc6b795ad89b8a37dd2c6725956440f8534e",
          "name": "init-container"
        },
        {
          "digest": "sha256:393fc395df12f109fc40c773224ceeffbab0bb1f9efd992b2d16d420f7090dbc",
          "image": "registry.connect.redhat.com/jfrog/xray-operator@sha256:393fc395df12f109fc40c773224ceeffbab0bb1f9efd992b2d16d420f7090dbc",
          "name": "xray-operator-393fc395df12f109fc40c773224ceeffbab0bb1f9efd992b2d16d420f7090dbc-annotation"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.12",
      "version": "2.1.1",
      "version_original": "2.1.1"
    },
    {
      "_id": "63846c778f4a976d04355086",
      "alm_examples": [
        {
          "api_version": "charts.helm.k8s.io/v1",
          "kind": "OpenshiftXray",
          "metadata": {
            "name": "openshiftxray"
          },
          "spec": {
            "xray": {
              "analysis": {
                "image": {
                  "registry": "registry.connect.redhat.com",
                  "repository": "jfrog/xray-analysis"
                },
                "name": "xray-analysis",
                "podManagementPolicy": "Parallel",
                "updateStrategy": "RollingUpdate"
              },
              "common": {
                "xrayGroupId": "1000721035",
                "xrayUserId": "1000721035"
              },
              "database": {
                "password": "OVERRIDE",
                "url": "OVERRIDE",
                "user": "OVERRIDE"
              },
              "global": {},
              "indexer": {
                "image": {
                  "registry": "registry.connect.redhat.com",
                  "repository": "jfrog/xray-indexer"
                },
                "name": "xray-indexer",
                "podManagementPolicy": "Parallel",
                "updateStrategy": "RollingUpdate"
              },
              "persist": {
                "image": {
                  "registry": "registry.connect.redhat.com",
                  "repository": "jfrog/xray-persist"
                },
                "name": "xray-persist",
                "persistence": {
                  "size": "10Gi"
                },
                "podManagementPolicy": "Parallel",
                "updateStrategy": "RollingUpdate"
              },
              "postgresql": {
                "enabled": false
              },
              "rabbitmq": {
                "auth": {
                  "erlangCookie": "XRAYRABBITMQCLUSTER",
                  "password": "xray",
                  "tls": {
                    "caCertificate": "",
                    "enabled": false,
                    "failIfNoPeerCert": true,
                    "serverCertificate": "",
                    "serverKey": "",
                    "sslOptionsVerify": "verify_peer"
                  },
                  "username": "xray"
                },
                "enabled": true,
                "image": {
                  "registry": "registry.connect.redhat.com",
                  "repository": "jfrog/xray-rabbitmq"
                },
                "podSecurityContext": {
                  "fsGroup": 1000721001,
                  "runAsUser": 1000721001
                },
                "rbac": {
                  "create": true
                },
                "replicaCount": 1
              },
              "rabbitmq-ha": {
                "enabled": false,
                "image": {
                  "repository": "registry.connect.redhat.com/jfrog/xray-rabbitmq"
                },
                "initContainer": {
                  "enabled": false
                },
                "managementPassword": "guest",
                "managementUsername": "guest",
                "rabbitmqPassword": "guest",
                "rabbitmqUsername": "guest",
                "replicaCount": 1,
                "securityContext": {
                  "fsGroup": 1000721035,
                  "runAsGroup": 1000721035,
                  "runAsUser": 1000721035
                }
              },
              "replicaCount": 1,
              "router": {
                "image": {
                  "imagePullPolicy": "IfNotPresent",
                  "registry": "registry.connect.redhat.com",
                  "repository": "jfrog/xray-router"
                },
                "name": "router"
              },
              "server": {
                "image": {
                  "registry": "registry.connect.redhat.com",
                  "repository": "jfrog/xray-server"
                },
                "name": "xray-server",
                "podManagementPolicy": "Parallel",
                "replicaCount": 1,
                "updateStrategy": "RollingUpdate"
              },
              "unifiedUpgradeAllowed": "true",
              "xray": {
                "consoleLog": false,
                "jfrogUrl": "OVERRIDE",
                "joinKey": "OVERRIDE",
                "masterKey": "OVERRIDE"
              }
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/jfrog/xray-operator-bundle@sha256:cd8fefb907e481fb2f1f12b50e347f5433d68dd4ab28e653fbecc44531f9ddcf",
      "bundle_path_digest": "sha256:cd8fefb907e481fb2f1f12b50e347f5433d68dd4ab28e653fbecc44531f9ddcf",
      "capabilities": [
        "Seamless Upgrades"
      ],
      "channel_name": "alpha",
      "creation_date": "2022-11-28T08:08:23.264000+00:00",
      "csv_description": "## Overview Openshift Operator to deploy JFrog Xray Continuous Security scanner into your Openshift cluster. NOTE: Artifactory is required for JFrog Xray to connect with and scan artifacts against.\n### Major version v2.0.4 Breaking change notification!!\nThis release has major bug fix for Rabbitmq Pod. Please update to version 2.0.4 of the operator.\n### Major version v2.0.0 Breaking change notification!! Xray is dependent upon Rabbitmq. The version of Rabbitmq that this chart uses has significantly changed from version 1.1.8 of the operator. For full details on how to migrate please visit: `https://github.com/jfrog/charts/blob/master/stable/xray/RABBITMQ_MIGRATION_NOTES.md`\nIf you are using operator version < 2.1.0 there is a issue with upgrade where xray version is not updated if you update operator version.As workaround you can update the Custom Resource OpenshiftXray which has hardcored image tag value.After upgrading to version 2.1.0 you can remove the image tags parameters from your CR so that future upgrade works without workaround.\n### Security Context Constraints To deploy this helm chart you will need to be a cluster admin w/ access to the anyuid scc and add the operator service account to the anyuid scc. ``` oc adm policy add-scc-to-user anyuid -z xray-operator ``` ``` oc adm policy add-scc-to-user anyuid -z openshiftxray ``` ``` oc adm policy add-scc-to-user anyuid -z openshiftxray-rabbitmq-ha ``` ## Usage An external DB is required. The operator will not deploy a DB but will require you to specify the configuration values to connect to it. Search for JFrog and click JFrog Xray Operator to install. Go to the Installed Operators. Wait for the JFrog Xray Operator to complete the installation. Open the Operator and click on the provided API: Xray Click Create New Instance and provide the following parameters for your DB configuration: ``` JFROG_URL DATABASE_URL DATABASE_USER DATABASE_PASSWORD ``` JFROG_URL is the external ip or DNS of your Artifactory to connect Xray to. Artifactory is required to use this operator. You can set your JFROG_URL to the service name of your Artifactory Nginx: ``` oc get svc -n my_namespace | grep nginx ``` DATABASE_URL must be a Postgresql URL in the format: `` postgres://postgres-postgresql:5432/xraydb?sslmode=disable `` DATABASE_USER and DATABASE_PASSWORD must supply a valid user on Postgresql. Click Create for Xray to deploy into OpenShift. Open Artifactory in a web browser to complete the onboarding wizard for Xray! ### Air gap environments To use Xray Operator in an air gap environment you will need to download the images as image streams into your Openshift air gap cluster manually. Use the image overrides to then specify the image stream locations that are local to your cluster. Next you will need to setup the database of index data for xray to use when scanning artifacts. Follow the link below for instructions on setup: https://www.jfrog.com/confluence/display/JFROG/Configuring+Xray#ConfiguringXray-SynchronizingtheDatabase ",
      "csv_display_name": "JFrog Xray Continuous Security Operator",
      "csv_metadata_description": "JFrog Xray Enterprise deploys Xray continuous security scanner into Openshift (Requires Jfrog Artifactory)",
      "csv_name": "xray-operator.v2.1.1",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-11-28T08:08:23.264000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.10",
      "organization": "certified-operators",
      "package": "openshiftxray-operator",
      "provided_apis": [
        {
          "group": "charts.helm.k8s.io",
          "kind": "OpenshiftXray",
          "plural": "openshiftxrays",
          "version": "v1"
        }
      ],
      "provider": "JFrog",
      "related_images": [
        {
          "digest": "sha256:c2cec947796e5c5e1b008aca8685bab865d45cc2a128c18e46f0f822782c334e",
          "image": "registry.connect.redhat.com/jfrog/xray-server@sha256:c2cec947796e5c5e1b008aca8685bab865d45cc2a128c18e46f0f822782c334e",
          "name": "xray-server"
        },
        {
          "digest": "sha256:069f198a2b76c141cdc6d5882d6b48ede535f1f0c74f567ec260942a4819a85e",
          "image": "registry.connect.redhat.com/jfrog/xray-analysis@sha256:069f198a2b76c141cdc6d5882d6b48ede535f1f0c74f567ec260942a4819a85e",
          "name": "xray-analysis"
        },
        {
          "digest": "sha256:fc419b7b34749ad4e0d07e6da4b8dcba345322b1789fdbd6039688e69a2089e7",
          "image": "registry.connect.redhat.com/jfrog/xray-persist@sha256:fc419b7b34749ad4e0d07e6da4b8dcba345322b1789fdbd6039688e69a2089e7",
          "name": "xray-persist"
        },
        {
          "digest": "sha256:37fbf31f2ff9c56d466884e310f694276bbc3056d0822d952eeee61800a99b73",
          "image": "registry.connect.redhat.com/jfrog/xray-indexer@sha256:37fbf31f2ff9c56d466884e310f694276bbc3056d0822d952eeee61800a99b73",
          "name": "xray-indexer"
        },
        {
          "digest": "sha256:1e432b7be09ac4901fbd070d2a9d0ceab310d70d5f037811328d5389d2c4b0d6",
          "image": "registry.connect.redhat.com/jfrog/xray-router@sha256:1e432b7be09ac4901fbd070d2a9d0ceab310d70d5f037811328d5389d2c4b0d6",
          "name": "xray-router"
        },
        {
          "digest": "sha256:580e6c5a19370de5c79e8d432d4c31379857de8d4dec73713d147b0e5714ac05",
          "image": "registry.connect.redhat.com/jfrog/xray-rabbitmq@sha256:580e6c5a19370de5c79e8d432d4c31379857de8d4dec73713d147b0e5714ac05",
          "name": "xray-rabbitmq"
        },
        {
          "digest": "sha256:393fc395df12f109fc40c773224ceeffbab0bb1f9efd992b2d16d420f7090dbc",
          "image": "registry.connect.redhat.com/jfrog/xray-operator@sha256:393fc395df12f109fc40c773224ceeffbab0bb1f9efd992b2d16d420f7090dbc",
          "name": "xray-operator"
        },
        {
          "digest": "sha256:43af37b35badaca5d49ae916214abc6b795ad89b8a37dd2c6725956440f8534e",
          "image": "registry.connect.redhat.com/jfrog/init@sha256:43af37b35badaca5d49ae916214abc6b795ad89b8a37dd2c6725956440f8534e",
          "name": "init-container"
        },
        {
          "digest": "sha256:393fc395df12f109fc40c773224ceeffbab0bb1f9efd992b2d16d420f7090dbc",
          "image": "registry.connect.redhat.com/jfrog/xray-operator@sha256:393fc395df12f109fc40c773224ceeffbab0bb1f9efd992b2d16d420f7090dbc",
          "name": "xray-operator-393fc395df12f109fc40c773224ceeffbab0bb1f9efd992b2d16d420f7090dbc-annotation"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.10",
      "version": "2.1.1",
      "version_original": "2.1.1"
    },
    {
      "_id": "63846c8bd1516305d20729bc",
      "alm_examples": [
        {
          "api_version": "charts.helm.k8s.io/v1",
          "kind": "OpenshiftXray",
          "metadata": {
            "name": "openshiftxray"
          },
          "spec": {
            "xray": {
              "analysis": {
                "image": {
                  "registry": "registry.connect.redhat.com",
                  "repository": "jfrog/xray-analysis"
                },
                "name": "xray-analysis",
                "podManagementPolicy": "Parallel",
                "updateStrategy": "RollingUpdate"
              },
              "common": {
                "xrayGroupId": "1000721035",
                "xrayUserId": "1000721035"
              },
              "database": {
                "password": "OVERRIDE",
                "url": "OVERRIDE",
                "user": "OVERRIDE"
              },
              "global": {},
              "indexer": {
                "image": {
                  "registry": "registry.connect.redhat.com",
                  "repository": "jfrog/xray-indexer"
                },
                "name": "xray-indexer",
                "podManagementPolicy": "Parallel",
                "updateStrategy": "RollingUpdate"
              },
              "persist": {
                "image": {
                  "registry": "registry.connect.redhat.com",
                  "repository": "jfrog/xray-persist"
                },
                "name": "xray-persist",
                "persistence": {
                  "size": "10Gi"
                },
                "podManagementPolicy": "Parallel",
                "updateStrategy": "RollingUpdate"
              },
              "postgresql": {
                "enabled": false
              },
              "rabbitmq": {
                "auth": {
                  "erlangCookie": "XRAYRABBITMQCLUSTER",
                  "password": "xray",
                  "tls": {
                    "caCertificate": "",
                    "enabled": false,
                    "failIfNoPeerCert": true,
                    "serverCertificate": "",
                    "serverKey": "",
                    "sslOptionsVerify": "verify_peer"
                  },
                  "username": "xray"
                },
                "enabled": true,
                "image": {
                  "registry": "registry.connect.redhat.com",
                  "repository": "jfrog/xray-rabbitmq"
                },
                "podSecurityContext": {
                  "fsGroup": 1000721001,
                  "runAsUser": 1000721001
                },
                "rbac": {
                  "create": true
                },
                "replicaCount": 1
              },
              "rabbitmq-ha": {
                "enabled": false,
                "image": {
                  "repository": "registry.connect.redhat.com/jfrog/xray-rabbitmq"
                },
                "initContainer": {
                  "enabled": false
                },
                "managementPassword": "guest",
                "managementUsername": "guest",
                "rabbitmqPassword": "guest",
                "rabbitmqUsername": "guest",
                "replicaCount": 1,
                "securityContext": {
                  "fsGroup": 1000721035,
                  "runAsGroup": 1000721035,
                  "runAsUser": 1000721035
                }
              },
              "replicaCount": 1,
              "router": {
                "image": {
                  "imagePullPolicy": "IfNotPresent",
                  "registry": "registry.connect.redhat.com",
                  "repository": "jfrog/xray-router"
                },
                "name": "router"
              },
              "server": {
                "image": {
                  "registry": "registry.connect.redhat.com",
                  "repository": "jfrog/xray-server"
                },
                "name": "xray-server",
                "podManagementPolicy": "Parallel",
                "replicaCount": 1,
                "updateStrategy": "RollingUpdate"
              },
              "unifiedUpgradeAllowed": "true",
              "xray": {
                "consoleLog": false,
                "jfrogUrl": "OVERRIDE",
                "joinKey": "OVERRIDE",
                "masterKey": "OVERRIDE"
              }
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/jfrog/xray-operator-bundle@sha256:cd8fefb907e481fb2f1f12b50e347f5433d68dd4ab28e653fbecc44531f9ddcf",
      "bundle_path_digest": "sha256:cd8fefb907e481fb2f1f12b50e347f5433d68dd4ab28e653fbecc44531f9ddcf",
      "capabilities": [
        "Seamless Upgrades"
      ],
      "channel_name": "alpha",
      "creation_date": "2022-11-28T08:08:43.878000+00:00",
      "csv_description": "## Overview Openshift Operator to deploy JFrog Xray Continuous Security scanner into your Openshift cluster. NOTE: Artifactory is required for JFrog Xray to connect with and scan artifacts against.\n### Major version v2.0.4 Breaking change notification!!\nThis release has major bug fix for Rabbitmq Pod. Please update to version 2.0.4 of the operator.\n### Major version v2.0.0 Breaking change notification!! Xray is dependent upon Rabbitmq. The version of Rabbitmq that this chart uses has significantly changed from version 1.1.8 of the operator. For full details on how to migrate please visit: `https://github.com/jfrog/charts/blob/master/stable/xray/RABBITMQ_MIGRATION_NOTES.md`\nIf you are using operator version < 2.1.0 there is a issue with upgrade where xray version is not updated if you update operator version.As workaround you can update the Custom Resource OpenshiftXray which has hardcored image tag value.After upgrading to version 2.1.0 you can remove the image tags parameters from your CR so that future upgrade works without workaround.\n### Security Context Constraints To deploy this helm chart you will need to be a cluster admin w/ access to the anyuid scc and add the operator service account to the anyuid scc. ``` oc adm policy add-scc-to-user anyuid -z xray-operator ``` ``` oc adm policy add-scc-to-user anyuid -z openshiftxray ``` ``` oc adm policy add-scc-to-user anyuid -z openshiftxray-rabbitmq-ha ``` ## Usage An external DB is required. The operator will not deploy a DB but will require you to specify the configuration values to connect to it. Search for JFrog and click JFrog Xray Operator to install. Go to the Installed Operators. Wait for the JFrog Xray Operator to complete the installation. Open the Operator and click on the provided API: Xray Click Create New Instance and provide the following parameters for your DB configuration: ``` JFROG_URL DATABASE_URL DATABASE_USER DATABASE_PASSWORD ``` JFROG_URL is the external ip or DNS of your Artifactory to connect Xray to. Artifactory is required to use this operator. You can set your JFROG_URL to the service name of your Artifactory Nginx: ``` oc get svc -n my_namespace | grep nginx ``` DATABASE_URL must be a Postgresql URL in the format: `` postgres://postgres-postgresql:5432/xraydb?sslmode=disable `` DATABASE_USER and DATABASE_PASSWORD must supply a valid user on Postgresql. Click Create for Xray to deploy into OpenShift. Open Artifactory in a web browser to complete the onboarding wizard for Xray! ### Air gap environments To use Xray Operator in an air gap environment you will need to download the images as image streams into your Openshift air gap cluster manually. Use the image overrides to then specify the image stream locations that are local to your cluster. Next you will need to setup the database of index data for xray to use when scanning artifacts. Follow the link below for instructions on setup: https://www.jfrog.com/confluence/display/JFROG/Configuring+Xray#ConfiguringXray-SynchronizingtheDatabase ",
      "csv_display_name": "JFrog Xray Continuous Security Operator",
      "csv_metadata_description": "JFrog Xray Enterprise deploys Xray continuous security scanner into Openshift (Requires Jfrog Artifactory)",
      "csv_name": "xray-operator.v2.1.1",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-11-28T08:08:43.878000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.9",
      "organization": "certified-operators",
      "package": "openshiftxray-operator",
      "provided_apis": [
        {
          "group": "charts.helm.k8s.io",
          "kind": "OpenshiftXray",
          "plural": "openshiftxrays",
          "version": "v1"
        }
      ],
      "provider": "JFrog",
      "related_images": [
        {
          "digest": "sha256:c2cec947796e5c5e1b008aca8685bab865d45cc2a128c18e46f0f822782c334e",
          "image": "registry.connect.redhat.com/jfrog/xray-server@sha256:c2cec947796e5c5e1b008aca8685bab865d45cc2a128c18e46f0f822782c334e",
          "name": "xray-server"
        },
        {
          "digest": "sha256:069f198a2b76c141cdc6d5882d6b48ede535f1f0c74f567ec260942a4819a85e",
          "image": "registry.connect.redhat.com/jfrog/xray-analysis@sha256:069f198a2b76c141cdc6d5882d6b48ede535f1f0c74f567ec260942a4819a85e",
          "name": "xray-analysis"
        },
        {
          "digest": "sha256:fc419b7b34749ad4e0d07e6da4b8dcba345322b1789fdbd6039688e69a2089e7",
          "image": "registry.connect.redhat.com/jfrog/xray-persist@sha256:fc419b7b34749ad4e0d07e6da4b8dcba345322b1789fdbd6039688e69a2089e7",
          "name": "xray-persist"
        },
        {
          "digest": "sha256:37fbf31f2ff9c56d466884e310f694276bbc3056d0822d952eeee61800a99b73",
          "image": "registry.connect.redhat.com/jfrog/xray-indexer@sha256:37fbf31f2ff9c56d466884e310f694276bbc3056d0822d952eeee61800a99b73",
          "name": "xray-indexer"
        },
        {
          "digest": "sha256:1e432b7be09ac4901fbd070d2a9d0ceab310d70d5f037811328d5389d2c4b0d6",
          "image": "registry.connect.redhat.com/jfrog/xray-router@sha256:1e432b7be09ac4901fbd070d2a9d0ceab310d70d5f037811328d5389d2c4b0d6",
          "name": "xray-router"
        },
        {
          "digest": "sha256:580e6c5a19370de5c79e8d432d4c31379857de8d4dec73713d147b0e5714ac05",
          "image": "registry.connect.redhat.com/jfrog/xray-rabbitmq@sha256:580e6c5a19370de5c79e8d432d4c31379857de8d4dec73713d147b0e5714ac05",
          "name": "xray-rabbitmq"
        },
        {
          "digest": "sha256:393fc395df12f109fc40c773224ceeffbab0bb1f9efd992b2d16d420f7090dbc",
          "image": "registry.connect.redhat.com/jfrog/xray-operator@sha256:393fc395df12f109fc40c773224ceeffbab0bb1f9efd992b2d16d420f7090dbc",
          "name": "xray-operator"
        },
        {
          "digest": "sha256:43af37b35badaca5d49ae916214abc6b795ad89b8a37dd2c6725956440f8534e",
          "image": "registry.connect.redhat.com/jfrog/init@sha256:43af37b35badaca5d49ae916214abc6b795ad89b8a37dd2c6725956440f8534e",
          "name": "init-container"
        },
        {
          "digest": "sha256:393fc395df12f109fc40c773224ceeffbab0bb1f9efd992b2d16d420f7090dbc",
          "image": "registry.connect.redhat.com/jfrog/xray-operator@sha256:393fc395df12f109fc40c773224ceeffbab0bb1f9efd992b2d16d420f7090dbc",
          "name": "xray-operator-393fc395df12f109fc40c773224ceeffbab0bb1f9efd992b2d16d420f7090dbc-annotation"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.9",
      "version": "2.1.1",
      "version_original": "2.1.1"
    },
    {
      "_id": "63846c9abb5a79c41aa5a579",
      "alm_examples": [
        {
          "api_version": "charts.helm.k8s.io/v1",
          "kind": "OpenshiftXray",
          "metadata": {
            "name": "openshiftxray"
          },
          "spec": {
            "xray": {
              "analysis": {
                "image": {
                  "registry": "registry.connect.redhat.com",
                  "repository": "jfrog/xray-analysis"
                },
                "name": "xray-analysis",
                "podManagementPolicy": "Parallel",
                "updateStrategy": "RollingUpdate"
              },
              "common": {
                "xrayGroupId": "1000721035",
                "xrayUserId": "1000721035"
              },
              "database": {
                "password": "OVERRIDE",
                "url": "OVERRIDE",
                "user": "OVERRIDE"
              },
              "global": {},
              "indexer": {
                "image": {
                  "registry": "registry.connect.redhat.com",
                  "repository": "jfrog/xray-indexer"
                },
                "name": "xray-indexer",
                "podManagementPolicy": "Parallel",
                "updateStrategy": "RollingUpdate"
              },
              "persist": {
                "image": {
                  "registry": "registry.connect.redhat.com",
                  "repository": "jfrog/xray-persist"
                },
                "name": "xray-persist",
                "persistence": {
                  "size": "10Gi"
                },
                "podManagementPolicy": "Parallel",
                "updateStrategy": "RollingUpdate"
              },
              "postgresql": {
                "enabled": false
              },
              "rabbitmq": {
                "auth": {
                  "erlangCookie": "XRAYRABBITMQCLUSTER",
                  "password": "xray",
                  "tls": {
                    "caCertificate": "",
                    "enabled": false,
                    "failIfNoPeerCert": true,
                    "serverCertificate": "",
                    "serverKey": "",
                    "sslOptionsVerify": "verify_peer"
                  },
                  "username": "xray"
                },
                "enabled": true,
                "image": {
                  "registry": "registry.connect.redhat.com",
                  "repository": "jfrog/xray-rabbitmq"
                },
                "podSecurityContext": {
                  "fsGroup": 1000721001,
                  "runAsUser": 1000721001
                },
                "rbac": {
                  "create": true
                },
                "replicaCount": 1
              },
              "rabbitmq-ha": {
                "enabled": false,
                "image": {
                  "repository": "registry.connect.redhat.com/jfrog/xray-rabbitmq"
                },
                "initContainer": {
                  "enabled": false
                },
                "managementPassword": "guest",
                "managementUsername": "guest",
                "rabbitmqPassword": "guest",
                "rabbitmqUsername": "guest",
                "replicaCount": 1,
                "securityContext": {
                  "fsGroup": 1000721035,
                  "runAsGroup": 1000721035,
                  "runAsUser": 1000721035
                }
              },
              "replicaCount": 1,
              "router": {
                "image": {
                  "imagePullPolicy": "IfNotPresent",
                  "registry": "registry.connect.redhat.com",
                  "repository": "jfrog/xray-router"
                },
                "name": "router"
              },
              "server": {
                "image": {
                  "registry": "registry.connect.redhat.com",
                  "repository": "jfrog/xray-server"
                },
                "name": "xray-server",
                "podManagementPolicy": "Parallel",
                "replicaCount": 1,
                "updateStrategy": "RollingUpdate"
              },
              "unifiedUpgradeAllowed": "true",
              "xray": {
                "consoleLog": false,
                "jfrogUrl": "OVERRIDE",
                "joinKey": "OVERRIDE",
                "masterKey": "OVERRIDE"
              }
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/jfrog/xray-operator-bundle@sha256:cd8fefb907e481fb2f1f12b50e347f5433d68dd4ab28e653fbecc44531f9ddcf",
      "bundle_path_digest": "sha256:cd8fefb907e481fb2f1f12b50e347f5433d68dd4ab28e653fbecc44531f9ddcf",
      "capabilities": [
        "Seamless Upgrades"
      ],
      "channel_name": "alpha",
      "creation_date": "2022-11-28T08:08:58.941000+00:00",
      "csv_description": "## Overview Openshift Operator to deploy JFrog Xray Continuous Security scanner into your Openshift cluster. NOTE: Artifactory is required for JFrog Xray to connect with and scan artifacts against.\n### Major version v2.0.4 Breaking change notification!!\nThis release has major bug fix for Rabbitmq Pod. Please update to version 2.0.4 of the operator.\n### Major version v2.0.0 Breaking change notification!! Xray is dependent upon Rabbitmq. The version of Rabbitmq that this chart uses has significantly changed from version 1.1.8 of the operator. For full details on how to migrate please visit: `https://github.com/jfrog/charts/blob/master/stable/xray/RABBITMQ_MIGRATION_NOTES.md`\nIf you are using operator version < 2.1.0 there is a issue with upgrade where xray version is not updated if you update operator version.As workaround you can update the Custom Resource OpenshiftXray which has hardcored image tag value.After upgrading to version 2.1.0 you can remove the image tags parameters from your CR so that future upgrade works without workaround.\n### Security Context Constraints To deploy this helm chart you will need to be a cluster admin w/ access to the anyuid scc and add the operator service account to the anyuid scc. ``` oc adm policy add-scc-to-user anyuid -z xray-operator ``` ``` oc adm policy add-scc-to-user anyuid -z openshiftxray ``` ``` oc adm policy add-scc-to-user anyuid -z openshiftxray-rabbitmq-ha ``` ## Usage An external DB is required. The operator will not deploy a DB but will require you to specify the configuration values to connect to it. Search for JFrog and click JFrog Xray Operator to install. Go to the Installed Operators. Wait for the JFrog Xray Operator to complete the installation. Open the Operator and click on the provided API: Xray Click Create New Instance and provide the following parameters for your DB configuration: ``` JFROG_URL DATABASE_URL DATABASE_USER DATABASE_PASSWORD ``` JFROG_URL is the external ip or DNS of your Artifactory to connect Xray to. Artifactory is required to use this operator. You can set your JFROG_URL to the service name of your Artifactory Nginx: ``` oc get svc -n my_namespace | grep nginx ``` DATABASE_URL must be a Postgresql URL in the format: `` postgres://postgres-postgresql:5432/xraydb?sslmode=disable `` DATABASE_USER and DATABASE_PASSWORD must supply a valid user on Postgresql. Click Create for Xray to deploy into OpenShift. Open Artifactory in a web browser to complete the onboarding wizard for Xray! ### Air gap environments To use Xray Operator in an air gap environment you will need to download the images as image streams into your Openshift air gap cluster manually. Use the image overrides to then specify the image stream locations that are local to your cluster. Next you will need to setup the database of index data for xray to use when scanning artifacts. Follow the link below for instructions on setup: https://www.jfrog.com/confluence/display/JFROG/Configuring+Xray#ConfiguringXray-SynchronizingtheDatabase ",
      "csv_display_name": "JFrog Xray Continuous Security Operator",
      "csv_metadata_description": "JFrog Xray Enterprise deploys Xray continuous security scanner into Openshift (Requires Jfrog Artifactory)",
      "csv_name": "xray-operator.v2.1.1",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-11-28T08:08:58.941000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.11",
      "organization": "certified-operators",
      "package": "openshiftxray-operator",
      "provided_apis": [
        {
          "group": "charts.helm.k8s.io",
          "kind": "OpenshiftXray",
          "version": "v1"
        }
      ],
      "provider": "JFrog",
      "related_images": [
        {
          "digest": "sha256:c2cec947796e5c5e1b008aca8685bab865d45cc2a128c18e46f0f822782c334e",
          "image": "registry.connect.redhat.com/jfrog/xray-server@sha256:c2cec947796e5c5e1b008aca8685bab865d45cc2a128c18e46f0f822782c334e",
          "name": "xray-server"
        },
        {
          "digest": "sha256:069f198a2b76c141cdc6d5882d6b48ede535f1f0c74f567ec260942a4819a85e",
          "image": "registry.connect.redhat.com/jfrog/xray-analysis@sha256:069f198a2b76c141cdc6d5882d6b48ede535f1f0c74f567ec260942a4819a85e",
          "name": "xray-analysis"
        },
        {
          "digest": "sha256:fc419b7b34749ad4e0d07e6da4b8dcba345322b1789fdbd6039688e69a2089e7",
          "image": "registry.connect.redhat.com/jfrog/xray-persist@sha256:fc419b7b34749ad4e0d07e6da4b8dcba345322b1789fdbd6039688e69a2089e7",
          "name": "xray-persist"
        },
        {
          "digest": "sha256:37fbf31f2ff9c56d466884e310f694276bbc3056d0822d952eeee61800a99b73",
          "image": "registry.connect.redhat.com/jfrog/xray-indexer@sha256:37fbf31f2ff9c56d466884e310f694276bbc3056d0822d952eeee61800a99b73",
          "name": "xray-indexer"
        },
        {
          "digest": "sha256:1e432b7be09ac4901fbd070d2a9d0ceab310d70d5f037811328d5389d2c4b0d6",
          "image": "registry.connect.redhat.com/jfrog/xray-router@sha256:1e432b7be09ac4901fbd070d2a9d0ceab310d70d5f037811328d5389d2c4b0d6",
          "name": "xray-router"
        },
        {
          "digest": "sha256:580e6c5a19370de5c79e8d432d4c31379857de8d4dec73713d147b0e5714ac05",
          "image": "registry.connect.redhat.com/jfrog/xray-rabbitmq@sha256:580e6c5a19370de5c79e8d432d4c31379857de8d4dec73713d147b0e5714ac05",
          "name": "xray-rabbitmq"
        },
        {
          "digest": "sha256:393fc395df12f109fc40c773224ceeffbab0bb1f9efd992b2d16d420f7090dbc",
          "image": "registry.connect.redhat.com/jfrog/xray-operator@sha256:393fc395df12f109fc40c773224ceeffbab0bb1f9efd992b2d16d420f7090dbc",
          "name": "xray-operator"
        },
        {
          "digest": "sha256:43af37b35badaca5d49ae916214abc6b795ad89b8a37dd2c6725956440f8534e",
          "image": "registry.connect.redhat.com/jfrog/init@sha256:43af37b35badaca5d49ae916214abc6b795ad89b8a37dd2c6725956440f8534e",
          "name": "init-container"
        },
        {
          "digest": "sha256:393fc395df12f109fc40c773224ceeffbab0bb1f9efd992b2d16d420f7090dbc",
          "image": "registry.connect.redhat.com/jfrog/xray-operator@sha256:393fc395df12f109fc40c773224ceeffbab0bb1f9efd992b2d16d420f7090dbc",
          "name": "xray-operator-393fc395df12f109fc40c773224ceeffbab0bb1f9efd992b2d16d420f7090dbc-annotation"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.11",
      "version": "2.1.1",
      "version_original": "2.1.1"
    },
    {
      "_id": "63846d2bbb5a79c41aa5a959",
      "alm_examples": [
        {
          "api_version": "charts.helm.k8s.io/v1",
          "kind": "OpenshiftXray",
          "metadata": {
            "name": "openshiftxray"
          },
          "spec": {
            "xray": {
              "analysis": {
                "image": {
                  "registry": "registry.connect.redhat.com",
                  "repository": "jfrog/xray-analysis"
                },
                "name": "xray-analysis",
                "podManagementPolicy": "Parallel",
                "updateStrategy": "RollingUpdate"
              },
              "common": {
                "xrayGroupId": "1000721035",
                "xrayUserId": "1000721035"
              },
              "database": {
                "password": "OVERRIDE",
                "url": "OVERRIDE",
                "user": "OVERRIDE"
              },
              "global": {},
              "indexer": {
                "image": {
                  "registry": "registry.connect.redhat.com",
                  "repository": "jfrog/xray-indexer"
                },
                "name": "xray-indexer",
                "podManagementPolicy": "Parallel",
                "updateStrategy": "RollingUpdate"
              },
              "persist": {
                "image": {
                  "registry": "registry.connect.redhat.com",
                  "repository": "jfrog/xray-persist"
                },
                "name": "xray-persist",
                "persistence": {
                  "size": "10Gi"
                },
                "podManagementPolicy": "Parallel",
                "updateStrategy": "RollingUpdate"
              },
              "postgresql": {
                "enabled": false
              },
              "rabbitmq": {
                "auth": {
                  "erlangCookie": "XRAYRABBITMQCLUSTER",
                  "password": "xray",
                  "tls": {
                    "caCertificate": "",
                    "enabled": false,
                    "failIfNoPeerCert": true,
                    "serverCertificate": "",
                    "serverKey": "",
                    "sslOptionsVerify": "verify_peer"
                  },
                  "username": "xray"
                },
                "enabled": true,
                "image": {
                  "registry": "registry.connect.redhat.com",
                  "repository": "jfrog/xray-rabbitmq"
                },
                "podSecurityContext": {
                  "fsGroup": 1000721001,
                  "runAsUser": 1000721001
                },
                "rbac": {
                  "create": true
                },
                "replicaCount": 1
              },
              "rabbitmq-ha": {
                "enabled": false,
                "image": {
                  "repository": "registry.connect.redhat.com/jfrog/xray-rabbitmq"
                },
                "initContainer": {
                  "enabled": false
                },
                "managementPassword": "guest",
                "managementUsername": "guest",
                "rabbitmqPassword": "guest",
                "rabbitmqUsername": "guest",
                "replicaCount": 1,
                "securityContext": {
                  "fsGroup": 1000721035,
                  "runAsGroup": 1000721035,
                  "runAsUser": 1000721035
                }
              },
              "replicaCount": 1,
              "router": {
                "image": {
                  "imagePullPolicy": "IfNotPresent",
                  "registry": "registry.connect.redhat.com",
                  "repository": "jfrog/xray-router"
                },
                "name": "router"
              },
              "server": {
                "image": {
                  "registry": "registry.connect.redhat.com",
                  "repository": "jfrog/xray-server"
                },
                "name": "xray-server",
                "podManagementPolicy": "Parallel",
                "replicaCount": 1,
                "updateStrategy": "RollingUpdate"
              },
              "unifiedUpgradeAllowed": "true",
              "xray": {
                "consoleLog": false,
                "jfrogUrl": "OVERRIDE",
                "joinKey": "OVERRIDE",
                "masterKey": "OVERRIDE"
              }
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/jfrog/xray-operator-bundle@sha256:cd8fefb907e481fb2f1f12b50e347f5433d68dd4ab28e653fbecc44531f9ddcf",
      "bundle_path_digest": "sha256:cd8fefb907e481fb2f1f12b50e347f5433d68dd4ab28e653fbecc44531f9ddcf",
      "capabilities": [
        "Seamless Upgrades"
      ],
      "channel_name": "alpha",
      "creation_date": "2022-11-28T08:11:23.274000+00:00",
      "csv_description": "## Overview Openshift Operator to deploy JFrog Xray Continuous Security scanner into your Openshift cluster. NOTE: Artifactory is required for JFrog Xray to connect with and scan artifacts against.\n### Major version v2.0.4 Breaking change notification!!\nThis release has major bug fix for Rabbitmq Pod. Please update to version 2.0.4 of the operator.\n### Major version v2.0.0 Breaking change notification!! Xray is dependent upon Rabbitmq. The version of Rabbitmq that this chart uses has significantly changed from version 1.1.8 of the operator. For full details on how to migrate please visit: `https://github.com/jfrog/charts/blob/master/stable/xray/RABBITMQ_MIGRATION_NOTES.md`\nIf you are using operator version < 2.1.0 there is a issue with upgrade where xray version is not updated if you update operator version.As workaround you can update the Custom Resource OpenshiftXray which has hardcored image tag value.After upgrading to version 2.1.0 you can remove the image tags parameters from your CR so that future upgrade works without workaround.\n### Security Context Constraints To deploy this helm chart you will need to be a cluster admin w/ access to the anyuid scc and add the operator service account to the anyuid scc. ``` oc adm policy add-scc-to-user anyuid -z xray-operator ``` ``` oc adm policy add-scc-to-user anyuid -z openshiftxray ``` ``` oc adm policy add-scc-to-user anyuid -z openshiftxray-rabbitmq-ha ``` ## Usage An external DB is required. The operator will not deploy a DB but will require you to specify the configuration values to connect to it. Search for JFrog and click JFrog Xray Operator to install. Go to the Installed Operators. Wait for the JFrog Xray Operator to complete the installation. Open the Operator and click on the provided API: Xray Click Create New Instance and provide the following parameters for your DB configuration: ``` JFROG_URL DATABASE_URL DATABASE_USER DATABASE_PASSWORD ``` JFROG_URL is the external ip or DNS of your Artifactory to connect Xray to. Artifactory is required to use this operator. You can set your JFROG_URL to the service name of your Artifactory Nginx: ``` oc get svc -n my_namespace | grep nginx ``` DATABASE_URL must be a Postgresql URL in the format: `` postgres://postgres-postgresql:5432/xraydb?sslmode=disable `` DATABASE_USER and DATABASE_PASSWORD must supply a valid user on Postgresql. Click Create for Xray to deploy into OpenShift. Open Artifactory in a web browser to complete the onboarding wizard for Xray! ### Air gap environments To use Xray Operator in an air gap environment you will need to download the images as image streams into your Openshift air gap cluster manually. Use the image overrides to then specify the image stream locations that are local to your cluster. Next you will need to setup the database of index data for xray to use when scanning artifacts. Follow the link below for instructions on setup: https://www.jfrog.com/confluence/display/JFROG/Configuring+Xray#ConfiguringXray-SynchronizingtheDatabase ",
      "csv_display_name": "JFrog Xray Continuous Security Operator",
      "csv_metadata_description": "JFrog Xray Enterprise deploys Xray continuous security scanner into Openshift (Requires Jfrog Artifactory)",
      "csv_name": "xray-operator.v2.1.1",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-11-28T08:11:23.274000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.8",
      "organization": "certified-operators",
      "package": "openshiftxray-operator",
      "provided_apis": [
        {
          "group": "charts.helm.k8s.io",
          "kind": "OpenshiftXray",
          "plural": "openshiftxrays",
          "version": "v1"
        }
      ],
      "provider": "JFrog",
      "related_images": [
        {
          "digest": "sha256:c2cec947796e5c5e1b008aca8685bab865d45cc2a128c18e46f0f822782c334e",
          "image": "registry.connect.redhat.com/jfrog/xray-server@sha256:c2cec947796e5c5e1b008aca8685bab865d45cc2a128c18e46f0f822782c334e",
          "name": "xray-server"
        },
        {
          "digest": "sha256:069f198a2b76c141cdc6d5882d6b48ede535f1f0c74f567ec260942a4819a85e",
          "image": "registry.connect.redhat.com/jfrog/xray-analysis@sha256:069f198a2b76c141cdc6d5882d6b48ede535f1f0c74f567ec260942a4819a85e",
          "name": "xray-analysis"
        },
        {
          "digest": "sha256:fc419b7b34749ad4e0d07e6da4b8dcba345322b1789fdbd6039688e69a2089e7",
          "image": "registry.connect.redhat.com/jfrog/xray-persist@sha256:fc419b7b34749ad4e0d07e6da4b8dcba345322b1789fdbd6039688e69a2089e7",
          "name": "xray-persist"
        },
        {
          "digest": "sha256:37fbf31f2ff9c56d466884e310f694276bbc3056d0822d952eeee61800a99b73",
          "image": "registry.connect.redhat.com/jfrog/xray-indexer@sha256:37fbf31f2ff9c56d466884e310f694276bbc3056d0822d952eeee61800a99b73",
          "name": "xray-indexer"
        },
        {
          "digest": "sha256:1e432b7be09ac4901fbd070d2a9d0ceab310d70d5f037811328d5389d2c4b0d6",
          "image": "registry.connect.redhat.com/jfrog/xray-router@sha256:1e432b7be09ac4901fbd070d2a9d0ceab310d70d5f037811328d5389d2c4b0d6",
          "name": "xray-router"
        },
        {
          "digest": "sha256:580e6c5a19370de5c79e8d432d4c31379857de8d4dec73713d147b0e5714ac05",
          "image": "registry.connect.redhat.com/jfrog/xray-rabbitmq@sha256:580e6c5a19370de5c79e8d432d4c31379857de8d4dec73713d147b0e5714ac05",
          "name": "xray-rabbitmq"
        },
        {
          "digest": "sha256:393fc395df12f109fc40c773224ceeffbab0bb1f9efd992b2d16d420f7090dbc",
          "image": "registry.connect.redhat.com/jfrog/xray-operator@sha256:393fc395df12f109fc40c773224ceeffbab0bb1f9efd992b2d16d420f7090dbc",
          "name": "xray-operator"
        },
        {
          "digest": "sha256:43af37b35badaca5d49ae916214abc6b795ad89b8a37dd2c6725956440f8534e",
          "image": "registry.connect.redhat.com/jfrog/init@sha256:43af37b35badaca5d49ae916214abc6b795ad89b8a37dd2c6725956440f8534e",
          "name": "init-container"
        },
        {
          "digest": "sha256:393fc395df12f109fc40c773224ceeffbab0bb1f9efd992b2d16d420f7090dbc",
          "image": "registry.connect.redhat.com/jfrog/xray-operator@sha256:393fc395df12f109fc40c773224ceeffbab0bb1f9efd992b2d16d420f7090dbc",
          "name": "xray-operator-393fc395df12f109fc40c773224ceeffbab0bb1f9efd992b2d16d420f7090dbc-annotation"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.8",
      "version": "2.1.1",
      "version_original": "2.1.1"
    },
    {
      "_id": "63847e16bb5a79c41aa606fb",
      "alm_examples": [
        {
          "api_version": "fep.fujitsu.io/v1",
          "kind": "FEPAction",
          "metadata": {
            "name": "new-fep-action"
          },
          "spec": {
            "fepAction": {
              "args": [
                "new-fep-sts-0",
                "new-fep-sts-1"
              ],
              "type": "reload"
            },
            "sysExtraLogging": false,
            "targetClusterName": "new-fep"
          }
        },
        {
          "api_version": "fep.fujitsu.io/v1",
          "kind": "FEPExporter",
          "metadata": {
            "name": "new-fep-exporter"
          },
          "spec": {
            "fepExporter": {
              "exporterLogLevel": "error",
              "fepClusterList": [
                "new-fep1"
              ],
              "image": {
                "pullPolicy": "IfNotPresent"
              },
              "mcSpec": {
                "limits": {
                  "cpu": "500m",
                  "memory": "700Mi"
                },
                "requests": {
                  "cpu": "200m",
                  "memory": "512Mi"
                }
              },
              "restartRequired": false,
              "sysExtraLogging": false,
              "userCustomQueries": "usr_example:\n  query: \"SELECT EXTRACT(EPOCH FROM (now() - pg_last_xact_replay_timestamp())) as lag\"\n  master: true\n  metrics:\n    - lag:\n        usage: \"GAUGE\"\n        description: \"Replication lag behind master in seconds\""
            }
          }
        },
        {
          "api_version": "fep.fujitsu.io/v1",
          "kind": "FEPPgpool2",
          "metadata": {
            "name": "new-fep-pgpool2"
          },
          "spec": {
            "count": 2,
            "customhba": "local   all         all                               trust\nhost    all         all         127.0.0.1/32          trust\nhost    all         all         ::1/128               trust\n",
            "customlogsize": "128Mi",
            "customparams": "listen_addresses = '*'\npcp_listen_addresses = '*'\nnum_init_children = 32\nreserved_connections = 0\nenable_pool_hba = off\nallow_clear_text_frontend_auth = off\nauthentication_timeout = 80\nbackend_weight0 = 1\nbackend_weight1 = 1\nbackend_flag0 = 'ALWAYS_PRIMARY'\nbackend_flag1 = 'DISALLOW_TO_FAILOVER'\nconnection_cache = on\nmax_pool = 4\nlisten_backlog_multiplier = 2\nserialize_accept = off\nchild_life_time = 300\nclient_idle_limit = 0\nchild_max_connections = 0\nconnection_life_time = 0\nreset_query_list = 'ABORT; DISCARD ALL'\nclient_min_messages = info\nlog_min_messages = debug1\nlog_statement = on\nlog_per_node_statement = on\nlog_client_messages = on\nlog_hostname = on\nlog_connections = on\nlog_line_prefix = '%t: pid %p: '\nload_balance_mode = on\nignore_leading_white_space = on\nwhite_function_list = ''\nblack_function_list = 'currval,lastval,nextval,setval'\nblack_query_pattern_list = ''\ndatabase_redirect_preference_list = ''\napp_name_redirect_preference_list = ''\nallow_sql_comments = off\ndisable_load_balance_on_write = 'transaction'\nstatement_level_load_balance = on\nsr_check_period = 0\nsr_check_user = 'postgres'\ndelay_threshold = 0\nlog_standby_delay = 'none'\nssl = on\nssl_ciphers = 'HIGH:MEDIUM:+3DES:!aNULL'\nssl_prefer_server_ciphers = off\nssl_ecdh_curve = 'prime256v1'\nssl_dh_params_file = ''\nrelcache_expire = 0\nrelcache_size = 256\ncheck_temp_table = catalog\ncheck_unlogged_table = on\nenable_shared_relcache = on\nrelcache_query_target = primary\nwd_port0 = 9000\nfailover_on_backend_error = off\n",
            "custompcp": "none",
            "customsslcacert": "none",
            "customsslcert": "-----BEGIN CERTIFICATE-----\nMIIDTzCCAjegAwIBAgIUYssQ8I74US5g+1+Z7CHuaDgkZnEwDQYJKoZIhvcNAQEL\nBQAwNzEQMA4GA1UECgwHRnVqaXRzdTEjMCEGA1UEAwwaRkVQIFJvb3QgQ0EgZm9y\nIEt1YmVybmV0ZXMwHhcNMjEwMjA2MDM1MjI4WhcNMzEwMjA0MDM1MjI4WjA3MRAw\nDgYDVQQKDAdGdWppdHN1MSMwIQYDVQQDDBpGRVAgUm9vdCBDQSBmb3IgS3ViZXJu\nZXRlczCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBAMs97gUF0xkUzCgL\n7MiiDju9ySr/ziwjvcYU7jA9ML+SLmftMs3HtcYbAmSntqI+MDBSR/FAJTOoytuT\npV+mCFcGj2YAjDpliHPeNcUpbryy4YMChF3+MovkIwGCksxo5rhiWhGmoBYpA48P\n4Xe8SPlzqMzhFvNeKzyiUhvjutS2Y1Ss38lsTaurFPx64vQ2PaC54XzdwMptXtpb\ntYmWSzCpJWwxZ6lF3vitdA2w0tnBWNyctAd0+RIM/fvArxiIqseAux9t0uogm5to\nlRIhvekuxOpXBPEqtIYQ4j9XUW2JH8vUDnzPkPvjrq+A3Ug8OyyfGVrW7+VYXozu\nc4aP7P0CAwEAAaNTMFEwHQYDVR0OBBYEFBzCutQ7S74WEhS5V2sNEJBGyLpmMB8G\nA1UdIwQYMBaAFBzCutQ7S74WEhS5V2sNEJBGyLpmMA8GA1UdEwEB/wQFMAMBAf8w\nDQYJKoZIhvcNAQELBQADggEBAMDwD85RAaWEBptFgLzKw+9xEUy1vcZaonAuA1qc\nT342XTueyAugxkC11HwdCGgGS34VyctfMGqj4AW6pA2ez4tLrbOps4DmV4sw8uBL\n8pgRDgfly3ob9FEg2wa0hmrwX9jH5Bt4vySUE2785uPAqaspT2UNtTBxS85BUi1T\nsKId2Rtil6an281Z81wyWVI6Jm2D4MG0mbsiGcTPlCtdg/UljvDYymXlAvd4vNhl\nk9hDa13TgDqJKgKdTIcmZoNQdpEVgFcO0h9AEUy5AuLqxHq60dLfZ6ESGPlMI7Lm\ni4PzYbCnBmOe+7TnHcPSyrnehs66Ik+oifRd82eYS7vKjFw=\n-----END CERTIFICATE-----",
            "customsslkey": "none",
            "fepclustername": "new-fep",
            "imagePullPolicy": "IfNotPresent",
            "limits": {
              "cpu": "400m",
              "memory": "512Mi"
            },
            "requests": {
              "cpu": "200m",
              "memory": "256Mi"
            },
            "serviceport": 9999,
            "statusport": 9898
          }
        },
        {
          "api_version": "fep.fujitsu.io/v1",
          "kind": "FEPRestore",
          "metadata": {
            "name": "new-fep-restore"
          },
          "spec": {
            "fromFEPcluster": "new-fep",
            "imagePullPolicy": "IfNotPresent",
            "mcSpec": {
              "limits": {
                "cpu": "200m",
                "memory": "300Mi"
              },
              "requests": {
                "cpu": "100m",
                "memory": "200Mi"
              }
            },
            "restoretype": "latest",
            "sysExtraLogging": false,
            "toFEPcluster": "new-fep-2"
          }
        },
        {
          "api_version": "fep.fujitsu.io/v2",
          "kind": "FEPCluster",
          "metadata": {
            "name": "new-fep"
          },
          "spec": {
            "fep": {
              "customAnnotations": {
                "allDeployments": {}
              },
              "forceSsl": true,
              "image": {
                "pullPolicy": "IfNotPresent"
              },
              "instances": 1,
              "mcSpec": {
                "limits": {
                  "cpu": "500m",
                  "memory": "700Mi"
                },
                "requests": {
                  "cpu": "200m",
                  "memory": "512Mi"
                }
              },
              "podAntiAffinity": false,
              "podDisruptionBudget": false,
              "servicePort": 27500,
              "syncMode": "off",
              "sysExtraLogging": false
            },
            "fepChildCrVal": {
              "backup": {
                "image": {
                  "pullPolicy": "IfNotPresent"
                },
                "mcSpec": {
                  "limits": {
                    "cpu": 0.2,
                    "memory": "300Mi"
                  },
                  "requests": {
                    "cpu": 0.1,
                    "memory": "200Mi"
                  }
                },
                "pgbackrestParams": "# if log volume is not defined, log_directory should be\n# changed to '/database/userdata/data/log'\n[global]\nrepo1-retention-full=7\nrepo1-retention-full-type=time\nlog-path=/database/log/backup\n",
                "postScript": " ",
                "preScript": " ",
                "schedule": {
                  "num": 2
                },
                "schedule1": {
                  "schedule": "15 0 * * 0",
                  "type": "full"
                },
                "schedule2": {
                  "schedule": "15 0 * * 1-6",
                  "type": "incr"
                },
                "schedule3": {
                  "schedule": " ",
                  "type": " "
                },
                "schedule4": {
                  "schedule": " ",
                  "type": " "
                },
                "schedule5": {
                  "schedule": " ",
                  "type": " "
                }
              },
              "customPgAudit": "# define pg audit custom params here to override defaults.\n# if log volume is not defined, log_directory should be\n# changed to '/database/userdata/data/log'\n[output]\nlogger = 'auditlog'\nlog_directory = '/database/log/audit'\nlog_truncate_on_rotation = on\nlog_filename = 'pgaudit-%a.log'\nlog_rotation_age = 1d\nlog_rotation_size = 0\n[rule]\n",
              "customPgHba": "# define pg_hba custom rules here to be merged with default rules.\n# TYPE     DATABASE        USER        ADDRESS        METHOD\n",
              "customPgParams": "# define custom postgresql.conf parameters below to override defaults.\n# Current values are as per default FEP deployment\nshared_preload_libraries='pgx_datamasking,pg_prewarm,pg_stat_statements'\nsession_preload_libraries='pg_prewarm'\nmax_prepared_transactions = 100\nmax_worker_processes = 30\nmax_connections = 100\nwork_mem = 1MB\nmaintenance_work_mem = 12MB\nshared_buffers = 128MB\neffective_cache_size = 384MB\ncheckpoint_completion_target = 0.8\n# tcp parameters\ntcp_keepalives_idle = 30\ntcp_keepalives_interval = 10\ntcp_keepalives_count = 3\n# logging parameters in default fep installation\n# if log volume is not defined, log_directory should be\n# changed to '/database/userdata/data/log'\nlog_directory = '/database/log'\nlog_filename = 'logfile-%a.log'\nlog_file_mode = 0600\nlog_truncate_on_rotation = on\nlog_rotation_age = 1d\nlog_rotation_size = 0\nlog_checkpoints = on\nlog_line_prefix = '%e %t [%p]: [%l-1] user=%u,db=%d,app=%a,client=%h'\nlog_lock_waits = on\nlog_autovacuum_min_duration = 60s\nlogging_collector = on\npgaudit.config_file='/opt/app-root/src/pgaudit-cfg/pgaudit.conf'\nlog_replication_commands = on\nlog_min_messages = WARNING\nlog_destination = stderr\n# wal_archive parameters in default fep installation\narchive_mode = on\narchive_command = 'pgbackrest --stanza=backupstanza --config=/database/userdata/pgbackrest.conf archive-push %p'\nwal_level = replica\nmax_wal_senders = 12\nwal_keep_segments = 64\ntrack_activities = on\ntrack_counts = on\n",
              "storage": {
                "archivewalVol": {
                  "size": "1Gi"
                },
                "backupVol": {
                  "size": "2Gi"
                },
                "dataVol": {
                  "size": "2Gi"
                },
                "logVol": {
                  "size": "1Gi"
                },
                "tablespaceVol": {
                  "size": "512Mi"
                },
                "walVol": {
                  "size": "1200Mi"
                }
              },
              "sysUsers": {
                "pgAdminPassword": "admin-password",
                "pgRewindPassword": "rewind_password",
                "pgRewindUser": "rewind_user",
                "pgdb": "mydb",
                "pgpassword": "mydbpassword",
                "pgreplpassword": "repluserpwd",
                "pgrepluser": "repluser",
                "pguser": "mydbuser",
                "tdepassphrase": "tde-passphrase"
              },
              "systemCertificates": {
                "cacrt": "-----BEGIN CERTIFICATE-----\nMIIDTzCCAjegAwIBAgIUYssQ8I74US5g+1+Z7CHuaDgkZnEwDQYJKoZIhvcNAQEL\nBQAwNzEQMA4GA1UECgwHRnVqaXRzdTEjMCEGA1UEAwwaRkVQIFJvb3QgQ0EgZm9y\nIEt1YmVybmV0ZXMwHhcNMjEwMjA2MDM1MjI4WhcNMzEwMjA0MDM1MjI4WjA3MRAw\nDgYDVQQKDAdGdWppdHN1MSMwIQYDVQQDDBpGRVAgUm9vdCBDQSBmb3IgS3ViZXJu\nZXRlczCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBAMs97gUF0xkUzCgL\n7MiiDju9ySr/ziwjvcYU7jA9ML+SLmftMs3HtcYbAmSntqI+MDBSR/FAJTOoytuT\npV+mCFcGj2YAjDpliHPeNcUpbryy4YMChF3+MovkIwGCksxo5rhiWhGmoBYpA48P\n4Xe8SPlzqMzhFvNeKzyiUhvjutS2Y1Ss38lsTaurFPx64vQ2PaC54XzdwMptXtpb\ntYmWSzCpJWwxZ6lF3vitdA2w0tnBWNyctAd0+RIM/fvArxiIqseAux9t0uogm5to\nlRIhvekuxOpXBPEqtIYQ4j9XUW2JH8vUDnzPkPvjrq+A3Ug8OyyfGVrW7+VYXozu\nc4aP7P0CAwEAAaNTMFEwHQYDVR0OBBYEFBzCutQ7S74WEhS5V2sNEJBGyLpmMB8G\nA1UdIwQYMBaAFBzCutQ7S74WEhS5V2sNEJBGyLpmMA8GA1UdEwEB/wQFMAMBAf8w\nDQYJKoZIhvcNAQELBQADggEBAMDwD85RAaWEBptFgLzKw+9xEUy1vcZaonAuA1qc\nT342XTueyAugxkC11HwdCGgGS34VyctfMGqj4AW6pA2ez4tLrbOps4DmV4sw8uBL\n8pgRDgfly3ob9FEg2wa0hmrwX9jH5Bt4vySUE2785uPAqaspT2UNtTBxS85BUi1T\nsKId2Rtil6an281Z81wyWVI6Jm2D4MG0mbsiGcTPlCtdg/UljvDYymXlAvd4vNhl\nk9hDa13TgDqJKgKdTIcmZoNQdpEVgFcO0h9AEUy5AuLqxHq60dLfZ6ESGPlMI7Lm\ni4PzYbCnBmOe+7TnHcPSyrnehs66Ik+oifRd82eYS7vKjFw=\n-----END CERTIFICATE-----",
                "crt": "-----BEGIN CERTIFICATE-----\nMIIDUTCCAjmgAwIBAgIRAMocW3qMoHrD6qRvMPppMkMwDQYJKoZIhvcNAQELBQAw\nNzEQMA4GA1UECgwHRnVqaXRzdTEjMCEGA1UEAwwaRkVQIFJvb3QgQ0EgZm9yIEt1\nYmVybmV0ZXMwHhcNMjEwMjA2MDQzMjM2WhcNMjYwMjA1MDQzMjM2WjA/MRAwDgYD\nVQQKEwdGdWppdHN1MSswKQYDVQQDEyJGVUpJVFNVIEVudGVycHJpc2UgUG9zdGdy\nZXMgU2VydmVyMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEA4AI33yvH\nZws+jta6qpV6wzJqF8odIfTIpCfbrVcUUtLFKJ1I2e4SceTKi6O3C/I1XuvWlpng\n5IO65+fQQLO06z1/AuQT78YUn/Wlm9x1aHVsv4ANB5JWWqDOjrRT3o7nRPGXfila\nbP0rGE2mJJcVR9nExJ3IeaktgT3sb8YlXvtchyYpmjdbfxabTz07ig0+6/cwKoRR\nxOK8Uf7f5euE0cI/490J6r5Rs4lgD8sIQNCUFlTFYvmAH7gcdssSFBt8NPlUATHE\nsoFmlW0DKCJWNhTLOht+s6L/1zwTHLjPG2pdkG6Wdgmu5H2pDml8CDNLDv98Aj7i\n+I5SRKKcVPlnuQIDAQABo1AwTjAdBgNVHSUEFjAUBggrBgEFBQcDAQYIKwYBBQUH\nAwIwDAYDVR0TAQH/BAIwADAfBgNVHSMEGDAWgBQcwrrUO0u+FhIUuVdrDRCQRsi6\nZjANBgkqhkiG9w0BAQsFAAOCAQEAm5dxBoI9pScOCvRAchg4CprdRDSJb9K6yB3O\nnCAxnM47iHeXnY3WlnI388kHu8DU7O4ba1tJbGs3KY9KzioPk43pU12jWkO1onoF\n+mTDjx/Ef1cYWA9r5q/LtgTa6Q2sxV4O2x67QW82aAnaxO34dV5zWCPIvAoovZBV\nHRT+BgCg3r2vD1RGKK2nl1aYJtWhO1SZubam+VttdZ/vbM9oOJctxmImsEtBXjkY\nKteePdQtLL5o03JhyXWyRshCq+HMmKf2KgyY8gvydGcP4eLQdBWcW40LcnVq6UjT\n0kJycJEKngMVademq1ZWHGaiYB7hyT6GhgIcHUJ2cKrPgbEh1Q==\n-----END CERTIFICATE-----",
                "key": "-----BEGIN RSA PRIVATE KEY-----\nMIIEowIBAAKCAQEA4AI33yvHZws+jta6qpV6wzJqF8odIfTIpCfbrVcUUtLFKJ1I\n2e4SceTKi6O3C/I1XuvWlpng5IO65+fQQLO06z1/AuQT78YUn/Wlm9x1aHVsv4AN\nB5JWWqDOjrRT3o7nRPGXfilabP0rGE2mJJcVR9nExJ3IeaktgT3sb8YlXvtchyYp\nmjdbfxabTz07ig0+6/cwKoRRxOK8Uf7f5euE0cI/490J6r5Rs4lgD8sIQNCUFlTF\nYvmAH7gcdssSFBt8NPlUATHEsoFmlW0DKCJWNhTLOht+s6L/1zwTHLjPG2pdkG6W\ndgmu5H2pDml8CDNLDv98Aj7i+I5SRKKcVPlnuQIDAQABAoIBAFPQYKlOzw/+BA0b\nyMIUpdctIMb/54CR/xR0mVw1DbSjigNVPjHUQvB8Y1B2FAITQObgJO06bAv0QdWN\nRb0/v/yYiNJDFjaLjaIAHlO/2+oWrXbFaZqgpVDJhB+e1xaZr2x7XGxm+p925k30\nl6pvIRY+I8JRKvZiV1VZHwL/R3JOtPr++xMZtLVjVOI+f+ySqJ+TZHuAjm49EKxj\ncEmmJ28b7QcziXsvKy00f+zbqLIBKXQdZAFU5eEr1BsDRXdRW+Kf0XIvftuy4BJZ\nvoKT+VGhEvF/qysswL4+6IAO6tpuYnnM0Y2d3sOGoWPkTcQK0MekYKzL/WmtCjNs\n9hodJtECgYEA5EWyhEOf4uOKe5TDp697UCUvXLoOR58FDe/S8XNvScn29jjOkqIg\nOMoqo9xAkJTNTzqn5UUdt1x/pgM2NxlPLFijrc0zQlX3SoOO2ryDd9WNi7YKtN16\nKJqa536WeZu2OEbuAZ+S3GALVy1RPeTNPnUOmKnF06DjDUGzLNCZy10CgYEA+zfw\n952DWuz1U0Z4wvAEqqcgUKXPKrkTXV/iUnjkDkrLYVr0ZofDNTXrdHl+UedFmaOC\ncieZn6DNhcdz5tKtyysGMH3g/qs9PfoGUngvcXsy0Egk04l3x1jc8TTCLqXZXYaQ\nHMsx51n+R58oncPtzYSUOr9qQ6PbC2CstTbFJA0CgYEAjGEsUliAB/jknfEzjXjG\nPdhQUxb8VyE864Az2lah9t/kJzFyIAziAeqZ5GE7t247AGFTBRTHHI8e1Qoemi3P\nWbc9GVIbFs1lIYbcIDpUIyrKPEP8O5QEXtoNLxXTFgAjRGKiVY87spjCAJ+W2ZhO\ne/1it5GYXfgQCYQA2yuBmOUCgYANRkR2YR1axaCk+NlSu6oTdmdPu6M5x7PNQE7O\nOtMaKjua9lppvIzFGAdMDUtueoEEAE7ZR1xnwfB6PDLUpJdIYAqgr1YfPt8qkjaZ\nTv56yZ7CwL0pbF8m6nwqRrZoDp1wwraEvvvxFKFKGY/k3kCHlpTakdjEoDjn3gDi\nRnWeVQKBgCEneMSzucei5LRppRtRaJw/Btll8qlPMlX3W7dxQ3cLwpmLOn0m51Fp\nPIZ44zYK8R6fu4+/sSrlfaIg86Ugeufp6YNxyNROKxUGza5vDIu5OftwWtBeg+UK\nZ8lLWNdX6pp7WMujmF3H1DrkBbauYMUKZ4UxUYtelgHERMePIxwb\n-----END RSA PRIVATE KEY-----"
              }
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [
        "amd64",
        "s390x"
      ],
      "bundle_path": "registry.connect.redhat.com/fujitsu-postgres/fujitsu-enterprise-postgres-13-bundle@sha256:af272c4338a870a6f4cf1276f16d5cb76696e3f86f96fc17a9ed7fbf4310553b",
      "bundle_path_digest": "sha256:af272c4338a870a6f4cf1276f16d5cb76696e3f86f96fc17a9ed7fbf4310553b",
      "capabilities": [
        "Auto Pilot"
      ],
      "channel_name": "stable-13",
      "creation_date": "2022-11-28T09:23:34.698000+00:00",
      "csv_description": "FUJITSU Enterprise Postgres 13 delivers an enterprise-grade PostgreSQL on OpenShift Container Platform. \n\nThis solution provides the flexibility of a hybrid cloud solution while delivering an enhanced distribution of PostgreSQL to support enterprise-level workloads and provide improved deployment and management, availability, performance, data governance and security.  \n\nAvailable as a multi-architecture container built for both amd64 and s390x. \n\nThe download and Use of the Product is strictly subject to the terms of the End User License Agreement with Fujitsu Limited found at https://www.fast.fujitsu.com/fujitsu-enterprise-postgres-license-agreements. Where the Product that has been embedded as a whole or part into a third party program, only Authorised Customers may download and use the Product.\n",
      "csv_display_name": "FUJITSU Enterprise Postgres 13 Operator",
      "csv_metadata_description": "OpenShift Operator for Fujitsu Enterprise Postgres 13",
      "csv_name": "fujitsu-enterprise-operator.v3.1.9",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": true,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-11-28T09:23:34.698000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.11",
      "organization": "certified-operators",
      "package": "fujitsu-enterprise-operator",
      "provided_apis": [
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPAction",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPAutoscale",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPBackup",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPCert",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPCluster",
          "version": "v2"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPConfig",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPExporter",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPPgpool2",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPPgpool2Cert",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPRestore",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPUser",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPVolume",
          "version": "v1"
        }
      ],
      "provider": "Fujitsu",
      "related_images": [
        {
          "digest": "sha256:80f5d30c66844eb05de5d67999198d9ea3d457f8fbe81358de0b37db0f15f0e3",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-13-server@sha256:80f5d30c66844eb05de5d67999198d9ea3d457f8fbe81358de0b37db0f15f0e3",
          "name": "fep"
        },
        {
          "digest": "sha256:c27c4b52361e73dd460eac5d8fe4765f582d0ffd4f4fc072bff4523b785f47e5",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-13-backup@sha256:c27c4b52361e73dd460eac5d8fe4765f582d0ffd4f4fc072bff4523b785f47e5",
          "name": "backup"
        },
        {
          "digest": "sha256:99fd4033d9f5e536911915410abfc636b55f09e63f88cf32dbe9f58f79124963",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-13-restore@sha256:99fd4033d9f5e536911915410abfc636b55f09e63f88cf32dbe9f58f79124963",
          "name": "restore"
        },
        {
          "digest": "sha256:2f23389e16635ff3e75774dac430c35dc4b314f291d9303d9296852eb0e436bc",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-13-pgpool2@sha256:2f23389e16635ff3e75774dac430c35dc4b314f291d9303d9296852eb0e436bc",
          "name": "pgpool2"
        },
        {
          "digest": "sha256:b8c93f9dc83332f547682e7c1bc9a691d20f91793a27cabe1798bd9ae8c96cb2",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-13-exporter@sha256:b8c93f9dc83332f547682e7c1bc9a691d20f91793a27cabe1798bd9ae8c96cb2",
          "name": "fepexporter"
        },
        {
          "digest": "sha256:130ea4b657170dd2d7ccf77ba7105d875593f6a8a33f4bb6f7871c9467c6b724",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-13-operator@sha256:130ea4b657170dd2d7ccf77ba7105d875593f6a8a33f4bb6f7871c9467c6b724",
          "name": "fujitsu-enterprise-postgres-13-operator-130ea4b657170dd2d7ccf77ba7105d875593f6a8a33f4bb6f7871c9467c6b724-annotation"
        },
        {
          "digest": "sha256:130ea4b657170dd2d7ccf77ba7105d875593f6a8a33f4bb6f7871c9467c6b724",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-13-operator@sha256:130ea4b657170dd2d7ccf77ba7105d875593f6a8a33f4bb6f7871c9467c6b724",
          "name": "fep-ansible-operator"
        }
      ],
      "replaces": null,
      "skip_range": "<3.1.9",
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.11",
      "version": "3.1.9",
      "version_original": "3.1.9"
    },
    {
      "_id": "63847e223b5f5f6a53095f33",
      "alm_examples": [
        {
          "api_version": "fep.fujitsu.io/v1",
          "kind": "FEPAction",
          "metadata": {
            "name": "new-fep-action"
          },
          "spec": {
            "fepAction": {
              "args": [
                "new-fep-sts-0",
                "new-fep-sts-1"
              ],
              "type": "reload"
            },
            "sysExtraLogging": false,
            "targetClusterName": "new-fep"
          }
        },
        {
          "api_version": "fep.fujitsu.io/v1",
          "kind": "FEPExporter",
          "metadata": {
            "name": "new-fep-exporter"
          },
          "spec": {
            "fepExporter": {
              "exporterLogLevel": "error",
              "fepClusterList": [
                "new-fep1"
              ],
              "image": {
                "pullPolicy": "IfNotPresent"
              },
              "mcSpec": {
                "limits": {
                  "cpu": "500m",
                  "memory": "700Mi"
                },
                "requests": {
                  "cpu": "200m",
                  "memory": "512Mi"
                }
              },
              "restartRequired": false,
              "sysExtraLogging": false,
              "userCustomQueries": "usr_example:\n  query: \"SELECT EXTRACT(EPOCH FROM (now() - pg_last_xact_replay_timestamp())) as lag\"\n  master: true\n  metrics:\n    - lag:\n        usage: \"GAUGE\"\n        description: \"Replication lag behind master in seconds\""
            }
          }
        },
        {
          "api_version": "fep.fujitsu.io/v1",
          "kind": "FEPPgpool2",
          "metadata": {
            "name": "new-fep-pgpool2"
          },
          "spec": {
            "count": 2,
            "customhba": "local   all         all                               trust\nhost    all         all         127.0.0.1/32          trust\nhost    all         all         ::1/128               trust\n",
            "customlogsize": "128Mi",
            "customparams": "listen_addresses = '*'\npcp_listen_addresses = '*'\nnum_init_children = 32\nreserved_connections = 0\nenable_pool_hba = off\nallow_clear_text_frontend_auth = off\nauthentication_timeout = 80\nbackend_weight0 = 1\nbackend_weight1 = 1\nbackend_flag0 = 'ALWAYS_PRIMARY'\nbackend_flag1 = 'DISALLOW_TO_FAILOVER'\nconnection_cache = on\nmax_pool = 4\nlisten_backlog_multiplier = 2\nserialize_accept = off\nchild_life_time = 300\nclient_idle_limit = 0\nchild_max_connections = 0\nconnection_life_time = 0\nreset_query_list = 'ABORT; DISCARD ALL'\nclient_min_messages = info\nlog_min_messages = debug1\nlog_statement = on\nlog_per_node_statement = on\nlog_client_messages = on\nlog_hostname = on\nlog_connections = on\nlog_line_prefix = '%t: pid %p: '\nload_balance_mode = on\nignore_leading_white_space = on\nwhite_function_list = ''\nblack_function_list = 'currval,lastval,nextval,setval'\nblack_query_pattern_list = ''\ndatabase_redirect_preference_list = ''\napp_name_redirect_preference_list = ''\nallow_sql_comments = off\ndisable_load_balance_on_write = 'transaction'\nstatement_level_load_balance = on\nsr_check_period = 0\nsr_check_user = 'postgres'\ndelay_threshold = 0\nlog_standby_delay = 'none'\nssl = on\nssl_ciphers = 'HIGH:MEDIUM:+3DES:!aNULL'\nssl_prefer_server_ciphers = off\nssl_ecdh_curve = 'prime256v1'\nssl_dh_params_file = ''\nrelcache_expire = 0\nrelcache_size = 256\ncheck_temp_table = catalog\ncheck_unlogged_table = on\nenable_shared_relcache = on\nrelcache_query_target = primary\nwd_port0 = 9000\nfailover_on_backend_error = off\n",
            "custompcp": "none",
            "customsslcacert": "none",
            "customsslcert": "-----BEGIN CERTIFICATE-----\nMIIDTzCCAjegAwIBAgIUYssQ8I74US5g+1+Z7CHuaDgkZnEwDQYJKoZIhvcNAQEL\nBQAwNzEQMA4GA1UECgwHRnVqaXRzdTEjMCEGA1UEAwwaRkVQIFJvb3QgQ0EgZm9y\nIEt1YmVybmV0ZXMwHhcNMjEwMjA2MDM1MjI4WhcNMzEwMjA0MDM1MjI4WjA3MRAw\nDgYDVQQKDAdGdWppdHN1MSMwIQYDVQQDDBpGRVAgUm9vdCBDQSBmb3IgS3ViZXJu\nZXRlczCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBAMs97gUF0xkUzCgL\n7MiiDju9ySr/ziwjvcYU7jA9ML+SLmftMs3HtcYbAmSntqI+MDBSR/FAJTOoytuT\npV+mCFcGj2YAjDpliHPeNcUpbryy4YMChF3+MovkIwGCksxo5rhiWhGmoBYpA48P\n4Xe8SPlzqMzhFvNeKzyiUhvjutS2Y1Ss38lsTaurFPx64vQ2PaC54XzdwMptXtpb\ntYmWSzCpJWwxZ6lF3vitdA2w0tnBWNyctAd0+RIM/fvArxiIqseAux9t0uogm5to\nlRIhvekuxOpXBPEqtIYQ4j9XUW2JH8vUDnzPkPvjrq+A3Ug8OyyfGVrW7+VYXozu\nc4aP7P0CAwEAAaNTMFEwHQYDVR0OBBYEFBzCutQ7S74WEhS5V2sNEJBGyLpmMB8G\nA1UdIwQYMBaAFBzCutQ7S74WEhS5V2sNEJBGyLpmMA8GA1UdEwEB/wQFMAMBAf8w\nDQYJKoZIhvcNAQELBQADggEBAMDwD85RAaWEBptFgLzKw+9xEUy1vcZaonAuA1qc\nT342XTueyAugxkC11HwdCGgGS34VyctfMGqj4AW6pA2ez4tLrbOps4DmV4sw8uBL\n8pgRDgfly3ob9FEg2wa0hmrwX9jH5Bt4vySUE2785uPAqaspT2UNtTBxS85BUi1T\nsKId2Rtil6an281Z81wyWVI6Jm2D4MG0mbsiGcTPlCtdg/UljvDYymXlAvd4vNhl\nk9hDa13TgDqJKgKdTIcmZoNQdpEVgFcO0h9AEUy5AuLqxHq60dLfZ6ESGPlMI7Lm\ni4PzYbCnBmOe+7TnHcPSyrnehs66Ik+oifRd82eYS7vKjFw=\n-----END CERTIFICATE-----",
            "customsslkey": "none",
            "fepclustername": "new-fep",
            "imagePullPolicy": "IfNotPresent",
            "limits": {
              "cpu": "400m",
              "memory": "512Mi"
            },
            "requests": {
              "cpu": "200m",
              "memory": "256Mi"
            },
            "serviceport": 9999,
            "statusport": 9898
          }
        },
        {
          "api_version": "fep.fujitsu.io/v1",
          "kind": "FEPRestore",
          "metadata": {
            "name": "new-fep-restore"
          },
          "spec": {
            "fromFEPcluster": "new-fep",
            "imagePullPolicy": "IfNotPresent",
            "mcSpec": {
              "limits": {
                "cpu": "200m",
                "memory": "300Mi"
              },
              "requests": {
                "cpu": "100m",
                "memory": "200Mi"
              }
            },
            "restoretype": "latest",
            "sysExtraLogging": false,
            "toFEPcluster": "new-fep-2"
          }
        },
        {
          "api_version": "fep.fujitsu.io/v2",
          "kind": "FEPCluster",
          "metadata": {
            "name": "new-fep"
          },
          "spec": {
            "fep": {
              "customAnnotations": {
                "allDeployments": {}
              },
              "forceSsl": true,
              "image": {
                "pullPolicy": "IfNotPresent"
              },
              "instances": 1,
              "mcSpec": {
                "limits": {
                  "cpu": "500m",
                  "memory": "700Mi"
                },
                "requests": {
                  "cpu": "200m",
                  "memory": "512Mi"
                }
              },
              "podAntiAffinity": false,
              "podDisruptionBudget": false,
              "servicePort": 27500,
              "syncMode": "off",
              "sysExtraLogging": false
            },
            "fepChildCrVal": {
              "backup": {
                "image": {
                  "pullPolicy": "IfNotPresent"
                },
                "mcSpec": {
                  "limits": {
                    "cpu": 0.2,
                    "memory": "300Mi"
                  },
                  "requests": {
                    "cpu": 0.1,
                    "memory": "200Mi"
                  }
                },
                "pgbackrestParams": "# if log volume is not defined, log_directory should be\n# changed to '/database/userdata/data/log'\n[global]\nrepo1-retention-full=7\nrepo1-retention-full-type=time\nlog-path=/database/log/backup\n",
                "postScript": " ",
                "preScript": " ",
                "schedule": {
                  "num": 2
                },
                "schedule1": {
                  "schedule": "15 0 * * 0",
                  "type": "full"
                },
                "schedule2": {
                  "schedule": "15 0 * * 1-6",
                  "type": "incr"
                },
                "schedule3": {
                  "schedule": " ",
                  "type": " "
                },
                "schedule4": {
                  "schedule": " ",
                  "type": " "
                },
                "schedule5": {
                  "schedule": " ",
                  "type": " "
                }
              },
              "customPgAudit": "# define pg audit custom params here to override defaults.\n# if log volume is not defined, log_directory should be\n# changed to '/database/userdata/data/log'\n[output]\nlogger = 'auditlog'\nlog_directory = '/database/log/audit'\nlog_truncate_on_rotation = on\nlog_filename = 'pgaudit-%a.log'\nlog_rotation_age = 1d\nlog_rotation_size = 0\n[rule]\n",
              "customPgHba": "# define pg_hba custom rules here to be merged with default rules.\n# TYPE     DATABASE        USER        ADDRESS        METHOD\n",
              "customPgParams": "# define custom postgresql.conf parameters below to override defaults.\n# Current values are as per default FEP deployment\nshared_preload_libraries='pgx_datamasking,pg_prewarm,pg_stat_statements'\nsession_preload_libraries='pg_prewarm'\nmax_prepared_transactions = 100\nmax_worker_processes = 30\nmax_connections = 100\nwork_mem = 1MB\nmaintenance_work_mem = 12MB\nshared_buffers = 128MB\neffective_cache_size = 384MB\ncheckpoint_completion_target = 0.8\n# tcp parameters\ntcp_keepalives_idle = 30\ntcp_keepalives_interval = 10\ntcp_keepalives_count = 3\n# logging parameters in default fep installation\n# if log volume is not defined, log_directory should be\n# changed to '/database/userdata/data/log'\nlog_directory = '/database/log'\nlog_filename = 'logfile-%a.log'\nlog_file_mode = 0600\nlog_truncate_on_rotation = on\nlog_rotation_age = 1d\nlog_rotation_size = 0\nlog_checkpoints = on\nlog_line_prefix = '%e %t [%p]: [%l-1] user=%u,db=%d,app=%a,client=%h'\nlog_lock_waits = on\nlog_autovacuum_min_duration = 60s\nlogging_collector = on\npgaudit.config_file='/opt/app-root/src/pgaudit-cfg/pgaudit.conf'\nlog_replication_commands = on\nlog_min_messages = WARNING\nlog_destination = stderr\n# wal_archive parameters in default fep installation\narchive_mode = on\narchive_command = 'pgbackrest --stanza=backupstanza --config=/database/userdata/pgbackrest.conf archive-push %p'\nwal_level = replica\nmax_wal_senders = 12\nwal_keep_segments = 64\ntrack_activities = on\ntrack_counts = on\n",
              "storage": {
                "archivewalVol": {
                  "size": "1Gi"
                },
                "backupVol": {
                  "size": "2Gi"
                },
                "dataVol": {
                  "size": "2Gi"
                },
                "logVol": {
                  "size": "1Gi"
                },
                "tablespaceVol": {
                  "size": "512Mi"
                },
                "walVol": {
                  "size": "1200Mi"
                }
              },
              "sysUsers": {
                "pgAdminPassword": "admin-password",
                "pgRewindPassword": "rewind_password",
                "pgRewindUser": "rewind_user",
                "pgdb": "mydb",
                "pgpassword": "mydbpassword",
                "pgreplpassword": "repluserpwd",
                "pgrepluser": "repluser",
                "pguser": "mydbuser",
                "tdepassphrase": "tde-passphrase"
              },
              "systemCertificates": {
                "cacrt": "-----BEGIN CERTIFICATE-----\nMIIDTzCCAjegAwIBAgIUYssQ8I74US5g+1+Z7CHuaDgkZnEwDQYJKoZIhvcNAQEL\nBQAwNzEQMA4GA1UECgwHRnVqaXRzdTEjMCEGA1UEAwwaRkVQIFJvb3QgQ0EgZm9y\nIEt1YmVybmV0ZXMwHhcNMjEwMjA2MDM1MjI4WhcNMzEwMjA0MDM1MjI4WjA3MRAw\nDgYDVQQKDAdGdWppdHN1MSMwIQYDVQQDDBpGRVAgUm9vdCBDQSBmb3IgS3ViZXJu\nZXRlczCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBAMs97gUF0xkUzCgL\n7MiiDju9ySr/ziwjvcYU7jA9ML+SLmftMs3HtcYbAmSntqI+MDBSR/FAJTOoytuT\npV+mCFcGj2YAjDpliHPeNcUpbryy4YMChF3+MovkIwGCksxo5rhiWhGmoBYpA48P\n4Xe8SPlzqMzhFvNeKzyiUhvjutS2Y1Ss38lsTaurFPx64vQ2PaC54XzdwMptXtpb\ntYmWSzCpJWwxZ6lF3vitdA2w0tnBWNyctAd0+RIM/fvArxiIqseAux9t0uogm5to\nlRIhvekuxOpXBPEqtIYQ4j9XUW2JH8vUDnzPkPvjrq+A3Ug8OyyfGVrW7+VYXozu\nc4aP7P0CAwEAAaNTMFEwHQYDVR0OBBYEFBzCutQ7S74WEhS5V2sNEJBGyLpmMB8G\nA1UdIwQYMBaAFBzCutQ7S74WEhS5V2sNEJBGyLpmMA8GA1UdEwEB/wQFMAMBAf8w\nDQYJKoZIhvcNAQELBQADggEBAMDwD85RAaWEBptFgLzKw+9xEUy1vcZaonAuA1qc\nT342XTueyAugxkC11HwdCGgGS34VyctfMGqj4AW6pA2ez4tLrbOps4DmV4sw8uBL\n8pgRDgfly3ob9FEg2wa0hmrwX9jH5Bt4vySUE2785uPAqaspT2UNtTBxS85BUi1T\nsKId2Rtil6an281Z81wyWVI6Jm2D4MG0mbsiGcTPlCtdg/UljvDYymXlAvd4vNhl\nk9hDa13TgDqJKgKdTIcmZoNQdpEVgFcO0h9AEUy5AuLqxHq60dLfZ6ESGPlMI7Lm\ni4PzYbCnBmOe+7TnHcPSyrnehs66Ik+oifRd82eYS7vKjFw=\n-----END CERTIFICATE-----",
                "crt": "-----BEGIN CERTIFICATE-----\nMIIDUTCCAjmgAwIBAgIRAMocW3qMoHrD6qRvMPppMkMwDQYJKoZIhvcNAQELBQAw\nNzEQMA4GA1UECgwHRnVqaXRzdTEjMCEGA1UEAwwaRkVQIFJvb3QgQ0EgZm9yIEt1\nYmVybmV0ZXMwHhcNMjEwMjA2MDQzMjM2WhcNMjYwMjA1MDQzMjM2WjA/MRAwDgYD\nVQQKEwdGdWppdHN1MSswKQYDVQQDEyJGVUpJVFNVIEVudGVycHJpc2UgUG9zdGdy\nZXMgU2VydmVyMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEA4AI33yvH\nZws+jta6qpV6wzJqF8odIfTIpCfbrVcUUtLFKJ1I2e4SceTKi6O3C/I1XuvWlpng\n5IO65+fQQLO06z1/AuQT78YUn/Wlm9x1aHVsv4ANB5JWWqDOjrRT3o7nRPGXfila\nbP0rGE2mJJcVR9nExJ3IeaktgT3sb8YlXvtchyYpmjdbfxabTz07ig0+6/cwKoRR\nxOK8Uf7f5euE0cI/490J6r5Rs4lgD8sIQNCUFlTFYvmAH7gcdssSFBt8NPlUATHE\nsoFmlW0DKCJWNhTLOht+s6L/1zwTHLjPG2pdkG6Wdgmu5H2pDml8CDNLDv98Aj7i\n+I5SRKKcVPlnuQIDAQABo1AwTjAdBgNVHSUEFjAUBggrBgEFBQcDAQYIKwYBBQUH\nAwIwDAYDVR0TAQH/BAIwADAfBgNVHSMEGDAWgBQcwrrUO0u+FhIUuVdrDRCQRsi6\nZjANBgkqhkiG9w0BAQsFAAOCAQEAm5dxBoI9pScOCvRAchg4CprdRDSJb9K6yB3O\nnCAxnM47iHeXnY3WlnI388kHu8DU7O4ba1tJbGs3KY9KzioPk43pU12jWkO1onoF\n+mTDjx/Ef1cYWA9r5q/LtgTa6Q2sxV4O2x67QW82aAnaxO34dV5zWCPIvAoovZBV\nHRT+BgCg3r2vD1RGKK2nl1aYJtWhO1SZubam+VttdZ/vbM9oOJctxmImsEtBXjkY\nKteePdQtLL5o03JhyXWyRshCq+HMmKf2KgyY8gvydGcP4eLQdBWcW40LcnVq6UjT\n0kJycJEKngMVademq1ZWHGaiYB7hyT6GhgIcHUJ2cKrPgbEh1Q==\n-----END CERTIFICATE-----",
                "key": "-----BEGIN RSA PRIVATE KEY-----\nMIIEowIBAAKCAQEA4AI33yvHZws+jta6qpV6wzJqF8odIfTIpCfbrVcUUtLFKJ1I\n2e4SceTKi6O3C/I1XuvWlpng5IO65+fQQLO06z1/AuQT78YUn/Wlm9x1aHVsv4AN\nB5JWWqDOjrRT3o7nRPGXfilabP0rGE2mJJcVR9nExJ3IeaktgT3sb8YlXvtchyYp\nmjdbfxabTz07ig0+6/cwKoRRxOK8Uf7f5euE0cI/490J6r5Rs4lgD8sIQNCUFlTF\nYvmAH7gcdssSFBt8NPlUATHEsoFmlW0DKCJWNhTLOht+s6L/1zwTHLjPG2pdkG6W\ndgmu5H2pDml8CDNLDv98Aj7i+I5SRKKcVPlnuQIDAQABAoIBAFPQYKlOzw/+BA0b\nyMIUpdctIMb/54CR/xR0mVw1DbSjigNVPjHUQvB8Y1B2FAITQObgJO06bAv0QdWN\nRb0/v/yYiNJDFjaLjaIAHlO/2+oWrXbFaZqgpVDJhB+e1xaZr2x7XGxm+p925k30\nl6pvIRY+I8JRKvZiV1VZHwL/R3JOtPr++xMZtLVjVOI+f+ySqJ+TZHuAjm49EKxj\ncEmmJ28b7QcziXsvKy00f+zbqLIBKXQdZAFU5eEr1BsDRXdRW+Kf0XIvftuy4BJZ\nvoKT+VGhEvF/qysswL4+6IAO6tpuYnnM0Y2d3sOGoWPkTcQK0MekYKzL/WmtCjNs\n9hodJtECgYEA5EWyhEOf4uOKe5TDp697UCUvXLoOR58FDe/S8XNvScn29jjOkqIg\nOMoqo9xAkJTNTzqn5UUdt1x/pgM2NxlPLFijrc0zQlX3SoOO2ryDd9WNi7YKtN16\nKJqa536WeZu2OEbuAZ+S3GALVy1RPeTNPnUOmKnF06DjDUGzLNCZy10CgYEA+zfw\n952DWuz1U0Z4wvAEqqcgUKXPKrkTXV/iUnjkDkrLYVr0ZofDNTXrdHl+UedFmaOC\ncieZn6DNhcdz5tKtyysGMH3g/qs9PfoGUngvcXsy0Egk04l3x1jc8TTCLqXZXYaQ\nHMsx51n+R58oncPtzYSUOr9qQ6PbC2CstTbFJA0CgYEAjGEsUliAB/jknfEzjXjG\nPdhQUxb8VyE864Az2lah9t/kJzFyIAziAeqZ5GE7t247AGFTBRTHHI8e1Qoemi3P\nWbc9GVIbFs1lIYbcIDpUIyrKPEP8O5QEXtoNLxXTFgAjRGKiVY87spjCAJ+W2ZhO\ne/1it5GYXfgQCYQA2yuBmOUCgYANRkR2YR1axaCk+NlSu6oTdmdPu6M5x7PNQE7O\nOtMaKjua9lppvIzFGAdMDUtueoEEAE7ZR1xnwfB6PDLUpJdIYAqgr1YfPt8qkjaZ\nTv56yZ7CwL0pbF8m6nwqRrZoDp1wwraEvvvxFKFKGY/k3kCHlpTakdjEoDjn3gDi\nRnWeVQKBgCEneMSzucei5LRppRtRaJw/Btll8qlPMlX3W7dxQ3cLwpmLOn0m51Fp\nPIZ44zYK8R6fu4+/sSrlfaIg86Ugeufp6YNxyNROKxUGza5vDIu5OftwWtBeg+UK\nZ8lLWNdX6pp7WMujmF3H1DrkBbauYMUKZ4UxUYtelgHERMePIxwb\n-----END RSA PRIVATE KEY-----"
              }
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [
        "amd64",
        "s390x"
      ],
      "bundle_path": "registry.connect.redhat.com/fujitsu-postgres/fujitsu-enterprise-postgres-13-bundle@sha256:af272c4338a870a6f4cf1276f16d5cb76696e3f86f96fc17a9ed7fbf4310553b",
      "bundle_path_digest": "sha256:af272c4338a870a6f4cf1276f16d5cb76696e3f86f96fc17a9ed7fbf4310553b",
      "capabilities": [
        "Auto Pilot"
      ],
      "channel_name": "stable-13",
      "creation_date": "2022-11-28T09:23:46.409000+00:00",
      "csv_description": "FUJITSU Enterprise Postgres 13 delivers an enterprise-grade PostgreSQL on OpenShift Container Platform. \n\nThis solution provides the flexibility of a hybrid cloud solution while delivering an enhanced distribution of PostgreSQL to support enterprise-level workloads and provide improved deployment and management, availability, performance, data governance and security.  \n\nAvailable as a multi-architecture container built for both amd64 and s390x. \n\nThe download and Use of the Product is strictly subject to the terms of the End User License Agreement with Fujitsu Limited found at https://www.fast.fujitsu.com/fujitsu-enterprise-postgres-license-agreements. Where the Product that has been embedded as a whole or part into a third party program, only Authorised Customers may download and use the Product.\n",
      "csv_display_name": "FUJITSU Enterprise Postgres 13 Operator",
      "csv_metadata_description": "OpenShift Operator for Fujitsu Enterprise Postgres 13",
      "csv_name": "fujitsu-enterprise-operator.v3.1.9",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": true,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-11-28T09:23:46.409000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.10",
      "organization": "certified-operators",
      "package": "fujitsu-enterprise-operator",
      "provided_apis": [
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPCluster",
          "plural": "fepclusters",
          "version": "v2"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPExporter",
          "plural": "fepexporters",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPVolume",
          "plural": "fepvolumes",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPBackup",
          "plural": "fepbackups",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPAutoscale",
          "plural": "fepautoscales",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPCert",
          "plural": "fepcerts",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPConfig",
          "plural": "fepconfigs",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPPgpool2Cert",
          "plural": "feppgpool2certs",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPPgpool2",
          "plural": "feppgpool2s",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPAction",
          "plural": "fepactions",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPRestore",
          "plural": "feprestores",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPUser",
          "plural": "fepusers",
          "version": "v1"
        }
      ],
      "provider": "Fujitsu",
      "related_images": [
        {
          "digest": "sha256:80f5d30c66844eb05de5d67999198d9ea3d457f8fbe81358de0b37db0f15f0e3",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-13-server@sha256:80f5d30c66844eb05de5d67999198d9ea3d457f8fbe81358de0b37db0f15f0e3",
          "name": "fep"
        },
        {
          "digest": "sha256:c27c4b52361e73dd460eac5d8fe4765f582d0ffd4f4fc072bff4523b785f47e5",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-13-backup@sha256:c27c4b52361e73dd460eac5d8fe4765f582d0ffd4f4fc072bff4523b785f47e5",
          "name": "backup"
        },
        {
          "digest": "sha256:99fd4033d9f5e536911915410abfc636b55f09e63f88cf32dbe9f58f79124963",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-13-restore@sha256:99fd4033d9f5e536911915410abfc636b55f09e63f88cf32dbe9f58f79124963",
          "name": "restore"
        },
        {
          "digest": "sha256:2f23389e16635ff3e75774dac430c35dc4b314f291d9303d9296852eb0e436bc",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-13-pgpool2@sha256:2f23389e16635ff3e75774dac430c35dc4b314f291d9303d9296852eb0e436bc",
          "name": "pgpool2"
        },
        {
          "digest": "sha256:b8c93f9dc83332f547682e7c1bc9a691d20f91793a27cabe1798bd9ae8c96cb2",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-13-exporter@sha256:b8c93f9dc83332f547682e7c1bc9a691d20f91793a27cabe1798bd9ae8c96cb2",
          "name": "fepexporter"
        },
        {
          "digest": "sha256:130ea4b657170dd2d7ccf77ba7105d875593f6a8a33f4bb6f7871c9467c6b724",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-13-operator@sha256:130ea4b657170dd2d7ccf77ba7105d875593f6a8a33f4bb6f7871c9467c6b724",
          "name": "fujitsu-enterprise-postgres-13-operator-130ea4b657170dd2d7ccf77ba7105d875593f6a8a33f4bb6f7871c9467c6b724-annotation"
        },
        {
          "digest": "sha256:130ea4b657170dd2d7ccf77ba7105d875593f6a8a33f4bb6f7871c9467c6b724",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-13-operator@sha256:130ea4b657170dd2d7ccf77ba7105d875593f6a8a33f4bb6f7871c9467c6b724",
          "name": "fep-ansible-operator"
        }
      ],
      "replaces": null,
      "skip_range": "<3.1.9",
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.10",
      "version": "3.1.9",
      "version_original": "3.1.9"
    },
    {
      "_id": "63847e3ab59d1c1edb22975b",
      "alm_examples": [
        {
          "api_version": "fep.fujitsu.io/v1",
          "kind": "FEPAction",
          "metadata": {
            "name": "new-fep-action"
          },
          "spec": {
            "fepAction": {
              "args": [
                "new-fep-sts-0",
                "new-fep-sts-1"
              ],
              "type": "reload"
            },
            "sysExtraLogging": false,
            "targetClusterName": "new-fep"
          }
        },
        {
          "api_version": "fep.fujitsu.io/v1",
          "kind": "FEPExporter",
          "metadata": {
            "name": "new-fep-exporter"
          },
          "spec": {
            "fepExporter": {
              "exporterLogLevel": "error",
              "fepClusterList": [
                "new-fep1"
              ],
              "image": {
                "pullPolicy": "IfNotPresent"
              },
              "mcSpec": {
                "limits": {
                  "cpu": "500m",
                  "memory": "700Mi"
                },
                "requests": {
                  "cpu": "200m",
                  "memory": "512Mi"
                }
              },
              "restartRequired": false,
              "sysExtraLogging": false,
              "userCustomQueries": "usr_example:\n  query: \"SELECT EXTRACT(EPOCH FROM (now() - pg_last_xact_replay_timestamp())) as lag\"\n  master: true\n  metrics:\n    - lag:\n        usage: \"GAUGE\"\n        description: \"Replication lag behind master in seconds\""
            }
          }
        },
        {
          "api_version": "fep.fujitsu.io/v1",
          "kind": "FEPPgpool2",
          "metadata": {
            "name": "new-fep-pgpool2"
          },
          "spec": {
            "count": 2,
            "customhba": "local   all         all                               trust\nhost    all         all         127.0.0.1/32          trust\nhost    all         all         ::1/128               trust\n",
            "customlogsize": "128Mi",
            "customparams": "listen_addresses = '*'\npcp_listen_addresses = '*'\nnum_init_children = 32\nreserved_connections = 0\nenable_pool_hba = off\nallow_clear_text_frontend_auth = off\nauthentication_timeout = 80\nbackend_weight0 = 1\nbackend_weight1 = 1\nbackend_flag0 = 'ALWAYS_PRIMARY'\nbackend_flag1 = 'DISALLOW_TO_FAILOVER'\nconnection_cache = on\nmax_pool = 4\nlisten_backlog_multiplier = 2\nserialize_accept = off\nchild_life_time = 300\nclient_idle_limit = 0\nchild_max_connections = 0\nconnection_life_time = 0\nreset_query_list = 'ABORT; DISCARD ALL'\nclient_min_messages = info\nlog_min_messages = debug1\nlog_statement = on\nlog_per_node_statement = on\nlog_client_messages = on\nlog_hostname = on\nlog_connections = on\nlog_line_prefix = '%t: pid %p: '\nload_balance_mode = on\nignore_leading_white_space = on\nwhite_function_list = ''\nblack_function_list = 'currval,lastval,nextval,setval'\nblack_query_pattern_list = ''\ndatabase_redirect_preference_list = ''\napp_name_redirect_preference_list = ''\nallow_sql_comments = off\ndisable_load_balance_on_write = 'transaction'\nstatement_level_load_balance = on\nsr_check_period = 0\nsr_check_user = 'postgres'\ndelay_threshold = 0\nlog_standby_delay = 'none'\nssl = on\nssl_ciphers = 'HIGH:MEDIUM:+3DES:!aNULL'\nssl_prefer_server_ciphers = off\nssl_ecdh_curve = 'prime256v1'\nssl_dh_params_file = ''\nrelcache_expire = 0\nrelcache_size = 256\ncheck_temp_table = catalog\ncheck_unlogged_table = on\nenable_shared_relcache = on\nrelcache_query_target = primary\nwd_port0 = 9000\nfailover_on_backend_error = off\n",
            "custompcp": "none",
            "customsslcacert": "none",
            "customsslcert": "-----BEGIN CERTIFICATE-----\nMIIDTzCCAjegAwIBAgIUYssQ8I74US5g+1+Z7CHuaDgkZnEwDQYJKoZIhvcNAQEL\nBQAwNzEQMA4GA1UECgwHRnVqaXRzdTEjMCEGA1UEAwwaRkVQIFJvb3QgQ0EgZm9y\nIEt1YmVybmV0ZXMwHhcNMjEwMjA2MDM1MjI4WhcNMzEwMjA0MDM1MjI4WjA3MRAw\nDgYDVQQKDAdGdWppdHN1MSMwIQYDVQQDDBpGRVAgUm9vdCBDQSBmb3IgS3ViZXJu\nZXRlczCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBAMs97gUF0xkUzCgL\n7MiiDju9ySr/ziwjvcYU7jA9ML+SLmftMs3HtcYbAmSntqI+MDBSR/FAJTOoytuT\npV+mCFcGj2YAjDpliHPeNcUpbryy4YMChF3+MovkIwGCksxo5rhiWhGmoBYpA48P\n4Xe8SPlzqMzhFvNeKzyiUhvjutS2Y1Ss38lsTaurFPx64vQ2PaC54XzdwMptXtpb\ntYmWSzCpJWwxZ6lF3vitdA2w0tnBWNyctAd0+RIM/fvArxiIqseAux9t0uogm5to\nlRIhvekuxOpXBPEqtIYQ4j9XUW2JH8vUDnzPkPvjrq+A3Ug8OyyfGVrW7+VYXozu\nc4aP7P0CAwEAAaNTMFEwHQYDVR0OBBYEFBzCutQ7S74WEhS5V2sNEJBGyLpmMB8G\nA1UdIwQYMBaAFBzCutQ7S74WEhS5V2sNEJBGyLpmMA8GA1UdEwEB/wQFMAMBAf8w\nDQYJKoZIhvcNAQELBQADggEBAMDwD85RAaWEBptFgLzKw+9xEUy1vcZaonAuA1qc\nT342XTueyAugxkC11HwdCGgGS34VyctfMGqj4AW6pA2ez4tLrbOps4DmV4sw8uBL\n8pgRDgfly3ob9FEg2wa0hmrwX9jH5Bt4vySUE2785uPAqaspT2UNtTBxS85BUi1T\nsKId2Rtil6an281Z81wyWVI6Jm2D4MG0mbsiGcTPlCtdg/UljvDYymXlAvd4vNhl\nk9hDa13TgDqJKgKdTIcmZoNQdpEVgFcO0h9AEUy5AuLqxHq60dLfZ6ESGPlMI7Lm\ni4PzYbCnBmOe+7TnHcPSyrnehs66Ik+oifRd82eYS7vKjFw=\n-----END CERTIFICATE-----",
            "customsslkey": "none",
            "fepclustername": "new-fep",
            "imagePullPolicy": "IfNotPresent",
            "limits": {
              "cpu": "400m",
              "memory": "512Mi"
            },
            "requests": {
              "cpu": "200m",
              "memory": "256Mi"
            },
            "serviceport": 9999,
            "statusport": 9898
          }
        },
        {
          "api_version": "fep.fujitsu.io/v1",
          "kind": "FEPRestore",
          "metadata": {
            "name": "new-fep-restore"
          },
          "spec": {
            "fromFEPcluster": "new-fep",
            "imagePullPolicy": "IfNotPresent",
            "mcSpec": {
              "limits": {
                "cpu": "200m",
                "memory": "300Mi"
              },
              "requests": {
                "cpu": "100m",
                "memory": "200Mi"
              }
            },
            "restoretype": "latest",
            "sysExtraLogging": false,
            "toFEPcluster": "new-fep-2"
          }
        },
        {
          "api_version": "fep.fujitsu.io/v2",
          "kind": "FEPCluster",
          "metadata": {
            "name": "new-fep"
          },
          "spec": {
            "fep": {
              "customAnnotations": {
                "allDeployments": {}
              },
              "forceSsl": true,
              "image": {
                "pullPolicy": "IfNotPresent"
              },
              "instances": 1,
              "mcSpec": {
                "limits": {
                  "cpu": "500m",
                  "memory": "700Mi"
                },
                "requests": {
                  "cpu": "200m",
                  "memory": "512Mi"
                }
              },
              "podAntiAffinity": false,
              "podDisruptionBudget": false,
              "servicePort": 27500,
              "syncMode": "off",
              "sysExtraLogging": false
            },
            "fepChildCrVal": {
              "backup": {
                "image": {
                  "pullPolicy": "IfNotPresent"
                },
                "mcSpec": {
                  "limits": {
                    "cpu": 0.2,
                    "memory": "300Mi"
                  },
                  "requests": {
                    "cpu": 0.1,
                    "memory": "200Mi"
                  }
                },
                "pgbackrestParams": "# if log volume is not defined, log_directory should be\n# changed to '/database/userdata/data/log'\n[global]\nrepo1-retention-full=7\nrepo1-retention-full-type=time\nlog-path=/database/log/backup\n",
                "postScript": " ",
                "preScript": " ",
                "schedule": {
                  "num": 2
                },
                "schedule1": {
                  "schedule": "15 0 * * 0",
                  "type": "full"
                },
                "schedule2": {
                  "schedule": "15 0 * * 1-6",
                  "type": "incr"
                },
                "schedule3": {
                  "schedule": " ",
                  "type": " "
                },
                "schedule4": {
                  "schedule": " ",
                  "type": " "
                },
                "schedule5": {
                  "schedule": " ",
                  "type": " "
                }
              },
              "customPgAudit": "# define pg audit custom params here to override defaults.\n# if log volume is not defined, log_directory should be\n# changed to '/database/userdata/data/log'\n[output]\nlogger = 'auditlog'\nlog_directory = '/database/log/audit'\nlog_truncate_on_rotation = on\nlog_filename = 'pgaudit-%a.log'\nlog_rotation_age = 1d\nlog_rotation_size = 0\n[rule]\n",
              "customPgHba": "# define pg_hba custom rules here to be merged with default rules.\n# TYPE     DATABASE        USER        ADDRESS        METHOD\n",
              "customPgParams": "# define custom postgresql.conf parameters below to override defaults.\n# Current values are as per default FEP deployment\nshared_preload_libraries='pgx_datamasking,pg_prewarm,pg_stat_statements'\nsession_preload_libraries='pg_prewarm'\nmax_prepared_transactions = 100\nmax_worker_processes = 30\nmax_connections = 100\nwork_mem = 1MB\nmaintenance_work_mem = 12MB\nshared_buffers = 128MB\neffective_cache_size = 384MB\ncheckpoint_completion_target = 0.8\n# tcp parameters\ntcp_keepalives_idle = 30\ntcp_keepalives_interval = 10\ntcp_keepalives_count = 3\n# logging parameters in default fep installation\n# if log volume is not defined, log_directory should be\n# changed to '/database/userdata/data/log'\nlog_directory = '/database/log'\nlog_filename = 'logfile-%a.log'\nlog_file_mode = 0600\nlog_truncate_on_rotation = on\nlog_rotation_age = 1d\nlog_rotation_size = 0\nlog_checkpoints = on\nlog_line_prefix = '%e %t [%p]: [%l-1] user=%u,db=%d,app=%a,client=%h'\nlog_lock_waits = on\nlog_autovacuum_min_duration = 60s\nlogging_collector = on\npgaudit.config_file='/opt/app-root/src/pgaudit-cfg/pgaudit.conf'\nlog_replication_commands = on\nlog_min_messages = WARNING\nlog_destination = stderr\n# wal_archive parameters in default fep installation\narchive_mode = on\narchive_command = 'pgbackrest --stanza=backupstanza --config=/database/userdata/pgbackrest.conf archive-push %p'\nwal_level = replica\nmax_wal_senders = 12\nwal_keep_segments = 64\ntrack_activities = on\ntrack_counts = on\n",
              "storage": {
                "archivewalVol": {
                  "size": "1Gi"
                },
                "backupVol": {
                  "size": "2Gi"
                },
                "dataVol": {
                  "size": "2Gi"
                },
                "logVol": {
                  "size": "1Gi"
                },
                "tablespaceVol": {
                  "size": "512Mi"
                },
                "walVol": {
                  "size": "1200Mi"
                }
              },
              "sysUsers": {
                "pgAdminPassword": "admin-password",
                "pgRewindPassword": "rewind_password",
                "pgRewindUser": "rewind_user",
                "pgdb": "mydb",
                "pgpassword": "mydbpassword",
                "pgreplpassword": "repluserpwd",
                "pgrepluser": "repluser",
                "pguser": "mydbuser",
                "tdepassphrase": "tde-passphrase"
              },
              "systemCertificates": {
                "cacrt": "-----BEGIN CERTIFICATE-----\nMIIDTzCCAjegAwIBAgIUYssQ8I74US5g+1+Z7CHuaDgkZnEwDQYJKoZIhvcNAQEL\nBQAwNzEQMA4GA1UECgwHRnVqaXRzdTEjMCEGA1UEAwwaRkVQIFJvb3QgQ0EgZm9y\nIEt1YmVybmV0ZXMwHhcNMjEwMjA2MDM1MjI4WhcNMzEwMjA0MDM1MjI4WjA3MRAw\nDgYDVQQKDAdGdWppdHN1MSMwIQYDVQQDDBpGRVAgUm9vdCBDQSBmb3IgS3ViZXJu\nZXRlczCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBAMs97gUF0xkUzCgL\n7MiiDju9ySr/ziwjvcYU7jA9ML+SLmftMs3HtcYbAmSntqI+MDBSR/FAJTOoytuT\npV+mCFcGj2YAjDpliHPeNcUpbryy4YMChF3+MovkIwGCksxo5rhiWhGmoBYpA48P\n4Xe8SPlzqMzhFvNeKzyiUhvjutS2Y1Ss38lsTaurFPx64vQ2PaC54XzdwMptXtpb\ntYmWSzCpJWwxZ6lF3vitdA2w0tnBWNyctAd0+RIM/fvArxiIqseAux9t0uogm5to\nlRIhvekuxOpXBPEqtIYQ4j9XUW2JH8vUDnzPkPvjrq+A3Ug8OyyfGVrW7+VYXozu\nc4aP7P0CAwEAAaNTMFEwHQYDVR0OBBYEFBzCutQ7S74WEhS5V2sNEJBGyLpmMB8G\nA1UdIwQYMBaAFBzCutQ7S74WEhS5V2sNEJBGyLpmMA8GA1UdEwEB/wQFMAMBAf8w\nDQYJKoZIhvcNAQELBQADggEBAMDwD85RAaWEBptFgLzKw+9xEUy1vcZaonAuA1qc\nT342XTueyAugxkC11HwdCGgGS34VyctfMGqj4AW6pA2ez4tLrbOps4DmV4sw8uBL\n8pgRDgfly3ob9FEg2wa0hmrwX9jH5Bt4vySUE2785uPAqaspT2UNtTBxS85BUi1T\nsKId2Rtil6an281Z81wyWVI6Jm2D4MG0mbsiGcTPlCtdg/UljvDYymXlAvd4vNhl\nk9hDa13TgDqJKgKdTIcmZoNQdpEVgFcO0h9AEUy5AuLqxHq60dLfZ6ESGPlMI7Lm\ni4PzYbCnBmOe+7TnHcPSyrnehs66Ik+oifRd82eYS7vKjFw=\n-----END CERTIFICATE-----",
                "crt": "-----BEGIN CERTIFICATE-----\nMIIDUTCCAjmgAwIBAgIRAMocW3qMoHrD6qRvMPppMkMwDQYJKoZIhvcNAQELBQAw\nNzEQMA4GA1UECgwHRnVqaXRzdTEjMCEGA1UEAwwaRkVQIFJvb3QgQ0EgZm9yIEt1\nYmVybmV0ZXMwHhcNMjEwMjA2MDQzMjM2WhcNMjYwMjA1MDQzMjM2WjA/MRAwDgYD\nVQQKEwdGdWppdHN1MSswKQYDVQQDEyJGVUpJVFNVIEVudGVycHJpc2UgUG9zdGdy\nZXMgU2VydmVyMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEA4AI33yvH\nZws+jta6qpV6wzJqF8odIfTIpCfbrVcUUtLFKJ1I2e4SceTKi6O3C/I1XuvWlpng\n5IO65+fQQLO06z1/AuQT78YUn/Wlm9x1aHVsv4ANB5JWWqDOjrRT3o7nRPGXfila\nbP0rGE2mJJcVR9nExJ3IeaktgT3sb8YlXvtchyYpmjdbfxabTz07ig0+6/cwKoRR\nxOK8Uf7f5euE0cI/490J6r5Rs4lgD8sIQNCUFlTFYvmAH7gcdssSFBt8NPlUATHE\nsoFmlW0DKCJWNhTLOht+s6L/1zwTHLjPG2pdkG6Wdgmu5H2pDml8CDNLDv98Aj7i\n+I5SRKKcVPlnuQIDAQABo1AwTjAdBgNVHSUEFjAUBggrBgEFBQcDAQYIKwYBBQUH\nAwIwDAYDVR0TAQH/BAIwADAfBgNVHSMEGDAWgBQcwrrUO0u+FhIUuVdrDRCQRsi6\nZjANBgkqhkiG9w0BAQsFAAOCAQEAm5dxBoI9pScOCvRAchg4CprdRDSJb9K6yB3O\nnCAxnM47iHeXnY3WlnI388kHu8DU7O4ba1tJbGs3KY9KzioPk43pU12jWkO1onoF\n+mTDjx/Ef1cYWA9r5q/LtgTa6Q2sxV4O2x67QW82aAnaxO34dV5zWCPIvAoovZBV\nHRT+BgCg3r2vD1RGKK2nl1aYJtWhO1SZubam+VttdZ/vbM9oOJctxmImsEtBXjkY\nKteePdQtLL5o03JhyXWyRshCq+HMmKf2KgyY8gvydGcP4eLQdBWcW40LcnVq6UjT\n0kJycJEKngMVademq1ZWHGaiYB7hyT6GhgIcHUJ2cKrPgbEh1Q==\n-----END CERTIFICATE-----",
                "key": "-----BEGIN RSA PRIVATE KEY-----\nMIIEowIBAAKCAQEA4AI33yvHZws+jta6qpV6wzJqF8odIfTIpCfbrVcUUtLFKJ1I\n2e4SceTKi6O3C/I1XuvWlpng5IO65+fQQLO06z1/AuQT78YUn/Wlm9x1aHVsv4AN\nB5JWWqDOjrRT3o7nRPGXfilabP0rGE2mJJcVR9nExJ3IeaktgT3sb8YlXvtchyYp\nmjdbfxabTz07ig0+6/cwKoRRxOK8Uf7f5euE0cI/490J6r5Rs4lgD8sIQNCUFlTF\nYvmAH7gcdssSFBt8NPlUATHEsoFmlW0DKCJWNhTLOht+s6L/1zwTHLjPG2pdkG6W\ndgmu5H2pDml8CDNLDv98Aj7i+I5SRKKcVPlnuQIDAQABAoIBAFPQYKlOzw/+BA0b\nyMIUpdctIMb/54CR/xR0mVw1DbSjigNVPjHUQvB8Y1B2FAITQObgJO06bAv0QdWN\nRb0/v/yYiNJDFjaLjaIAHlO/2+oWrXbFaZqgpVDJhB+e1xaZr2x7XGxm+p925k30\nl6pvIRY+I8JRKvZiV1VZHwL/R3JOtPr++xMZtLVjVOI+f+ySqJ+TZHuAjm49EKxj\ncEmmJ28b7QcziXsvKy00f+zbqLIBKXQdZAFU5eEr1BsDRXdRW+Kf0XIvftuy4BJZ\nvoKT+VGhEvF/qysswL4+6IAO6tpuYnnM0Y2d3sOGoWPkTcQK0MekYKzL/WmtCjNs\n9hodJtECgYEA5EWyhEOf4uOKe5TDp697UCUvXLoOR58FDe/S8XNvScn29jjOkqIg\nOMoqo9xAkJTNTzqn5UUdt1x/pgM2NxlPLFijrc0zQlX3SoOO2ryDd9WNi7YKtN16\nKJqa536WeZu2OEbuAZ+S3GALVy1RPeTNPnUOmKnF06DjDUGzLNCZy10CgYEA+zfw\n952DWuz1U0Z4wvAEqqcgUKXPKrkTXV/iUnjkDkrLYVr0ZofDNTXrdHl+UedFmaOC\ncieZn6DNhcdz5tKtyysGMH3g/qs9PfoGUngvcXsy0Egk04l3x1jc8TTCLqXZXYaQ\nHMsx51n+R58oncPtzYSUOr9qQ6PbC2CstTbFJA0CgYEAjGEsUliAB/jknfEzjXjG\nPdhQUxb8VyE864Az2lah9t/kJzFyIAziAeqZ5GE7t247AGFTBRTHHI8e1Qoemi3P\nWbc9GVIbFs1lIYbcIDpUIyrKPEP8O5QEXtoNLxXTFgAjRGKiVY87spjCAJ+W2ZhO\ne/1it5GYXfgQCYQA2yuBmOUCgYANRkR2YR1axaCk+NlSu6oTdmdPu6M5x7PNQE7O\nOtMaKjua9lppvIzFGAdMDUtueoEEAE7ZR1xnwfB6PDLUpJdIYAqgr1YfPt8qkjaZ\nTv56yZ7CwL0pbF8m6nwqRrZoDp1wwraEvvvxFKFKGY/k3kCHlpTakdjEoDjn3gDi\nRnWeVQKBgCEneMSzucei5LRppRtRaJw/Btll8qlPMlX3W7dxQ3cLwpmLOn0m51Fp\nPIZ44zYK8R6fu4+/sSrlfaIg86Ugeufp6YNxyNROKxUGza5vDIu5OftwWtBeg+UK\nZ8lLWNdX6pp7WMujmF3H1DrkBbauYMUKZ4UxUYtelgHERMePIxwb\n-----END RSA PRIVATE KEY-----"
              }
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [
        "amd64",
        "s390x"
      ],
      "bundle_path": "registry.connect.redhat.com/fujitsu-postgres/fujitsu-enterprise-postgres-13-bundle@sha256:af272c4338a870a6f4cf1276f16d5cb76696e3f86f96fc17a9ed7fbf4310553b",
      "bundle_path_digest": "sha256:af272c4338a870a6f4cf1276f16d5cb76696e3f86f96fc17a9ed7fbf4310553b",
      "capabilities": [
        "Auto Pilot"
      ],
      "channel_name": "stable-13",
      "creation_date": "2022-11-28T09:24:10.143000+00:00",
      "csv_description": "FUJITSU Enterprise Postgres 13 delivers an enterprise-grade PostgreSQL on OpenShift Container Platform. \n\nThis solution provides the flexibility of a hybrid cloud solution while delivering an enhanced distribution of PostgreSQL to support enterprise-level workloads and provide improved deployment and management, availability, performance, data governance and security.  \n\nAvailable as a multi-architecture container built for both amd64 and s390x. \n\nThe download and Use of the Product is strictly subject to the terms of the End User License Agreement with Fujitsu Limited found at https://www.fast.fujitsu.com/fujitsu-enterprise-postgres-license-agreements. Where the Product that has been embedded as a whole or part into a third party program, only Authorised Customers may download and use the Product.\n",
      "csv_display_name": "FUJITSU Enterprise Postgres 13 Operator",
      "csv_metadata_description": "OpenShift Operator for Fujitsu Enterprise Postgres 13",
      "csv_name": "fujitsu-enterprise-operator.v3.1.9",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": true,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-11-28T09:24:10.143000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.9",
      "organization": "certified-operators",
      "package": "fujitsu-enterprise-operator",
      "provided_apis": [
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPBackup",
          "plural": "fepbackups",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPPgpool2Cert",
          "plural": "feppgpool2certs",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPConfig",
          "plural": "fepconfigs",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPPgpool2",
          "plural": "feppgpool2s",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPRestore",
          "plural": "feprestores",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPAction",
          "plural": "fepactions",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPVolume",
          "plural": "fepvolumes",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPAutoscale",
          "plural": "fepautoscales",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPCert",
          "plural": "fepcerts",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPCluster",
          "plural": "fepclusters",
          "version": "v2"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPExporter",
          "plural": "fepexporters",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPUser",
          "plural": "fepusers",
          "version": "v1"
        }
      ],
      "provider": "Fujitsu",
      "related_images": [
        {
          "digest": "sha256:80f5d30c66844eb05de5d67999198d9ea3d457f8fbe81358de0b37db0f15f0e3",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-13-server@sha256:80f5d30c66844eb05de5d67999198d9ea3d457f8fbe81358de0b37db0f15f0e3",
          "name": "fep"
        },
        {
          "digest": "sha256:c27c4b52361e73dd460eac5d8fe4765f582d0ffd4f4fc072bff4523b785f47e5",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-13-backup@sha256:c27c4b52361e73dd460eac5d8fe4765f582d0ffd4f4fc072bff4523b785f47e5",
          "name": "backup"
        },
        {
          "digest": "sha256:99fd4033d9f5e536911915410abfc636b55f09e63f88cf32dbe9f58f79124963",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-13-restore@sha256:99fd4033d9f5e536911915410abfc636b55f09e63f88cf32dbe9f58f79124963",
          "name": "restore"
        },
        {
          "digest": "sha256:2f23389e16635ff3e75774dac430c35dc4b314f291d9303d9296852eb0e436bc",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-13-pgpool2@sha256:2f23389e16635ff3e75774dac430c35dc4b314f291d9303d9296852eb0e436bc",
          "name": "pgpool2"
        },
        {
          "digest": "sha256:b8c93f9dc83332f547682e7c1bc9a691d20f91793a27cabe1798bd9ae8c96cb2",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-13-exporter@sha256:b8c93f9dc83332f547682e7c1bc9a691d20f91793a27cabe1798bd9ae8c96cb2",
          "name": "fepexporter"
        },
        {
          "digest": "sha256:130ea4b657170dd2d7ccf77ba7105d875593f6a8a33f4bb6f7871c9467c6b724",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-13-operator@sha256:130ea4b657170dd2d7ccf77ba7105d875593f6a8a33f4bb6f7871c9467c6b724",
          "name": "fujitsu-enterprise-postgres-13-operator-130ea4b657170dd2d7ccf77ba7105d875593f6a8a33f4bb6f7871c9467c6b724-annotation"
        },
        {
          "digest": "sha256:130ea4b657170dd2d7ccf77ba7105d875593f6a8a33f4bb6f7871c9467c6b724",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-13-operator@sha256:130ea4b657170dd2d7ccf77ba7105d875593f6a8a33f4bb6f7871c9467c6b724",
          "name": "fep-ansible-operator"
        }
      ],
      "replaces": null,
      "skip_range": "<3.1.9",
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.9",
      "version": "3.1.9",
      "version_original": "3.1.9"
    },
    {
      "_id": "63847ed1d1516305d2078eb0",
      "alm_examples": [
        {
          "api_version": "fep.fujitsu.io/v1",
          "kind": "FEPAction",
          "metadata": {
            "name": "new-fep-action"
          },
          "spec": {
            "fepAction": {
              "args": [
                "new-fep-sts-0",
                "new-fep-sts-1"
              ],
              "type": "reload"
            },
            "sysExtraLogging": false,
            "targetClusterName": "new-fep"
          }
        },
        {
          "api_version": "fep.fujitsu.io/v1",
          "kind": "FEPExporter",
          "metadata": {
            "name": "new-fep-exporter"
          },
          "spec": {
            "fepExporter": {
              "exporterLogLevel": "error",
              "fepClusterList": [
                "new-fep1"
              ],
              "image": {
                "pullPolicy": "IfNotPresent"
              },
              "mcSpec": {
                "limits": {
                  "cpu": "500m",
                  "memory": "700Mi"
                },
                "requests": {
                  "cpu": "200m",
                  "memory": "512Mi"
                }
              },
              "restartRequired": false,
              "sysExtraLogging": false,
              "userCustomQueries": "usr_example:\n  query: \"SELECT EXTRACT(EPOCH FROM (now() - pg_last_xact_replay_timestamp())) as lag\"\n  master: true\n  metrics:\n    - lag:\n        usage: \"GAUGE\"\n        description: \"Replication lag behind master in seconds\""
            }
          }
        },
        {
          "api_version": "fep.fujitsu.io/v1",
          "kind": "FEPPgpool2",
          "metadata": {
            "name": "new-fep-pgpool2"
          },
          "spec": {
            "count": 2,
            "customhba": "local   all         all                               trust\nhost    all         all         127.0.0.1/32          trust\nhost    all         all         ::1/128               trust\n",
            "customlogsize": "128Mi",
            "customparams": "listen_addresses = '*'\npcp_listen_addresses = '*'\nnum_init_children = 32\nreserved_connections = 0\nenable_pool_hba = off\nallow_clear_text_frontend_auth = off\nauthentication_timeout = 80\nbackend_weight0 = 1\nbackend_weight1 = 1\nbackend_flag0 = 'ALWAYS_PRIMARY'\nbackend_flag1 = 'DISALLOW_TO_FAILOVER'\nconnection_cache = on\nmax_pool = 4\nlisten_backlog_multiplier = 2\nserialize_accept = off\nchild_life_time = 300\nclient_idle_limit = 0\nchild_max_connections = 0\nconnection_life_time = 0\nreset_query_list = 'ABORT; DISCARD ALL'\nclient_min_messages = info\nlog_min_messages = debug1\nlog_statement = on\nlog_per_node_statement = on\nlog_client_messages = on\nlog_hostname = on\nlog_connections = on\nlog_line_prefix = '%t: pid %p: '\nload_balance_mode = on\nignore_leading_white_space = on\nwhite_function_list = ''\nblack_function_list = 'currval,lastval,nextval,setval'\nblack_query_pattern_list = ''\ndatabase_redirect_preference_list = ''\napp_name_redirect_preference_list = ''\nallow_sql_comments = off\ndisable_load_balance_on_write = 'transaction'\nstatement_level_load_balance = on\nsr_check_period = 0\nsr_check_user = 'postgres'\ndelay_threshold = 0\nlog_standby_delay = 'none'\nssl = on\nssl_ciphers = 'HIGH:MEDIUM:+3DES:!aNULL'\nssl_prefer_server_ciphers = off\nssl_ecdh_curve = 'prime256v1'\nssl_dh_params_file = ''\nrelcache_expire = 0\nrelcache_size = 256\ncheck_temp_table = catalog\ncheck_unlogged_table = on\nenable_shared_relcache = on\nrelcache_query_target = primary\nwd_port0 = 9000\nfailover_on_backend_error = off\n",
            "custompcp": "none",
            "customsslcacert": "none",
            "customsslcert": "-----BEGIN CERTIFICATE-----\nMIIDTzCCAjegAwIBAgIUYssQ8I74US5g+1+Z7CHuaDgkZnEwDQYJKoZIhvcNAQEL\nBQAwNzEQMA4GA1UECgwHRnVqaXRzdTEjMCEGA1UEAwwaRkVQIFJvb3QgQ0EgZm9y\nIEt1YmVybmV0ZXMwHhcNMjEwMjA2MDM1MjI4WhcNMzEwMjA0MDM1MjI4WjA3MRAw\nDgYDVQQKDAdGdWppdHN1MSMwIQYDVQQDDBpGRVAgUm9vdCBDQSBmb3IgS3ViZXJu\nZXRlczCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBAMs97gUF0xkUzCgL\n7MiiDju9ySr/ziwjvcYU7jA9ML+SLmftMs3HtcYbAmSntqI+MDBSR/FAJTOoytuT\npV+mCFcGj2YAjDpliHPeNcUpbryy4YMChF3+MovkIwGCksxo5rhiWhGmoBYpA48P\n4Xe8SPlzqMzhFvNeKzyiUhvjutS2Y1Ss38lsTaurFPx64vQ2PaC54XzdwMptXtpb\ntYmWSzCpJWwxZ6lF3vitdA2w0tnBWNyctAd0+RIM/fvArxiIqseAux9t0uogm5to\nlRIhvekuxOpXBPEqtIYQ4j9XUW2JH8vUDnzPkPvjrq+A3Ug8OyyfGVrW7+VYXozu\nc4aP7P0CAwEAAaNTMFEwHQYDVR0OBBYEFBzCutQ7S74WEhS5V2sNEJBGyLpmMB8G\nA1UdIwQYMBaAFBzCutQ7S74WEhS5V2sNEJBGyLpmMA8GA1UdEwEB/wQFMAMBAf8w\nDQYJKoZIhvcNAQELBQADggEBAMDwD85RAaWEBptFgLzKw+9xEUy1vcZaonAuA1qc\nT342XTueyAugxkC11HwdCGgGS34VyctfMGqj4AW6pA2ez4tLrbOps4DmV4sw8uBL\n8pgRDgfly3ob9FEg2wa0hmrwX9jH5Bt4vySUE2785uPAqaspT2UNtTBxS85BUi1T\nsKId2Rtil6an281Z81wyWVI6Jm2D4MG0mbsiGcTPlCtdg/UljvDYymXlAvd4vNhl\nk9hDa13TgDqJKgKdTIcmZoNQdpEVgFcO0h9AEUy5AuLqxHq60dLfZ6ESGPlMI7Lm\ni4PzYbCnBmOe+7TnHcPSyrnehs66Ik+oifRd82eYS7vKjFw=\n-----END CERTIFICATE-----",
            "customsslkey": "none",
            "fepclustername": "new-fep",
            "imagePullPolicy": "IfNotPresent",
            "limits": {
              "cpu": "400m",
              "memory": "512Mi"
            },
            "requests": {
              "cpu": "200m",
              "memory": "256Mi"
            },
            "serviceport": 9999,
            "statusport": 9898
          }
        },
        {
          "api_version": "fep.fujitsu.io/v1",
          "kind": "FEPRestore",
          "metadata": {
            "name": "new-fep-restore"
          },
          "spec": {
            "fromFEPcluster": "new-fep",
            "imagePullPolicy": "IfNotPresent",
            "mcSpec": {
              "limits": {
                "cpu": "200m",
                "memory": "300Mi"
              },
              "requests": {
                "cpu": "100m",
                "memory": "200Mi"
              }
            },
            "restoretype": "latest",
            "sysExtraLogging": false,
            "toFEPcluster": "new-fep-2"
          }
        },
        {
          "api_version": "fep.fujitsu.io/v2",
          "kind": "FEPCluster",
          "metadata": {
            "name": "new-fep"
          },
          "spec": {
            "fep": {
              "customAnnotations": {
                "allDeployments": {}
              },
              "forceSsl": true,
              "image": {
                "pullPolicy": "IfNotPresent"
              },
              "instances": 1,
              "mcSpec": {
                "limits": {
                  "cpu": "500m",
                  "memory": "700Mi"
                },
                "requests": {
                  "cpu": "200m",
                  "memory": "512Mi"
                }
              },
              "podAntiAffinity": false,
              "podDisruptionBudget": false,
              "servicePort": 27500,
              "syncMode": "off",
              "sysExtraLogging": false
            },
            "fepChildCrVal": {
              "backup": {
                "image": {
                  "pullPolicy": "IfNotPresent"
                },
                "mcSpec": {
                  "limits": {
                    "cpu": 0.2,
                    "memory": "300Mi"
                  },
                  "requests": {
                    "cpu": 0.1,
                    "memory": "200Mi"
                  }
                },
                "pgbackrestParams": "# if log volume is not defined, log_directory should be\n# changed to '/database/userdata/data/log'\n[global]\nrepo1-retention-full=7\nrepo1-retention-full-type=time\nlog-path=/database/log/backup\n",
                "postScript": " ",
                "preScript": " ",
                "schedule": {
                  "num": 2
                },
                "schedule1": {
                  "schedule": "15 0 * * 0",
                  "type": "full"
                },
                "schedule2": {
                  "schedule": "15 0 * * 1-6",
                  "type": "incr"
                },
                "schedule3": {
                  "schedule": " ",
                  "type": " "
                },
                "schedule4": {
                  "schedule": " ",
                  "type": " "
                },
                "schedule5": {
                  "schedule": " ",
                  "type": " "
                }
              },
              "customPgAudit": "# define pg audit custom params here to override defaults.\n# if log volume is not defined, log_directory should be\n# changed to '/database/userdata/data/log'\n[output]\nlogger = 'auditlog'\nlog_directory = '/database/log/audit'\nlog_truncate_on_rotation = on\nlog_filename = 'pgaudit-%a.log'\nlog_rotation_age = 1d\nlog_rotation_size = 0\n[rule]\n",
              "customPgHba": "# define pg_hba custom rules here to be merged with default rules.\n# TYPE     DATABASE        USER        ADDRESS        METHOD\n",
              "customPgParams": "# define custom postgresql.conf parameters below to override defaults.\n# Current values are as per default FEP deployment\nshared_preload_libraries='pgx_datamasking,pg_prewarm,pg_stat_statements'\nsession_preload_libraries='pg_prewarm'\nmax_prepared_transactions = 100\nmax_worker_processes = 30\nmax_connections = 100\nwork_mem = 1MB\nmaintenance_work_mem = 12MB\nshared_buffers = 128MB\neffective_cache_size = 384MB\ncheckpoint_completion_target = 0.8\n# tcp parameters\ntcp_keepalives_idle = 30\ntcp_keepalives_interval = 10\ntcp_keepalives_count = 3\n# logging parameters in default fep installation\n# if log volume is not defined, log_directory should be\n# changed to '/database/userdata/data/log'\nlog_directory = '/database/log'\nlog_filename = 'logfile-%a.log'\nlog_file_mode = 0600\nlog_truncate_on_rotation = on\nlog_rotation_age = 1d\nlog_rotation_size = 0\nlog_checkpoints = on\nlog_line_prefix = '%e %t [%p]: [%l-1] user=%u,db=%d,app=%a,client=%h'\nlog_lock_waits = on\nlog_autovacuum_min_duration = 60s\nlogging_collector = on\npgaudit.config_file='/opt/app-root/src/pgaudit-cfg/pgaudit.conf'\nlog_replication_commands = on\nlog_min_messages = WARNING\nlog_destination = stderr\n# wal_archive parameters in default fep installation\narchive_mode = on\narchive_command = 'pgbackrest --stanza=backupstanza --config=/database/userdata/pgbackrest.conf archive-push %p'\nwal_level = replica\nmax_wal_senders = 12\nwal_keep_segments = 64\ntrack_activities = on\ntrack_counts = on\n",
              "storage": {
                "archivewalVol": {
                  "size": "1Gi"
                },
                "backupVol": {
                  "size": "2Gi"
                },
                "dataVol": {
                  "size": "2Gi"
                },
                "logVol": {
                  "size": "1Gi"
                },
                "tablespaceVol": {
                  "size": "512Mi"
                },
                "walVol": {
                  "size": "1200Mi"
                }
              },
              "sysUsers": {
                "pgAdminPassword": "admin-password",
                "pgRewindPassword": "rewind_password",
                "pgRewindUser": "rewind_user",
                "pgdb": "mydb",
                "pgpassword": "mydbpassword",
                "pgreplpassword": "repluserpwd",
                "pgrepluser": "repluser",
                "pguser": "mydbuser",
                "tdepassphrase": "tde-passphrase"
              },
              "systemCertificates": {
                "cacrt": "-----BEGIN CERTIFICATE-----\nMIIDTzCCAjegAwIBAgIUYssQ8I74US5g+1+Z7CHuaDgkZnEwDQYJKoZIhvcNAQEL\nBQAwNzEQMA4GA1UECgwHRnVqaXRzdTEjMCEGA1UEAwwaRkVQIFJvb3QgQ0EgZm9y\nIEt1YmVybmV0ZXMwHhcNMjEwMjA2MDM1MjI4WhcNMzEwMjA0MDM1MjI4WjA3MRAw\nDgYDVQQKDAdGdWppdHN1MSMwIQYDVQQDDBpGRVAgUm9vdCBDQSBmb3IgS3ViZXJu\nZXRlczCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBAMs97gUF0xkUzCgL\n7MiiDju9ySr/ziwjvcYU7jA9ML+SLmftMs3HtcYbAmSntqI+MDBSR/FAJTOoytuT\npV+mCFcGj2YAjDpliHPeNcUpbryy4YMChF3+MovkIwGCksxo5rhiWhGmoBYpA48P\n4Xe8SPlzqMzhFvNeKzyiUhvjutS2Y1Ss38lsTaurFPx64vQ2PaC54XzdwMptXtpb\ntYmWSzCpJWwxZ6lF3vitdA2w0tnBWNyctAd0+RIM/fvArxiIqseAux9t0uogm5to\nlRIhvekuxOpXBPEqtIYQ4j9XUW2JH8vUDnzPkPvjrq+A3Ug8OyyfGVrW7+VYXozu\nc4aP7P0CAwEAAaNTMFEwHQYDVR0OBBYEFBzCutQ7S74WEhS5V2sNEJBGyLpmMB8G\nA1UdIwQYMBaAFBzCutQ7S74WEhS5V2sNEJBGyLpmMA8GA1UdEwEB/wQFMAMBAf8w\nDQYJKoZIhvcNAQELBQADggEBAMDwD85RAaWEBptFgLzKw+9xEUy1vcZaonAuA1qc\nT342XTueyAugxkC11HwdCGgGS34VyctfMGqj4AW6pA2ez4tLrbOps4DmV4sw8uBL\n8pgRDgfly3ob9FEg2wa0hmrwX9jH5Bt4vySUE2785uPAqaspT2UNtTBxS85BUi1T\nsKId2Rtil6an281Z81wyWVI6Jm2D4MG0mbsiGcTPlCtdg/UljvDYymXlAvd4vNhl\nk9hDa13TgDqJKgKdTIcmZoNQdpEVgFcO0h9AEUy5AuLqxHq60dLfZ6ESGPlMI7Lm\ni4PzYbCnBmOe+7TnHcPSyrnehs66Ik+oifRd82eYS7vKjFw=\n-----END CERTIFICATE-----",
                "crt": "-----BEGIN CERTIFICATE-----\nMIIDUTCCAjmgAwIBAgIRAMocW3qMoHrD6qRvMPppMkMwDQYJKoZIhvcNAQELBQAw\nNzEQMA4GA1UECgwHRnVqaXRzdTEjMCEGA1UEAwwaRkVQIFJvb3QgQ0EgZm9yIEt1\nYmVybmV0ZXMwHhcNMjEwMjA2MDQzMjM2WhcNMjYwMjA1MDQzMjM2WjA/MRAwDgYD\nVQQKEwdGdWppdHN1MSswKQYDVQQDEyJGVUpJVFNVIEVudGVycHJpc2UgUG9zdGdy\nZXMgU2VydmVyMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEA4AI33yvH\nZws+jta6qpV6wzJqF8odIfTIpCfbrVcUUtLFKJ1I2e4SceTKi6O3C/I1XuvWlpng\n5IO65+fQQLO06z1/AuQT78YUn/Wlm9x1aHVsv4ANB5JWWqDOjrRT3o7nRPGXfila\nbP0rGE2mJJcVR9nExJ3IeaktgT3sb8YlXvtchyYpmjdbfxabTz07ig0+6/cwKoRR\nxOK8Uf7f5euE0cI/490J6r5Rs4lgD8sIQNCUFlTFYvmAH7gcdssSFBt8NPlUATHE\nsoFmlW0DKCJWNhTLOht+s6L/1zwTHLjPG2pdkG6Wdgmu5H2pDml8CDNLDv98Aj7i\n+I5SRKKcVPlnuQIDAQABo1AwTjAdBgNVHSUEFjAUBggrBgEFBQcDAQYIKwYBBQUH\nAwIwDAYDVR0TAQH/BAIwADAfBgNVHSMEGDAWgBQcwrrUO0u+FhIUuVdrDRCQRsi6\nZjANBgkqhkiG9w0BAQsFAAOCAQEAm5dxBoI9pScOCvRAchg4CprdRDSJb9K6yB3O\nnCAxnM47iHeXnY3WlnI388kHu8DU7O4ba1tJbGs3KY9KzioPk43pU12jWkO1onoF\n+mTDjx/Ef1cYWA9r5q/LtgTa6Q2sxV4O2x67QW82aAnaxO34dV5zWCPIvAoovZBV\nHRT+BgCg3r2vD1RGKK2nl1aYJtWhO1SZubam+VttdZ/vbM9oOJctxmImsEtBXjkY\nKteePdQtLL5o03JhyXWyRshCq+HMmKf2KgyY8gvydGcP4eLQdBWcW40LcnVq6UjT\n0kJycJEKngMVademq1ZWHGaiYB7hyT6GhgIcHUJ2cKrPgbEh1Q==\n-----END CERTIFICATE-----",
                "key": "-----BEGIN RSA PRIVATE KEY-----\nMIIEowIBAAKCAQEA4AI33yvHZws+jta6qpV6wzJqF8odIfTIpCfbrVcUUtLFKJ1I\n2e4SceTKi6O3C/I1XuvWlpng5IO65+fQQLO06z1/AuQT78YUn/Wlm9x1aHVsv4AN\nB5JWWqDOjrRT3o7nRPGXfilabP0rGE2mJJcVR9nExJ3IeaktgT3sb8YlXvtchyYp\nmjdbfxabTz07ig0+6/cwKoRRxOK8Uf7f5euE0cI/490J6r5Rs4lgD8sIQNCUFlTF\nYvmAH7gcdssSFBt8NPlUATHEsoFmlW0DKCJWNhTLOht+s6L/1zwTHLjPG2pdkG6W\ndgmu5H2pDml8CDNLDv98Aj7i+I5SRKKcVPlnuQIDAQABAoIBAFPQYKlOzw/+BA0b\nyMIUpdctIMb/54CR/xR0mVw1DbSjigNVPjHUQvB8Y1B2FAITQObgJO06bAv0QdWN\nRb0/v/yYiNJDFjaLjaIAHlO/2+oWrXbFaZqgpVDJhB+e1xaZr2x7XGxm+p925k30\nl6pvIRY+I8JRKvZiV1VZHwL/R3JOtPr++xMZtLVjVOI+f+ySqJ+TZHuAjm49EKxj\ncEmmJ28b7QcziXsvKy00f+zbqLIBKXQdZAFU5eEr1BsDRXdRW+Kf0XIvftuy4BJZ\nvoKT+VGhEvF/qysswL4+6IAO6tpuYnnM0Y2d3sOGoWPkTcQK0MekYKzL/WmtCjNs\n9hodJtECgYEA5EWyhEOf4uOKe5TDp697UCUvXLoOR58FDe/S8XNvScn29jjOkqIg\nOMoqo9xAkJTNTzqn5UUdt1x/pgM2NxlPLFijrc0zQlX3SoOO2ryDd9WNi7YKtN16\nKJqa536WeZu2OEbuAZ+S3GALVy1RPeTNPnUOmKnF06DjDUGzLNCZy10CgYEA+zfw\n952DWuz1U0Z4wvAEqqcgUKXPKrkTXV/iUnjkDkrLYVr0ZofDNTXrdHl+UedFmaOC\ncieZn6DNhcdz5tKtyysGMH3g/qs9PfoGUngvcXsy0Egk04l3x1jc8TTCLqXZXYaQ\nHMsx51n+R58oncPtzYSUOr9qQ6PbC2CstTbFJA0CgYEAjGEsUliAB/jknfEzjXjG\nPdhQUxb8VyE864Az2lah9t/kJzFyIAziAeqZ5GE7t247AGFTBRTHHI8e1Qoemi3P\nWbc9GVIbFs1lIYbcIDpUIyrKPEP8O5QEXtoNLxXTFgAjRGKiVY87spjCAJ+W2ZhO\ne/1it5GYXfgQCYQA2yuBmOUCgYANRkR2YR1axaCk+NlSu6oTdmdPu6M5x7PNQE7O\nOtMaKjua9lppvIzFGAdMDUtueoEEAE7ZR1xnwfB6PDLUpJdIYAqgr1YfPt8qkjaZ\nTv56yZ7CwL0pbF8m6nwqRrZoDp1wwraEvvvxFKFKGY/k3kCHlpTakdjEoDjn3gDi\nRnWeVQKBgCEneMSzucei5LRppRtRaJw/Btll8qlPMlX3W7dxQ3cLwpmLOn0m51Fp\nPIZ44zYK8R6fu4+/sSrlfaIg86Ugeufp6YNxyNROKxUGza5vDIu5OftwWtBeg+UK\nZ8lLWNdX6pp7WMujmF3H1DrkBbauYMUKZ4UxUYtelgHERMePIxwb\n-----END RSA PRIVATE KEY-----"
              }
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [
        "amd64",
        "s390x"
      ],
      "bundle_path": "registry.connect.redhat.com/fujitsu-postgres/fujitsu-enterprise-postgres-13-bundle@sha256:af272c4338a870a6f4cf1276f16d5cb76696e3f86f96fc17a9ed7fbf4310553b",
      "bundle_path_digest": "sha256:af272c4338a870a6f4cf1276f16d5cb76696e3f86f96fc17a9ed7fbf4310553b",
      "capabilities": [
        "Auto Pilot"
      ],
      "channel_name": "stable-13",
      "creation_date": "2022-11-28T09:26:41.745000+00:00",
      "csv_description": "FUJITSU Enterprise Postgres 13 delivers an enterprise-grade PostgreSQL on OpenShift Container Platform. \n\nThis solution provides the flexibility of a hybrid cloud solution while delivering an enhanced distribution of PostgreSQL to support enterprise-level workloads and provide improved deployment and management, availability, performance, data governance and security.  \n\nAvailable as a multi-architecture container built for both amd64 and s390x. \n\nThe download and Use of the Product is strictly subject to the terms of the End User License Agreement with Fujitsu Limited found at https://www.fast.fujitsu.com/fujitsu-enterprise-postgres-license-agreements. Where the Product that has been embedded as a whole or part into a third party program, only Authorised Customers may download and use the Product.\n",
      "csv_display_name": "FUJITSU Enterprise Postgres 13 Operator",
      "csv_metadata_description": "OpenShift Operator for Fujitsu Enterprise Postgres 13",
      "csv_name": "fujitsu-enterprise-operator.v3.1.9",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": true,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-11-28T09:26:41.745000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.8",
      "organization": "certified-operators",
      "package": "fujitsu-enterprise-operator",
      "provided_apis": [
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPCluster",
          "plural": "fepclusters",
          "version": "v2"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPExporter",
          "plural": "fepexporters",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPPgpool2Cert",
          "plural": "feppgpool2certs",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPPgpool2",
          "plural": "feppgpool2s",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPAutoscale",
          "plural": "fepautoscales",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPUser",
          "plural": "fepusers",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPVolume",
          "plural": "fepvolumes",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPConfig",
          "plural": "fepconfigs",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPBackup",
          "plural": "fepbackups",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPCert",
          "plural": "fepcerts",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPRestore",
          "plural": "feprestores",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPAction",
          "plural": "fepactions",
          "version": "v1"
        }
      ],
      "provider": "Fujitsu",
      "related_images": [
        {
          "digest": "sha256:80f5d30c66844eb05de5d67999198d9ea3d457f8fbe81358de0b37db0f15f0e3",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-13-server@sha256:80f5d30c66844eb05de5d67999198d9ea3d457f8fbe81358de0b37db0f15f0e3",
          "name": "fep"
        },
        {
          "digest": "sha256:c27c4b52361e73dd460eac5d8fe4765f582d0ffd4f4fc072bff4523b785f47e5",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-13-backup@sha256:c27c4b52361e73dd460eac5d8fe4765f582d0ffd4f4fc072bff4523b785f47e5",
          "name": "backup"
        },
        {
          "digest": "sha256:99fd4033d9f5e536911915410abfc636b55f09e63f88cf32dbe9f58f79124963",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-13-restore@sha256:99fd4033d9f5e536911915410abfc636b55f09e63f88cf32dbe9f58f79124963",
          "name": "restore"
        },
        {
          "digest": "sha256:2f23389e16635ff3e75774dac430c35dc4b314f291d9303d9296852eb0e436bc",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-13-pgpool2@sha256:2f23389e16635ff3e75774dac430c35dc4b314f291d9303d9296852eb0e436bc",
          "name": "pgpool2"
        },
        {
          "digest": "sha256:b8c93f9dc83332f547682e7c1bc9a691d20f91793a27cabe1798bd9ae8c96cb2",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-13-exporter@sha256:b8c93f9dc83332f547682e7c1bc9a691d20f91793a27cabe1798bd9ae8c96cb2",
          "name": "fepexporter"
        },
        {
          "digest": "sha256:130ea4b657170dd2d7ccf77ba7105d875593f6a8a33f4bb6f7871c9467c6b724",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-13-operator@sha256:130ea4b657170dd2d7ccf77ba7105d875593f6a8a33f4bb6f7871c9467c6b724",
          "name": "fujitsu-enterprise-postgres-13-operator-130ea4b657170dd2d7ccf77ba7105d875593f6a8a33f4bb6f7871c9467c6b724-annotation"
        },
        {
          "digest": "sha256:130ea4b657170dd2d7ccf77ba7105d875593f6a8a33f4bb6f7871c9467c6b724",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-13-operator@sha256:130ea4b657170dd2d7ccf77ba7105d875593f6a8a33f4bb6f7871c9467c6b724",
          "name": "fep-ansible-operator"
        }
      ],
      "replaces": null,
      "skip_range": "<3.1.9",
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.8",
      "version": "3.1.9",
      "version_original": "3.1.9"
    },
    {
      "_id": "63848080b59d1c1edb22a4b3",
      "alm_examples": [
        {
          "api_version": "fep.fujitsu.io/v1",
          "kind": "FEPAction",
          "metadata": {
            "name": "new-fep-action"
          },
          "spec": {
            "fepAction": {
              "args": [
                "new-fep-sts-0",
                "new-fep-sts-1"
              ],
              "type": "reload"
            },
            "sysExtraLogging": false,
            "targetClusterName": "new-fep"
          }
        },
        {
          "api_version": "fep.fujitsu.io/v1",
          "kind": "FEPPgpool2",
          "metadata": {
            "name": "new-fep-pgpool2"
          },
          "spec": {
            "count": 2,
            "customhba": "local   all         all                               trust\nhost    all         all         127.0.0.1/32          trust\nhost    all         all         ::1/128               trust\n",
            "customlogsize": "128Mi",
            "customparams": "listen_addresses = '*'\npcp_listen_addresses = '*'\nnum_init_children = 32\nreserved_connections = 0\nenable_pool_hba = off\nallow_clear_text_frontend_auth = off\nauthentication_timeout = 80\nbackend_weight0 = 1\nbackend_weight1 = 1\nbackend_flag0 = 'ALWAYS_MASTER'\nbackend_flag1 = 'ALLOW_TO_FAILOVER'\nconnection_cache = on\nmax_pool = 4\nlisten_backlog_multiplier = 2\nserialize_accept = off\nchild_life_time = 300\nclient_idle_limit = 0\nchild_max_connections = 0\nconnection_life_time = 0\nreset_query_list = 'ABORT; DISCARD ALL'\nclient_min_messages = info\nlog_min_messages = debug1\nlog_statement = on\nlog_per_node_statement = on\nlog_client_messages = on\nlog_hostname = on\nlog_connections = on\nlog_line_prefix = '%t: pid %p: '\nload_balance_mode = on\nignore_leading_white_space = on\nwhite_function_list = ''\nblack_function_list = 'currval,lastval,nextval,setval'\nblack_query_pattern_list = ''\ndatabase_redirect_preference_list = ''\napp_name_redirect_preference_list = ''\nallow_sql_comments = off\ndisable_load_balance_on_write = 'transaction'\nstatement_level_load_balance = on\nconnect_timeout = 10000\nsr_check_period = 0\nsr_check_user = 'postgres'\ndelay_threshold = 0\nlog_standby_delay = 'none'\nssl = on\nssl_ciphers = 'HIGH:MEDIUM:+3DES:!aNULL'\nssl_prefer_server_ciphers = off\nssl_ecdh_curve = 'prime256v1'\nssl_dh_params_file = ''\nrelcache_expire = 0\nrelcache_size = 256\ncheck_temp_table = catalog\ncheck_unlogged_table = on\nenable_shared_relcache = on\nrelcache_query_target = master\nfailover_on_backend_error = off\n",
            "custompcp": "none",
            "customsslcacert": "none",
            "customsslcert": "-----BEGIN CERTIFICATE-----\nMIIDTzCCAjegAwIBAgIUYssQ8I74US5g+1+Z7CHuaDgkZnEwDQYJKoZIhvcNAQEL\nBQAwNzEQMA4GA1UECgwHRnVqaXRzdTEjMCEGA1UEAwwaRkVQIFJvb3QgQ0EgZm9y\nIEt1YmVybmV0ZXMwHhcNMjEwMjA2MDM1MjI4WhcNMzEwMjA0MDM1MjI4WjA3MRAw\nDgYDVQQKDAdGdWppdHN1MSMwIQYDVQQDDBpGRVAgUm9vdCBDQSBmb3IgS3ViZXJu\nZXRlczCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBAMs97gUF0xkUzCgL\n7MiiDju9ySr/ziwjvcYU7jA9ML+SLmftMs3HtcYbAmSntqI+MDBSR/FAJTOoytuT\npV+mCFcGj2YAjDpliHPeNcUpbryy4YMChF3+MovkIwGCksxo5rhiWhGmoBYpA48P\n4Xe8SPlzqMzhFvNeKzyiUhvjutS2Y1Ss38lsTaurFPx64vQ2PaC54XzdwMptXtpb\ntYmWSzCpJWwxZ6lF3vitdA2w0tnBWNyctAd0+RIM/fvArxiIqseAux9t0uogm5to\nlRIhvekuxOpXBPEqtIYQ4j9XUW2JH8vUDnzPkPvjrq+A3Ug8OyyfGVrW7+VYXozu\nc4aP7P0CAwEAAaNTMFEwHQYDVR0OBBYEFBzCutQ7S74WEhS5V2sNEJBGyLpmMB8G\nA1UdIwQYMBaAFBzCutQ7S74WEhS5V2sNEJBGyLpmMA8GA1UdEwEB/wQFMAMBAf8w\nDQYJKoZIhvcNAQELBQADggEBAMDwD85RAaWEBptFgLzKw+9xEUy1vcZaonAuA1qc\nT342XTueyAugxkC11HwdCGgGS34VyctfMGqj4AW6pA2ez4tLrbOps4DmV4sw8uBL\n8pgRDgfly3ob9FEg2wa0hmrwX9jH5Bt4vySUE2785uPAqaspT2UNtTBxS85BUi1T\nsKId2Rtil6an281Z81wyWVI6Jm2D4MG0mbsiGcTPlCtdg/UljvDYymXlAvd4vNhl\nk9hDa13TgDqJKgKdTIcmZoNQdpEVgFcO0h9AEUy5AuLqxHq60dLfZ6ESGPlMI7Lm\ni4PzYbCnBmOe+7TnHcPSyrnehs66Ik+oifRd82eYS7vKjFw=\n-----END CERTIFICATE-----",
            "customsslkey": "none",
            "fepclustername": "new-fep",
            "imagePullPolicy": "IfNotPresent",
            "limits": {
              "cpu": "400m",
              "memory": "512Mi"
            },
            "requests": {
              "cpu": "200m",
              "memory": "256Mi"
            },
            "serviceport": 9999,
            "statusport": 9898
          }
        },
        {
          "api_version": "fep.fujitsu.io/v1",
          "kind": "FEPRestore",
          "metadata": {
            "name": "new-fep-restore"
          },
          "spec": {
            "fromFEPcluster": "new-fep",
            "imagePullPolicy": "IfNotPresent",
            "mcSpec": {
              "limits": {
                "cpu": "200m",
                "memory": "300Mi"
              },
              "requests": {
                "cpu": "100m",
                "memory": "200Mi"
              }
            },
            "restoretype": "latest",
            "sysExtraLogging": false,
            "toFEPcluster": "new-fep-2"
          }
        },
        {
          "api_version": "fep.fujitsu.io/v2",
          "kind": "FEPCluster",
          "metadata": {
            "name": "new-fep"
          },
          "spec": {
            "fep": {
              "customAnnotations": {
                "allDeployments": {}
              },
              "forceSsl": true,
              "image": {
                "pullPolicy": "IfNotPresent"
              },
              "instances": 1,
              "mcSpec": {
                "limits": {
                  "cpu": "500m",
                  "memory": "700Mi"
                },
                "requests": {
                  "cpu": "200m",
                  "memory": "512Mi"
                }
              },
              "podAntiAffinity": false,
              "podDisruptionBudget": false,
              "servicePort": 27500,
              "syncMode": "off",
              "sysExtraLogging": false
            },
            "fepChildCrVal": {
              "backup": {
                "image": {
                  "pullPolicy": "IfNotPresent"
                },
                "mcSpec": {
                  "limits": {
                    "cpu": 0.2,
                    "memory": "300Mi"
                  },
                  "requests": {
                    "cpu": 0.1,
                    "memory": "200Mi"
                  }
                },
                "pgbackrestParams": "# if log volume is not defined, log_directory should be\n# changed to '/database/userdata/data/log'\n[global]\nrepo1-retention-full=7\nrepo1-retention-full-type=time\nlog-path=/database/log/backup\n",
                "postScript": " ",
                "preScript": " ",
                "schedule": {
                  "num": 2
                },
                "schedule1": {
                  "schedule": "15 0 * * 0",
                  "type": "full"
                },
                "schedule2": {
                  "schedule": "15 0 * * 1-6",
                  "type": "incr"
                },
                "schedule3": {
                  "schedule": " ",
                  "type": " "
                },
                "schedule4": {
                  "schedule": " ",
                  "type": " "
                },
                "schedule5": {
                  "schedule": " ",
                  "type": " "
                }
              },
              "customPgAudit": "# define pg audit custom params here to override defaults.\n# if log volume is not defined, log_directory should be\n# changed to '/database/userdata/data/log'\n[output]\nlog_truncate_on_rotation = on\nlog_filename = 'pgaudit-%a.log'\nlog_rotation_age = 1d\nlog_rotation_size = 0\n[rule]\n",
              "customPgHba": "# define pg_hba custom rules here to be merged with default rules.\n# TYPE     DATABASE        USER        ADDRESS        METHOD\n",
              "customPgParams": "# define custom postgresql.conf parameters below to override defaults.\n# Current values are as per default FEP deployment\nshared_preload_libraries='pgx_datamasking,pg_prewarm,pg_stat_statements'\nsession_preload_libraries='pg_prewarm'\nmax_prepared_transactions = 100\nmax_worker_processes = 20\nmax_connections = 100\nwork_mem = 1MB\nmaintenance_work_mem = 12MB\nshared_buffers = 128MB\neffective_cache_size = 384MB\ncheckpoint_completion_target = 0.8\n\n# tcp parameters\ntcp_keepalives_idle = 30\ntcp_keepalives_interval = 10\ntcp_keepalives_count = 3\n\n# logging parameters in default fep installation\n# if log volume is not defined, log_directory should be\n# changed to '/database/userdata/data/log'\nlog_directory = '/database/log'\nlog_filename = 'logfile-%a.log'\nlog_file_mode = 0600\nlog_truncate_on_rotation = on\nlog_rotation_age = 1d\nlog_rotation_size = 0\nlog_checkpoints = on\nlog_line_prefix = '%e %t [%p]: [%l-1] user=%u,db=%d,app=%a,client=%h'\nlog_lock_waits = on\nlog_autovacuum_min_duration = 60s\nlogging_collector = on\npgaudit.config_file='/opt/app-root/src/pgaudit-cfg/pgaudit.conf'\nlog_replication_commands = on\nlog_min_messages = WARNING\nlog_destination = stderr\n\n# wal_archive parameters in default fep installation\narchive_mode = on\narchive_command = 'pgbackrest --stanza=backupstanza --config=/database/userdata/pgbackrest.conf archive-push %p'\nwal_level = replica\nmax_wal_senders = 12\nwal_keep_segments = 64\n",
              "storage": {
                "archivewalVol": {
                  "size": "1Gi"
                },
                "backupVol": {
                  "size": "2Gi"
                },
                "dataVol": {
                  "size": "2Gi"
                },
                "logVol": {
                  "size": "1Gi"
                },
                "tablespaceVol": {
                  "size": "512Mi"
                },
                "walVol": {
                  "size": "1200Mi"
                }
              },
              "sysUsers": {
                "pgAdminPassword": "admin-password",
                "pgdb": "mydb",
                "pgpassword": "mydbpassword",
                "pgreplpassword": "repluserpwd",
                "pgrepluser": "repluser",
                "pguser": "mydbuser",
                "tdepassphrase": "tde-passphrase"
              },
              "systemCertificates": {
                "cacrt": "-----BEGIN CERTIFICATE-----\nMIIDTzCCAjegAwIBAgIUYssQ8I74US5g+1+Z7CHuaDgkZnEwDQYJKoZIhvcNAQEL\nBQAwNzEQMA4GA1UECgwHRnVqaXRzdTEjMCEGA1UEAwwaRkVQIFJvb3QgQ0EgZm9y\nIEt1YmVybmV0ZXMwHhcNMjEwMjA2MDM1MjI4WhcNMzEwMjA0MDM1MjI4WjA3MRAw\nDgYDVQQKDAdGdWppdHN1MSMwIQYDVQQDDBpGRVAgUm9vdCBDQSBmb3IgS3ViZXJu\nZXRlczCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBAMs97gUF0xkUzCgL\n7MiiDju9ySr/ziwjvcYU7jA9ML+SLmftMs3HtcYbAmSntqI+MDBSR/FAJTOoytuT\npV+mCFcGj2YAjDpliHPeNcUpbryy4YMChF3+MovkIwGCksxo5rhiWhGmoBYpA48P\n4Xe8SPlzqMzhFvNeKzyiUhvjutS2Y1Ss38lsTaurFPx64vQ2PaC54XzdwMptXtpb\ntYmWSzCpJWwxZ6lF3vitdA2w0tnBWNyctAd0+RIM/fvArxiIqseAux9t0uogm5to\nlRIhvekuxOpXBPEqtIYQ4j9XUW2JH8vUDnzPkPvjrq+A3Ug8OyyfGVrW7+VYXozu\nc4aP7P0CAwEAAaNTMFEwHQYDVR0OBBYEFBzCutQ7S74WEhS5V2sNEJBGyLpmMB8G\nA1UdIwQYMBaAFBzCutQ7S74WEhS5V2sNEJBGyLpmMA8GA1UdEwEB/wQFMAMBAf8w\nDQYJKoZIhvcNAQELBQADggEBAMDwD85RAaWEBptFgLzKw+9xEUy1vcZaonAuA1qc\nT342XTueyAugxkC11HwdCGgGS34VyctfMGqj4AW6pA2ez4tLrbOps4DmV4sw8uBL\n8pgRDgfly3ob9FEg2wa0hmrwX9jH5Bt4vySUE2785uPAqaspT2UNtTBxS85BUi1T\nsKId2Rtil6an281Z81wyWVI6Jm2D4MG0mbsiGcTPlCtdg/UljvDYymXlAvd4vNhl\nk9hDa13TgDqJKgKdTIcmZoNQdpEVgFcO0h9AEUy5AuLqxHq60dLfZ6ESGPlMI7Lm\ni4PzYbCnBmOe+7TnHcPSyrnehs66Ik+oifRd82eYS7vKjFw=\n-----END CERTIFICATE-----",
                "crt": "-----BEGIN CERTIFICATE-----\nMIIDUTCCAjmgAwIBAgIRAMocW3qMoHrD6qRvMPppMkMwDQYJKoZIhvcNAQELBQAw\nNzEQMA4GA1UECgwHRnVqaXRzdTEjMCEGA1UEAwwaRkVQIFJvb3QgQ0EgZm9yIEt1\nYmVybmV0ZXMwHhcNMjEwMjA2MDQzMjM2WhcNMjYwMjA1MDQzMjM2WjA/MRAwDgYD\nVQQKEwdGdWppdHN1MSswKQYDVQQDEyJGVUpJVFNVIEVudGVycHJpc2UgUG9zdGdy\nZXMgU2VydmVyMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEA4AI33yvH\nZws+jta6qpV6wzJqF8odIfTIpCfbrVcUUtLFKJ1I2e4SceTKi6O3C/I1XuvWlpng\n5IO65+fQQLO06z1/AuQT78YUn/Wlm9x1aHVsv4ANB5JWWqDOjrRT3o7nRPGXfila\nbP0rGE2mJJcVR9nExJ3IeaktgT3sb8YlXvtchyYpmjdbfxabTz07ig0+6/cwKoRR\nxOK8Uf7f5euE0cI/490J6r5Rs4lgD8sIQNCUFlTFYvmAH7gcdssSFBt8NPlUATHE\nsoFmlW0DKCJWNhTLOht+s6L/1zwTHLjPG2pdkG6Wdgmu5H2pDml8CDNLDv98Aj7i\n+I5SRKKcVPlnuQIDAQABo1AwTjAdBgNVHSUEFjAUBggrBgEFBQcDAQYIKwYBBQUH\nAwIwDAYDVR0TAQH/BAIwADAfBgNVHSMEGDAWgBQcwrrUO0u+FhIUuVdrDRCQRsi6\nZjANBgkqhkiG9w0BAQsFAAOCAQEAm5dxBoI9pScOCvRAchg4CprdRDSJb9K6yB3O\nnCAxnM47iHeXnY3WlnI388kHu8DU7O4ba1tJbGs3KY9KzioPk43pU12jWkO1onoF\n+mTDjx/Ef1cYWA9r5q/LtgTa6Q2sxV4O2x67QW82aAnaxO34dV5zWCPIvAoovZBV\nHRT+BgCg3r2vD1RGKK2nl1aYJtWhO1SZubam+VttdZ/vbM9oOJctxmImsEtBXjkY\nKteePdQtLL5o03JhyXWyRshCq+HMmKf2KgyY8gvydGcP4eLQdBWcW40LcnVq6UjT\n0kJycJEKngMVademq1ZWHGaiYB7hyT6GhgIcHUJ2cKrPgbEh1Q==\n-----END CERTIFICATE-----",
                "key": "-----BEGIN RSA PRIVATE KEY-----\nMIIEowIBAAKCAQEA4AI33yvHZws+jta6qpV6wzJqF8odIfTIpCfbrVcUUtLFKJ1I\n2e4SceTKi6O3C/I1XuvWlpng5IO65+fQQLO06z1/AuQT78YUn/Wlm9x1aHVsv4AN\nB5JWWqDOjrRT3o7nRPGXfilabP0rGE2mJJcVR9nExJ3IeaktgT3sb8YlXvtchyYp\nmjdbfxabTz07ig0+6/cwKoRRxOK8Uf7f5euE0cI/490J6r5Rs4lgD8sIQNCUFlTF\nYvmAH7gcdssSFBt8NPlUATHEsoFmlW0DKCJWNhTLOht+s6L/1zwTHLjPG2pdkG6W\ndgmu5H2pDml8CDNLDv98Aj7i+I5SRKKcVPlnuQIDAQABAoIBAFPQYKlOzw/+BA0b\nyMIUpdctIMb/54CR/xR0mVw1DbSjigNVPjHUQvB8Y1B2FAITQObgJO06bAv0QdWN\nRb0/v/yYiNJDFjaLjaIAHlO/2+oWrXbFaZqgpVDJhB+e1xaZr2x7XGxm+p925k30\nl6pvIRY+I8JRKvZiV1VZHwL/R3JOtPr++xMZtLVjVOI+f+ySqJ+TZHuAjm49EKxj\ncEmmJ28b7QcziXsvKy00f+zbqLIBKXQdZAFU5eEr1BsDRXdRW+Kf0XIvftuy4BJZ\nvoKT+VGhEvF/qysswL4+6IAO6tpuYnnM0Y2d3sOGoWPkTcQK0MekYKzL/WmtCjNs\n9hodJtECgYEA5EWyhEOf4uOKe5TDp697UCUvXLoOR58FDe/S8XNvScn29jjOkqIg\nOMoqo9xAkJTNTzqn5UUdt1x/pgM2NxlPLFijrc0zQlX3SoOO2ryDd9WNi7YKtN16\nKJqa536WeZu2OEbuAZ+S3GALVy1RPeTNPnUOmKnF06DjDUGzLNCZy10CgYEA+zfw\n952DWuz1U0Z4wvAEqqcgUKXPKrkTXV/iUnjkDkrLYVr0ZofDNTXrdHl+UedFmaOC\ncieZn6DNhcdz5tKtyysGMH3g/qs9PfoGUngvcXsy0Egk04l3x1jc8TTCLqXZXYaQ\nHMsx51n+R58oncPtzYSUOr9qQ6PbC2CstTbFJA0CgYEAjGEsUliAB/jknfEzjXjG\nPdhQUxb8VyE864Az2lah9t/kJzFyIAziAeqZ5GE7t247AGFTBRTHHI8e1Qoemi3P\nWbc9GVIbFs1lIYbcIDpUIyrKPEP8O5QEXtoNLxXTFgAjRGKiVY87spjCAJ+W2ZhO\ne/1it5GYXfgQCYQA2yuBmOUCgYANRkR2YR1axaCk+NlSu6oTdmdPu6M5x7PNQE7O\nOtMaKjua9lppvIzFGAdMDUtueoEEAE7ZR1xnwfB6PDLUpJdIYAqgr1YfPt8qkjaZ\nTv56yZ7CwL0pbF8m6nwqRrZoDp1wwraEvvvxFKFKGY/k3kCHlpTakdjEoDjn3gDi\nRnWeVQKBgCEneMSzucei5LRppRtRaJw/Btll8qlPMlX3W7dxQ3cLwpmLOn0m51Fp\nPIZ44zYK8R6fu4+/sSrlfaIg86Ugeufp6YNxyNROKxUGza5vDIu5OftwWtBeg+UK\nZ8lLWNdX6pp7WMujmF3H1DrkBbauYMUKZ4UxUYtelgHERMePIxwb\n-----END RSA PRIVATE KEY-----"
              }
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [
          "disconnected"
        ],
        "valid_subscription": []
      },
      "architectures": [
        "amd64",
        "s390x"
      ],
      "bundle_path": "registry.connect.redhat.com/fujitsu-postgres/fujitsu-enterprise-postgres-bundle@sha256:50004c77a8d75e30f232c0465979b3764ccde8df2fa601d617c03085b03e1571",
      "bundle_path_digest": "sha256:50004c77a8d75e30f232c0465979b3764ccde8df2fa601d617c03085b03e1571",
      "capabilities": [
        "Full Lifecycle"
      ],
      "channel_name": "stable",
      "creation_date": "2022-11-28T09:33:52.937000+00:00",
      "csv_description": "FUJITSU Enterprise Postgres 12 delivers an enterprise-grade PostgreSQL on OpenShift Container Platform.\n\nThis solution provides the flexibility of a hybrid cloud solution while delivering an enhanced distribution\nof PostgreSQL to support enterprise-level workloads and provide improved deployment and management,\navailability, performance, data governance and security.\n\nAvailable as a multi-architecture container built for both amd64 and s390x.\n\nThe download and Use of the Product is strictly subject to the terms of the End User License Agreement with Fujitsu Limited found at\nhttps://www.fast.fujitsu.com/fujitsu-enterprise-postgres-license-agreements\nWhere the Product that has been embedded as a whole or part into a third party program, only Authorised Customers may download and use the Product.\n",
      "csv_display_name": "FUJITSU Enterprise Postgres 12 Operator",
      "csv_metadata_description": "OpenShift Operator for Fujitsu Enterprise Postgres 12",
      "csv_name": "fep-ansible-operator.v2.2.12",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": true,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-11-28T09:33:52.937000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.9",
      "organization": "certified-operators",
      "package": "fep-ansible-operator",
      "provided_apis": [
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPCluster",
          "plural": "fepclusters",
          "version": "v2"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPConfig",
          "plural": "fepconfigs",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPCert",
          "plural": "fepcerts",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPVolume",
          "plural": "fepvolumes",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPUser",
          "plural": "fepusers",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPAction",
          "plural": "fepactions",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPBackup",
          "plural": "fepbackups",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPPgpool2",
          "plural": "feppgpool2s",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPPgpool2Cert",
          "plural": "feppgpool2certs",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPRestore",
          "plural": "feprestores",
          "version": "v1"
        }
      ],
      "provider": "Fujitsu",
      "related_images": [
        {
          "digest": "sha256:61592bcccccee2e72521d0640f005d1902730319ad697aa0f4fc5485bf22989d",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-12-server@sha256:61592bcccccee2e72521d0640f005d1902730319ad697aa0f4fc5485bf22989d",
          "name": "fep"
        },
        {
          "digest": "sha256:f126c128c1301afa684bdf3258c78d1ec68c08aa6e5c085c5f939940947e5c94",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-12-backup@sha256:f126c128c1301afa684bdf3258c78d1ec68c08aa6e5c085c5f939940947e5c94",
          "name": "backup"
        },
        {
          "digest": "sha256:240da72ba4ef71031c1587a1f44b7631697346bb7699be52dcebd64e9f93d66e",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-12-restore@sha256:240da72ba4ef71031c1587a1f44b7631697346bb7699be52dcebd64e9f93d66e",
          "name": "restore"
        },
        {
          "digest": "sha256:6d58e56e6e7ab0c07d6095b1cab429c47ea3d294bf1656bd0a7d6efed098cb02",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-12-pgpool2@sha256:6d58e56e6e7ab0c07d6095b1cab429c47ea3d294bf1656bd0a7d6efed098cb02",
          "name": "pgpool2"
        },
        {
          "digest": "sha256:37ee807c70476b71d4f3d1275d47b43e170e7023070259f80f798091be0b1d81",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-12-operator@sha256:37ee807c70476b71d4f3d1275d47b43e170e7023070259f80f798091be0b1d81",
          "name": "fujitsu-enterprise-postgres-12-operator-37ee807c70476b71d4f3d1275d47b43e170e7023070259f80f798091be0b1d81-annotation"
        },
        {
          "digest": "sha256:37ee807c70476b71d4f3d1275d47b43e170e7023070259f80f798091be0b1d81",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-12-operator@sha256:37ee807c70476b71d4f3d1275d47b43e170e7023070259f80f798091be0b1d81",
          "name": "fep-ansible-operator"
        }
      ],
      "replaces": null,
      "skip_range": "<2.2.12",
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.9",
      "version": "2.2.12",
      "version_original": "2.2.12"
    },
    {
      "_id": "6384817e3b5f5f6a53097339",
      "alm_examples": [
        {
          "api_version": "fep.fujitsu.io/v1",
          "kind": "FEPAction",
          "metadata": {
            "name": "new-fep-action"
          },
          "spec": {
            "fepAction": {
              "args": [
                "new-fep-sts-0",
                "new-fep-sts-1"
              ],
              "type": "reload"
            },
            "sysExtraLogging": false,
            "targetClusterName": "new-fep"
          }
        },
        {
          "api_version": "fep.fujitsu.io/v1",
          "kind": "FEPPgpool2",
          "metadata": {
            "name": "new-fep-pgpool2"
          },
          "spec": {
            "count": 2,
            "customhba": "local   all         all                               trust\nhost    all         all         127.0.0.1/32          trust\nhost    all         all         ::1/128               trust\n",
            "customlogsize": "128Mi",
            "customparams": "listen_addresses = '*'\npcp_listen_addresses = '*'\nnum_init_children = 32\nreserved_connections = 0\nenable_pool_hba = off\nallow_clear_text_frontend_auth = off\nauthentication_timeout = 80\nbackend_weight0 = 1\nbackend_weight1 = 1\nbackend_flag0 = 'ALWAYS_MASTER'\nbackend_flag1 = 'ALLOW_TO_FAILOVER'\nconnection_cache = on\nmax_pool = 4\nlisten_backlog_multiplier = 2\nserialize_accept = off\nchild_life_time = 300\nclient_idle_limit = 0\nchild_max_connections = 0\nconnection_life_time = 0\nreset_query_list = 'ABORT; DISCARD ALL'\nclient_min_messages = info\nlog_min_messages = debug1\nlog_statement = on\nlog_per_node_statement = on\nlog_client_messages = on\nlog_hostname = on\nlog_connections = on\nlog_line_prefix = '%t: pid %p: '\nload_balance_mode = on\nignore_leading_white_space = on\nwhite_function_list = ''\nblack_function_list = 'currval,lastval,nextval,setval'\nblack_query_pattern_list = ''\ndatabase_redirect_preference_list = ''\napp_name_redirect_preference_list = ''\nallow_sql_comments = off\ndisable_load_balance_on_write = 'transaction'\nstatement_level_load_balance = on\nconnect_timeout = 10000\nsr_check_period = 0\nsr_check_user = 'postgres'\ndelay_threshold = 0\nlog_standby_delay = 'none'\nssl = on\nssl_ciphers = 'HIGH:MEDIUM:+3DES:!aNULL'\nssl_prefer_server_ciphers = off\nssl_ecdh_curve = 'prime256v1'\nssl_dh_params_file = ''\nrelcache_expire = 0\nrelcache_size = 256\ncheck_temp_table = catalog\ncheck_unlogged_table = on\nenable_shared_relcache = on\nrelcache_query_target = master\nfailover_on_backend_error = off\n",
            "custompcp": "none",
            "customsslcacert": "none",
            "customsslcert": "-----BEGIN CERTIFICATE-----\nMIIDTzCCAjegAwIBAgIUYssQ8I74US5g+1+Z7CHuaDgkZnEwDQYJKoZIhvcNAQEL\nBQAwNzEQMA4GA1UECgwHRnVqaXRzdTEjMCEGA1UEAwwaRkVQIFJvb3QgQ0EgZm9y\nIEt1YmVybmV0ZXMwHhcNMjEwMjA2MDM1MjI4WhcNMzEwMjA0MDM1MjI4WjA3MRAw\nDgYDVQQKDAdGdWppdHN1MSMwIQYDVQQDDBpGRVAgUm9vdCBDQSBmb3IgS3ViZXJu\nZXRlczCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBAMs97gUF0xkUzCgL\n7MiiDju9ySr/ziwjvcYU7jA9ML+SLmftMs3HtcYbAmSntqI+MDBSR/FAJTOoytuT\npV+mCFcGj2YAjDpliHPeNcUpbryy4YMChF3+MovkIwGCksxo5rhiWhGmoBYpA48P\n4Xe8SPlzqMzhFvNeKzyiUhvjutS2Y1Ss38lsTaurFPx64vQ2PaC54XzdwMptXtpb\ntYmWSzCpJWwxZ6lF3vitdA2w0tnBWNyctAd0+RIM/fvArxiIqseAux9t0uogm5to\nlRIhvekuxOpXBPEqtIYQ4j9XUW2JH8vUDnzPkPvjrq+A3Ug8OyyfGVrW7+VYXozu\nc4aP7P0CAwEAAaNTMFEwHQYDVR0OBBYEFBzCutQ7S74WEhS5V2sNEJBGyLpmMB8G\nA1UdIwQYMBaAFBzCutQ7S74WEhS5V2sNEJBGyLpmMA8GA1UdEwEB/wQFMAMBAf8w\nDQYJKoZIhvcNAQELBQADggEBAMDwD85RAaWEBptFgLzKw+9xEUy1vcZaonAuA1qc\nT342XTueyAugxkC11HwdCGgGS34VyctfMGqj4AW6pA2ez4tLrbOps4DmV4sw8uBL\n8pgRDgfly3ob9FEg2wa0hmrwX9jH5Bt4vySUE2785uPAqaspT2UNtTBxS85BUi1T\nsKId2Rtil6an281Z81wyWVI6Jm2D4MG0mbsiGcTPlCtdg/UljvDYymXlAvd4vNhl\nk9hDa13TgDqJKgKdTIcmZoNQdpEVgFcO0h9AEUy5AuLqxHq60dLfZ6ESGPlMI7Lm\ni4PzYbCnBmOe+7TnHcPSyrnehs66Ik+oifRd82eYS7vKjFw=\n-----END CERTIFICATE-----",
            "customsslkey": "none",
            "fepclustername": "new-fep",
            "imagePullPolicy": "IfNotPresent",
            "limits": {
              "cpu": "400m",
              "memory": "512Mi"
            },
            "requests": {
              "cpu": "200m",
              "memory": "256Mi"
            },
            "serviceport": 9999,
            "statusport": 9898
          }
        },
        {
          "api_version": "fep.fujitsu.io/v1",
          "kind": "FEPRestore",
          "metadata": {
            "name": "new-fep-restore"
          },
          "spec": {
            "fromFEPcluster": "new-fep",
            "imagePullPolicy": "IfNotPresent",
            "mcSpec": {
              "limits": {
                "cpu": "200m",
                "memory": "300Mi"
              },
              "requests": {
                "cpu": "100m",
                "memory": "200Mi"
              }
            },
            "restoretype": "latest",
            "sysExtraLogging": false,
            "toFEPcluster": "new-fep-2"
          }
        },
        {
          "api_version": "fep.fujitsu.io/v2",
          "kind": "FEPCluster",
          "metadata": {
            "name": "new-fep"
          },
          "spec": {
            "fep": {
              "customAnnotations": {
                "allDeployments": {}
              },
              "forceSsl": true,
              "image": {
                "pullPolicy": "IfNotPresent"
              },
              "instances": 1,
              "mcSpec": {
                "limits": {
                  "cpu": "500m",
                  "memory": "700Mi"
                },
                "requests": {
                  "cpu": "200m",
                  "memory": "512Mi"
                }
              },
              "podAntiAffinity": false,
              "podDisruptionBudget": false,
              "servicePort": 27500,
              "syncMode": "off",
              "sysExtraLogging": false
            },
            "fepChildCrVal": {
              "backup": {
                "image": {
                  "pullPolicy": "IfNotPresent"
                },
                "mcSpec": {
                  "limits": {
                    "cpu": 0.2,
                    "memory": "300Mi"
                  },
                  "requests": {
                    "cpu": 0.1,
                    "memory": "200Mi"
                  }
                },
                "pgbackrestParams": "# if log volume is not defined, log_directory should be\n# changed to '/database/userdata/data/log'\n[global]\nrepo1-retention-full=7\nrepo1-retention-full-type=time\nlog-path=/database/log/backup\n",
                "postScript": " ",
                "preScript": " ",
                "schedule": {
                  "num": 2
                },
                "schedule1": {
                  "schedule": "15 0 * * 0",
                  "type": "full"
                },
                "schedule2": {
                  "schedule": "15 0 * * 1-6",
                  "type": "incr"
                },
                "schedule3": {
                  "schedule": " ",
                  "type": " "
                },
                "schedule4": {
                  "schedule": " ",
                  "type": " "
                },
                "schedule5": {
                  "schedule": " ",
                  "type": " "
                }
              },
              "customPgAudit": "# define pg audit custom params here to override defaults.\n# if log volume is not defined, log_directory should be\n# changed to '/database/userdata/data/log'\n[output]\nlog_truncate_on_rotation = on\nlog_filename = 'pgaudit-%a.log'\nlog_rotation_age = 1d\nlog_rotation_size = 0\n[rule]\n",
              "customPgHba": "# define pg_hba custom rules here to be merged with default rules.\n# TYPE     DATABASE        USER        ADDRESS        METHOD\n",
              "customPgParams": "# define custom postgresql.conf parameters below to override defaults.\n# Current values are as per default FEP deployment\nshared_preload_libraries='pgx_datamasking,pg_prewarm,pg_stat_statements'\nsession_preload_libraries='pg_prewarm'\nmax_prepared_transactions = 100\nmax_worker_processes = 20\nmax_connections = 100\nwork_mem = 1MB\nmaintenance_work_mem = 12MB\nshared_buffers = 128MB\neffective_cache_size = 384MB\ncheckpoint_completion_target = 0.8\n\n# tcp parameters\ntcp_keepalives_idle = 30\ntcp_keepalives_interval = 10\ntcp_keepalives_count = 3\n\n# logging parameters in default fep installation\n# if log volume is not defined, log_directory should be\n# changed to '/database/userdata/data/log'\nlog_directory = '/database/log'\nlog_filename = 'logfile-%a.log'\nlog_file_mode = 0600\nlog_truncate_on_rotation = on\nlog_rotation_age = 1d\nlog_rotation_size = 0\nlog_checkpoints = on\nlog_line_prefix = '%e %t [%p]: [%l-1] user=%u,db=%d,app=%a,client=%h'\nlog_lock_waits = on\nlog_autovacuum_min_duration = 60s\nlogging_collector = on\npgaudit.config_file='/opt/app-root/src/pgaudit-cfg/pgaudit.conf'\nlog_replication_commands = on\nlog_min_messages = WARNING\nlog_destination = stderr\n\n# wal_archive parameters in default fep installation\narchive_mode = on\narchive_command = 'pgbackrest --stanza=backupstanza --config=/database/userdata/pgbackrest.conf archive-push %p'\nwal_level = replica\nmax_wal_senders = 12\nwal_keep_segments = 64\n",
              "storage": {
                "archivewalVol": {
                  "size": "1Gi"
                },
                "backupVol": {
                  "size": "2Gi"
                },
                "dataVol": {
                  "size": "2Gi"
                },
                "logVol": {
                  "size": "1Gi"
                },
                "tablespaceVol": {
                  "size": "512Mi"
                },
                "walVol": {
                  "size": "1200Mi"
                }
              },
              "sysUsers": {
                "pgAdminPassword": "admin-password",
                "pgdb": "mydb",
                "pgpassword": "mydbpassword",
                "pgreplpassword": "repluserpwd",
                "pgrepluser": "repluser",
                "pguser": "mydbuser",
                "tdepassphrase": "tde-passphrase"
              },
              "systemCertificates": {
                "cacrt": "-----BEGIN CERTIFICATE-----\nMIIDTzCCAjegAwIBAgIUYssQ8I74US5g+1+Z7CHuaDgkZnEwDQYJKoZIhvcNAQEL\nBQAwNzEQMA4GA1UECgwHRnVqaXRzdTEjMCEGA1UEAwwaRkVQIFJvb3QgQ0EgZm9y\nIEt1YmVybmV0ZXMwHhcNMjEwMjA2MDM1MjI4WhcNMzEwMjA0MDM1MjI4WjA3MRAw\nDgYDVQQKDAdGdWppdHN1MSMwIQYDVQQDDBpGRVAgUm9vdCBDQSBmb3IgS3ViZXJu\nZXRlczCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBAMs97gUF0xkUzCgL\n7MiiDju9ySr/ziwjvcYU7jA9ML+SLmftMs3HtcYbAmSntqI+MDBSR/FAJTOoytuT\npV+mCFcGj2YAjDpliHPeNcUpbryy4YMChF3+MovkIwGCksxo5rhiWhGmoBYpA48P\n4Xe8SPlzqMzhFvNeKzyiUhvjutS2Y1Ss38lsTaurFPx64vQ2PaC54XzdwMptXtpb\ntYmWSzCpJWwxZ6lF3vitdA2w0tnBWNyctAd0+RIM/fvArxiIqseAux9t0uogm5to\nlRIhvekuxOpXBPEqtIYQ4j9XUW2JH8vUDnzPkPvjrq+A3Ug8OyyfGVrW7+VYXozu\nc4aP7P0CAwEAAaNTMFEwHQYDVR0OBBYEFBzCutQ7S74WEhS5V2sNEJBGyLpmMB8G\nA1UdIwQYMBaAFBzCutQ7S74WEhS5V2sNEJBGyLpmMA8GA1UdEwEB/wQFMAMBAf8w\nDQYJKoZIhvcNAQELBQADggEBAMDwD85RAaWEBptFgLzKw+9xEUy1vcZaonAuA1qc\nT342XTueyAugxkC11HwdCGgGS34VyctfMGqj4AW6pA2ez4tLrbOps4DmV4sw8uBL\n8pgRDgfly3ob9FEg2wa0hmrwX9jH5Bt4vySUE2785uPAqaspT2UNtTBxS85BUi1T\nsKId2Rtil6an281Z81wyWVI6Jm2D4MG0mbsiGcTPlCtdg/UljvDYymXlAvd4vNhl\nk9hDa13TgDqJKgKdTIcmZoNQdpEVgFcO0h9AEUy5AuLqxHq60dLfZ6ESGPlMI7Lm\ni4PzYbCnBmOe+7TnHcPSyrnehs66Ik+oifRd82eYS7vKjFw=\n-----END CERTIFICATE-----",
                "crt": "-----BEGIN CERTIFICATE-----\nMIIDUTCCAjmgAwIBAgIRAMocW3qMoHrD6qRvMPppMkMwDQYJKoZIhvcNAQELBQAw\nNzEQMA4GA1UECgwHRnVqaXRzdTEjMCEGA1UEAwwaRkVQIFJvb3QgQ0EgZm9yIEt1\nYmVybmV0ZXMwHhcNMjEwMjA2MDQzMjM2WhcNMjYwMjA1MDQzMjM2WjA/MRAwDgYD\nVQQKEwdGdWppdHN1MSswKQYDVQQDEyJGVUpJVFNVIEVudGVycHJpc2UgUG9zdGdy\nZXMgU2VydmVyMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEA4AI33yvH\nZws+jta6qpV6wzJqF8odIfTIpCfbrVcUUtLFKJ1I2e4SceTKi6O3C/I1XuvWlpng\n5IO65+fQQLO06z1/AuQT78YUn/Wlm9x1aHVsv4ANB5JWWqDOjrRT3o7nRPGXfila\nbP0rGE2mJJcVR9nExJ3IeaktgT3sb8YlXvtchyYpmjdbfxabTz07ig0+6/cwKoRR\nxOK8Uf7f5euE0cI/490J6r5Rs4lgD8sIQNCUFlTFYvmAH7gcdssSFBt8NPlUATHE\nsoFmlW0DKCJWNhTLOht+s6L/1zwTHLjPG2pdkG6Wdgmu5H2pDml8CDNLDv98Aj7i\n+I5SRKKcVPlnuQIDAQABo1AwTjAdBgNVHSUEFjAUBggrBgEFBQcDAQYIKwYBBQUH\nAwIwDAYDVR0TAQH/BAIwADAfBgNVHSMEGDAWgBQcwrrUO0u+FhIUuVdrDRCQRsi6\nZjANBgkqhkiG9w0BAQsFAAOCAQEAm5dxBoI9pScOCvRAchg4CprdRDSJb9K6yB3O\nnCAxnM47iHeXnY3WlnI388kHu8DU7O4ba1tJbGs3KY9KzioPk43pU12jWkO1onoF\n+mTDjx/Ef1cYWA9r5q/LtgTa6Q2sxV4O2x67QW82aAnaxO34dV5zWCPIvAoovZBV\nHRT+BgCg3r2vD1RGKK2nl1aYJtWhO1SZubam+VttdZ/vbM9oOJctxmImsEtBXjkY\nKteePdQtLL5o03JhyXWyRshCq+HMmKf2KgyY8gvydGcP4eLQdBWcW40LcnVq6UjT\n0kJycJEKngMVademq1ZWHGaiYB7hyT6GhgIcHUJ2cKrPgbEh1Q==\n-----END CERTIFICATE-----",
                "key": "-----BEGIN RSA PRIVATE KEY-----\nMIIEowIBAAKCAQEA4AI33yvHZws+jta6qpV6wzJqF8odIfTIpCfbrVcUUtLFKJ1I\n2e4SceTKi6O3C/I1XuvWlpng5IO65+fQQLO06z1/AuQT78YUn/Wlm9x1aHVsv4AN\nB5JWWqDOjrRT3o7nRPGXfilabP0rGE2mJJcVR9nExJ3IeaktgT3sb8YlXvtchyYp\nmjdbfxabTz07ig0+6/cwKoRRxOK8Uf7f5euE0cI/490J6r5Rs4lgD8sIQNCUFlTF\nYvmAH7gcdssSFBt8NPlUATHEsoFmlW0DKCJWNhTLOht+s6L/1zwTHLjPG2pdkG6W\ndgmu5H2pDml8CDNLDv98Aj7i+I5SRKKcVPlnuQIDAQABAoIBAFPQYKlOzw/+BA0b\nyMIUpdctIMb/54CR/xR0mVw1DbSjigNVPjHUQvB8Y1B2FAITQObgJO06bAv0QdWN\nRb0/v/yYiNJDFjaLjaIAHlO/2+oWrXbFaZqgpVDJhB+e1xaZr2x7XGxm+p925k30\nl6pvIRY+I8JRKvZiV1VZHwL/R3JOtPr++xMZtLVjVOI+f+ySqJ+TZHuAjm49EKxj\ncEmmJ28b7QcziXsvKy00f+zbqLIBKXQdZAFU5eEr1BsDRXdRW+Kf0XIvftuy4BJZ\nvoKT+VGhEvF/qysswL4+6IAO6tpuYnnM0Y2d3sOGoWPkTcQK0MekYKzL/WmtCjNs\n9hodJtECgYEA5EWyhEOf4uOKe5TDp697UCUvXLoOR58FDe/S8XNvScn29jjOkqIg\nOMoqo9xAkJTNTzqn5UUdt1x/pgM2NxlPLFijrc0zQlX3SoOO2ryDd9WNi7YKtN16\nKJqa536WeZu2OEbuAZ+S3GALVy1RPeTNPnUOmKnF06DjDUGzLNCZy10CgYEA+zfw\n952DWuz1U0Z4wvAEqqcgUKXPKrkTXV/iUnjkDkrLYVr0ZofDNTXrdHl+UedFmaOC\ncieZn6DNhcdz5tKtyysGMH3g/qs9PfoGUngvcXsy0Egk04l3x1jc8TTCLqXZXYaQ\nHMsx51n+R58oncPtzYSUOr9qQ6PbC2CstTbFJA0CgYEAjGEsUliAB/jknfEzjXjG\nPdhQUxb8VyE864Az2lah9t/kJzFyIAziAeqZ5GE7t247AGFTBRTHHI8e1Qoemi3P\nWbc9GVIbFs1lIYbcIDpUIyrKPEP8O5QEXtoNLxXTFgAjRGKiVY87spjCAJ+W2ZhO\ne/1it5GYXfgQCYQA2yuBmOUCgYANRkR2YR1axaCk+NlSu6oTdmdPu6M5x7PNQE7O\nOtMaKjua9lppvIzFGAdMDUtueoEEAE7ZR1xnwfB6PDLUpJdIYAqgr1YfPt8qkjaZ\nTv56yZ7CwL0pbF8m6nwqRrZoDp1wwraEvvvxFKFKGY/k3kCHlpTakdjEoDjn3gDi\nRnWeVQKBgCEneMSzucei5LRppRtRaJw/Btll8qlPMlX3W7dxQ3cLwpmLOn0m51Fp\nPIZ44zYK8R6fu4+/sSrlfaIg86Ugeufp6YNxyNROKxUGza5vDIu5OftwWtBeg+UK\nZ8lLWNdX6pp7WMujmF3H1DrkBbauYMUKZ4UxUYtelgHERMePIxwb\n-----END RSA PRIVATE KEY-----"
              }
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [
          "disconnected"
        ],
        "valid_subscription": []
      },
      "architectures": [
        "amd64",
        "s390x"
      ],
      "bundle_path": "registry.connect.redhat.com/fujitsu-postgres/fujitsu-enterprise-postgres-bundle@sha256:50004c77a8d75e30f232c0465979b3764ccde8df2fa601d617c03085b03e1571",
      "bundle_path_digest": "sha256:50004c77a8d75e30f232c0465979b3764ccde8df2fa601d617c03085b03e1571",
      "capabilities": [
        "Full Lifecycle"
      ],
      "channel_name": "stable",
      "creation_date": "2022-11-28T09:38:06.413000+00:00",
      "csv_description": "FUJITSU Enterprise Postgres 12 delivers an enterprise-grade PostgreSQL on OpenShift Container Platform.\n\nThis solution provides the flexibility of a hybrid cloud solution while delivering an enhanced distribution\nof PostgreSQL to support enterprise-level workloads and provide improved deployment and management,\navailability, performance, data governance and security.\n\nAvailable as a multi-architecture container built for both amd64 and s390x.\n\nThe download and Use of the Product is strictly subject to the terms of the End User License Agreement with Fujitsu Limited found at\nhttps://www.fast.fujitsu.com/fujitsu-enterprise-postgres-license-agreements\nWhere the Product that has been embedded as a whole or part into a third party program, only Authorised Customers may download and use the Product.\n",
      "csv_display_name": "FUJITSU Enterprise Postgres 12 Operator",
      "csv_metadata_description": "OpenShift Operator for Fujitsu Enterprise Postgres 12",
      "csv_name": "fep-ansible-operator.v2.2.12",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": true,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-11-28T09:38:06.413000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.11",
      "organization": "certified-operators",
      "package": "fep-ansible-operator",
      "provided_apis": [
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPAction",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPBackup",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPCert",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPCluster",
          "version": "v2"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPConfig",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPPgpool2",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPPgpool2Cert",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPRestore",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPUser",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPVolume",
          "version": "v1"
        }
      ],
      "provider": "Fujitsu",
      "related_images": [
        {
          "digest": "sha256:61592bcccccee2e72521d0640f005d1902730319ad697aa0f4fc5485bf22989d",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-12-server@sha256:61592bcccccee2e72521d0640f005d1902730319ad697aa0f4fc5485bf22989d",
          "name": "fep"
        },
        {
          "digest": "sha256:f126c128c1301afa684bdf3258c78d1ec68c08aa6e5c085c5f939940947e5c94",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-12-backup@sha256:f126c128c1301afa684bdf3258c78d1ec68c08aa6e5c085c5f939940947e5c94",
          "name": "backup"
        },
        {
          "digest": "sha256:240da72ba4ef71031c1587a1f44b7631697346bb7699be52dcebd64e9f93d66e",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-12-restore@sha256:240da72ba4ef71031c1587a1f44b7631697346bb7699be52dcebd64e9f93d66e",
          "name": "restore"
        },
        {
          "digest": "sha256:6d58e56e6e7ab0c07d6095b1cab429c47ea3d294bf1656bd0a7d6efed098cb02",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-12-pgpool2@sha256:6d58e56e6e7ab0c07d6095b1cab429c47ea3d294bf1656bd0a7d6efed098cb02",
          "name": "pgpool2"
        },
        {
          "digest": "sha256:37ee807c70476b71d4f3d1275d47b43e170e7023070259f80f798091be0b1d81",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-12-operator@sha256:37ee807c70476b71d4f3d1275d47b43e170e7023070259f80f798091be0b1d81",
          "name": "fujitsu-enterprise-postgres-12-operator-37ee807c70476b71d4f3d1275d47b43e170e7023070259f80f798091be0b1d81-annotation"
        },
        {
          "digest": "sha256:37ee807c70476b71d4f3d1275d47b43e170e7023070259f80f798091be0b1d81",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-12-operator@sha256:37ee807c70476b71d4f3d1275d47b43e170e7023070259f80f798091be0b1d81",
          "name": "fep-ansible-operator"
        }
      ],
      "replaces": null,
      "skip_range": "<2.2.12",
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.11",
      "version": "2.2.12",
      "version_original": "2.2.12"
    },
    {
      "_id": "6384818abb5a79c41aa61ba5",
      "alm_examples": [
        {
          "api_version": "fep.fujitsu.io/v1",
          "kind": "FEPAction",
          "metadata": {
            "name": "new-fep-action"
          },
          "spec": {
            "fepAction": {
              "args": [
                "new-fep-sts-0",
                "new-fep-sts-1"
              ],
              "type": "reload"
            },
            "sysExtraLogging": false,
            "targetClusterName": "new-fep"
          }
        },
        {
          "api_version": "fep.fujitsu.io/v1",
          "kind": "FEPPgpool2",
          "metadata": {
            "name": "new-fep-pgpool2"
          },
          "spec": {
            "count": 2,
            "customhba": "local   all         all                               trust\nhost    all         all         127.0.0.1/32          trust\nhost    all         all         ::1/128               trust\n",
            "customlogsize": "128Mi",
            "customparams": "listen_addresses = '*'\npcp_listen_addresses = '*'\nnum_init_children = 32\nreserved_connections = 0\nenable_pool_hba = off\nallow_clear_text_frontend_auth = off\nauthentication_timeout = 80\nbackend_weight0 = 1\nbackend_weight1 = 1\nbackend_flag0 = 'ALWAYS_MASTER'\nbackend_flag1 = 'ALLOW_TO_FAILOVER'\nconnection_cache = on\nmax_pool = 4\nlisten_backlog_multiplier = 2\nserialize_accept = off\nchild_life_time = 300\nclient_idle_limit = 0\nchild_max_connections = 0\nconnection_life_time = 0\nreset_query_list = 'ABORT; DISCARD ALL'\nclient_min_messages = info\nlog_min_messages = debug1\nlog_statement = on\nlog_per_node_statement = on\nlog_client_messages = on\nlog_hostname = on\nlog_connections = on\nlog_line_prefix = '%t: pid %p: '\nload_balance_mode = on\nignore_leading_white_space = on\nwhite_function_list = ''\nblack_function_list = 'currval,lastval,nextval,setval'\nblack_query_pattern_list = ''\ndatabase_redirect_preference_list = ''\napp_name_redirect_preference_list = ''\nallow_sql_comments = off\ndisable_load_balance_on_write = 'transaction'\nstatement_level_load_balance = on\nconnect_timeout = 10000\nsr_check_period = 0\nsr_check_user = 'postgres'\ndelay_threshold = 0\nlog_standby_delay = 'none'\nssl = on\nssl_ciphers = 'HIGH:MEDIUM:+3DES:!aNULL'\nssl_prefer_server_ciphers = off\nssl_ecdh_curve = 'prime256v1'\nssl_dh_params_file = ''\nrelcache_expire = 0\nrelcache_size = 256\ncheck_temp_table = catalog\ncheck_unlogged_table = on\nenable_shared_relcache = on\nrelcache_query_target = master\nfailover_on_backend_error = off\n",
            "custompcp": "none",
            "customsslcacert": "none",
            "customsslcert": "-----BEGIN CERTIFICATE-----\nMIIDTzCCAjegAwIBAgIUYssQ8I74US5g+1+Z7CHuaDgkZnEwDQYJKoZIhvcNAQEL\nBQAwNzEQMA4GA1UECgwHRnVqaXRzdTEjMCEGA1UEAwwaRkVQIFJvb3QgQ0EgZm9y\nIEt1YmVybmV0ZXMwHhcNMjEwMjA2MDM1MjI4WhcNMzEwMjA0MDM1MjI4WjA3MRAw\nDgYDVQQKDAdGdWppdHN1MSMwIQYDVQQDDBpGRVAgUm9vdCBDQSBmb3IgS3ViZXJu\nZXRlczCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBAMs97gUF0xkUzCgL\n7MiiDju9ySr/ziwjvcYU7jA9ML+SLmftMs3HtcYbAmSntqI+MDBSR/FAJTOoytuT\npV+mCFcGj2YAjDpliHPeNcUpbryy4YMChF3+MovkIwGCksxo5rhiWhGmoBYpA48P\n4Xe8SPlzqMzhFvNeKzyiUhvjutS2Y1Ss38lsTaurFPx64vQ2PaC54XzdwMptXtpb\ntYmWSzCpJWwxZ6lF3vitdA2w0tnBWNyctAd0+RIM/fvArxiIqseAux9t0uogm5to\nlRIhvekuxOpXBPEqtIYQ4j9XUW2JH8vUDnzPkPvjrq+A3Ug8OyyfGVrW7+VYXozu\nc4aP7P0CAwEAAaNTMFEwHQYDVR0OBBYEFBzCutQ7S74WEhS5V2sNEJBGyLpmMB8G\nA1UdIwQYMBaAFBzCutQ7S74WEhS5V2sNEJBGyLpmMA8GA1UdEwEB/wQFMAMBAf8w\nDQYJKoZIhvcNAQELBQADggEBAMDwD85RAaWEBptFgLzKw+9xEUy1vcZaonAuA1qc\nT342XTueyAugxkC11HwdCGgGS34VyctfMGqj4AW6pA2ez4tLrbOps4DmV4sw8uBL\n8pgRDgfly3ob9FEg2wa0hmrwX9jH5Bt4vySUE2785uPAqaspT2UNtTBxS85BUi1T\nsKId2Rtil6an281Z81wyWVI6Jm2D4MG0mbsiGcTPlCtdg/UljvDYymXlAvd4vNhl\nk9hDa13TgDqJKgKdTIcmZoNQdpEVgFcO0h9AEUy5AuLqxHq60dLfZ6ESGPlMI7Lm\ni4PzYbCnBmOe+7TnHcPSyrnehs66Ik+oifRd82eYS7vKjFw=\n-----END CERTIFICATE-----",
            "customsslkey": "none",
            "fepclustername": "new-fep",
            "imagePullPolicy": "IfNotPresent",
            "limits": {
              "cpu": "400m",
              "memory": "512Mi"
            },
            "requests": {
              "cpu": "200m",
              "memory": "256Mi"
            },
            "serviceport": 9999,
            "statusport": 9898
          }
        },
        {
          "api_version": "fep.fujitsu.io/v1",
          "kind": "FEPRestore",
          "metadata": {
            "name": "new-fep-restore"
          },
          "spec": {
            "fromFEPcluster": "new-fep",
            "imagePullPolicy": "IfNotPresent",
            "mcSpec": {
              "limits": {
                "cpu": "200m",
                "memory": "300Mi"
              },
              "requests": {
                "cpu": "100m",
                "memory": "200Mi"
              }
            },
            "restoretype": "latest",
            "sysExtraLogging": false,
            "toFEPcluster": "new-fep-2"
          }
        },
        {
          "api_version": "fep.fujitsu.io/v2",
          "kind": "FEPCluster",
          "metadata": {
            "name": "new-fep"
          },
          "spec": {
            "fep": {
              "customAnnotations": {
                "allDeployments": {}
              },
              "forceSsl": true,
              "image": {
                "pullPolicy": "IfNotPresent"
              },
              "instances": 1,
              "mcSpec": {
                "limits": {
                  "cpu": "500m",
                  "memory": "700Mi"
                },
                "requests": {
                  "cpu": "200m",
                  "memory": "512Mi"
                }
              },
              "podAntiAffinity": false,
              "podDisruptionBudget": false,
              "servicePort": 27500,
              "syncMode": "off",
              "sysExtraLogging": false
            },
            "fepChildCrVal": {
              "backup": {
                "image": {
                  "pullPolicy": "IfNotPresent"
                },
                "mcSpec": {
                  "limits": {
                    "cpu": 0.2,
                    "memory": "300Mi"
                  },
                  "requests": {
                    "cpu": 0.1,
                    "memory": "200Mi"
                  }
                },
                "pgbackrestParams": "# if log volume is not defined, log_directory should be\n# changed to '/database/userdata/data/log'\n[global]\nrepo1-retention-full=7\nrepo1-retention-full-type=time\nlog-path=/database/log/backup\n",
                "postScript": " ",
                "preScript": " ",
                "schedule": {
                  "num": 2
                },
                "schedule1": {
                  "schedule": "15 0 * * 0",
                  "type": "full"
                },
                "schedule2": {
                  "schedule": "15 0 * * 1-6",
                  "type": "incr"
                },
                "schedule3": {
                  "schedule": " ",
                  "type": " "
                },
                "schedule4": {
                  "schedule": " ",
                  "type": " "
                },
                "schedule5": {
                  "schedule": " ",
                  "type": " "
                }
              },
              "customPgAudit": "# define pg audit custom params here to override defaults.\n# if log volume is not defined, log_directory should be\n# changed to '/database/userdata/data/log'\n[output]\nlog_truncate_on_rotation = on\nlog_filename = 'pgaudit-%a.log'\nlog_rotation_age = 1d\nlog_rotation_size = 0\n[rule]\n",
              "customPgHba": "# define pg_hba custom rules here to be merged with default rules.\n# TYPE     DATABASE        USER        ADDRESS        METHOD\n",
              "customPgParams": "# define custom postgresql.conf parameters below to override defaults.\n# Current values are as per default FEP deployment\nshared_preload_libraries='pgx_datamasking,pg_prewarm,pg_stat_statements'\nsession_preload_libraries='pg_prewarm'\nmax_prepared_transactions = 100\nmax_worker_processes = 20\nmax_connections = 100\nwork_mem = 1MB\nmaintenance_work_mem = 12MB\nshared_buffers = 128MB\neffective_cache_size = 384MB\ncheckpoint_completion_target = 0.8\n\n# tcp parameters\ntcp_keepalives_idle = 30\ntcp_keepalives_interval = 10\ntcp_keepalives_count = 3\n\n# logging parameters in default fep installation\n# if log volume is not defined, log_directory should be\n# changed to '/database/userdata/data/log'\nlog_directory = '/database/log'\nlog_filename = 'logfile-%a.log'\nlog_file_mode = 0600\nlog_truncate_on_rotation = on\nlog_rotation_age = 1d\nlog_rotation_size = 0\nlog_checkpoints = on\nlog_line_prefix = '%e %t [%p]: [%l-1] user=%u,db=%d,app=%a,client=%h'\nlog_lock_waits = on\nlog_autovacuum_min_duration = 60s\nlogging_collector = on\npgaudit.config_file='/opt/app-root/src/pgaudit-cfg/pgaudit.conf'\nlog_replication_commands = on\nlog_min_messages = WARNING\nlog_destination = stderr\n\n# wal_archive parameters in default fep installation\narchive_mode = on\narchive_command = 'pgbackrest --stanza=backupstanza --config=/database/userdata/pgbackrest.conf archive-push %p'\nwal_level = replica\nmax_wal_senders = 12\nwal_keep_segments = 64\n",
              "storage": {
                "archivewalVol": {
                  "size": "1Gi"
                },
                "backupVol": {
                  "size": "2Gi"
                },
                "dataVol": {
                  "size": "2Gi"
                },
                "logVol": {
                  "size": "1Gi"
                },
                "tablespaceVol": {
                  "size": "512Mi"
                },
                "walVol": {
                  "size": "1200Mi"
                }
              },
              "sysUsers": {
                "pgAdminPassword": "admin-password",
                "pgdb": "mydb",
                "pgpassword": "mydbpassword",
                "pgreplpassword": "repluserpwd",
                "pgrepluser": "repluser",
                "pguser": "mydbuser",
                "tdepassphrase": "tde-passphrase"
              },
              "systemCertificates": {
                "cacrt": "-----BEGIN CERTIFICATE-----\nMIIDTzCCAjegAwIBAgIUYssQ8I74US5g+1+Z7CHuaDgkZnEwDQYJKoZIhvcNAQEL\nBQAwNzEQMA4GA1UECgwHRnVqaXRzdTEjMCEGA1UEAwwaRkVQIFJvb3QgQ0EgZm9y\nIEt1YmVybmV0ZXMwHhcNMjEwMjA2MDM1MjI4WhcNMzEwMjA0MDM1MjI4WjA3MRAw\nDgYDVQQKDAdGdWppdHN1MSMwIQYDVQQDDBpGRVAgUm9vdCBDQSBmb3IgS3ViZXJu\nZXRlczCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBAMs97gUF0xkUzCgL\n7MiiDju9ySr/ziwjvcYU7jA9ML+SLmftMs3HtcYbAmSntqI+MDBSR/FAJTOoytuT\npV+mCFcGj2YAjDpliHPeNcUpbryy4YMChF3+MovkIwGCksxo5rhiWhGmoBYpA48P\n4Xe8SPlzqMzhFvNeKzyiUhvjutS2Y1Ss38lsTaurFPx64vQ2PaC54XzdwMptXtpb\ntYmWSzCpJWwxZ6lF3vitdA2w0tnBWNyctAd0+RIM/fvArxiIqseAux9t0uogm5to\nlRIhvekuxOpXBPEqtIYQ4j9XUW2JH8vUDnzPkPvjrq+A3Ug8OyyfGVrW7+VYXozu\nc4aP7P0CAwEAAaNTMFEwHQYDVR0OBBYEFBzCutQ7S74WEhS5V2sNEJBGyLpmMB8G\nA1UdIwQYMBaAFBzCutQ7S74WEhS5V2sNEJBGyLpmMA8GA1UdEwEB/wQFMAMBAf8w\nDQYJKoZIhvcNAQELBQADggEBAMDwD85RAaWEBptFgLzKw+9xEUy1vcZaonAuA1qc\nT342XTueyAugxkC11HwdCGgGS34VyctfMGqj4AW6pA2ez4tLrbOps4DmV4sw8uBL\n8pgRDgfly3ob9FEg2wa0hmrwX9jH5Bt4vySUE2785uPAqaspT2UNtTBxS85BUi1T\nsKId2Rtil6an281Z81wyWVI6Jm2D4MG0mbsiGcTPlCtdg/UljvDYymXlAvd4vNhl\nk9hDa13TgDqJKgKdTIcmZoNQdpEVgFcO0h9AEUy5AuLqxHq60dLfZ6ESGPlMI7Lm\ni4PzYbCnBmOe+7TnHcPSyrnehs66Ik+oifRd82eYS7vKjFw=\n-----END CERTIFICATE-----",
                "crt": "-----BEGIN CERTIFICATE-----\nMIIDUTCCAjmgAwIBAgIRAMocW3qMoHrD6qRvMPppMkMwDQYJKoZIhvcNAQELBQAw\nNzEQMA4GA1UECgwHRnVqaXRzdTEjMCEGA1UEAwwaRkVQIFJvb3QgQ0EgZm9yIEt1\nYmVybmV0ZXMwHhcNMjEwMjA2MDQzMjM2WhcNMjYwMjA1MDQzMjM2WjA/MRAwDgYD\nVQQKEwdGdWppdHN1MSswKQYDVQQDEyJGVUpJVFNVIEVudGVycHJpc2UgUG9zdGdy\nZXMgU2VydmVyMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEA4AI33yvH\nZws+jta6qpV6wzJqF8odIfTIpCfbrVcUUtLFKJ1I2e4SceTKi6O3C/I1XuvWlpng\n5IO65+fQQLO06z1/AuQT78YUn/Wlm9x1aHVsv4ANB5JWWqDOjrRT3o7nRPGXfila\nbP0rGE2mJJcVR9nExJ3IeaktgT3sb8YlXvtchyYpmjdbfxabTz07ig0+6/cwKoRR\nxOK8Uf7f5euE0cI/490J6r5Rs4lgD8sIQNCUFlTFYvmAH7gcdssSFBt8NPlUATHE\nsoFmlW0DKCJWNhTLOht+s6L/1zwTHLjPG2pdkG6Wdgmu5H2pDml8CDNLDv98Aj7i\n+I5SRKKcVPlnuQIDAQABo1AwTjAdBgNVHSUEFjAUBggrBgEFBQcDAQYIKwYBBQUH\nAwIwDAYDVR0TAQH/BAIwADAfBgNVHSMEGDAWgBQcwrrUO0u+FhIUuVdrDRCQRsi6\nZjANBgkqhkiG9w0BAQsFAAOCAQEAm5dxBoI9pScOCvRAchg4CprdRDSJb9K6yB3O\nnCAxnM47iHeXnY3WlnI388kHu8DU7O4ba1tJbGs3KY9KzioPk43pU12jWkO1onoF\n+mTDjx/Ef1cYWA9r5q/LtgTa6Q2sxV4O2x67QW82aAnaxO34dV5zWCPIvAoovZBV\nHRT+BgCg3r2vD1RGKK2nl1aYJtWhO1SZubam+VttdZ/vbM9oOJctxmImsEtBXjkY\nKteePdQtLL5o03JhyXWyRshCq+HMmKf2KgyY8gvydGcP4eLQdBWcW40LcnVq6UjT\n0kJycJEKngMVademq1ZWHGaiYB7hyT6GhgIcHUJ2cKrPgbEh1Q==\n-----END CERTIFICATE-----",
                "key": "-----BEGIN RSA PRIVATE KEY-----\nMIIEowIBAAKCAQEA4AI33yvHZws+jta6qpV6wzJqF8odIfTIpCfbrVcUUtLFKJ1I\n2e4SceTKi6O3C/I1XuvWlpng5IO65+fQQLO06z1/AuQT78YUn/Wlm9x1aHVsv4AN\nB5JWWqDOjrRT3o7nRPGXfilabP0rGE2mJJcVR9nExJ3IeaktgT3sb8YlXvtchyYp\nmjdbfxabTz07ig0+6/cwKoRRxOK8Uf7f5euE0cI/490J6r5Rs4lgD8sIQNCUFlTF\nYvmAH7gcdssSFBt8NPlUATHEsoFmlW0DKCJWNhTLOht+s6L/1zwTHLjPG2pdkG6W\ndgmu5H2pDml8CDNLDv98Aj7i+I5SRKKcVPlnuQIDAQABAoIBAFPQYKlOzw/+BA0b\nyMIUpdctIMb/54CR/xR0mVw1DbSjigNVPjHUQvB8Y1B2FAITQObgJO06bAv0QdWN\nRb0/v/yYiNJDFjaLjaIAHlO/2+oWrXbFaZqgpVDJhB+e1xaZr2x7XGxm+p925k30\nl6pvIRY+I8JRKvZiV1VZHwL/R3JOtPr++xMZtLVjVOI+f+ySqJ+TZHuAjm49EKxj\ncEmmJ28b7QcziXsvKy00f+zbqLIBKXQdZAFU5eEr1BsDRXdRW+Kf0XIvftuy4BJZ\nvoKT+VGhEvF/qysswL4+6IAO6tpuYnnM0Y2d3sOGoWPkTcQK0MekYKzL/WmtCjNs\n9hodJtECgYEA5EWyhEOf4uOKe5TDp697UCUvXLoOR58FDe/S8XNvScn29jjOkqIg\nOMoqo9xAkJTNTzqn5UUdt1x/pgM2NxlPLFijrc0zQlX3SoOO2ryDd9WNi7YKtN16\nKJqa536WeZu2OEbuAZ+S3GALVy1RPeTNPnUOmKnF06DjDUGzLNCZy10CgYEA+zfw\n952DWuz1U0Z4wvAEqqcgUKXPKrkTXV/iUnjkDkrLYVr0ZofDNTXrdHl+UedFmaOC\ncieZn6DNhcdz5tKtyysGMH3g/qs9PfoGUngvcXsy0Egk04l3x1jc8TTCLqXZXYaQ\nHMsx51n+R58oncPtzYSUOr9qQ6PbC2CstTbFJA0CgYEAjGEsUliAB/jknfEzjXjG\nPdhQUxb8VyE864Az2lah9t/kJzFyIAziAeqZ5GE7t247AGFTBRTHHI8e1Qoemi3P\nWbc9GVIbFs1lIYbcIDpUIyrKPEP8O5QEXtoNLxXTFgAjRGKiVY87spjCAJ+W2ZhO\ne/1it5GYXfgQCYQA2yuBmOUCgYANRkR2YR1axaCk+NlSu6oTdmdPu6M5x7PNQE7O\nOtMaKjua9lppvIzFGAdMDUtueoEEAE7ZR1xnwfB6PDLUpJdIYAqgr1YfPt8qkjaZ\nTv56yZ7CwL0pbF8m6nwqRrZoDp1wwraEvvvxFKFKGY/k3kCHlpTakdjEoDjn3gDi\nRnWeVQKBgCEneMSzucei5LRppRtRaJw/Btll8qlPMlX3W7dxQ3cLwpmLOn0m51Fp\nPIZ44zYK8R6fu4+/sSrlfaIg86Ugeufp6YNxyNROKxUGza5vDIu5OftwWtBeg+UK\nZ8lLWNdX6pp7WMujmF3H1DrkBbauYMUKZ4UxUYtelgHERMePIxwb\n-----END RSA PRIVATE KEY-----"
              }
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [
          "disconnected"
        ],
        "valid_subscription": []
      },
      "architectures": [
        "amd64",
        "s390x"
      ],
      "bundle_path": "registry.connect.redhat.com/fujitsu-postgres/fujitsu-enterprise-postgres-bundle@sha256:50004c77a8d75e30f232c0465979b3764ccde8df2fa601d617c03085b03e1571",
      "bundle_path_digest": "sha256:50004c77a8d75e30f232c0465979b3764ccde8df2fa601d617c03085b03e1571",
      "capabilities": [
        "Full Lifecycle"
      ],
      "channel_name": "stable",
      "creation_date": "2022-11-28T09:38:18.298000+00:00",
      "csv_description": "FUJITSU Enterprise Postgres 12 delivers an enterprise-grade PostgreSQL on OpenShift Container Platform.\n\nThis solution provides the flexibility of a hybrid cloud solution while delivering an enhanced distribution\nof PostgreSQL to support enterprise-level workloads and provide improved deployment and management,\navailability, performance, data governance and security.\n\nAvailable as a multi-architecture container built for both amd64 and s390x.\n\nThe download and Use of the Product is strictly subject to the terms of the End User License Agreement with Fujitsu Limited found at\nhttps://www.fast.fujitsu.com/fujitsu-enterprise-postgres-license-agreements\nWhere the Product that has been embedded as a whole or part into a third party program, only Authorised Customers may download and use the Product.\n",
      "csv_display_name": "FUJITSU Enterprise Postgres 12 Operator",
      "csv_metadata_description": "OpenShift Operator for Fujitsu Enterprise Postgres 12",
      "csv_name": "fep-ansible-operator.v2.2.12",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": true,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-11-28T09:38:18.298000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.12",
      "organization": "certified-operators",
      "package": "fep-ansible-operator",
      "provided_apis": [
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPAction",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPBackup",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPCert",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPCluster",
          "version": "v2"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPConfig",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPPgpool2",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPPgpool2Cert",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPRestore",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPUser",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPVolume",
          "version": "v1"
        }
      ],
      "provider": "Fujitsu",
      "related_images": [
        {
          "digest": "sha256:61592bcccccee2e72521d0640f005d1902730319ad697aa0f4fc5485bf22989d",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-12-server@sha256:61592bcccccee2e72521d0640f005d1902730319ad697aa0f4fc5485bf22989d",
          "name": "fep"
        },
        {
          "digest": "sha256:f126c128c1301afa684bdf3258c78d1ec68c08aa6e5c085c5f939940947e5c94",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-12-backup@sha256:f126c128c1301afa684bdf3258c78d1ec68c08aa6e5c085c5f939940947e5c94",
          "name": "backup"
        },
        {
          "digest": "sha256:240da72ba4ef71031c1587a1f44b7631697346bb7699be52dcebd64e9f93d66e",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-12-restore@sha256:240da72ba4ef71031c1587a1f44b7631697346bb7699be52dcebd64e9f93d66e",
          "name": "restore"
        },
        {
          "digest": "sha256:6d58e56e6e7ab0c07d6095b1cab429c47ea3d294bf1656bd0a7d6efed098cb02",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-12-pgpool2@sha256:6d58e56e6e7ab0c07d6095b1cab429c47ea3d294bf1656bd0a7d6efed098cb02",
          "name": "pgpool2"
        },
        {
          "digest": "sha256:37ee807c70476b71d4f3d1275d47b43e170e7023070259f80f798091be0b1d81",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-12-operator@sha256:37ee807c70476b71d4f3d1275d47b43e170e7023070259f80f798091be0b1d81",
          "name": "fujitsu-enterprise-postgres-12-operator-37ee807c70476b71d4f3d1275d47b43e170e7023070259f80f798091be0b1d81-annotation"
        },
        {
          "digest": "sha256:37ee807c70476b71d4f3d1275d47b43e170e7023070259f80f798091be0b1d81",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-12-operator@sha256:37ee807c70476b71d4f3d1275d47b43e170e7023070259f80f798091be0b1d81",
          "name": "fep-ansible-operator"
        }
      ],
      "replaces": null,
      "skip_range": "<2.2.12",
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.12",
      "version": "2.2.12",
      "version_original": "2.2.12"
    },
    {
      "_id": "6384818ad1516305d2079e02",
      "alm_examples": [
        {
          "api_version": "fep.fujitsu.io/v1",
          "kind": "FEPAction",
          "metadata": {
            "name": "new-fep-action"
          },
          "spec": {
            "fepAction": {
              "args": [
                "new-fep-sts-0",
                "new-fep-sts-1"
              ],
              "type": "reload"
            },
            "sysExtraLogging": false,
            "targetClusterName": "new-fep"
          }
        },
        {
          "api_version": "fep.fujitsu.io/v1",
          "kind": "FEPPgpool2",
          "metadata": {
            "name": "new-fep-pgpool2"
          },
          "spec": {
            "count": 2,
            "customhba": "local   all         all                               trust\nhost    all         all         127.0.0.1/32          trust\nhost    all         all         ::1/128               trust\n",
            "customlogsize": "128Mi",
            "customparams": "listen_addresses = '*'\npcp_listen_addresses = '*'\nnum_init_children = 32\nreserved_connections = 0\nenable_pool_hba = off\nallow_clear_text_frontend_auth = off\nauthentication_timeout = 80\nbackend_weight0 = 1\nbackend_weight1 = 1\nbackend_flag0 = 'ALWAYS_MASTER'\nbackend_flag1 = 'ALLOW_TO_FAILOVER'\nconnection_cache = on\nmax_pool = 4\nlisten_backlog_multiplier = 2\nserialize_accept = off\nchild_life_time = 300\nclient_idle_limit = 0\nchild_max_connections = 0\nconnection_life_time = 0\nreset_query_list = 'ABORT; DISCARD ALL'\nclient_min_messages = info\nlog_min_messages = debug1\nlog_statement = on\nlog_per_node_statement = on\nlog_client_messages = on\nlog_hostname = on\nlog_connections = on\nlog_line_prefix = '%t: pid %p: '\nload_balance_mode = on\nignore_leading_white_space = on\nwhite_function_list = ''\nblack_function_list = 'currval,lastval,nextval,setval'\nblack_query_pattern_list = ''\ndatabase_redirect_preference_list = ''\napp_name_redirect_preference_list = ''\nallow_sql_comments = off\ndisable_load_balance_on_write = 'transaction'\nstatement_level_load_balance = on\nconnect_timeout = 10000\nsr_check_period = 0\nsr_check_user = 'postgres'\ndelay_threshold = 0\nlog_standby_delay = 'none'\nssl = on\nssl_ciphers = 'HIGH:MEDIUM:+3DES:!aNULL'\nssl_prefer_server_ciphers = off\nssl_ecdh_curve = 'prime256v1'\nssl_dh_params_file = ''\nrelcache_expire = 0\nrelcache_size = 256\ncheck_temp_table = catalog\ncheck_unlogged_table = on\nenable_shared_relcache = on\nrelcache_query_target = master\nfailover_on_backend_error = off\n",
            "custompcp": "none",
            "customsslcacert": "none",
            "customsslcert": "-----BEGIN CERTIFICATE-----\nMIIDTzCCAjegAwIBAgIUYssQ8I74US5g+1+Z7CHuaDgkZnEwDQYJKoZIhvcNAQEL\nBQAwNzEQMA4GA1UECgwHRnVqaXRzdTEjMCEGA1UEAwwaRkVQIFJvb3QgQ0EgZm9y\nIEt1YmVybmV0ZXMwHhcNMjEwMjA2MDM1MjI4WhcNMzEwMjA0MDM1MjI4WjA3MRAw\nDgYDVQQKDAdGdWppdHN1MSMwIQYDVQQDDBpGRVAgUm9vdCBDQSBmb3IgS3ViZXJu\nZXRlczCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBAMs97gUF0xkUzCgL\n7MiiDju9ySr/ziwjvcYU7jA9ML+SLmftMs3HtcYbAmSntqI+MDBSR/FAJTOoytuT\npV+mCFcGj2YAjDpliHPeNcUpbryy4YMChF3+MovkIwGCksxo5rhiWhGmoBYpA48P\n4Xe8SPlzqMzhFvNeKzyiUhvjutS2Y1Ss38lsTaurFPx64vQ2PaC54XzdwMptXtpb\ntYmWSzCpJWwxZ6lF3vitdA2w0tnBWNyctAd0+RIM/fvArxiIqseAux9t0uogm5to\nlRIhvekuxOpXBPEqtIYQ4j9XUW2JH8vUDnzPkPvjrq+A3Ug8OyyfGVrW7+VYXozu\nc4aP7P0CAwEAAaNTMFEwHQYDVR0OBBYEFBzCutQ7S74WEhS5V2sNEJBGyLpmMB8G\nA1UdIwQYMBaAFBzCutQ7S74WEhS5V2sNEJBGyLpmMA8GA1UdEwEB/wQFMAMBAf8w\nDQYJKoZIhvcNAQELBQADggEBAMDwD85RAaWEBptFgLzKw+9xEUy1vcZaonAuA1qc\nT342XTueyAugxkC11HwdCGgGS34VyctfMGqj4AW6pA2ez4tLrbOps4DmV4sw8uBL\n8pgRDgfly3ob9FEg2wa0hmrwX9jH5Bt4vySUE2785uPAqaspT2UNtTBxS85BUi1T\nsKId2Rtil6an281Z81wyWVI6Jm2D4MG0mbsiGcTPlCtdg/UljvDYymXlAvd4vNhl\nk9hDa13TgDqJKgKdTIcmZoNQdpEVgFcO0h9AEUy5AuLqxHq60dLfZ6ESGPlMI7Lm\ni4PzYbCnBmOe+7TnHcPSyrnehs66Ik+oifRd82eYS7vKjFw=\n-----END CERTIFICATE-----",
            "customsslkey": "none",
            "fepclustername": "new-fep",
            "imagePullPolicy": "IfNotPresent",
            "limits": {
              "cpu": "400m",
              "memory": "512Mi"
            },
            "requests": {
              "cpu": "200m",
              "memory": "256Mi"
            },
            "serviceport": 9999,
            "statusport": 9898
          }
        },
        {
          "api_version": "fep.fujitsu.io/v1",
          "kind": "FEPRestore",
          "metadata": {
            "name": "new-fep-restore"
          },
          "spec": {
            "fromFEPcluster": "new-fep",
            "imagePullPolicy": "IfNotPresent",
            "mcSpec": {
              "limits": {
                "cpu": "200m",
                "memory": "300Mi"
              },
              "requests": {
                "cpu": "100m",
                "memory": "200Mi"
              }
            },
            "restoretype": "latest",
            "sysExtraLogging": false,
            "toFEPcluster": "new-fep-2"
          }
        },
        {
          "api_version": "fep.fujitsu.io/v2",
          "kind": "FEPCluster",
          "metadata": {
            "name": "new-fep"
          },
          "spec": {
            "fep": {
              "customAnnotations": {
                "allDeployments": {}
              },
              "forceSsl": true,
              "image": {
                "pullPolicy": "IfNotPresent"
              },
              "instances": 1,
              "mcSpec": {
                "limits": {
                  "cpu": "500m",
                  "memory": "700Mi"
                },
                "requests": {
                  "cpu": "200m",
                  "memory": "512Mi"
                }
              },
              "podAntiAffinity": false,
              "podDisruptionBudget": false,
              "servicePort": 27500,
              "syncMode": "off",
              "sysExtraLogging": false
            },
            "fepChildCrVal": {
              "backup": {
                "image": {
                  "pullPolicy": "IfNotPresent"
                },
                "mcSpec": {
                  "limits": {
                    "cpu": 0.2,
                    "memory": "300Mi"
                  },
                  "requests": {
                    "cpu": 0.1,
                    "memory": "200Mi"
                  }
                },
                "pgbackrestParams": "# if log volume is not defined, log_directory should be\n# changed to '/database/userdata/data/log'\n[global]\nrepo1-retention-full=7\nrepo1-retention-full-type=time\nlog-path=/database/log/backup\n",
                "postScript": " ",
                "preScript": " ",
                "schedule": {
                  "num": 2
                },
                "schedule1": {
                  "schedule": "15 0 * * 0",
                  "type": "full"
                },
                "schedule2": {
                  "schedule": "15 0 * * 1-6",
                  "type": "incr"
                },
                "schedule3": {
                  "schedule": " ",
                  "type": " "
                },
                "schedule4": {
                  "schedule": " ",
                  "type": " "
                },
                "schedule5": {
                  "schedule": " ",
                  "type": " "
                }
              },
              "customPgAudit": "# define pg audit custom params here to override defaults.\n# if log volume is not defined, log_directory should be\n# changed to '/database/userdata/data/log'\n[output]\nlog_truncate_on_rotation = on\nlog_filename = 'pgaudit-%a.log'\nlog_rotation_age = 1d\nlog_rotation_size = 0\n[rule]\n",
              "customPgHba": "# define pg_hba custom rules here to be merged with default rules.\n# TYPE     DATABASE        USER        ADDRESS        METHOD\n",
              "customPgParams": "# define custom postgresql.conf parameters below to override defaults.\n# Current values are as per default FEP deployment\nshared_preload_libraries='pgx_datamasking,pg_prewarm,pg_stat_statements'\nsession_preload_libraries='pg_prewarm'\nmax_prepared_transactions = 100\nmax_worker_processes = 20\nmax_connections = 100\nwork_mem = 1MB\nmaintenance_work_mem = 12MB\nshared_buffers = 128MB\neffective_cache_size = 384MB\ncheckpoint_completion_target = 0.8\n\n# tcp parameters\ntcp_keepalives_idle = 30\ntcp_keepalives_interval = 10\ntcp_keepalives_count = 3\n\n# logging parameters in default fep installation\n# if log volume is not defined, log_directory should be\n# changed to '/database/userdata/data/log'\nlog_directory = '/database/log'\nlog_filename = 'logfile-%a.log'\nlog_file_mode = 0600\nlog_truncate_on_rotation = on\nlog_rotation_age = 1d\nlog_rotation_size = 0\nlog_checkpoints = on\nlog_line_prefix = '%e %t [%p]: [%l-1] user=%u,db=%d,app=%a,client=%h'\nlog_lock_waits = on\nlog_autovacuum_min_duration = 60s\nlogging_collector = on\npgaudit.config_file='/opt/app-root/src/pgaudit-cfg/pgaudit.conf'\nlog_replication_commands = on\nlog_min_messages = WARNING\nlog_destination = stderr\n\n# wal_archive parameters in default fep installation\narchive_mode = on\narchive_command = 'pgbackrest --stanza=backupstanza --config=/database/userdata/pgbackrest.conf archive-push %p'\nwal_level = replica\nmax_wal_senders = 12\nwal_keep_segments = 64\n",
              "storage": {
                "archivewalVol": {
                  "size": "1Gi"
                },
                "backupVol": {
                  "size": "2Gi"
                },
                "dataVol": {
                  "size": "2Gi"
                },
                "logVol": {
                  "size": "1Gi"
                },
                "tablespaceVol": {
                  "size": "512Mi"
                },
                "walVol": {
                  "size": "1200Mi"
                }
              },
              "sysUsers": {
                "pgAdminPassword": "admin-password",
                "pgdb": "mydb",
                "pgpassword": "mydbpassword",
                "pgreplpassword": "repluserpwd",
                "pgrepluser": "repluser",
                "pguser": "mydbuser",
                "tdepassphrase": "tde-passphrase"
              },
              "systemCertificates": {
                "cacrt": "-----BEGIN CERTIFICATE-----\nMIIDTzCCAjegAwIBAgIUYssQ8I74US5g+1+Z7CHuaDgkZnEwDQYJKoZIhvcNAQEL\nBQAwNzEQMA4GA1UECgwHRnVqaXRzdTEjMCEGA1UEAwwaRkVQIFJvb3QgQ0EgZm9y\nIEt1YmVybmV0ZXMwHhcNMjEwMjA2MDM1MjI4WhcNMzEwMjA0MDM1MjI4WjA3MRAw\nDgYDVQQKDAdGdWppdHN1MSMwIQYDVQQDDBpGRVAgUm9vdCBDQSBmb3IgS3ViZXJu\nZXRlczCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBAMs97gUF0xkUzCgL\n7MiiDju9ySr/ziwjvcYU7jA9ML+SLmftMs3HtcYbAmSntqI+MDBSR/FAJTOoytuT\npV+mCFcGj2YAjDpliHPeNcUpbryy4YMChF3+MovkIwGCksxo5rhiWhGmoBYpA48P\n4Xe8SPlzqMzhFvNeKzyiUhvjutS2Y1Ss38lsTaurFPx64vQ2PaC54XzdwMptXtpb\ntYmWSzCpJWwxZ6lF3vitdA2w0tnBWNyctAd0+RIM/fvArxiIqseAux9t0uogm5to\nlRIhvekuxOpXBPEqtIYQ4j9XUW2JH8vUDnzPkPvjrq+A3Ug8OyyfGVrW7+VYXozu\nc4aP7P0CAwEAAaNTMFEwHQYDVR0OBBYEFBzCutQ7S74WEhS5V2sNEJBGyLpmMB8G\nA1UdIwQYMBaAFBzCutQ7S74WEhS5V2sNEJBGyLpmMA8GA1UdEwEB/wQFMAMBAf8w\nDQYJKoZIhvcNAQELBQADggEBAMDwD85RAaWEBptFgLzKw+9xEUy1vcZaonAuA1qc\nT342XTueyAugxkC11HwdCGgGS34VyctfMGqj4AW6pA2ez4tLrbOps4DmV4sw8uBL\n8pgRDgfly3ob9FEg2wa0hmrwX9jH5Bt4vySUE2785uPAqaspT2UNtTBxS85BUi1T\nsKId2Rtil6an281Z81wyWVI6Jm2D4MG0mbsiGcTPlCtdg/UljvDYymXlAvd4vNhl\nk9hDa13TgDqJKgKdTIcmZoNQdpEVgFcO0h9AEUy5AuLqxHq60dLfZ6ESGPlMI7Lm\ni4PzYbCnBmOe+7TnHcPSyrnehs66Ik+oifRd82eYS7vKjFw=\n-----END CERTIFICATE-----",
                "crt": "-----BEGIN CERTIFICATE-----\nMIIDUTCCAjmgAwIBAgIRAMocW3qMoHrD6qRvMPppMkMwDQYJKoZIhvcNAQELBQAw\nNzEQMA4GA1UECgwHRnVqaXRzdTEjMCEGA1UEAwwaRkVQIFJvb3QgQ0EgZm9yIEt1\nYmVybmV0ZXMwHhcNMjEwMjA2MDQzMjM2WhcNMjYwMjA1MDQzMjM2WjA/MRAwDgYD\nVQQKEwdGdWppdHN1MSswKQYDVQQDEyJGVUpJVFNVIEVudGVycHJpc2UgUG9zdGdy\nZXMgU2VydmVyMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEA4AI33yvH\nZws+jta6qpV6wzJqF8odIfTIpCfbrVcUUtLFKJ1I2e4SceTKi6O3C/I1XuvWlpng\n5IO65+fQQLO06z1/AuQT78YUn/Wlm9x1aHVsv4ANB5JWWqDOjrRT3o7nRPGXfila\nbP0rGE2mJJcVR9nExJ3IeaktgT3sb8YlXvtchyYpmjdbfxabTz07ig0+6/cwKoRR\nxOK8Uf7f5euE0cI/490J6r5Rs4lgD8sIQNCUFlTFYvmAH7gcdssSFBt8NPlUATHE\nsoFmlW0DKCJWNhTLOht+s6L/1zwTHLjPG2pdkG6Wdgmu5H2pDml8CDNLDv98Aj7i\n+I5SRKKcVPlnuQIDAQABo1AwTjAdBgNVHSUEFjAUBggrBgEFBQcDAQYIKwYBBQUH\nAwIwDAYDVR0TAQH/BAIwADAfBgNVHSMEGDAWgBQcwrrUO0u+FhIUuVdrDRCQRsi6\nZjANBgkqhkiG9w0BAQsFAAOCAQEAm5dxBoI9pScOCvRAchg4CprdRDSJb9K6yB3O\nnCAxnM47iHeXnY3WlnI388kHu8DU7O4ba1tJbGs3KY9KzioPk43pU12jWkO1onoF\n+mTDjx/Ef1cYWA9r5q/LtgTa6Q2sxV4O2x67QW82aAnaxO34dV5zWCPIvAoovZBV\nHRT+BgCg3r2vD1RGKK2nl1aYJtWhO1SZubam+VttdZ/vbM9oOJctxmImsEtBXjkY\nKteePdQtLL5o03JhyXWyRshCq+HMmKf2KgyY8gvydGcP4eLQdBWcW40LcnVq6UjT\n0kJycJEKngMVademq1ZWHGaiYB7hyT6GhgIcHUJ2cKrPgbEh1Q==\n-----END CERTIFICATE-----",
                "key": "-----BEGIN RSA PRIVATE KEY-----\nMIIEowIBAAKCAQEA4AI33yvHZws+jta6qpV6wzJqF8odIfTIpCfbrVcUUtLFKJ1I\n2e4SceTKi6O3C/I1XuvWlpng5IO65+fQQLO06z1/AuQT78YUn/Wlm9x1aHVsv4AN\nB5JWWqDOjrRT3o7nRPGXfilabP0rGE2mJJcVR9nExJ3IeaktgT3sb8YlXvtchyYp\nmjdbfxabTz07ig0+6/cwKoRRxOK8Uf7f5euE0cI/490J6r5Rs4lgD8sIQNCUFlTF\nYvmAH7gcdssSFBt8NPlUATHEsoFmlW0DKCJWNhTLOht+s6L/1zwTHLjPG2pdkG6W\ndgmu5H2pDml8CDNLDv98Aj7i+I5SRKKcVPlnuQIDAQABAoIBAFPQYKlOzw/+BA0b\nyMIUpdctIMb/54CR/xR0mVw1DbSjigNVPjHUQvB8Y1B2FAITQObgJO06bAv0QdWN\nRb0/v/yYiNJDFjaLjaIAHlO/2+oWrXbFaZqgpVDJhB+e1xaZr2x7XGxm+p925k30\nl6pvIRY+I8JRKvZiV1VZHwL/R3JOtPr++xMZtLVjVOI+f+ySqJ+TZHuAjm49EKxj\ncEmmJ28b7QcziXsvKy00f+zbqLIBKXQdZAFU5eEr1BsDRXdRW+Kf0XIvftuy4BJZ\nvoKT+VGhEvF/qysswL4+6IAO6tpuYnnM0Y2d3sOGoWPkTcQK0MekYKzL/WmtCjNs\n9hodJtECgYEA5EWyhEOf4uOKe5TDp697UCUvXLoOR58FDe/S8XNvScn29jjOkqIg\nOMoqo9xAkJTNTzqn5UUdt1x/pgM2NxlPLFijrc0zQlX3SoOO2ryDd9WNi7YKtN16\nKJqa536WeZu2OEbuAZ+S3GALVy1RPeTNPnUOmKnF06DjDUGzLNCZy10CgYEA+zfw\n952DWuz1U0Z4wvAEqqcgUKXPKrkTXV/iUnjkDkrLYVr0ZofDNTXrdHl+UedFmaOC\ncieZn6DNhcdz5tKtyysGMH3g/qs9PfoGUngvcXsy0Egk04l3x1jc8TTCLqXZXYaQ\nHMsx51n+R58oncPtzYSUOr9qQ6PbC2CstTbFJA0CgYEAjGEsUliAB/jknfEzjXjG\nPdhQUxb8VyE864Az2lah9t/kJzFyIAziAeqZ5GE7t247AGFTBRTHHI8e1Qoemi3P\nWbc9GVIbFs1lIYbcIDpUIyrKPEP8O5QEXtoNLxXTFgAjRGKiVY87spjCAJ+W2ZhO\ne/1it5GYXfgQCYQA2yuBmOUCgYANRkR2YR1axaCk+NlSu6oTdmdPu6M5x7PNQE7O\nOtMaKjua9lppvIzFGAdMDUtueoEEAE7ZR1xnwfB6PDLUpJdIYAqgr1YfPt8qkjaZ\nTv56yZ7CwL0pbF8m6nwqRrZoDp1wwraEvvvxFKFKGY/k3kCHlpTakdjEoDjn3gDi\nRnWeVQKBgCEneMSzucei5LRppRtRaJw/Btll8qlPMlX3W7dxQ3cLwpmLOn0m51Fp\nPIZ44zYK8R6fu4+/sSrlfaIg86Ugeufp6YNxyNROKxUGza5vDIu5OftwWtBeg+UK\nZ8lLWNdX6pp7WMujmF3H1DrkBbauYMUKZ4UxUYtelgHERMePIxwb\n-----END RSA PRIVATE KEY-----"
              }
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [
          "disconnected"
        ],
        "valid_subscription": []
      },
      "architectures": [
        "amd64",
        "s390x"
      ],
      "bundle_path": "registry.connect.redhat.com/fujitsu-postgres/fujitsu-enterprise-postgres-bundle@sha256:50004c77a8d75e30f232c0465979b3764ccde8df2fa601d617c03085b03e1571",
      "bundle_path_digest": "sha256:50004c77a8d75e30f232c0465979b3764ccde8df2fa601d617c03085b03e1571",
      "capabilities": [
        "Full Lifecycle"
      ],
      "channel_name": "stable",
      "creation_date": "2022-11-28T09:38:18.733000+00:00",
      "csv_description": "FUJITSU Enterprise Postgres 12 delivers an enterprise-grade PostgreSQL on OpenShift Container Platform.\n\nThis solution provides the flexibility of a hybrid cloud solution while delivering an enhanced distribution\nof PostgreSQL to support enterprise-level workloads and provide improved deployment and management,\navailability, performance, data governance and security.\n\nAvailable as a multi-architecture container built for both amd64 and s390x.\n\nThe download and Use of the Product is strictly subject to the terms of the End User License Agreement with Fujitsu Limited found at\nhttps://www.fast.fujitsu.com/fujitsu-enterprise-postgres-license-agreements\nWhere the Product that has been embedded as a whole or part into a third party program, only Authorised Customers may download and use the Product.\n",
      "csv_display_name": "FUJITSU Enterprise Postgres 12 Operator",
      "csv_metadata_description": "OpenShift Operator for Fujitsu Enterprise Postgres 12",
      "csv_name": "fep-ansible-operator.v2.2.12",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": true,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-11-28T09:38:18.733000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.10",
      "organization": "certified-operators",
      "package": "fep-ansible-operator",
      "provided_apis": [
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPRestore",
          "plural": "feprestores",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPConfig",
          "plural": "fepconfigs",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPVolume",
          "plural": "fepvolumes",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPCert",
          "plural": "fepcerts",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPAction",
          "plural": "fepactions",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPBackup",
          "plural": "fepbackups",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPUser",
          "plural": "fepusers",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPCluster",
          "plural": "fepclusters",
          "version": "v2"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPPgpool2",
          "plural": "feppgpool2s",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPPgpool2Cert",
          "plural": "feppgpool2certs",
          "version": "v1"
        }
      ],
      "provider": "Fujitsu",
      "related_images": [
        {
          "digest": "sha256:61592bcccccee2e72521d0640f005d1902730319ad697aa0f4fc5485bf22989d",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-12-server@sha256:61592bcccccee2e72521d0640f005d1902730319ad697aa0f4fc5485bf22989d",
          "name": "fep"
        },
        {
          "digest": "sha256:f126c128c1301afa684bdf3258c78d1ec68c08aa6e5c085c5f939940947e5c94",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-12-backup@sha256:f126c128c1301afa684bdf3258c78d1ec68c08aa6e5c085c5f939940947e5c94",
          "name": "backup"
        },
        {
          "digest": "sha256:240da72ba4ef71031c1587a1f44b7631697346bb7699be52dcebd64e9f93d66e",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-12-restore@sha256:240da72ba4ef71031c1587a1f44b7631697346bb7699be52dcebd64e9f93d66e",
          "name": "restore"
        },
        {
          "digest": "sha256:6d58e56e6e7ab0c07d6095b1cab429c47ea3d294bf1656bd0a7d6efed098cb02",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-12-pgpool2@sha256:6d58e56e6e7ab0c07d6095b1cab429c47ea3d294bf1656bd0a7d6efed098cb02",
          "name": "pgpool2"
        },
        {
          "digest": "sha256:37ee807c70476b71d4f3d1275d47b43e170e7023070259f80f798091be0b1d81",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-12-operator@sha256:37ee807c70476b71d4f3d1275d47b43e170e7023070259f80f798091be0b1d81",
          "name": "fujitsu-enterprise-postgres-12-operator-37ee807c70476b71d4f3d1275d47b43e170e7023070259f80f798091be0b1d81-annotation"
        },
        {
          "digest": "sha256:37ee807c70476b71d4f3d1275d47b43e170e7023070259f80f798091be0b1d81",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-12-operator@sha256:37ee807c70476b71d4f3d1275d47b43e170e7023070259f80f798091be0b1d81",
          "name": "fep-ansible-operator"
        }
      ],
      "replaces": null,
      "skip_range": "<2.2.12",
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.10",
      "version": "2.2.12",
      "version_original": "2.2.12"
    },
    {
      "_id": "6384820bb59d1c1edb22ae6c",
      "alm_examples": [
        {
          "api_version": "fep.fujitsu.io/v1",
          "kind": "FEPAction",
          "metadata": {
            "name": "new-fep-action"
          },
          "spec": {
            "fepAction": {
              "args": [
                "new-fep-sts-0",
                "new-fep-sts-1"
              ],
              "type": "reload"
            },
            "sysExtraLogging": false,
            "targetClusterName": "new-fep"
          }
        },
        {
          "api_version": "fep.fujitsu.io/v1",
          "kind": "FEPPgpool2",
          "metadata": {
            "name": "new-fep-pgpool2"
          },
          "spec": {
            "count": 2,
            "customhba": "local   all         all                               trust\nhost    all         all         127.0.0.1/32          trust\nhost    all         all         ::1/128               trust\n",
            "customlogsize": "128Mi",
            "customparams": "listen_addresses = '*'\npcp_listen_addresses = '*'\nnum_init_children = 32\nreserved_connections = 0\nenable_pool_hba = off\nallow_clear_text_frontend_auth = off\nauthentication_timeout = 80\nbackend_weight0 = 1\nbackend_weight1 = 1\nbackend_flag0 = 'ALWAYS_MASTER'\nbackend_flag1 = 'ALLOW_TO_FAILOVER'\nconnection_cache = on\nmax_pool = 4\nlisten_backlog_multiplier = 2\nserialize_accept = off\nchild_life_time = 300\nclient_idle_limit = 0\nchild_max_connections = 0\nconnection_life_time = 0\nreset_query_list = 'ABORT; DISCARD ALL'\nclient_min_messages = info\nlog_min_messages = debug1\nlog_statement = on\nlog_per_node_statement = on\nlog_client_messages = on\nlog_hostname = on\nlog_connections = on\nlog_line_prefix = '%t: pid %p: '\nload_balance_mode = on\nignore_leading_white_space = on\nwhite_function_list = ''\nblack_function_list = 'currval,lastval,nextval,setval'\nblack_query_pattern_list = ''\ndatabase_redirect_preference_list = ''\napp_name_redirect_preference_list = ''\nallow_sql_comments = off\ndisable_load_balance_on_write = 'transaction'\nstatement_level_load_balance = on\nconnect_timeout = 10000\nsr_check_period = 0\nsr_check_user = 'postgres'\ndelay_threshold = 0\nlog_standby_delay = 'none'\nssl = on\nssl_ciphers = 'HIGH:MEDIUM:+3DES:!aNULL'\nssl_prefer_server_ciphers = off\nssl_ecdh_curve = 'prime256v1'\nssl_dh_params_file = ''\nrelcache_expire = 0\nrelcache_size = 256\ncheck_temp_table = catalog\ncheck_unlogged_table = on\nenable_shared_relcache = on\nrelcache_query_target = master\nfailover_on_backend_error = off\n",
            "custompcp": "none",
            "customsslcacert": "none",
            "customsslcert": "-----BEGIN CERTIFICATE-----\nMIIDTzCCAjegAwIBAgIUYssQ8I74US5g+1+Z7CHuaDgkZnEwDQYJKoZIhvcNAQEL\nBQAwNzEQMA4GA1UECgwHRnVqaXRzdTEjMCEGA1UEAwwaRkVQIFJvb3QgQ0EgZm9y\nIEt1YmVybmV0ZXMwHhcNMjEwMjA2MDM1MjI4WhcNMzEwMjA0MDM1MjI4WjA3MRAw\nDgYDVQQKDAdGdWppdHN1MSMwIQYDVQQDDBpGRVAgUm9vdCBDQSBmb3IgS3ViZXJu\nZXRlczCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBAMs97gUF0xkUzCgL\n7MiiDju9ySr/ziwjvcYU7jA9ML+SLmftMs3HtcYbAmSntqI+MDBSR/FAJTOoytuT\npV+mCFcGj2YAjDpliHPeNcUpbryy4YMChF3+MovkIwGCksxo5rhiWhGmoBYpA48P\n4Xe8SPlzqMzhFvNeKzyiUhvjutS2Y1Ss38lsTaurFPx64vQ2PaC54XzdwMptXtpb\ntYmWSzCpJWwxZ6lF3vitdA2w0tnBWNyctAd0+RIM/fvArxiIqseAux9t0uogm5to\nlRIhvekuxOpXBPEqtIYQ4j9XUW2JH8vUDnzPkPvjrq+A3Ug8OyyfGVrW7+VYXozu\nc4aP7P0CAwEAAaNTMFEwHQYDVR0OBBYEFBzCutQ7S74WEhS5V2sNEJBGyLpmMB8G\nA1UdIwQYMBaAFBzCutQ7S74WEhS5V2sNEJBGyLpmMA8GA1UdEwEB/wQFMAMBAf8w\nDQYJKoZIhvcNAQELBQADggEBAMDwD85RAaWEBptFgLzKw+9xEUy1vcZaonAuA1qc\nT342XTueyAugxkC11HwdCGgGS34VyctfMGqj4AW6pA2ez4tLrbOps4DmV4sw8uBL\n8pgRDgfly3ob9FEg2wa0hmrwX9jH5Bt4vySUE2785uPAqaspT2UNtTBxS85BUi1T\nsKId2Rtil6an281Z81wyWVI6Jm2D4MG0mbsiGcTPlCtdg/UljvDYymXlAvd4vNhl\nk9hDa13TgDqJKgKdTIcmZoNQdpEVgFcO0h9AEUy5AuLqxHq60dLfZ6ESGPlMI7Lm\ni4PzYbCnBmOe+7TnHcPSyrnehs66Ik+oifRd82eYS7vKjFw=\n-----END CERTIFICATE-----",
            "customsslkey": "none",
            "fepclustername": "new-fep",
            "imagePullPolicy": "IfNotPresent",
            "limits": {
              "cpu": "400m",
              "memory": "512Mi"
            },
            "requests": {
              "cpu": "200m",
              "memory": "256Mi"
            },
            "serviceport": 9999,
            "statusport": 9898
          }
        },
        {
          "api_version": "fep.fujitsu.io/v1",
          "kind": "FEPRestore",
          "metadata": {
            "name": "new-fep-restore"
          },
          "spec": {
            "fromFEPcluster": "new-fep",
            "imagePullPolicy": "IfNotPresent",
            "mcSpec": {
              "limits": {
                "cpu": "200m",
                "memory": "300Mi"
              },
              "requests": {
                "cpu": "100m",
                "memory": "200Mi"
              }
            },
            "restoretype": "latest",
            "sysExtraLogging": false,
            "toFEPcluster": "new-fep-2"
          }
        },
        {
          "api_version": "fep.fujitsu.io/v2",
          "kind": "FEPCluster",
          "metadata": {
            "name": "new-fep"
          },
          "spec": {
            "fep": {
              "customAnnotations": {
                "allDeployments": {}
              },
              "forceSsl": true,
              "image": {
                "pullPolicy": "IfNotPresent"
              },
              "instances": 1,
              "mcSpec": {
                "limits": {
                  "cpu": "500m",
                  "memory": "700Mi"
                },
                "requests": {
                  "cpu": "200m",
                  "memory": "512Mi"
                }
              },
              "podAntiAffinity": false,
              "podDisruptionBudget": false,
              "servicePort": 27500,
              "syncMode": "off",
              "sysExtraLogging": false
            },
            "fepChildCrVal": {
              "backup": {
                "image": {
                  "pullPolicy": "IfNotPresent"
                },
                "mcSpec": {
                  "limits": {
                    "cpu": 0.2,
                    "memory": "300Mi"
                  },
                  "requests": {
                    "cpu": 0.1,
                    "memory": "200Mi"
                  }
                },
                "pgbackrestParams": "# if log volume is not defined, log_directory should be\n# changed to '/database/userdata/data/log'\n[global]\nrepo1-retention-full=7\nrepo1-retention-full-type=time\nlog-path=/database/log/backup\n",
                "postScript": " ",
                "preScript": " ",
                "schedule": {
                  "num": 2
                },
                "schedule1": {
                  "schedule": "15 0 * * 0",
                  "type": "full"
                },
                "schedule2": {
                  "schedule": "15 0 * * 1-6",
                  "type": "incr"
                },
                "schedule3": {
                  "schedule": " ",
                  "type": " "
                },
                "schedule4": {
                  "schedule": " ",
                  "type": " "
                },
                "schedule5": {
                  "schedule": " ",
                  "type": " "
                }
              },
              "customPgAudit": "# define pg audit custom params here to override defaults.\n# if log volume is not defined, log_directory should be\n# changed to '/database/userdata/data/log'\n[output]\nlog_truncate_on_rotation = on\nlog_filename = 'pgaudit-%a.log'\nlog_rotation_age = 1d\nlog_rotation_size = 0\n[rule]\n",
              "customPgHba": "# define pg_hba custom rules here to be merged with default rules.\n# TYPE     DATABASE        USER        ADDRESS        METHOD\n",
              "customPgParams": "# define custom postgresql.conf parameters below to override defaults.\n# Current values are as per default FEP deployment\nshared_preload_libraries='pgx_datamasking,pg_prewarm,pg_stat_statements'\nsession_preload_libraries='pg_prewarm'\nmax_prepared_transactions = 100\nmax_worker_processes = 20\nmax_connections = 100\nwork_mem = 1MB\nmaintenance_work_mem = 12MB\nshared_buffers = 128MB\neffective_cache_size = 384MB\ncheckpoint_completion_target = 0.8\n\n# tcp parameters\ntcp_keepalives_idle = 30\ntcp_keepalives_interval = 10\ntcp_keepalives_count = 3\n\n# logging parameters in default fep installation\n# if log volume is not defined, log_directory should be\n# changed to '/database/userdata/data/log'\nlog_directory = '/database/log'\nlog_filename = 'logfile-%a.log'\nlog_file_mode = 0600\nlog_truncate_on_rotation = on\nlog_rotation_age = 1d\nlog_rotation_size = 0\nlog_checkpoints = on\nlog_line_prefix = '%e %t [%p]: [%l-1] user=%u,db=%d,app=%a,client=%h'\nlog_lock_waits = on\nlog_autovacuum_min_duration = 60s\nlogging_collector = on\npgaudit.config_file='/opt/app-root/src/pgaudit-cfg/pgaudit.conf'\nlog_replication_commands = on\nlog_min_messages = WARNING\nlog_destination = stderr\n\n# wal_archive parameters in default fep installation\narchive_mode = on\narchive_command = 'pgbackrest --stanza=backupstanza --config=/database/userdata/pgbackrest.conf archive-push %p'\nwal_level = replica\nmax_wal_senders = 12\nwal_keep_segments = 64\n",
              "storage": {
                "archivewalVol": {
                  "size": "1Gi"
                },
                "backupVol": {
                  "size": "2Gi"
                },
                "dataVol": {
                  "size": "2Gi"
                },
                "logVol": {
                  "size": "1Gi"
                },
                "tablespaceVol": {
                  "size": "512Mi"
                },
                "walVol": {
                  "size": "1200Mi"
                }
              },
              "sysUsers": {
                "pgAdminPassword": "admin-password",
                "pgdb": "mydb",
                "pgpassword": "mydbpassword",
                "pgreplpassword": "repluserpwd",
                "pgrepluser": "repluser",
                "pguser": "mydbuser",
                "tdepassphrase": "tde-passphrase"
              },
              "systemCertificates": {
                "cacrt": "-----BEGIN CERTIFICATE-----\nMIIDTzCCAjegAwIBAgIUYssQ8I74US5g+1+Z7CHuaDgkZnEwDQYJKoZIhvcNAQEL\nBQAwNzEQMA4GA1UECgwHRnVqaXRzdTEjMCEGA1UEAwwaRkVQIFJvb3QgQ0EgZm9y\nIEt1YmVybmV0ZXMwHhcNMjEwMjA2MDM1MjI4WhcNMzEwMjA0MDM1MjI4WjA3MRAw\nDgYDVQQKDAdGdWppdHN1MSMwIQYDVQQDDBpGRVAgUm9vdCBDQSBmb3IgS3ViZXJu\nZXRlczCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBAMs97gUF0xkUzCgL\n7MiiDju9ySr/ziwjvcYU7jA9ML+SLmftMs3HtcYbAmSntqI+MDBSR/FAJTOoytuT\npV+mCFcGj2YAjDpliHPeNcUpbryy4YMChF3+MovkIwGCksxo5rhiWhGmoBYpA48P\n4Xe8SPlzqMzhFvNeKzyiUhvjutS2Y1Ss38lsTaurFPx64vQ2PaC54XzdwMptXtpb\ntYmWSzCpJWwxZ6lF3vitdA2w0tnBWNyctAd0+RIM/fvArxiIqseAux9t0uogm5to\nlRIhvekuxOpXBPEqtIYQ4j9XUW2JH8vUDnzPkPvjrq+A3Ug8OyyfGVrW7+VYXozu\nc4aP7P0CAwEAAaNTMFEwHQYDVR0OBBYEFBzCutQ7S74WEhS5V2sNEJBGyLpmMB8G\nA1UdIwQYMBaAFBzCutQ7S74WEhS5V2sNEJBGyLpmMA8GA1UdEwEB/wQFMAMBAf8w\nDQYJKoZIhvcNAQELBQADggEBAMDwD85RAaWEBptFgLzKw+9xEUy1vcZaonAuA1qc\nT342XTueyAugxkC11HwdCGgGS34VyctfMGqj4AW6pA2ez4tLrbOps4DmV4sw8uBL\n8pgRDgfly3ob9FEg2wa0hmrwX9jH5Bt4vySUE2785uPAqaspT2UNtTBxS85BUi1T\nsKId2Rtil6an281Z81wyWVI6Jm2D4MG0mbsiGcTPlCtdg/UljvDYymXlAvd4vNhl\nk9hDa13TgDqJKgKdTIcmZoNQdpEVgFcO0h9AEUy5AuLqxHq60dLfZ6ESGPlMI7Lm\ni4PzYbCnBmOe+7TnHcPSyrnehs66Ik+oifRd82eYS7vKjFw=\n-----END CERTIFICATE-----",
                "crt": "-----BEGIN CERTIFICATE-----\nMIIDUTCCAjmgAwIBAgIRAMocW3qMoHrD6qRvMPppMkMwDQYJKoZIhvcNAQELBQAw\nNzEQMA4GA1UECgwHRnVqaXRzdTEjMCEGA1UEAwwaRkVQIFJvb3QgQ0EgZm9yIEt1\nYmVybmV0ZXMwHhcNMjEwMjA2MDQzMjM2WhcNMjYwMjA1MDQzMjM2WjA/MRAwDgYD\nVQQKEwdGdWppdHN1MSswKQYDVQQDEyJGVUpJVFNVIEVudGVycHJpc2UgUG9zdGdy\nZXMgU2VydmVyMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEA4AI33yvH\nZws+jta6qpV6wzJqF8odIfTIpCfbrVcUUtLFKJ1I2e4SceTKi6O3C/I1XuvWlpng\n5IO65+fQQLO06z1/AuQT78YUn/Wlm9x1aHVsv4ANB5JWWqDOjrRT3o7nRPGXfila\nbP0rGE2mJJcVR9nExJ3IeaktgT3sb8YlXvtchyYpmjdbfxabTz07ig0+6/cwKoRR\nxOK8Uf7f5euE0cI/490J6r5Rs4lgD8sIQNCUFlTFYvmAH7gcdssSFBt8NPlUATHE\nsoFmlW0DKCJWNhTLOht+s6L/1zwTHLjPG2pdkG6Wdgmu5H2pDml8CDNLDv98Aj7i\n+I5SRKKcVPlnuQIDAQABo1AwTjAdBgNVHSUEFjAUBggrBgEFBQcDAQYIKwYBBQUH\nAwIwDAYDVR0TAQH/BAIwADAfBgNVHSMEGDAWgBQcwrrUO0u+FhIUuVdrDRCQRsi6\nZjANBgkqhkiG9w0BAQsFAAOCAQEAm5dxBoI9pScOCvRAchg4CprdRDSJb9K6yB3O\nnCAxnM47iHeXnY3WlnI388kHu8DU7O4ba1tJbGs3KY9KzioPk43pU12jWkO1onoF\n+mTDjx/Ef1cYWA9r5q/LtgTa6Q2sxV4O2x67QW82aAnaxO34dV5zWCPIvAoovZBV\nHRT+BgCg3r2vD1RGKK2nl1aYJtWhO1SZubam+VttdZ/vbM9oOJctxmImsEtBXjkY\nKteePdQtLL5o03JhyXWyRshCq+HMmKf2KgyY8gvydGcP4eLQdBWcW40LcnVq6UjT\n0kJycJEKngMVademq1ZWHGaiYB7hyT6GhgIcHUJ2cKrPgbEh1Q==\n-----END CERTIFICATE-----",
                "key": "-----BEGIN RSA PRIVATE KEY-----\nMIIEowIBAAKCAQEA4AI33yvHZws+jta6qpV6wzJqF8odIfTIpCfbrVcUUtLFKJ1I\n2e4SceTKi6O3C/I1XuvWlpng5IO65+fQQLO06z1/AuQT78YUn/Wlm9x1aHVsv4AN\nB5JWWqDOjrRT3o7nRPGXfilabP0rGE2mJJcVR9nExJ3IeaktgT3sb8YlXvtchyYp\nmjdbfxabTz07ig0+6/cwKoRRxOK8Uf7f5euE0cI/490J6r5Rs4lgD8sIQNCUFlTF\nYvmAH7gcdssSFBt8NPlUATHEsoFmlW0DKCJWNhTLOht+s6L/1zwTHLjPG2pdkG6W\ndgmu5H2pDml8CDNLDv98Aj7i+I5SRKKcVPlnuQIDAQABAoIBAFPQYKlOzw/+BA0b\nyMIUpdctIMb/54CR/xR0mVw1DbSjigNVPjHUQvB8Y1B2FAITQObgJO06bAv0QdWN\nRb0/v/yYiNJDFjaLjaIAHlO/2+oWrXbFaZqgpVDJhB+e1xaZr2x7XGxm+p925k30\nl6pvIRY+I8JRKvZiV1VZHwL/R3JOtPr++xMZtLVjVOI+f+ySqJ+TZHuAjm49EKxj\ncEmmJ28b7QcziXsvKy00f+zbqLIBKXQdZAFU5eEr1BsDRXdRW+Kf0XIvftuy4BJZ\nvoKT+VGhEvF/qysswL4+6IAO6tpuYnnM0Y2d3sOGoWPkTcQK0MekYKzL/WmtCjNs\n9hodJtECgYEA5EWyhEOf4uOKe5TDp697UCUvXLoOR58FDe/S8XNvScn29jjOkqIg\nOMoqo9xAkJTNTzqn5UUdt1x/pgM2NxlPLFijrc0zQlX3SoOO2ryDd9WNi7YKtN16\nKJqa536WeZu2OEbuAZ+S3GALVy1RPeTNPnUOmKnF06DjDUGzLNCZy10CgYEA+zfw\n952DWuz1U0Z4wvAEqqcgUKXPKrkTXV/iUnjkDkrLYVr0ZofDNTXrdHl+UedFmaOC\ncieZn6DNhcdz5tKtyysGMH3g/qs9PfoGUngvcXsy0Egk04l3x1jc8TTCLqXZXYaQ\nHMsx51n+R58oncPtzYSUOr9qQ6PbC2CstTbFJA0CgYEAjGEsUliAB/jknfEzjXjG\nPdhQUxb8VyE864Az2lah9t/kJzFyIAziAeqZ5GE7t247AGFTBRTHHI8e1Qoemi3P\nWbc9GVIbFs1lIYbcIDpUIyrKPEP8O5QEXtoNLxXTFgAjRGKiVY87spjCAJ+W2ZhO\ne/1it5GYXfgQCYQA2yuBmOUCgYANRkR2YR1axaCk+NlSu6oTdmdPu6M5x7PNQE7O\nOtMaKjua9lppvIzFGAdMDUtueoEEAE7ZR1xnwfB6PDLUpJdIYAqgr1YfPt8qkjaZ\nTv56yZ7CwL0pbF8m6nwqRrZoDp1wwraEvvvxFKFKGY/k3kCHlpTakdjEoDjn3gDi\nRnWeVQKBgCEneMSzucei5LRppRtRaJw/Btll8qlPMlX3W7dxQ3cLwpmLOn0m51Fp\nPIZ44zYK8R6fu4+/sSrlfaIg86Ugeufp6YNxyNROKxUGza5vDIu5OftwWtBeg+UK\nZ8lLWNdX6pp7WMujmF3H1DrkBbauYMUKZ4UxUYtelgHERMePIxwb\n-----END RSA PRIVATE KEY-----"
              }
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [
          "disconnected"
        ],
        "valid_subscription": []
      },
      "architectures": [
        "amd64",
        "s390x"
      ],
      "bundle_path": "registry.connect.redhat.com/fujitsu-postgres/fujitsu-enterprise-postgres-bundle@sha256:50004c77a8d75e30f232c0465979b3764ccde8df2fa601d617c03085b03e1571",
      "bundle_path_digest": "sha256:50004c77a8d75e30f232c0465979b3764ccde8df2fa601d617c03085b03e1571",
      "capabilities": [
        "Full Lifecycle"
      ],
      "channel_name": "stable",
      "creation_date": "2022-11-28T09:40:27.256000+00:00",
      "csv_description": "FUJITSU Enterprise Postgres 12 delivers an enterprise-grade PostgreSQL on OpenShift Container Platform.\n\nThis solution provides the flexibility of a hybrid cloud solution while delivering an enhanced distribution\nof PostgreSQL to support enterprise-level workloads and provide improved deployment and management,\navailability, performance, data governance and security.\n\nAvailable as a multi-architecture container built for both amd64 and s390x.\n\nThe download and Use of the Product is strictly subject to the terms of the End User License Agreement with Fujitsu Limited found at\nhttps://www.fast.fujitsu.com/fujitsu-enterprise-postgres-license-agreements\nWhere the Product that has been embedded as a whole or part into a third party program, only Authorised Customers may download and use the Product.\n",
      "csv_display_name": "FUJITSU Enterprise Postgres 12 Operator",
      "csv_metadata_description": "OpenShift Operator for Fujitsu Enterprise Postgres 12",
      "csv_name": "fep-ansible-operator.v2.2.12",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": true,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-11-28T09:40:27.256000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.8",
      "organization": "certified-operators",
      "package": "fep-ansible-operator",
      "provided_apis": [
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPConfig",
          "plural": "fepconfigs",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPRestore",
          "plural": "feprestores",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPVolume",
          "plural": "fepvolumes",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPAction",
          "plural": "fepactions",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPCert",
          "plural": "fepcerts",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPPgpool2",
          "plural": "feppgpool2s",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPCluster",
          "plural": "fepclusters",
          "version": "v2"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPPgpool2Cert",
          "plural": "feppgpool2certs",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPUser",
          "plural": "fepusers",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPBackup",
          "plural": "fepbackups",
          "version": "v1"
        }
      ],
      "provider": "Fujitsu",
      "related_images": [
        {
          "digest": "sha256:61592bcccccee2e72521d0640f005d1902730319ad697aa0f4fc5485bf22989d",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-12-server@sha256:61592bcccccee2e72521d0640f005d1902730319ad697aa0f4fc5485bf22989d",
          "name": "fep"
        },
        {
          "digest": "sha256:f126c128c1301afa684bdf3258c78d1ec68c08aa6e5c085c5f939940947e5c94",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-12-backup@sha256:f126c128c1301afa684bdf3258c78d1ec68c08aa6e5c085c5f939940947e5c94",
          "name": "backup"
        },
        {
          "digest": "sha256:240da72ba4ef71031c1587a1f44b7631697346bb7699be52dcebd64e9f93d66e",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-12-restore@sha256:240da72ba4ef71031c1587a1f44b7631697346bb7699be52dcebd64e9f93d66e",
          "name": "restore"
        },
        {
          "digest": "sha256:6d58e56e6e7ab0c07d6095b1cab429c47ea3d294bf1656bd0a7d6efed098cb02",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-12-pgpool2@sha256:6d58e56e6e7ab0c07d6095b1cab429c47ea3d294bf1656bd0a7d6efed098cb02",
          "name": "pgpool2"
        },
        {
          "digest": "sha256:37ee807c70476b71d4f3d1275d47b43e170e7023070259f80f798091be0b1d81",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-12-operator@sha256:37ee807c70476b71d4f3d1275d47b43e170e7023070259f80f798091be0b1d81",
          "name": "fujitsu-enterprise-postgres-12-operator-37ee807c70476b71d4f3d1275d47b43e170e7023070259f80f798091be0b1d81-annotation"
        },
        {
          "digest": "sha256:37ee807c70476b71d4f3d1275d47b43e170e7023070259f80f798091be0b1d81",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-12-operator@sha256:37ee807c70476b71d4f3d1275d47b43e170e7023070259f80f798091be0b1d81",
          "name": "fep-ansible-operator"
        }
      ],
      "replaces": null,
      "skip_range": "<2.2.12",
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.8",
      "version": "2.2.12",
      "version_original": "2.2.12"
    },
    {
      "_id": "6386061b33e711a2e505ac33",
      "alm_examples": [
        {
          "api_version": "fep.fujitsu.io/v1",
          "kind": "FEPAction",
          "metadata": {
            "name": "new-fep-action"
          },
          "spec": {
            "fepAction": {
              "args": [
                "new-fep-sts-0",
                "new-fep-sts-1"
              ],
              "type": "reload"
            },
            "sysExtraLogging": false,
            "targetClusterName": "new-fep"
          }
        },
        {
          "api_version": "fep.fujitsu.io/v1",
          "kind": "FEPExporter",
          "metadata": {
            "name": "new-fep-exporter"
          },
          "spec": {
            "fepExporter": {
              "exporterLogLevel": "error",
              "fepClusterList": [
                "new-fep1"
              ],
              "image": {
                "pullPolicy": "IfNotPresent"
              },
              "mcSpec": {
                "limits": {
                  "cpu": "500m",
                  "memory": "700Mi"
                },
                "requests": {
                  "cpu": "200m",
                  "memory": "512Mi"
                }
              },
              "restartRequired": false,
              "sysExtraLogging": false,
              "userCustomQueries": "usr_example:\n  query: \"SELECT EXTRACT(EPOCH FROM (now() - pg_last_xact_replay_timestamp())) as lag\"\n  master: true\n  metrics:\n    - lag:\n        usage: \"GAUGE\"\n        description: \"Replication lag behind master in seconds\""
            }
          }
        },
        {
          "api_version": "fep.fujitsu.io/v1",
          "kind": "FEPLogging",
          "metadata": {
            "name": "new-fep-logging"
          },
          "spec": {
            "fepLogging": {
              "image": {
                "pullPolicy": "IfNotPresent"
              },
              "mcSpec": {
                "limits": {
                  "cpu": "500m",
                  "memory": "700Mi"
                },
                "requests": {
                  "cpu": "200m",
                  "memory": "512Mi"
                }
              },
              "restartRequired": false,
              "sysExtraLogging": false
            }
          }
        },
        {
          "api_version": "fep.fujitsu.io/v1",
          "kind": "FEPPgpool2",
          "metadata": {
            "name": "new-fep-pgpool2"
          },
          "spec": {
            "count": 2,
            "customhba": "local   all         all                               trust\nhost    all         all         127.0.0.1/32          trust\nhost    all         all         ::1/128               trust\n",
            "customlogsize": "128Mi",
            "customparams": "listen_addresses = '*'\npcp_listen_addresses = '*'\nnum_init_children = 32\nreserved_connections = 0\nenable_pool_hba = off\nallow_clear_text_frontend_auth = off\nauthentication_timeout = 80\nbackend_weight0 = 1\nbackend_weight1 = 1\nbackend_flag0 = 'ALWAYS_PRIMARY'\nbackend_flag1 = 'DISALLOW_TO_FAILOVER'\nconnection_cache = on\nmax_pool = 4\nlisten_backlog_multiplier = 2\nserialize_accept = off\nchild_life_time = 300\nclient_idle_limit = 0\nchild_max_connections = 0\nconnection_life_time = 0\nreset_query_list = 'ABORT; DISCARD ALL'\nclient_min_messages = info\nlog_min_messages = debug1\nlog_statement = on\nlog_per_node_statement = on\nlog_client_messages = on\nlog_hostname = on\nlog_connections = on\nlog_line_prefix = '%t: pid %p: '\nload_balance_mode = on\nignore_leading_white_space = on\nwhite_function_list = ''\nblack_function_list = 'currval,lastval,nextval,setval'\nblack_query_pattern_list = ''\ndatabase_redirect_preference_list = ''\napp_name_redirect_preference_list = ''\nallow_sql_comments = off\ndisable_load_balance_on_write = 'transaction'\nstatement_level_load_balance = on\nsr_check_period = 0\nsr_check_user = 'postgres'\ndelay_threshold = 0\nlog_standby_delay = 'none'\nssl = on\nssl_ciphers = 'HIGH:MEDIUM:+3DES:!aNULL'\nssl_prefer_server_ciphers = off\nssl_ecdh_curve = 'prime256v1'\nssl_dh_params_file = ''\nrelcache_expire = 0\nrelcache_size = 256\ncheck_temp_table = catalog\ncheck_unlogged_table = on\nenable_shared_relcache = on\nrelcache_query_target = primary\nwd_port0 = 9000\nfailover_on_backend_error = off\n",
            "custompcp": "none",
            "customsslcacert": "none",
            "customsslcert": "-----BEGIN CERTIFICATE-----\nMIIDTzCCAjegAwIBAgIUYssQ8I74US5g+1+Z7CHuaDgkZnEwDQYJKoZIhvcNAQEL\nBQAwNzEQMA4GA1UECgwHRnVqaXRzdTEjMCEGA1UEAwwaRkVQIFJvb3QgQ0EgZm9y\nIEt1YmVybmV0ZXMwHhcNMjEwMjA2MDM1MjI4WhcNMzEwMjA0MDM1MjI4WjA3MRAw\nDgYDVQQKDAdGdWppdHN1MSMwIQYDVQQDDBpGRVAgUm9vdCBDQSBmb3IgS3ViZXJu\nZXRlczCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBAMs97gUF0xkUzCgL\n7MiiDju9ySr/ziwjvcYU7jA9ML+SLmftMs3HtcYbAmSntqI+MDBSR/FAJTOoytuT\npV+mCFcGj2YAjDpliHPeNcUpbryy4YMChF3+MovkIwGCksxo5rhiWhGmoBYpA48P\n4Xe8SPlzqMzhFvNeKzyiUhvjutS2Y1Ss38lsTaurFPx64vQ2PaC54XzdwMptXtpb\ntYmWSzCpJWwxZ6lF3vitdA2w0tnBWNyctAd0+RIM/fvArxiIqseAux9t0uogm5to\nlRIhvekuxOpXBPEqtIYQ4j9XUW2JH8vUDnzPkPvjrq+A3Ug8OyyfGVrW7+VYXozu\nc4aP7P0CAwEAAaNTMFEwHQYDVR0OBBYEFBzCutQ7S74WEhS5V2sNEJBGyLpmMB8G\nA1UdIwQYMBaAFBzCutQ7S74WEhS5V2sNEJBGyLpmMA8GA1UdEwEB/wQFMAMBAf8w\nDQYJKoZIhvcNAQELBQADggEBAMDwD85RAaWEBptFgLzKw+9xEUy1vcZaonAuA1qc\nT342XTueyAugxkC11HwdCGgGS34VyctfMGqj4AW6pA2ez4tLrbOps4DmV4sw8uBL\n8pgRDgfly3ob9FEg2wa0hmrwX9jH5Bt4vySUE2785uPAqaspT2UNtTBxS85BUi1T\nsKId2Rtil6an281Z81wyWVI6Jm2D4MG0mbsiGcTPlCtdg/UljvDYymXlAvd4vNhl\nk9hDa13TgDqJKgKdTIcmZoNQdpEVgFcO0h9AEUy5AuLqxHq60dLfZ6ESGPlMI7Lm\ni4PzYbCnBmOe+7TnHcPSyrnehs66Ik+oifRd82eYS7vKjFw=\n-----END CERTIFICATE-----",
            "customsslkey": "none",
            "fepclustername": "new-fep",
            "imagePullPolicy": "IfNotPresent",
            "limits": {
              "cpu": "400m",
              "memory": "512Mi"
            },
            "requests": {
              "cpu": "200m",
              "memory": "256Mi"
            },
            "serviceport": 9999,
            "statusport": 9898
          }
        },
        {
          "api_version": "fep.fujitsu.io/v1",
          "kind": "FEPRestore",
          "metadata": {
            "name": "new-fep-restore"
          },
          "spec": {
            "fromFEPcluster": "new-fep",
            "imagePullPolicy": "IfNotPresent",
            "mcSpec": {
              "limits": {
                "cpu": "200m",
                "memory": "300Mi"
              },
              "requests": {
                "cpu": "100m",
                "memory": "200Mi"
              }
            },
            "restoretype": "latest",
            "sysExtraLogging": false,
            "toFEPcluster": "new-fep-2"
          }
        },
        {
          "api_version": "fep.fujitsu.io/v2",
          "kind": "FEPCluster",
          "metadata": {
            "name": "new-fep"
          },
          "spec": {
            "fep": {
              "customAnnotations": {
                "allDeployments": {}
              },
              "forceSsl": true,
              "image": {
                "pullPolicy": "IfNotPresent"
              },
              "instances": 1,
              "mcSpec": {
                "limits": {
                  "cpu": "500m",
                  "memory": "700Mi"
                },
                "requests": {
                  "cpu": "200m",
                  "memory": "512Mi"
                }
              },
              "podAntiAffinity": false,
              "podDisruptionBudget": false,
              "servicePort": 27500,
              "syncMode": "off",
              "sysExtraLogging": false
            },
            "fepChildCrVal": {
              "backup": {
                "image": {
                  "pullPolicy": "IfNotPresent"
                },
                "mcSpec": {
                  "limits": {
                    "cpu": 0.2,
                    "memory": "300Mi"
                  },
                  "requests": {
                    "cpu": 0.1,
                    "memory": "200Mi"
                  }
                },
                "pgbackrestParams": "# if log volume is not defined, log_directory should be\n# changed to '/database/userdata/data/log'\n[global]\nrepo1-retention-full=7\nrepo1-retention-full-type=time\nlog-path=/database/log/backup\n",
                "postScript": " ",
                "preScript": " ",
                "schedule": {
                  "num": 2
                },
                "schedule1": {
                  "schedule": "15 0 * * 0",
                  "type": "full"
                },
                "schedule2": {
                  "schedule": "15 0 * * 1-6",
                  "type": "incr"
                },
                "schedule3": {
                  "schedule": " ",
                  "type": " "
                },
                "schedule4": {
                  "schedule": " ",
                  "type": " "
                },
                "schedule5": {
                  "schedule": " ",
                  "type": " "
                }
              },
              "customPgAudit": "# define pg audit custom params here to override defaults.\n# if log volume is not defined, log_directory should be\n# changed to '/database/userdata/data/log'\n[output]\nlogger = 'auditlog'\nlog_directory = '/database/log/audit'\nlog_truncate_on_rotation = on\nlog_filename = 'pgaudit-%a.log'\nlog_rotation_age = 1d\nlog_rotation_size = 0\n[rule]\n",
              "customPgHba": "# define pg_hba custom rules here to be merged with default rules.\n# TYPE     DATABASE        USER        ADDRESS        METHOD\n",
              "customPgParams": "# define custom postgresql.conf parameters below to override defaults.\n# Current values are as per default FEP deployment\nshared_preload_libraries='pgx_datamasking,pg_prewarm,pg_stat_statements'\nsession_preload_libraries='pg_prewarm'\nmax_prepared_transactions = 100\nmax_worker_processes = 30\nmax_connections = 100\nwork_mem = 1MB\nmaintenance_work_mem = 12MB\nshared_buffers = 128MB\neffective_cache_size = 384MB\ncheckpoint_completion_target = 0.8\n# tcp parameters\ntcp_keepalives_idle = 30\ntcp_keepalives_interval = 10\ntcp_keepalives_count = 3\n# logging parameters in default fep installation\n# if log volume is not defined, log_directory should be\n# changed to '/database/userdata/data/log'\nlog_directory = '/database/log'\nlog_filename = 'logfile-%a.log'\nlog_file_mode = 0600\nlog_truncate_on_rotation = on\nlog_rotation_age = 1d\nlog_rotation_size = 0\nlog_checkpoints = on\nlog_line_prefix = '%e %t [%p]: [%l-1] user=%u,db=%d,app=%a,client=%h'\nlog_lock_waits = on\nlog_autovacuum_min_duration = 60s\nlogging_collector = on\npgaudit.config_file='/opt/app-root/src/pgaudit-cfg/pgaudit.conf'\nlog_replication_commands = on\nlog_min_messages = WARNING\nlog_destination = csvlog\n# wal_archive parameters in default fep installation\narchive_mode = on\narchive_command = 'pgbackrest --stanza=backupstanza --config=/database/userdata/pgbackrest.conf archive-push %p'\nwal_level = replica\nmax_wal_senders = 12\nwal_keep_segments = 64\ntrack_activities = on\ntrack_counts = on\npassword_encryption = 'md5'\n",
              "storage": {
                "archivewalVol": {
                  "size": "1Gi"
                },
                "backupVol": {
                  "size": "2Gi"
                },
                "dataVol": {
                  "size": "2Gi"
                },
                "logVol": {
                  "size": "1Gi"
                },
                "tablespaceVol": {
                  "size": "512Mi"
                },
                "walVol": {
                  "size": "1200Mi"
                }
              },
              "sysUsers": {
                "pgAdminPassword": "admin-password",
                "pgRewindPassword": "rewind_password",
                "pgRewindUser": "rewind_user",
                "pgdb": "mydb",
                "pgpassword": "mydbpassword",
                "pgreplpassword": "repluserpwd",
                "pgrepluser": "repluser",
                "pguser": "mydbuser",
                "tdepassphrase": "tde-passphrase"
              },
              "systemCertificates": {
                "cacrt": "-----BEGIN CERTIFICATE-----\nMIIDTzCCAjegAwIBAgIUYssQ8I74US5g+1+Z7CHuaDgkZnEwDQYJKoZIhvcNAQEL\nBQAwNzEQMA4GA1UECgwHRnVqaXRzdTEjMCEGA1UEAwwaRkVQIFJvb3QgQ0EgZm9y\nIEt1YmVybmV0ZXMwHhcNMjEwMjA2MDM1MjI4WhcNMzEwMjA0MDM1MjI4WjA3MRAw\nDgYDVQQKDAdGdWppdHN1MSMwIQYDVQQDDBpGRVAgUm9vdCBDQSBmb3IgS3ViZXJu\nZXRlczCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBAMs97gUF0xkUzCgL\n7MiiDju9ySr/ziwjvcYU7jA9ML+SLmftMs3HtcYbAmSntqI+MDBSR/FAJTOoytuT\npV+mCFcGj2YAjDpliHPeNcUpbryy4YMChF3+MovkIwGCksxo5rhiWhGmoBYpA48P\n4Xe8SPlzqMzhFvNeKzyiUhvjutS2Y1Ss38lsTaurFPx64vQ2PaC54XzdwMptXtpb\ntYmWSzCpJWwxZ6lF3vitdA2w0tnBWNyctAd0+RIM/fvArxiIqseAux9t0uogm5to\nlRIhvekuxOpXBPEqtIYQ4j9XUW2JH8vUDnzPkPvjrq+A3Ug8OyyfGVrW7+VYXozu\nc4aP7P0CAwEAAaNTMFEwHQYDVR0OBBYEFBzCutQ7S74WEhS5V2sNEJBGyLpmMB8G\nA1UdIwQYMBaAFBzCutQ7S74WEhS5V2sNEJBGyLpmMA8GA1UdEwEB/wQFMAMBAf8w\nDQYJKoZIhvcNAQELBQADggEBAMDwD85RAaWEBptFgLzKw+9xEUy1vcZaonAuA1qc\nT342XTueyAugxkC11HwdCGgGS34VyctfMGqj4AW6pA2ez4tLrbOps4DmV4sw8uBL\n8pgRDgfly3ob9FEg2wa0hmrwX9jH5Bt4vySUE2785uPAqaspT2UNtTBxS85BUi1T\nsKId2Rtil6an281Z81wyWVI6Jm2D4MG0mbsiGcTPlCtdg/UljvDYymXlAvd4vNhl\nk9hDa13TgDqJKgKdTIcmZoNQdpEVgFcO0h9AEUy5AuLqxHq60dLfZ6ESGPlMI7Lm\ni4PzYbCnBmOe+7TnHcPSyrnehs66Ik+oifRd82eYS7vKjFw=\n-----END CERTIFICATE-----",
                "crt": "-----BEGIN CERTIFICATE-----\nMIIDUTCCAjmgAwIBAgIRAMocW3qMoHrD6qRvMPppMkMwDQYJKoZIhvcNAQELBQAw\nNzEQMA4GA1UECgwHRnVqaXRzdTEjMCEGA1UEAwwaRkVQIFJvb3QgQ0EgZm9yIEt1\nYmVybmV0ZXMwHhcNMjEwMjA2MDQzMjM2WhcNMjYwMjA1MDQzMjM2WjA/MRAwDgYD\nVQQKEwdGdWppdHN1MSswKQYDVQQDEyJGVUpJVFNVIEVudGVycHJpc2UgUG9zdGdy\nZXMgU2VydmVyMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEA4AI33yvH\nZws+jta6qpV6wzJqF8odIfTIpCfbrVcUUtLFKJ1I2e4SceTKi6O3C/I1XuvWlpng\n5IO65+fQQLO06z1/AuQT78YUn/Wlm9x1aHVsv4ANB5JWWqDOjrRT3o7nRPGXfila\nbP0rGE2mJJcVR9nExJ3IeaktgT3sb8YlXvtchyYpmjdbfxabTz07ig0+6/cwKoRR\nxOK8Uf7f5euE0cI/490J6r5Rs4lgD8sIQNCUFlTFYvmAH7gcdssSFBt8NPlUATHE\nsoFmlW0DKCJWNhTLOht+s6L/1zwTHLjPG2pdkG6Wdgmu5H2pDml8CDNLDv98Aj7i\n+I5SRKKcVPlnuQIDAQABo1AwTjAdBgNVHSUEFjAUBggrBgEFBQcDAQYIKwYBBQUH\nAwIwDAYDVR0TAQH/BAIwADAfBgNVHSMEGDAWgBQcwrrUO0u+FhIUuVdrDRCQRsi6\nZjANBgkqhkiG9w0BAQsFAAOCAQEAm5dxBoI9pScOCvRAchg4CprdRDSJb9K6yB3O\nnCAxnM47iHeXnY3WlnI388kHu8DU7O4ba1tJbGs3KY9KzioPk43pU12jWkO1onoF\n+mTDjx/Ef1cYWA9r5q/LtgTa6Q2sxV4O2x67QW82aAnaxO34dV5zWCPIvAoovZBV\nHRT+BgCg3r2vD1RGKK2nl1aYJtWhO1SZubam+VttdZ/vbM9oOJctxmImsEtBXjkY\nKteePdQtLL5o03JhyXWyRshCq+HMmKf2KgyY8gvydGcP4eLQdBWcW40LcnVq6UjT\n0kJycJEKngMVademq1ZWHGaiYB7hyT6GhgIcHUJ2cKrPgbEh1Q==\n-----END CERTIFICATE-----",
                "key": "-----BEGIN RSA PRIVATE KEY-----\nMIIEowIBAAKCAQEA4AI33yvHZws+jta6qpV6wzJqF8odIfTIpCfbrVcUUtLFKJ1I\n2e4SceTKi6O3C/I1XuvWlpng5IO65+fQQLO06z1/AuQT78YUn/Wlm9x1aHVsv4AN\nB5JWWqDOjrRT3o7nRPGXfilabP0rGE2mJJcVR9nExJ3IeaktgT3sb8YlXvtchyYp\nmjdbfxabTz07ig0+6/cwKoRRxOK8Uf7f5euE0cI/490J6r5Rs4lgD8sIQNCUFlTF\nYvmAH7gcdssSFBt8NPlUATHEsoFmlW0DKCJWNhTLOht+s6L/1zwTHLjPG2pdkG6W\ndgmu5H2pDml8CDNLDv98Aj7i+I5SRKKcVPlnuQIDAQABAoIBAFPQYKlOzw/+BA0b\nyMIUpdctIMb/54CR/xR0mVw1DbSjigNVPjHUQvB8Y1B2FAITQObgJO06bAv0QdWN\nRb0/v/yYiNJDFjaLjaIAHlO/2+oWrXbFaZqgpVDJhB+e1xaZr2x7XGxm+p925k30\nl6pvIRY+I8JRKvZiV1VZHwL/R3JOtPr++xMZtLVjVOI+f+ySqJ+TZHuAjm49EKxj\ncEmmJ28b7QcziXsvKy00f+zbqLIBKXQdZAFU5eEr1BsDRXdRW+Kf0XIvftuy4BJZ\nvoKT+VGhEvF/qysswL4+6IAO6tpuYnnM0Y2d3sOGoWPkTcQK0MekYKzL/WmtCjNs\n9hodJtECgYEA5EWyhEOf4uOKe5TDp697UCUvXLoOR58FDe/S8XNvScn29jjOkqIg\nOMoqo9xAkJTNTzqn5UUdt1x/pgM2NxlPLFijrc0zQlX3SoOO2ryDd9WNi7YKtN16\nKJqa536WeZu2OEbuAZ+S3GALVy1RPeTNPnUOmKnF06DjDUGzLNCZy10CgYEA+zfw\n952DWuz1U0Z4wvAEqqcgUKXPKrkTXV/iUnjkDkrLYVr0ZofDNTXrdHl+UedFmaOC\ncieZn6DNhcdz5tKtyysGMH3g/qs9PfoGUngvcXsy0Egk04l3x1jc8TTCLqXZXYaQ\nHMsx51n+R58oncPtzYSUOr9qQ6PbC2CstTbFJA0CgYEAjGEsUliAB/jknfEzjXjG\nPdhQUxb8VyE864Az2lah9t/kJzFyIAziAeqZ5GE7t247AGFTBRTHHI8e1Qoemi3P\nWbc9GVIbFs1lIYbcIDpUIyrKPEP8O5QEXtoNLxXTFgAjRGKiVY87spjCAJ+W2ZhO\ne/1it5GYXfgQCYQA2yuBmOUCgYANRkR2YR1axaCk+NlSu6oTdmdPu6M5x7PNQE7O\nOtMaKjua9lppvIzFGAdMDUtueoEEAE7ZR1xnwfB6PDLUpJdIYAqgr1YfPt8qkjaZ\nTv56yZ7CwL0pbF8m6nwqRrZoDp1wwraEvvvxFKFKGY/k3kCHlpTakdjEoDjn3gDi\nRnWeVQKBgCEneMSzucei5LRppRtRaJw/Btll8qlPMlX3W7dxQ3cLwpmLOn0m51Fp\nPIZ44zYK8R6fu4+/sSrlfaIg86Ugeufp6YNxyNROKxUGza5vDIu5OftwWtBeg+UK\nZ8lLWNdX6pp7WMujmF3H1DrkBbauYMUKZ4UxUYtelgHERMePIxwb\n-----END RSA PRIVATE KEY-----"
              }
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [
        "amd64",
        "ppc64le",
        "s390x"
      ],
      "bundle_path": "registry.connect.redhat.com/fujitsu-postgres/fujitsu-enterprise-postgres-operator-bundle@sha256:94fa991b1fdaa56ee907acd562a163dcc52284719b61188f823f2b0f6dc6f436",
      "bundle_path_digest": "sha256:94fa991b1fdaa56ee907acd562a163dcc52284719b61188f823f2b0f6dc6f436",
      "capabilities": [
        "Auto Pilot"
      ],
      "channel_name": "stable",
      "creation_date": "2022-11-29T13:16:11.068000+00:00",
      "csv_description": "FUJITSU Enterprise Postgres 14 delivers an enterprise-grade PostgreSQL on OpenShift Container Platform.\n\nThis solution provides the flexibility of a hybrid cloud solution while delivering an enhanced distribution of PostgreSQL to support enterprise-level workloads and provide improved deployment and management, availability, performance, data governance and security.  \n\nAvailable as a multi-architecture container built for both amd64, s390x and ppc64le.\n\nThe download and Use of the Product is strictly subject to the terms of the End User License Agreement with Fujitsu Limited found at https://www.fast.fujitsu.com/fujitsu-enterprise-postgres-license-agreements. Where the Product that has been embedded as a whole or part into a third party program, only Authorised Customers may download and use the Product.\n",
      "csv_display_name": "FUJITSU Enterprise Postgres Operator",
      "csv_metadata_description": "OpenShift Operator for Fujitsu Enterprise Postgres",
      "csv_name": "fujitsu-enterprise-postgres-operator.v4.1.9",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": true,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-11-29T13:16:11.068000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.12",
      "organization": "certified-operators",
      "package": "fujitsu-enterprise-postgres-operator",
      "provided_apis": [
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPAction",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPAutoscale",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPBackup",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPCert",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPCluster",
          "version": "v2"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPConfig",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPExporter",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPLogging",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPPgpool2",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPPgpool2Cert",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPRestore",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPUpgrade",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPUser",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPVolume",
          "version": "v1"
        }
      ],
      "provider": "Fujitsu",
      "related_images": [
        {
          "digest": "sha256:a07987c0b4d47c6ec53c2127ead56bb420ea493e9b5d1bb79064b5f1a1f57d91",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-14-server@sha256:a07987c0b4d47c6ec53c2127ead56bb420ea493e9b5d1bb79064b5f1a1f57d91",
          "name": "fep-14"
        },
        {
          "digest": "sha256:8690a50c3c900686fd5e8a65b04fee0125e4cf500c44845f3c4392a1ea1ef873",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-14-backup@sha256:8690a50c3c900686fd5e8a65b04fee0125e4cf500c44845f3c4392a1ea1ef873",
          "name": "backup-14"
        },
        {
          "digest": "sha256:3759bd7513228b49733b23db24fdeb2c6a31b3f93c7ac5f84ab0d8b9939e07ce",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-14-restore@sha256:3759bd7513228b49733b23db24fdeb2c6a31b3f93c7ac5f84ab0d8b9939e07ce",
          "name": "restore-14"
        },
        {
          "digest": "sha256:37729f8332f647eba5f8209ea9ce95aaa9ab6674774afc2931c4aa59dc51ae90",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-14-pgpool2@sha256:37729f8332f647eba5f8209ea9ce95aaa9ab6674774afc2931c4aa59dc51ae90",
          "name": "pgpool2-14"
        },
        {
          "digest": "sha256:f9e28fe00b2caee1b15e4867e86dad39c8e6913f137fe689fd30c18b9f19881c",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-exporter@sha256:f9e28fe00b2caee1b15e4867e86dad39c8e6913f137fe689fd30c18b9f19881c",
          "name": "exporter-14"
        },
        {
          "digest": "sha256:fb8bffe2c1833b7061a4856ecabda811ed16ff661d38b8ea6a5bc826bc0ccbc3",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-fluentd@sha256:fb8bffe2c1833b7061a4856ecabda811ed16ff661d38b8ea6a5bc826bc0ccbc3",
          "name": "fluentd-14"
        },
        {
          "digest": "sha256:326005d0cfb049f36759e6ef022c67029dbac1865810be3c850c1bc1cf604e8d",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-fluentbit@sha256:326005d0cfb049f36759e6ef022c67029dbac1865810be3c850c1bc1cf604e8d",
          "name": "fluentbit-14"
        },
        {
          "digest": "sha256:4f75b63a09ad3da34684db497814bfa8eae8356e43963ce67fbfcb48e4f350ac",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-cronjob@sha256:4f75b63a09ad3da34684db497814bfa8eae8356e43963ce67fbfcb48e4f350ac",
          "name": "cronjob-14"
        },
        {
          "digest": "sha256:793a06eefe93a7613cfdf1a89febf5603244f453a1e804760d93e6f182cc522a",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-14-upgrade@sha256:793a06eefe93a7613cfdf1a89febf5603244f453a1e804760d93e6f182cc522a",
          "name": "upgrade-14"
        },
        {
          "digest": "sha256:80f5d30c66844eb05de5d67999198d9ea3d457f8fbe81358de0b37db0f15f0e3",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-13-server@sha256:80f5d30c66844eb05de5d67999198d9ea3d457f8fbe81358de0b37db0f15f0e3",
          "name": "fep-13"
        },
        {
          "digest": "sha256:c27c4b52361e73dd460eac5d8fe4765f582d0ffd4f4fc072bff4523b785f47e5",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-13-backup@sha256:c27c4b52361e73dd460eac5d8fe4765f582d0ffd4f4fc072bff4523b785f47e5",
          "name": "backup-13"
        },
        {
          "digest": "sha256:99fd4033d9f5e536911915410abfc636b55f09e63f88cf32dbe9f58f79124963",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-13-restore@sha256:99fd4033d9f5e536911915410abfc636b55f09e63f88cf32dbe9f58f79124963",
          "name": "restore-13"
        },
        {
          "digest": "sha256:2f23389e16635ff3e75774dac430c35dc4b314f291d9303d9296852eb0e436bc",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-13-pgpool2@sha256:2f23389e16635ff3e75774dac430c35dc4b314f291d9303d9296852eb0e436bc",
          "name": "pgpool2-13"
        },
        {
          "digest": "sha256:b8c93f9dc83332f547682e7c1bc9a691d20f91793a27cabe1798bd9ae8c96cb2",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-13-exporter@sha256:b8c93f9dc83332f547682e7c1bc9a691d20f91793a27cabe1798bd9ae8c96cb2",
          "name": "fepexporter-13"
        },
        {
          "digest": "sha256:61592bcccccee2e72521d0640f005d1902730319ad697aa0f4fc5485bf22989d",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-12-server@sha256:61592bcccccee2e72521d0640f005d1902730319ad697aa0f4fc5485bf22989d",
          "name": "fep-12"
        },
        {
          "digest": "sha256:f126c128c1301afa684bdf3258c78d1ec68c08aa6e5c085c5f939940947e5c94",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-12-backup@sha256:f126c128c1301afa684bdf3258c78d1ec68c08aa6e5c085c5f939940947e5c94",
          "name": "backup-12"
        },
        {
          "digest": "sha256:240da72ba4ef71031c1587a1f44b7631697346bb7699be52dcebd64e9f93d66e",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-12-restore@sha256:240da72ba4ef71031c1587a1f44b7631697346bb7699be52dcebd64e9f93d66e",
          "name": "restore-12"
        },
        {
          "digest": "sha256:6d58e56e6e7ab0c07d6095b1cab429c47ea3d294bf1656bd0a7d6efed098cb02",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-12-pgpool2@sha256:6d58e56e6e7ab0c07d6095b1cab429c47ea3d294bf1656bd0a7d6efed098cb02",
          "name": "pgpool2-12"
        },
        {
          "digest": "sha256:ecf7b692793b5b7ad2e848f335456742901ef0ef6b0b171d4771f0642e76714e",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-operator@sha256:ecf7b692793b5b7ad2e848f335456742901ef0ef6b0b171d4771f0642e76714e",
          "name": "fujitsu-enterprise-postgres-operator-ecf7b692793b5b7ad2e848f335456742901ef0ef6b0b171d4771f0642e76714e-annotation"
        },
        {
          "digest": "sha256:ecf7b692793b5b7ad2e848f335456742901ef0ef6b0b171d4771f0642e76714e",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-operator@sha256:ecf7b692793b5b7ad2e848f335456742901ef0ef6b0b171d4771f0642e76714e",
          "name": "fep-ansible-operator"
        },
        {
          "digest": "sha256:a07987c0b4d47c6ec53c2127ead56bb420ea493e9b5d1bb79064b5f1a1f57d91",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-14-server@sha256:a07987c0b4d47c6ec53c2127ead56bb420ea493e9b5d1bb79064b5f1a1f57d91",
          "name": "fep"
        },
        {
          "digest": "sha256:8690a50c3c900686fd5e8a65b04fee0125e4cf500c44845f3c4392a1ea1ef873",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-14-backup@sha256:8690a50c3c900686fd5e8a65b04fee0125e4cf500c44845f3c4392a1ea1ef873",
          "name": "backup"
        },
        {
          "digest": "sha256:3759bd7513228b49733b23db24fdeb2c6a31b3f93c7ac5f84ab0d8b9939e07ce",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-14-restore@sha256:3759bd7513228b49733b23db24fdeb2c6a31b3f93c7ac5f84ab0d8b9939e07ce",
          "name": "restore"
        },
        {
          "digest": "sha256:37729f8332f647eba5f8209ea9ce95aaa9ab6674774afc2931c4aa59dc51ae90",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-14-pgpool2@sha256:37729f8332f647eba5f8209ea9ce95aaa9ab6674774afc2931c4aa59dc51ae90",
          "name": "pgpool2"
        },
        {
          "digest": "sha256:f9e28fe00b2caee1b15e4867e86dad39c8e6913f137fe689fd30c18b9f19881c",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-exporter@sha256:f9e28fe00b2caee1b15e4867e86dad39c8e6913f137fe689fd30c18b9f19881c",
          "name": "fepexporter"
        },
        {
          "digest": "sha256:fb8bffe2c1833b7061a4856ecabda811ed16ff661d38b8ea6a5bc826bc0ccbc3",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-fluentd@sha256:fb8bffe2c1833b7061a4856ecabda811ed16ff661d38b8ea6a5bc826bc0ccbc3",
          "name": "feplogging"
        },
        {
          "digest": "sha256:326005d0cfb049f36759e6ef022c67029dbac1865810be3c850c1bc1cf604e8d",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-fluentbit@sha256:326005d0cfb049f36759e6ef022c67029dbac1865810be3c850c1bc1cf604e8d",
          "name": "feplogging_fluentbit"
        },
        {
          "digest": "sha256:4f75b63a09ad3da34684db497814bfa8eae8356e43963ce67fbfcb48e4f350ac",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-cronjob@sha256:4f75b63a09ad3da34684db497814bfa8eae8356e43963ce67fbfcb48e4f350ac",
          "name": "cronjob"
        },
        {
          "digest": "sha256:793a06eefe93a7613cfdf1a89febf5603244f453a1e804760d93e6f182cc522a",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-14-upgrade@sha256:793a06eefe93a7613cfdf1a89febf5603244f453a1e804760d93e6f182cc522a",
          "name": "upgrade"
        },
        {
          "digest": "sha256:a07987c0b4d47c6ec53c2127ead56bb420ea493e9b5d1bb79064b5f1a1f57d91",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-14-server@sha256:a07987c0b4d47c6ec53c2127ead56bb420ea493e9b5d1bb79064b5f1a1f57d91",
          "name": "fep_14"
        },
        {
          "digest": "sha256:8690a50c3c900686fd5e8a65b04fee0125e4cf500c44845f3c4392a1ea1ef873",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-14-backup@sha256:8690a50c3c900686fd5e8a65b04fee0125e4cf500c44845f3c4392a1ea1ef873",
          "name": "backup_14"
        },
        {
          "digest": "sha256:3759bd7513228b49733b23db24fdeb2c6a31b3f93c7ac5f84ab0d8b9939e07ce",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-14-restore@sha256:3759bd7513228b49733b23db24fdeb2c6a31b3f93c7ac5f84ab0d8b9939e07ce",
          "name": "restore_14"
        },
        {
          "digest": "sha256:37729f8332f647eba5f8209ea9ce95aaa9ab6674774afc2931c4aa59dc51ae90",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-14-pgpool2@sha256:37729f8332f647eba5f8209ea9ce95aaa9ab6674774afc2931c4aa59dc51ae90",
          "name": "pgpool2_14"
        },
        {
          "digest": "sha256:f9e28fe00b2caee1b15e4867e86dad39c8e6913f137fe689fd30c18b9f19881c",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-exporter@sha256:f9e28fe00b2caee1b15e4867e86dad39c8e6913f137fe689fd30c18b9f19881c",
          "name": "fepexporter_14"
        },
        {
          "digest": "sha256:fb8bffe2c1833b7061a4856ecabda811ed16ff661d38b8ea6a5bc826bc0ccbc3",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-fluentd@sha256:fb8bffe2c1833b7061a4856ecabda811ed16ff661d38b8ea6a5bc826bc0ccbc3",
          "name": "feplogging_14"
        },
        {
          "digest": "sha256:326005d0cfb049f36759e6ef022c67029dbac1865810be3c850c1bc1cf604e8d",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-fluentbit@sha256:326005d0cfb049f36759e6ef022c67029dbac1865810be3c850c1bc1cf604e8d",
          "name": "feplogging_fluentbit_14"
        },
        {
          "digest": "sha256:4f75b63a09ad3da34684db497814bfa8eae8356e43963ce67fbfcb48e4f350ac",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-cronjob@sha256:4f75b63a09ad3da34684db497814bfa8eae8356e43963ce67fbfcb48e4f350ac",
          "name": "cronjob_14"
        },
        {
          "digest": "sha256:793a06eefe93a7613cfdf1a89febf5603244f453a1e804760d93e6f182cc522a",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-14-upgrade@sha256:793a06eefe93a7613cfdf1a89febf5603244f453a1e804760d93e6f182cc522a",
          "name": "upgrade_14"
        },
        {
          "digest": "sha256:80f5d30c66844eb05de5d67999198d9ea3d457f8fbe81358de0b37db0f15f0e3",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-13-server@sha256:80f5d30c66844eb05de5d67999198d9ea3d457f8fbe81358de0b37db0f15f0e3",
          "name": "fep_13"
        },
        {
          "digest": "sha256:c27c4b52361e73dd460eac5d8fe4765f582d0ffd4f4fc072bff4523b785f47e5",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-13-backup@sha256:c27c4b52361e73dd460eac5d8fe4765f582d0ffd4f4fc072bff4523b785f47e5",
          "name": "backup_13"
        },
        {
          "digest": "sha256:99fd4033d9f5e536911915410abfc636b55f09e63f88cf32dbe9f58f79124963",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-13-restore@sha256:99fd4033d9f5e536911915410abfc636b55f09e63f88cf32dbe9f58f79124963",
          "name": "restore_13"
        },
        {
          "digest": "sha256:2f23389e16635ff3e75774dac430c35dc4b314f291d9303d9296852eb0e436bc",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-13-pgpool2@sha256:2f23389e16635ff3e75774dac430c35dc4b314f291d9303d9296852eb0e436bc",
          "name": "pgpool2_13"
        },
        {
          "digest": "sha256:b8c93f9dc83332f547682e7c1bc9a691d20f91793a27cabe1798bd9ae8c96cb2",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-13-exporter@sha256:b8c93f9dc83332f547682e7c1bc9a691d20f91793a27cabe1798bd9ae8c96cb2",
          "name": "fepexporter_13"
        },
        {
          "digest": "sha256:61592bcccccee2e72521d0640f005d1902730319ad697aa0f4fc5485bf22989d",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-12-server@sha256:61592bcccccee2e72521d0640f005d1902730319ad697aa0f4fc5485bf22989d",
          "name": "fep_12"
        },
        {
          "digest": "sha256:f126c128c1301afa684bdf3258c78d1ec68c08aa6e5c085c5f939940947e5c94",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-12-backup@sha256:f126c128c1301afa684bdf3258c78d1ec68c08aa6e5c085c5f939940947e5c94",
          "name": "backup_12"
        },
        {
          "digest": "sha256:240da72ba4ef71031c1587a1f44b7631697346bb7699be52dcebd64e9f93d66e",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-12-restore@sha256:240da72ba4ef71031c1587a1f44b7631697346bb7699be52dcebd64e9f93d66e",
          "name": "restore_12"
        },
        {
          "digest": "sha256:6d58e56e6e7ab0c07d6095b1cab429c47ea3d294bf1656bd0a7d6efed098cb02",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-12-pgpool2@sha256:6d58e56e6e7ab0c07d6095b1cab429c47ea3d294bf1656bd0a7d6efed098cb02",
          "name": "pgpool2_12"
        }
      ],
      "replaces": null,
      "skip_range": "<4.1.9",
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.12",
      "version": "4.1.9",
      "version_original": "4.1.9"
    },
    {
      "_id": "6386071def2c70d7276b44d7",
      "alm_examples": [
        {
          "api_version": "fep.fujitsu.io/v1",
          "kind": "FEPAction",
          "metadata": {
            "name": "new-fep-action"
          },
          "spec": {
            "fepAction": {
              "args": [
                "new-fep-sts-0",
                "new-fep-sts-1"
              ],
              "type": "reload"
            },
            "sysExtraLogging": false,
            "targetClusterName": "new-fep"
          }
        },
        {
          "api_version": "fep.fujitsu.io/v1",
          "kind": "FEPExporter",
          "metadata": {
            "name": "new-fep-exporter"
          },
          "spec": {
            "fepExporter": {
              "exporterLogLevel": "error",
              "fepClusterList": [
                "new-fep1"
              ],
              "image": {
                "pullPolicy": "IfNotPresent"
              },
              "mcSpec": {
                "limits": {
                  "cpu": "500m",
                  "memory": "700Mi"
                },
                "requests": {
                  "cpu": "200m",
                  "memory": "512Mi"
                }
              },
              "restartRequired": false,
              "sysExtraLogging": false,
              "userCustomQueries": "usr_example:\n  query: \"SELECT EXTRACT(EPOCH FROM (now() - pg_last_xact_replay_timestamp())) as lag\"\n  master: true\n  metrics:\n    - lag:\n        usage: \"GAUGE\"\n        description: \"Replication lag behind master in seconds\""
            }
          }
        },
        {
          "api_version": "fep.fujitsu.io/v1",
          "kind": "FEPLogging",
          "metadata": {
            "name": "new-fep-logging"
          },
          "spec": {
            "fepLogging": {
              "image": {
                "pullPolicy": "IfNotPresent"
              },
              "mcSpec": {
                "limits": {
                  "cpu": "500m",
                  "memory": "700Mi"
                },
                "requests": {
                  "cpu": "200m",
                  "memory": "512Mi"
                }
              },
              "restartRequired": false,
              "sysExtraLogging": false
            }
          }
        },
        {
          "api_version": "fep.fujitsu.io/v1",
          "kind": "FEPPgpool2",
          "metadata": {
            "name": "new-fep-pgpool2"
          },
          "spec": {
            "count": 2,
            "customhba": "local   all         all                               trust\nhost    all         all         127.0.0.1/32          trust\nhost    all         all         ::1/128               trust\n",
            "customlogsize": "128Mi",
            "customparams": "listen_addresses = '*'\npcp_listen_addresses = '*'\nnum_init_children = 32\nreserved_connections = 0\nenable_pool_hba = off\nallow_clear_text_frontend_auth = off\nauthentication_timeout = 80\nbackend_weight0 = 1\nbackend_weight1 = 1\nbackend_flag0 = 'ALWAYS_PRIMARY'\nbackend_flag1 = 'DISALLOW_TO_FAILOVER'\nconnection_cache = on\nmax_pool = 4\nlisten_backlog_multiplier = 2\nserialize_accept = off\nchild_life_time = 300\nclient_idle_limit = 0\nchild_max_connections = 0\nconnection_life_time = 0\nreset_query_list = 'ABORT; DISCARD ALL'\nclient_min_messages = info\nlog_min_messages = debug1\nlog_statement = on\nlog_per_node_statement = on\nlog_client_messages = on\nlog_hostname = on\nlog_connections = on\nlog_line_prefix = '%t: pid %p: '\nload_balance_mode = on\nignore_leading_white_space = on\nwhite_function_list = ''\nblack_function_list = 'currval,lastval,nextval,setval'\nblack_query_pattern_list = ''\ndatabase_redirect_preference_list = ''\napp_name_redirect_preference_list = ''\nallow_sql_comments = off\ndisable_load_balance_on_write = 'transaction'\nstatement_level_load_balance = on\nsr_check_period = 0\nsr_check_user = 'postgres'\ndelay_threshold = 0\nlog_standby_delay = 'none'\nssl = on\nssl_ciphers = 'HIGH:MEDIUM:+3DES:!aNULL'\nssl_prefer_server_ciphers = off\nssl_ecdh_curve = 'prime256v1'\nssl_dh_params_file = ''\nrelcache_expire = 0\nrelcache_size = 256\ncheck_temp_table = catalog\ncheck_unlogged_table = on\nenable_shared_relcache = on\nrelcache_query_target = primary\nwd_port0 = 9000\nfailover_on_backend_error = off\n",
            "custompcp": "none",
            "customsslcacert": "none",
            "customsslcert": "-----BEGIN CERTIFICATE-----\nMIIDTzCCAjegAwIBAgIUYssQ8I74US5g+1+Z7CHuaDgkZnEwDQYJKoZIhvcNAQEL\nBQAwNzEQMA4GA1UECgwHRnVqaXRzdTEjMCEGA1UEAwwaRkVQIFJvb3QgQ0EgZm9y\nIEt1YmVybmV0ZXMwHhcNMjEwMjA2MDM1MjI4WhcNMzEwMjA0MDM1MjI4WjA3MRAw\nDgYDVQQKDAdGdWppdHN1MSMwIQYDVQQDDBpGRVAgUm9vdCBDQSBmb3IgS3ViZXJu\nZXRlczCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBAMs97gUF0xkUzCgL\n7MiiDju9ySr/ziwjvcYU7jA9ML+SLmftMs3HtcYbAmSntqI+MDBSR/FAJTOoytuT\npV+mCFcGj2YAjDpliHPeNcUpbryy4YMChF3+MovkIwGCksxo5rhiWhGmoBYpA48P\n4Xe8SPlzqMzhFvNeKzyiUhvjutS2Y1Ss38lsTaurFPx64vQ2PaC54XzdwMptXtpb\ntYmWSzCpJWwxZ6lF3vitdA2w0tnBWNyctAd0+RIM/fvArxiIqseAux9t0uogm5to\nlRIhvekuxOpXBPEqtIYQ4j9XUW2JH8vUDnzPkPvjrq+A3Ug8OyyfGVrW7+VYXozu\nc4aP7P0CAwEAAaNTMFEwHQYDVR0OBBYEFBzCutQ7S74WEhS5V2sNEJBGyLpmMB8G\nA1UdIwQYMBaAFBzCutQ7S74WEhS5V2sNEJBGyLpmMA8GA1UdEwEB/wQFMAMBAf8w\nDQYJKoZIhvcNAQELBQADggEBAMDwD85RAaWEBptFgLzKw+9xEUy1vcZaonAuA1qc\nT342XTueyAugxkC11HwdCGgGS34VyctfMGqj4AW6pA2ez4tLrbOps4DmV4sw8uBL\n8pgRDgfly3ob9FEg2wa0hmrwX9jH5Bt4vySUE2785uPAqaspT2UNtTBxS85BUi1T\nsKId2Rtil6an281Z81wyWVI6Jm2D4MG0mbsiGcTPlCtdg/UljvDYymXlAvd4vNhl\nk9hDa13TgDqJKgKdTIcmZoNQdpEVgFcO0h9AEUy5AuLqxHq60dLfZ6ESGPlMI7Lm\ni4PzYbCnBmOe+7TnHcPSyrnehs66Ik+oifRd82eYS7vKjFw=\n-----END CERTIFICATE-----",
            "customsslkey": "none",
            "fepclustername": "new-fep",
            "imagePullPolicy": "IfNotPresent",
            "limits": {
              "cpu": "400m",
              "memory": "512Mi"
            },
            "requests": {
              "cpu": "200m",
              "memory": "256Mi"
            },
            "serviceport": 9999,
            "statusport": 9898
          }
        },
        {
          "api_version": "fep.fujitsu.io/v1",
          "kind": "FEPRestore",
          "metadata": {
            "name": "new-fep-restore"
          },
          "spec": {
            "fromFEPcluster": "new-fep",
            "imagePullPolicy": "IfNotPresent",
            "mcSpec": {
              "limits": {
                "cpu": "200m",
                "memory": "300Mi"
              },
              "requests": {
                "cpu": "100m",
                "memory": "200Mi"
              }
            },
            "restoretype": "latest",
            "sysExtraLogging": false,
            "toFEPcluster": "new-fep-2"
          }
        },
        {
          "api_version": "fep.fujitsu.io/v2",
          "kind": "FEPCluster",
          "metadata": {
            "name": "new-fep"
          },
          "spec": {
            "fep": {
              "customAnnotations": {
                "allDeployments": {}
              },
              "forceSsl": true,
              "image": {
                "pullPolicy": "IfNotPresent"
              },
              "instances": 1,
              "mcSpec": {
                "limits": {
                  "cpu": "500m",
                  "memory": "700Mi"
                },
                "requests": {
                  "cpu": "200m",
                  "memory": "512Mi"
                }
              },
              "podAntiAffinity": false,
              "podDisruptionBudget": false,
              "servicePort": 27500,
              "syncMode": "off",
              "sysExtraLogging": false
            },
            "fepChildCrVal": {
              "backup": {
                "image": {
                  "pullPolicy": "IfNotPresent"
                },
                "mcSpec": {
                  "limits": {
                    "cpu": 0.2,
                    "memory": "300Mi"
                  },
                  "requests": {
                    "cpu": 0.1,
                    "memory": "200Mi"
                  }
                },
                "pgbackrestParams": "# if log volume is not defined, log_directory should be\n# changed to '/database/userdata/data/log'\n[global]\nrepo1-retention-full=7\nrepo1-retention-full-type=time\nlog-path=/database/log/backup\n",
                "postScript": " ",
                "preScript": " ",
                "schedule": {
                  "num": 2
                },
                "schedule1": {
                  "schedule": "15 0 * * 0",
                  "type": "full"
                },
                "schedule2": {
                  "schedule": "15 0 * * 1-6",
                  "type": "incr"
                },
                "schedule3": {
                  "schedule": " ",
                  "type": " "
                },
                "schedule4": {
                  "schedule": " ",
                  "type": " "
                },
                "schedule5": {
                  "schedule": " ",
                  "type": " "
                }
              },
              "customPgAudit": "# define pg audit custom params here to override defaults.\n# if log volume is not defined, log_directory should be\n# changed to '/database/userdata/data/log'\n[output]\nlogger = 'auditlog'\nlog_directory = '/database/log/audit'\nlog_truncate_on_rotation = on\nlog_filename = 'pgaudit-%a.log'\nlog_rotation_age = 1d\nlog_rotation_size = 0\n[rule]\n",
              "customPgHba": "# define pg_hba custom rules here to be merged with default rules.\n# TYPE     DATABASE        USER        ADDRESS        METHOD\n",
              "customPgParams": "# define custom postgresql.conf parameters below to override defaults.\n# Current values are as per default FEP deployment\nshared_preload_libraries='pgx_datamasking,pg_prewarm,pg_stat_statements'\nsession_preload_libraries='pg_prewarm'\nmax_prepared_transactions = 100\nmax_worker_processes = 30\nmax_connections = 100\nwork_mem = 1MB\nmaintenance_work_mem = 12MB\nshared_buffers = 128MB\neffective_cache_size = 384MB\ncheckpoint_completion_target = 0.8\n# tcp parameters\ntcp_keepalives_idle = 30\ntcp_keepalives_interval = 10\ntcp_keepalives_count = 3\n# logging parameters in default fep installation\n# if log volume is not defined, log_directory should be\n# changed to '/database/userdata/data/log'\nlog_directory = '/database/log'\nlog_filename = 'logfile-%a.log'\nlog_file_mode = 0600\nlog_truncate_on_rotation = on\nlog_rotation_age = 1d\nlog_rotation_size = 0\nlog_checkpoints = on\nlog_line_prefix = '%e %t [%p]: [%l-1] user=%u,db=%d,app=%a,client=%h'\nlog_lock_waits = on\nlog_autovacuum_min_duration = 60s\nlogging_collector = on\npgaudit.config_file='/opt/app-root/src/pgaudit-cfg/pgaudit.conf'\nlog_replication_commands = on\nlog_min_messages = WARNING\nlog_destination = csvlog\n# wal_archive parameters in default fep installation\narchive_mode = on\narchive_command = 'pgbackrest --stanza=backupstanza --config=/database/userdata/pgbackrest.conf archive-push %p'\nwal_level = replica\nmax_wal_senders = 12\nwal_keep_segments = 64\ntrack_activities = on\ntrack_counts = on\npassword_encryption = 'md5'\n",
              "storage": {
                "archivewalVol": {
                  "size": "1Gi"
                },
                "backupVol": {
                  "size": "2Gi"
                },
                "dataVol": {
                  "size": "2Gi"
                },
                "logVol": {
                  "size": "1Gi"
                },
                "tablespaceVol": {
                  "size": "512Mi"
                },
                "walVol": {
                  "size": "1200Mi"
                }
              },
              "sysUsers": {
                "pgAdminPassword": "admin-password",
                "pgRewindPassword": "rewind_password",
                "pgRewindUser": "rewind_user",
                "pgdb": "mydb",
                "pgpassword": "mydbpassword",
                "pgreplpassword": "repluserpwd",
                "pgrepluser": "repluser",
                "pguser": "mydbuser",
                "tdepassphrase": "tde-passphrase"
              },
              "systemCertificates": {
                "cacrt": "-----BEGIN CERTIFICATE-----\nMIIDTzCCAjegAwIBAgIUYssQ8I74US5g+1+Z7CHuaDgkZnEwDQYJKoZIhvcNAQEL\nBQAwNzEQMA4GA1UECgwHRnVqaXRzdTEjMCEGA1UEAwwaRkVQIFJvb3QgQ0EgZm9y\nIEt1YmVybmV0ZXMwHhcNMjEwMjA2MDM1MjI4WhcNMzEwMjA0MDM1MjI4WjA3MRAw\nDgYDVQQKDAdGdWppdHN1MSMwIQYDVQQDDBpGRVAgUm9vdCBDQSBmb3IgS3ViZXJu\nZXRlczCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBAMs97gUF0xkUzCgL\n7MiiDju9ySr/ziwjvcYU7jA9ML+SLmftMs3HtcYbAmSntqI+MDBSR/FAJTOoytuT\npV+mCFcGj2YAjDpliHPeNcUpbryy4YMChF3+MovkIwGCksxo5rhiWhGmoBYpA48P\n4Xe8SPlzqMzhFvNeKzyiUhvjutS2Y1Ss38lsTaurFPx64vQ2PaC54XzdwMptXtpb\ntYmWSzCpJWwxZ6lF3vitdA2w0tnBWNyctAd0+RIM/fvArxiIqseAux9t0uogm5to\nlRIhvekuxOpXBPEqtIYQ4j9XUW2JH8vUDnzPkPvjrq+A3Ug8OyyfGVrW7+VYXozu\nc4aP7P0CAwEAAaNTMFEwHQYDVR0OBBYEFBzCutQ7S74WEhS5V2sNEJBGyLpmMB8G\nA1UdIwQYMBaAFBzCutQ7S74WEhS5V2sNEJBGyLpmMA8GA1UdEwEB/wQFMAMBAf8w\nDQYJKoZIhvcNAQELBQADggEBAMDwD85RAaWEBptFgLzKw+9xEUy1vcZaonAuA1qc\nT342XTueyAugxkC11HwdCGgGS34VyctfMGqj4AW6pA2ez4tLrbOps4DmV4sw8uBL\n8pgRDgfly3ob9FEg2wa0hmrwX9jH5Bt4vySUE2785uPAqaspT2UNtTBxS85BUi1T\nsKId2Rtil6an281Z81wyWVI6Jm2D4MG0mbsiGcTPlCtdg/UljvDYymXlAvd4vNhl\nk9hDa13TgDqJKgKdTIcmZoNQdpEVgFcO0h9AEUy5AuLqxHq60dLfZ6ESGPlMI7Lm\ni4PzYbCnBmOe+7TnHcPSyrnehs66Ik+oifRd82eYS7vKjFw=\n-----END CERTIFICATE-----",
                "crt": "-----BEGIN CERTIFICATE-----\nMIIDUTCCAjmgAwIBAgIRAMocW3qMoHrD6qRvMPppMkMwDQYJKoZIhvcNAQELBQAw\nNzEQMA4GA1UECgwHRnVqaXRzdTEjMCEGA1UEAwwaRkVQIFJvb3QgQ0EgZm9yIEt1\nYmVybmV0ZXMwHhcNMjEwMjA2MDQzMjM2WhcNMjYwMjA1MDQzMjM2WjA/MRAwDgYD\nVQQKEwdGdWppdHN1MSswKQYDVQQDEyJGVUpJVFNVIEVudGVycHJpc2UgUG9zdGdy\nZXMgU2VydmVyMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEA4AI33yvH\nZws+jta6qpV6wzJqF8odIfTIpCfbrVcUUtLFKJ1I2e4SceTKi6O3C/I1XuvWlpng\n5IO65+fQQLO06z1/AuQT78YUn/Wlm9x1aHVsv4ANB5JWWqDOjrRT3o7nRPGXfila\nbP0rGE2mJJcVR9nExJ3IeaktgT3sb8YlXvtchyYpmjdbfxabTz07ig0+6/cwKoRR\nxOK8Uf7f5euE0cI/490J6r5Rs4lgD8sIQNCUFlTFYvmAH7gcdssSFBt8NPlUATHE\nsoFmlW0DKCJWNhTLOht+s6L/1zwTHLjPG2pdkG6Wdgmu5H2pDml8CDNLDv98Aj7i\n+I5SRKKcVPlnuQIDAQABo1AwTjAdBgNVHSUEFjAUBggrBgEFBQcDAQYIKwYBBQUH\nAwIwDAYDVR0TAQH/BAIwADAfBgNVHSMEGDAWgBQcwrrUO0u+FhIUuVdrDRCQRsi6\nZjANBgkqhkiG9w0BAQsFAAOCAQEAm5dxBoI9pScOCvRAchg4CprdRDSJb9K6yB3O\nnCAxnM47iHeXnY3WlnI388kHu8DU7O4ba1tJbGs3KY9KzioPk43pU12jWkO1onoF\n+mTDjx/Ef1cYWA9r5q/LtgTa6Q2sxV4O2x67QW82aAnaxO34dV5zWCPIvAoovZBV\nHRT+BgCg3r2vD1RGKK2nl1aYJtWhO1SZubam+VttdZ/vbM9oOJctxmImsEtBXjkY\nKteePdQtLL5o03JhyXWyRshCq+HMmKf2KgyY8gvydGcP4eLQdBWcW40LcnVq6UjT\n0kJycJEKngMVademq1ZWHGaiYB7hyT6GhgIcHUJ2cKrPgbEh1Q==\n-----END CERTIFICATE-----",
                "key": "-----BEGIN RSA PRIVATE KEY-----\nMIIEowIBAAKCAQEA4AI33yvHZws+jta6qpV6wzJqF8odIfTIpCfbrVcUUtLFKJ1I\n2e4SceTKi6O3C/I1XuvWlpng5IO65+fQQLO06z1/AuQT78YUn/Wlm9x1aHVsv4AN\nB5JWWqDOjrRT3o7nRPGXfilabP0rGE2mJJcVR9nExJ3IeaktgT3sb8YlXvtchyYp\nmjdbfxabTz07ig0+6/cwKoRRxOK8Uf7f5euE0cI/490J6r5Rs4lgD8sIQNCUFlTF\nYvmAH7gcdssSFBt8NPlUATHEsoFmlW0DKCJWNhTLOht+s6L/1zwTHLjPG2pdkG6W\ndgmu5H2pDml8CDNLDv98Aj7i+I5SRKKcVPlnuQIDAQABAoIBAFPQYKlOzw/+BA0b\nyMIUpdctIMb/54CR/xR0mVw1DbSjigNVPjHUQvB8Y1B2FAITQObgJO06bAv0QdWN\nRb0/v/yYiNJDFjaLjaIAHlO/2+oWrXbFaZqgpVDJhB+e1xaZr2x7XGxm+p925k30\nl6pvIRY+I8JRKvZiV1VZHwL/R3JOtPr++xMZtLVjVOI+f+ySqJ+TZHuAjm49EKxj\ncEmmJ28b7QcziXsvKy00f+zbqLIBKXQdZAFU5eEr1BsDRXdRW+Kf0XIvftuy4BJZ\nvoKT+VGhEvF/qysswL4+6IAO6tpuYnnM0Y2d3sOGoWPkTcQK0MekYKzL/WmtCjNs\n9hodJtECgYEA5EWyhEOf4uOKe5TDp697UCUvXLoOR58FDe/S8XNvScn29jjOkqIg\nOMoqo9xAkJTNTzqn5UUdt1x/pgM2NxlPLFijrc0zQlX3SoOO2ryDd9WNi7YKtN16\nKJqa536WeZu2OEbuAZ+S3GALVy1RPeTNPnUOmKnF06DjDUGzLNCZy10CgYEA+zfw\n952DWuz1U0Z4wvAEqqcgUKXPKrkTXV/iUnjkDkrLYVr0ZofDNTXrdHl+UedFmaOC\ncieZn6DNhcdz5tKtyysGMH3g/qs9PfoGUngvcXsy0Egk04l3x1jc8TTCLqXZXYaQ\nHMsx51n+R58oncPtzYSUOr9qQ6PbC2CstTbFJA0CgYEAjGEsUliAB/jknfEzjXjG\nPdhQUxb8VyE864Az2lah9t/kJzFyIAziAeqZ5GE7t247AGFTBRTHHI8e1Qoemi3P\nWbc9GVIbFs1lIYbcIDpUIyrKPEP8O5QEXtoNLxXTFgAjRGKiVY87spjCAJ+W2ZhO\ne/1it5GYXfgQCYQA2yuBmOUCgYANRkR2YR1axaCk+NlSu6oTdmdPu6M5x7PNQE7O\nOtMaKjua9lppvIzFGAdMDUtueoEEAE7ZR1xnwfB6PDLUpJdIYAqgr1YfPt8qkjaZ\nTv56yZ7CwL0pbF8m6nwqRrZoDp1wwraEvvvxFKFKGY/k3kCHlpTakdjEoDjn3gDi\nRnWeVQKBgCEneMSzucei5LRppRtRaJw/Btll8qlPMlX3W7dxQ3cLwpmLOn0m51Fp\nPIZ44zYK8R6fu4+/sSrlfaIg86Ugeufp6YNxyNROKxUGza5vDIu5OftwWtBeg+UK\nZ8lLWNdX6pp7WMujmF3H1DrkBbauYMUKZ4UxUYtelgHERMePIxwb\n-----END RSA PRIVATE KEY-----"
              }
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [
        "amd64",
        "ppc64le",
        "s390x"
      ],
      "bundle_path": "registry.connect.redhat.com/fujitsu-postgres/fujitsu-enterprise-postgres-operator-bundle@sha256:94fa991b1fdaa56ee907acd562a163dcc52284719b61188f823f2b0f6dc6f436",
      "bundle_path_digest": "sha256:94fa991b1fdaa56ee907acd562a163dcc52284719b61188f823f2b0f6dc6f436",
      "capabilities": [
        "Auto Pilot"
      ],
      "channel_name": "stable",
      "creation_date": "2022-11-29T13:20:29.391000+00:00",
      "csv_description": "FUJITSU Enterprise Postgres 14 delivers an enterprise-grade PostgreSQL on OpenShift Container Platform.\n\nThis solution provides the flexibility of a hybrid cloud solution while delivering an enhanced distribution of PostgreSQL to support enterprise-level workloads and provide improved deployment and management, availability, performance, data governance and security.  \n\nAvailable as a multi-architecture container built for both amd64, s390x and ppc64le.\n\nThe download and Use of the Product is strictly subject to the terms of the End User License Agreement with Fujitsu Limited found at https://www.fast.fujitsu.com/fujitsu-enterprise-postgres-license-agreements. Where the Product that has been embedded as a whole or part into a third party program, only Authorised Customers may download and use the Product.\n",
      "csv_display_name": "FUJITSU Enterprise Postgres Operator",
      "csv_metadata_description": "OpenShift Operator for Fujitsu Enterprise Postgres",
      "csv_name": "fujitsu-enterprise-postgres-operator.v4.1.9",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": true,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-11-29T13:20:29.391000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.11",
      "organization": "certified-operators",
      "package": "fujitsu-enterprise-postgres-operator",
      "provided_apis": [
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPAction",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPAutoscale",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPBackup",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPCert",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPCluster",
          "version": "v2"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPConfig",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPExporter",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPLogging",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPPgpool2",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPPgpool2Cert",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPRestore",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPUpgrade",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPUser",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPVolume",
          "version": "v1"
        }
      ],
      "provider": "Fujitsu",
      "related_images": [
        {
          "digest": "sha256:a07987c0b4d47c6ec53c2127ead56bb420ea493e9b5d1bb79064b5f1a1f57d91",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-14-server@sha256:a07987c0b4d47c6ec53c2127ead56bb420ea493e9b5d1bb79064b5f1a1f57d91",
          "name": "fep-14"
        },
        {
          "digest": "sha256:8690a50c3c900686fd5e8a65b04fee0125e4cf500c44845f3c4392a1ea1ef873",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-14-backup@sha256:8690a50c3c900686fd5e8a65b04fee0125e4cf500c44845f3c4392a1ea1ef873",
          "name": "backup-14"
        },
        {
          "digest": "sha256:3759bd7513228b49733b23db24fdeb2c6a31b3f93c7ac5f84ab0d8b9939e07ce",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-14-restore@sha256:3759bd7513228b49733b23db24fdeb2c6a31b3f93c7ac5f84ab0d8b9939e07ce",
          "name": "restore-14"
        },
        {
          "digest": "sha256:37729f8332f647eba5f8209ea9ce95aaa9ab6674774afc2931c4aa59dc51ae90",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-14-pgpool2@sha256:37729f8332f647eba5f8209ea9ce95aaa9ab6674774afc2931c4aa59dc51ae90",
          "name": "pgpool2-14"
        },
        {
          "digest": "sha256:f9e28fe00b2caee1b15e4867e86dad39c8e6913f137fe689fd30c18b9f19881c",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-exporter@sha256:f9e28fe00b2caee1b15e4867e86dad39c8e6913f137fe689fd30c18b9f19881c",
          "name": "exporter-14"
        },
        {
          "digest": "sha256:fb8bffe2c1833b7061a4856ecabda811ed16ff661d38b8ea6a5bc826bc0ccbc3",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-fluentd@sha256:fb8bffe2c1833b7061a4856ecabda811ed16ff661d38b8ea6a5bc826bc0ccbc3",
          "name": "fluentd-14"
        },
        {
          "digest": "sha256:326005d0cfb049f36759e6ef022c67029dbac1865810be3c850c1bc1cf604e8d",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-fluentbit@sha256:326005d0cfb049f36759e6ef022c67029dbac1865810be3c850c1bc1cf604e8d",
          "name": "fluentbit-14"
        },
        {
          "digest": "sha256:4f75b63a09ad3da34684db497814bfa8eae8356e43963ce67fbfcb48e4f350ac",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-cronjob@sha256:4f75b63a09ad3da34684db497814bfa8eae8356e43963ce67fbfcb48e4f350ac",
          "name": "cronjob-14"
        },
        {
          "digest": "sha256:793a06eefe93a7613cfdf1a89febf5603244f453a1e804760d93e6f182cc522a",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-14-upgrade@sha256:793a06eefe93a7613cfdf1a89febf5603244f453a1e804760d93e6f182cc522a",
          "name": "upgrade-14"
        },
        {
          "digest": "sha256:80f5d30c66844eb05de5d67999198d9ea3d457f8fbe81358de0b37db0f15f0e3",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-13-server@sha256:80f5d30c66844eb05de5d67999198d9ea3d457f8fbe81358de0b37db0f15f0e3",
          "name": "fep-13"
        },
        {
          "digest": "sha256:c27c4b52361e73dd460eac5d8fe4765f582d0ffd4f4fc072bff4523b785f47e5",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-13-backup@sha256:c27c4b52361e73dd460eac5d8fe4765f582d0ffd4f4fc072bff4523b785f47e5",
          "name": "backup-13"
        },
        {
          "digest": "sha256:99fd4033d9f5e536911915410abfc636b55f09e63f88cf32dbe9f58f79124963",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-13-restore@sha256:99fd4033d9f5e536911915410abfc636b55f09e63f88cf32dbe9f58f79124963",
          "name": "restore-13"
        },
        {
          "digest": "sha256:2f23389e16635ff3e75774dac430c35dc4b314f291d9303d9296852eb0e436bc",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-13-pgpool2@sha256:2f23389e16635ff3e75774dac430c35dc4b314f291d9303d9296852eb0e436bc",
          "name": "pgpool2-13"
        },
        {
          "digest": "sha256:b8c93f9dc83332f547682e7c1bc9a691d20f91793a27cabe1798bd9ae8c96cb2",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-13-exporter@sha256:b8c93f9dc83332f547682e7c1bc9a691d20f91793a27cabe1798bd9ae8c96cb2",
          "name": "fepexporter-13"
        },
        {
          "digest": "sha256:61592bcccccee2e72521d0640f005d1902730319ad697aa0f4fc5485bf22989d",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-12-server@sha256:61592bcccccee2e72521d0640f005d1902730319ad697aa0f4fc5485bf22989d",
          "name": "fep-12"
        },
        {
          "digest": "sha256:f126c128c1301afa684bdf3258c78d1ec68c08aa6e5c085c5f939940947e5c94",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-12-backup@sha256:f126c128c1301afa684bdf3258c78d1ec68c08aa6e5c085c5f939940947e5c94",
          "name": "backup-12"
        },
        {
          "digest": "sha256:240da72ba4ef71031c1587a1f44b7631697346bb7699be52dcebd64e9f93d66e",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-12-restore@sha256:240da72ba4ef71031c1587a1f44b7631697346bb7699be52dcebd64e9f93d66e",
          "name": "restore-12"
        },
        {
          "digest": "sha256:6d58e56e6e7ab0c07d6095b1cab429c47ea3d294bf1656bd0a7d6efed098cb02",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-12-pgpool2@sha256:6d58e56e6e7ab0c07d6095b1cab429c47ea3d294bf1656bd0a7d6efed098cb02",
          "name": "pgpool2-12"
        },
        {
          "digest": "sha256:ecf7b692793b5b7ad2e848f335456742901ef0ef6b0b171d4771f0642e76714e",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-operator@sha256:ecf7b692793b5b7ad2e848f335456742901ef0ef6b0b171d4771f0642e76714e",
          "name": "fujitsu-enterprise-postgres-operator-ecf7b692793b5b7ad2e848f335456742901ef0ef6b0b171d4771f0642e76714e-annotation"
        },
        {
          "digest": "sha256:ecf7b692793b5b7ad2e848f335456742901ef0ef6b0b171d4771f0642e76714e",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-operator@sha256:ecf7b692793b5b7ad2e848f335456742901ef0ef6b0b171d4771f0642e76714e",
          "name": "fep-ansible-operator"
        },
        {
          "digest": "sha256:a07987c0b4d47c6ec53c2127ead56bb420ea493e9b5d1bb79064b5f1a1f57d91",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-14-server@sha256:a07987c0b4d47c6ec53c2127ead56bb420ea493e9b5d1bb79064b5f1a1f57d91",
          "name": "fep"
        },
        {
          "digest": "sha256:8690a50c3c900686fd5e8a65b04fee0125e4cf500c44845f3c4392a1ea1ef873",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-14-backup@sha256:8690a50c3c900686fd5e8a65b04fee0125e4cf500c44845f3c4392a1ea1ef873",
          "name": "backup"
        },
        {
          "digest": "sha256:3759bd7513228b49733b23db24fdeb2c6a31b3f93c7ac5f84ab0d8b9939e07ce",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-14-restore@sha256:3759bd7513228b49733b23db24fdeb2c6a31b3f93c7ac5f84ab0d8b9939e07ce",
          "name": "restore"
        },
        {
          "digest": "sha256:37729f8332f647eba5f8209ea9ce95aaa9ab6674774afc2931c4aa59dc51ae90",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-14-pgpool2@sha256:37729f8332f647eba5f8209ea9ce95aaa9ab6674774afc2931c4aa59dc51ae90",
          "name": "pgpool2"
        },
        {
          "digest": "sha256:f9e28fe00b2caee1b15e4867e86dad39c8e6913f137fe689fd30c18b9f19881c",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-exporter@sha256:f9e28fe00b2caee1b15e4867e86dad39c8e6913f137fe689fd30c18b9f19881c",
          "name": "fepexporter"
        },
        {
          "digest": "sha256:fb8bffe2c1833b7061a4856ecabda811ed16ff661d38b8ea6a5bc826bc0ccbc3",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-fluentd@sha256:fb8bffe2c1833b7061a4856ecabda811ed16ff661d38b8ea6a5bc826bc0ccbc3",
          "name": "feplogging"
        },
        {
          "digest": "sha256:326005d0cfb049f36759e6ef022c67029dbac1865810be3c850c1bc1cf604e8d",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-fluentbit@sha256:326005d0cfb049f36759e6ef022c67029dbac1865810be3c850c1bc1cf604e8d",
          "name": "feplogging_fluentbit"
        },
        {
          "digest": "sha256:4f75b63a09ad3da34684db497814bfa8eae8356e43963ce67fbfcb48e4f350ac",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-cronjob@sha256:4f75b63a09ad3da34684db497814bfa8eae8356e43963ce67fbfcb48e4f350ac",
          "name": "cronjob"
        },
        {
          "digest": "sha256:793a06eefe93a7613cfdf1a89febf5603244f453a1e804760d93e6f182cc522a",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-14-upgrade@sha256:793a06eefe93a7613cfdf1a89febf5603244f453a1e804760d93e6f182cc522a",
          "name": "upgrade"
        },
        {
          "digest": "sha256:a07987c0b4d47c6ec53c2127ead56bb420ea493e9b5d1bb79064b5f1a1f57d91",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-14-server@sha256:a07987c0b4d47c6ec53c2127ead56bb420ea493e9b5d1bb79064b5f1a1f57d91",
          "name": "fep_14"
        },
        {
          "digest": "sha256:8690a50c3c900686fd5e8a65b04fee0125e4cf500c44845f3c4392a1ea1ef873",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-14-backup@sha256:8690a50c3c900686fd5e8a65b04fee0125e4cf500c44845f3c4392a1ea1ef873",
          "name": "backup_14"
        },
        {
          "digest": "sha256:3759bd7513228b49733b23db24fdeb2c6a31b3f93c7ac5f84ab0d8b9939e07ce",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-14-restore@sha256:3759bd7513228b49733b23db24fdeb2c6a31b3f93c7ac5f84ab0d8b9939e07ce",
          "name": "restore_14"
        },
        {
          "digest": "sha256:37729f8332f647eba5f8209ea9ce95aaa9ab6674774afc2931c4aa59dc51ae90",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-14-pgpool2@sha256:37729f8332f647eba5f8209ea9ce95aaa9ab6674774afc2931c4aa59dc51ae90",
          "name": "pgpool2_14"
        },
        {
          "digest": "sha256:f9e28fe00b2caee1b15e4867e86dad39c8e6913f137fe689fd30c18b9f19881c",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-exporter@sha256:f9e28fe00b2caee1b15e4867e86dad39c8e6913f137fe689fd30c18b9f19881c",
          "name": "fepexporter_14"
        },
        {
          "digest": "sha256:fb8bffe2c1833b7061a4856ecabda811ed16ff661d38b8ea6a5bc826bc0ccbc3",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-fluentd@sha256:fb8bffe2c1833b7061a4856ecabda811ed16ff661d38b8ea6a5bc826bc0ccbc3",
          "name": "feplogging_14"
        },
        {
          "digest": "sha256:326005d0cfb049f36759e6ef022c67029dbac1865810be3c850c1bc1cf604e8d",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-fluentbit@sha256:326005d0cfb049f36759e6ef022c67029dbac1865810be3c850c1bc1cf604e8d",
          "name": "feplogging_fluentbit_14"
        },
        {
          "digest": "sha256:4f75b63a09ad3da34684db497814bfa8eae8356e43963ce67fbfcb48e4f350ac",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-cronjob@sha256:4f75b63a09ad3da34684db497814bfa8eae8356e43963ce67fbfcb48e4f350ac",
          "name": "cronjob_14"
        },
        {
          "digest": "sha256:793a06eefe93a7613cfdf1a89febf5603244f453a1e804760d93e6f182cc522a",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-14-upgrade@sha256:793a06eefe93a7613cfdf1a89febf5603244f453a1e804760d93e6f182cc522a",
          "name": "upgrade_14"
        },
        {
          "digest": "sha256:80f5d30c66844eb05de5d67999198d9ea3d457f8fbe81358de0b37db0f15f0e3",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-13-server@sha256:80f5d30c66844eb05de5d67999198d9ea3d457f8fbe81358de0b37db0f15f0e3",
          "name": "fep_13"
        },
        {
          "digest": "sha256:c27c4b52361e73dd460eac5d8fe4765f582d0ffd4f4fc072bff4523b785f47e5",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-13-backup@sha256:c27c4b52361e73dd460eac5d8fe4765f582d0ffd4f4fc072bff4523b785f47e5",
          "name": "backup_13"
        },
        {
          "digest": "sha256:99fd4033d9f5e536911915410abfc636b55f09e63f88cf32dbe9f58f79124963",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-13-restore@sha256:99fd4033d9f5e536911915410abfc636b55f09e63f88cf32dbe9f58f79124963",
          "name": "restore_13"
        },
        {
          "digest": "sha256:2f23389e16635ff3e75774dac430c35dc4b314f291d9303d9296852eb0e436bc",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-13-pgpool2@sha256:2f23389e16635ff3e75774dac430c35dc4b314f291d9303d9296852eb0e436bc",
          "name": "pgpool2_13"
        },
        {
          "digest": "sha256:b8c93f9dc83332f547682e7c1bc9a691d20f91793a27cabe1798bd9ae8c96cb2",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-13-exporter@sha256:b8c93f9dc83332f547682e7c1bc9a691d20f91793a27cabe1798bd9ae8c96cb2",
          "name": "fepexporter_13"
        },
        {
          "digest": "sha256:61592bcccccee2e72521d0640f005d1902730319ad697aa0f4fc5485bf22989d",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-12-server@sha256:61592bcccccee2e72521d0640f005d1902730319ad697aa0f4fc5485bf22989d",
          "name": "fep_12"
        },
        {
          "digest": "sha256:f126c128c1301afa684bdf3258c78d1ec68c08aa6e5c085c5f939940947e5c94",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-12-backup@sha256:f126c128c1301afa684bdf3258c78d1ec68c08aa6e5c085c5f939940947e5c94",
          "name": "backup_12"
        },
        {
          "digest": "sha256:240da72ba4ef71031c1587a1f44b7631697346bb7699be52dcebd64e9f93d66e",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-12-restore@sha256:240da72ba4ef71031c1587a1f44b7631697346bb7699be52dcebd64e9f93d66e",
          "name": "restore_12"
        },
        {
          "digest": "sha256:6d58e56e6e7ab0c07d6095b1cab429c47ea3d294bf1656bd0a7d6efed098cb02",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-12-pgpool2@sha256:6d58e56e6e7ab0c07d6095b1cab429c47ea3d294bf1656bd0a7d6efed098cb02",
          "name": "pgpool2_12"
        }
      ],
      "replaces": null,
      "skip_range": "<4.1.9",
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.11",
      "version": "4.1.9",
      "version_original": "4.1.9"
    },
    {
      "_id": "638609bf9135a67a7f3cb238",
      "alm_examples": [
        {
          "api_version": "fep.fujitsu.io/v1",
          "kind": "FEPAction",
          "metadata": {
            "name": "new-fep-action"
          },
          "spec": {
            "fepAction": {
              "args": [
                "new-fep-sts-0",
                "new-fep-sts-1"
              ],
              "type": "reload"
            },
            "sysExtraLogging": false,
            "targetClusterName": "new-fep"
          }
        },
        {
          "api_version": "fep.fujitsu.io/v1",
          "kind": "FEPExporter",
          "metadata": {
            "name": "new-fep-exporter"
          },
          "spec": {
            "fepExporter": {
              "exporterLogLevel": "error",
              "fepClusterList": [
                "new-fep1"
              ],
              "image": {
                "pullPolicy": "IfNotPresent"
              },
              "mcSpec": {
                "limits": {
                  "cpu": "500m",
                  "memory": "700Mi"
                },
                "requests": {
                  "cpu": "200m",
                  "memory": "512Mi"
                }
              },
              "restartRequired": false,
              "sysExtraLogging": false,
              "userCustomQueries": "usr_example:\n  query: \"SELECT EXTRACT(EPOCH FROM (now() - pg_last_xact_replay_timestamp())) as lag\"\n  master: true\n  metrics:\n    - lag:\n        usage: \"GAUGE\"\n        description: \"Replication lag behind master in seconds\""
            }
          }
        },
        {
          "api_version": "fep.fujitsu.io/v1",
          "kind": "FEPLogging",
          "metadata": {
            "name": "new-fep-logging"
          },
          "spec": {
            "fepLogging": {
              "image": {
                "pullPolicy": "IfNotPresent"
              },
              "mcSpec": {
                "limits": {
                  "cpu": "500m",
                  "memory": "700Mi"
                },
                "requests": {
                  "cpu": "200m",
                  "memory": "512Mi"
                }
              },
              "restartRequired": false,
              "sysExtraLogging": false
            }
          }
        },
        {
          "api_version": "fep.fujitsu.io/v1",
          "kind": "FEPPgpool2",
          "metadata": {
            "name": "new-fep-pgpool2"
          },
          "spec": {
            "count": 2,
            "customhba": "local   all         all                               trust\nhost    all         all         127.0.0.1/32          trust\nhost    all         all         ::1/128               trust\n",
            "customlogsize": "128Mi",
            "customparams": "listen_addresses = '*'\npcp_listen_addresses = '*'\nnum_init_children = 32\nreserved_connections = 0\nenable_pool_hba = off\nallow_clear_text_frontend_auth = off\nauthentication_timeout = 80\nbackend_weight0 = 1\nbackend_weight1 = 1\nbackend_flag0 = 'ALWAYS_PRIMARY'\nbackend_flag1 = 'DISALLOW_TO_FAILOVER'\nconnection_cache = on\nmax_pool = 4\nlisten_backlog_multiplier = 2\nserialize_accept = off\nchild_life_time = 300\nclient_idle_limit = 0\nchild_max_connections = 0\nconnection_life_time = 0\nreset_query_list = 'ABORT; DISCARD ALL'\nclient_min_messages = info\nlog_min_messages = debug1\nlog_statement = on\nlog_per_node_statement = on\nlog_client_messages = on\nlog_hostname = on\nlog_connections = on\nlog_line_prefix = '%t: pid %p: '\nload_balance_mode = on\nignore_leading_white_space = on\nwhite_function_list = ''\nblack_function_list = 'currval,lastval,nextval,setval'\nblack_query_pattern_list = ''\ndatabase_redirect_preference_list = ''\napp_name_redirect_preference_list = ''\nallow_sql_comments = off\ndisable_load_balance_on_write = 'transaction'\nstatement_level_load_balance = on\nsr_check_period = 0\nsr_check_user = 'postgres'\ndelay_threshold = 0\nlog_standby_delay = 'none'\nssl = on\nssl_ciphers = 'HIGH:MEDIUM:+3DES:!aNULL'\nssl_prefer_server_ciphers = off\nssl_ecdh_curve = 'prime256v1'\nssl_dh_params_file = ''\nrelcache_expire = 0\nrelcache_size = 256\ncheck_temp_table = catalog\ncheck_unlogged_table = on\nenable_shared_relcache = on\nrelcache_query_target = primary\nwd_port0 = 9000\nfailover_on_backend_error = off\n",
            "custompcp": "none",
            "customsslcacert": "none",
            "customsslcert": "-----BEGIN CERTIFICATE-----\nMIIDTzCCAjegAwIBAgIUYssQ8I74US5g+1+Z7CHuaDgkZnEwDQYJKoZIhvcNAQEL\nBQAwNzEQMA4GA1UECgwHRnVqaXRzdTEjMCEGA1UEAwwaRkVQIFJvb3QgQ0EgZm9y\nIEt1YmVybmV0ZXMwHhcNMjEwMjA2MDM1MjI4WhcNMzEwMjA0MDM1MjI4WjA3MRAw\nDgYDVQQKDAdGdWppdHN1MSMwIQYDVQQDDBpGRVAgUm9vdCBDQSBmb3IgS3ViZXJu\nZXRlczCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBAMs97gUF0xkUzCgL\n7MiiDju9ySr/ziwjvcYU7jA9ML+SLmftMs3HtcYbAmSntqI+MDBSR/FAJTOoytuT\npV+mCFcGj2YAjDpliHPeNcUpbryy4YMChF3+MovkIwGCksxo5rhiWhGmoBYpA48P\n4Xe8SPlzqMzhFvNeKzyiUhvjutS2Y1Ss38lsTaurFPx64vQ2PaC54XzdwMptXtpb\ntYmWSzCpJWwxZ6lF3vitdA2w0tnBWNyctAd0+RIM/fvArxiIqseAux9t0uogm5to\nlRIhvekuxOpXBPEqtIYQ4j9XUW2JH8vUDnzPkPvjrq+A3Ug8OyyfGVrW7+VYXozu\nc4aP7P0CAwEAAaNTMFEwHQYDVR0OBBYEFBzCutQ7S74WEhS5V2sNEJBGyLpmMB8G\nA1UdIwQYMBaAFBzCutQ7S74WEhS5V2sNEJBGyLpmMA8GA1UdEwEB/wQFMAMBAf8w\nDQYJKoZIhvcNAQELBQADggEBAMDwD85RAaWEBptFgLzKw+9xEUy1vcZaonAuA1qc\nT342XTueyAugxkC11HwdCGgGS34VyctfMGqj4AW6pA2ez4tLrbOps4DmV4sw8uBL\n8pgRDgfly3ob9FEg2wa0hmrwX9jH5Bt4vySUE2785uPAqaspT2UNtTBxS85BUi1T\nsKId2Rtil6an281Z81wyWVI6Jm2D4MG0mbsiGcTPlCtdg/UljvDYymXlAvd4vNhl\nk9hDa13TgDqJKgKdTIcmZoNQdpEVgFcO0h9AEUy5AuLqxHq60dLfZ6ESGPlMI7Lm\ni4PzYbCnBmOe+7TnHcPSyrnehs66Ik+oifRd82eYS7vKjFw=\n-----END CERTIFICATE-----",
            "customsslkey": "none",
            "fepclustername": "new-fep",
            "imagePullPolicy": "IfNotPresent",
            "limits": {
              "cpu": "400m",
              "memory": "512Mi"
            },
            "requests": {
              "cpu": "200m",
              "memory": "256Mi"
            },
            "serviceport": 9999,
            "statusport": 9898
          }
        },
        {
          "api_version": "fep.fujitsu.io/v1",
          "kind": "FEPRestore",
          "metadata": {
            "name": "new-fep-restore"
          },
          "spec": {
            "fromFEPcluster": "new-fep",
            "imagePullPolicy": "IfNotPresent",
            "mcSpec": {
              "limits": {
                "cpu": "200m",
                "memory": "300Mi"
              },
              "requests": {
                "cpu": "100m",
                "memory": "200Mi"
              }
            },
            "restoretype": "latest",
            "sysExtraLogging": false,
            "toFEPcluster": "new-fep-2"
          }
        },
        {
          "api_version": "fep.fujitsu.io/v2",
          "kind": "FEPCluster",
          "metadata": {
            "name": "new-fep"
          },
          "spec": {
            "fep": {
              "customAnnotations": {
                "allDeployments": {}
              },
              "forceSsl": true,
              "image": {
                "pullPolicy": "IfNotPresent"
              },
              "instances": 1,
              "mcSpec": {
                "limits": {
                  "cpu": "500m",
                  "memory": "700Mi"
                },
                "requests": {
                  "cpu": "200m",
                  "memory": "512Mi"
                }
              },
              "podAntiAffinity": false,
              "podDisruptionBudget": false,
              "servicePort": 27500,
              "syncMode": "off",
              "sysExtraLogging": false
            },
            "fepChildCrVal": {
              "backup": {
                "image": {
                  "pullPolicy": "IfNotPresent"
                },
                "mcSpec": {
                  "limits": {
                    "cpu": 0.2,
                    "memory": "300Mi"
                  },
                  "requests": {
                    "cpu": 0.1,
                    "memory": "200Mi"
                  }
                },
                "pgbackrestParams": "# if log volume is not defined, log_directory should be\n# changed to '/database/userdata/data/log'\n[global]\nrepo1-retention-full=7\nrepo1-retention-full-type=time\nlog-path=/database/log/backup\n",
                "postScript": " ",
                "preScript": " ",
                "schedule": {
                  "num": 2
                },
                "schedule1": {
                  "schedule": "15 0 * * 0",
                  "type": "full"
                },
                "schedule2": {
                  "schedule": "15 0 * * 1-6",
                  "type": "incr"
                },
                "schedule3": {
                  "schedule": " ",
                  "type": " "
                },
                "schedule4": {
                  "schedule": " ",
                  "type": " "
                },
                "schedule5": {
                  "schedule": " ",
                  "type": " "
                }
              },
              "customPgAudit": "# define pg audit custom params here to override defaults.\n# if log volume is not defined, log_directory should be\n# changed to '/database/userdata/data/log'\n[output]\nlogger = 'auditlog'\nlog_directory = '/database/log/audit'\nlog_truncate_on_rotation = on\nlog_filename = 'pgaudit-%a.log'\nlog_rotation_age = 1d\nlog_rotation_size = 0\n[rule]\n",
              "customPgHba": "# define pg_hba custom rules here to be merged with default rules.\n# TYPE     DATABASE        USER        ADDRESS        METHOD\n",
              "customPgParams": "# define custom postgresql.conf parameters below to override defaults.\n# Current values are as per default FEP deployment\nshared_preload_libraries='pgx_datamasking,pg_prewarm,pg_stat_statements'\nsession_preload_libraries='pg_prewarm'\nmax_prepared_transactions = 100\nmax_worker_processes = 30\nmax_connections = 100\nwork_mem = 1MB\nmaintenance_work_mem = 12MB\nshared_buffers = 128MB\neffective_cache_size = 384MB\ncheckpoint_completion_target = 0.8\n# tcp parameters\ntcp_keepalives_idle = 30\ntcp_keepalives_interval = 10\ntcp_keepalives_count = 3\n# logging parameters in default fep installation\n# if log volume is not defined, log_directory should be\n# changed to '/database/userdata/data/log'\nlog_directory = '/database/log'\nlog_filename = 'logfile-%a.log'\nlog_file_mode = 0600\nlog_truncate_on_rotation = on\nlog_rotation_age = 1d\nlog_rotation_size = 0\nlog_checkpoints = on\nlog_line_prefix = '%e %t [%p]: [%l-1] user=%u,db=%d,app=%a,client=%h'\nlog_lock_waits = on\nlog_autovacuum_min_duration = 60s\nlogging_collector = on\npgaudit.config_file='/opt/app-root/src/pgaudit-cfg/pgaudit.conf'\nlog_replication_commands = on\nlog_min_messages = WARNING\nlog_destination = csvlog\n# wal_archive parameters in default fep installation\narchive_mode = on\narchive_command = 'pgbackrest --stanza=backupstanza --config=/database/userdata/pgbackrest.conf archive-push %p'\nwal_level = replica\nmax_wal_senders = 12\nwal_keep_segments = 64\ntrack_activities = on\ntrack_counts = on\npassword_encryption = 'md5'\n",
              "storage": {
                "archivewalVol": {
                  "size": "1Gi"
                },
                "backupVol": {
                  "size": "2Gi"
                },
                "dataVol": {
                  "size": "2Gi"
                },
                "logVol": {
                  "size": "1Gi"
                },
                "tablespaceVol": {
                  "size": "512Mi"
                },
                "walVol": {
                  "size": "1200Mi"
                }
              },
              "sysUsers": {
                "pgAdminPassword": "admin-password",
                "pgRewindPassword": "rewind_password",
                "pgRewindUser": "rewind_user",
                "pgdb": "mydb",
                "pgpassword": "mydbpassword",
                "pgreplpassword": "repluserpwd",
                "pgrepluser": "repluser",
                "pguser": "mydbuser",
                "tdepassphrase": "tde-passphrase"
              },
              "systemCertificates": {
                "cacrt": "-----BEGIN CERTIFICATE-----\nMIIDTzCCAjegAwIBAgIUYssQ8I74US5g+1+Z7CHuaDgkZnEwDQYJKoZIhvcNAQEL\nBQAwNzEQMA4GA1UECgwHRnVqaXRzdTEjMCEGA1UEAwwaRkVQIFJvb3QgQ0EgZm9y\nIEt1YmVybmV0ZXMwHhcNMjEwMjA2MDM1MjI4WhcNMzEwMjA0MDM1MjI4WjA3MRAw\nDgYDVQQKDAdGdWppdHN1MSMwIQYDVQQDDBpGRVAgUm9vdCBDQSBmb3IgS3ViZXJu\nZXRlczCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBAMs97gUF0xkUzCgL\n7MiiDju9ySr/ziwjvcYU7jA9ML+SLmftMs3HtcYbAmSntqI+MDBSR/FAJTOoytuT\npV+mCFcGj2YAjDpliHPeNcUpbryy4YMChF3+MovkIwGCksxo5rhiWhGmoBYpA48P\n4Xe8SPlzqMzhFvNeKzyiUhvjutS2Y1Ss38lsTaurFPx64vQ2PaC54XzdwMptXtpb\ntYmWSzCpJWwxZ6lF3vitdA2w0tnBWNyctAd0+RIM/fvArxiIqseAux9t0uogm5to\nlRIhvekuxOpXBPEqtIYQ4j9XUW2JH8vUDnzPkPvjrq+A3Ug8OyyfGVrW7+VYXozu\nc4aP7P0CAwEAAaNTMFEwHQYDVR0OBBYEFBzCutQ7S74WEhS5V2sNEJBGyLpmMB8G\nA1UdIwQYMBaAFBzCutQ7S74WEhS5V2sNEJBGyLpmMA8GA1UdEwEB/wQFMAMBAf8w\nDQYJKoZIhvcNAQELBQADggEBAMDwD85RAaWEBptFgLzKw+9xEUy1vcZaonAuA1qc\nT342XTueyAugxkC11HwdCGgGS34VyctfMGqj4AW6pA2ez4tLrbOps4DmV4sw8uBL\n8pgRDgfly3ob9FEg2wa0hmrwX9jH5Bt4vySUE2785uPAqaspT2UNtTBxS85BUi1T\nsKId2Rtil6an281Z81wyWVI6Jm2D4MG0mbsiGcTPlCtdg/UljvDYymXlAvd4vNhl\nk9hDa13TgDqJKgKdTIcmZoNQdpEVgFcO0h9AEUy5AuLqxHq60dLfZ6ESGPlMI7Lm\ni4PzYbCnBmOe+7TnHcPSyrnehs66Ik+oifRd82eYS7vKjFw=\n-----END CERTIFICATE-----",
                "crt": "-----BEGIN CERTIFICATE-----\nMIIDUTCCAjmgAwIBAgIRAMocW3qMoHrD6qRvMPppMkMwDQYJKoZIhvcNAQELBQAw\nNzEQMA4GA1UECgwHRnVqaXRzdTEjMCEGA1UEAwwaRkVQIFJvb3QgQ0EgZm9yIEt1\nYmVybmV0ZXMwHhcNMjEwMjA2MDQzMjM2WhcNMjYwMjA1MDQzMjM2WjA/MRAwDgYD\nVQQKEwdGdWppdHN1MSswKQYDVQQDEyJGVUpJVFNVIEVudGVycHJpc2UgUG9zdGdy\nZXMgU2VydmVyMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEA4AI33yvH\nZws+jta6qpV6wzJqF8odIfTIpCfbrVcUUtLFKJ1I2e4SceTKi6O3C/I1XuvWlpng\n5IO65+fQQLO06z1/AuQT78YUn/Wlm9x1aHVsv4ANB5JWWqDOjrRT3o7nRPGXfila\nbP0rGE2mJJcVR9nExJ3IeaktgT3sb8YlXvtchyYpmjdbfxabTz07ig0+6/cwKoRR\nxOK8Uf7f5euE0cI/490J6r5Rs4lgD8sIQNCUFlTFYvmAH7gcdssSFBt8NPlUATHE\nsoFmlW0DKCJWNhTLOht+s6L/1zwTHLjPG2pdkG6Wdgmu5H2pDml8CDNLDv98Aj7i\n+I5SRKKcVPlnuQIDAQABo1AwTjAdBgNVHSUEFjAUBggrBgEFBQcDAQYIKwYBBQUH\nAwIwDAYDVR0TAQH/BAIwADAfBgNVHSMEGDAWgBQcwrrUO0u+FhIUuVdrDRCQRsi6\nZjANBgkqhkiG9w0BAQsFAAOCAQEAm5dxBoI9pScOCvRAchg4CprdRDSJb9K6yB3O\nnCAxnM47iHeXnY3WlnI388kHu8DU7O4ba1tJbGs3KY9KzioPk43pU12jWkO1onoF\n+mTDjx/Ef1cYWA9r5q/LtgTa6Q2sxV4O2x67QW82aAnaxO34dV5zWCPIvAoovZBV\nHRT+BgCg3r2vD1RGKK2nl1aYJtWhO1SZubam+VttdZ/vbM9oOJctxmImsEtBXjkY\nKteePdQtLL5o03JhyXWyRshCq+HMmKf2KgyY8gvydGcP4eLQdBWcW40LcnVq6UjT\n0kJycJEKngMVademq1ZWHGaiYB7hyT6GhgIcHUJ2cKrPgbEh1Q==\n-----END CERTIFICATE-----",
                "key": "-----BEGIN RSA PRIVATE KEY-----\nMIIEowIBAAKCAQEA4AI33yvHZws+jta6qpV6wzJqF8odIfTIpCfbrVcUUtLFKJ1I\n2e4SceTKi6O3C/I1XuvWlpng5IO65+fQQLO06z1/AuQT78YUn/Wlm9x1aHVsv4AN\nB5JWWqDOjrRT3o7nRPGXfilabP0rGE2mJJcVR9nExJ3IeaktgT3sb8YlXvtchyYp\nmjdbfxabTz07ig0+6/cwKoRRxOK8Uf7f5euE0cI/490J6r5Rs4lgD8sIQNCUFlTF\nYvmAH7gcdssSFBt8NPlUATHEsoFmlW0DKCJWNhTLOht+s6L/1zwTHLjPG2pdkG6W\ndgmu5H2pDml8CDNLDv98Aj7i+I5SRKKcVPlnuQIDAQABAoIBAFPQYKlOzw/+BA0b\nyMIUpdctIMb/54CR/xR0mVw1DbSjigNVPjHUQvB8Y1B2FAITQObgJO06bAv0QdWN\nRb0/v/yYiNJDFjaLjaIAHlO/2+oWrXbFaZqgpVDJhB+e1xaZr2x7XGxm+p925k30\nl6pvIRY+I8JRKvZiV1VZHwL/R3JOtPr++xMZtLVjVOI+f+ySqJ+TZHuAjm49EKxj\ncEmmJ28b7QcziXsvKy00f+zbqLIBKXQdZAFU5eEr1BsDRXdRW+Kf0XIvftuy4BJZ\nvoKT+VGhEvF/qysswL4+6IAO6tpuYnnM0Y2d3sOGoWPkTcQK0MekYKzL/WmtCjNs\n9hodJtECgYEA5EWyhEOf4uOKe5TDp697UCUvXLoOR58FDe/S8XNvScn29jjOkqIg\nOMoqo9xAkJTNTzqn5UUdt1x/pgM2NxlPLFijrc0zQlX3SoOO2ryDd9WNi7YKtN16\nKJqa536WeZu2OEbuAZ+S3GALVy1RPeTNPnUOmKnF06DjDUGzLNCZy10CgYEA+zfw\n952DWuz1U0Z4wvAEqqcgUKXPKrkTXV/iUnjkDkrLYVr0ZofDNTXrdHl+UedFmaOC\ncieZn6DNhcdz5tKtyysGMH3g/qs9PfoGUngvcXsy0Egk04l3x1jc8TTCLqXZXYaQ\nHMsx51n+R58oncPtzYSUOr9qQ6PbC2CstTbFJA0CgYEAjGEsUliAB/jknfEzjXjG\nPdhQUxb8VyE864Az2lah9t/kJzFyIAziAeqZ5GE7t247AGFTBRTHHI8e1Qoemi3P\nWbc9GVIbFs1lIYbcIDpUIyrKPEP8O5QEXtoNLxXTFgAjRGKiVY87spjCAJ+W2ZhO\ne/1it5GYXfgQCYQA2yuBmOUCgYANRkR2YR1axaCk+NlSu6oTdmdPu6M5x7PNQE7O\nOtMaKjua9lppvIzFGAdMDUtueoEEAE7ZR1xnwfB6PDLUpJdIYAqgr1YfPt8qkjaZ\nTv56yZ7CwL0pbF8m6nwqRrZoDp1wwraEvvvxFKFKGY/k3kCHlpTakdjEoDjn3gDi\nRnWeVQKBgCEneMSzucei5LRppRtRaJw/Btll8qlPMlX3W7dxQ3cLwpmLOn0m51Fp\nPIZ44zYK8R6fu4+/sSrlfaIg86Ugeufp6YNxyNROKxUGza5vDIu5OftwWtBeg+UK\nZ8lLWNdX6pp7WMujmF3H1DrkBbauYMUKZ4UxUYtelgHERMePIxwb\n-----END RSA PRIVATE KEY-----"
              }
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [
        "amd64",
        "ppc64le",
        "s390x"
      ],
      "bundle_path": "registry.connect.redhat.com/fujitsu-postgres/fujitsu-enterprise-postgres-operator-bundle@sha256:94fa991b1fdaa56ee907acd562a163dcc52284719b61188f823f2b0f6dc6f436",
      "bundle_path_digest": "sha256:94fa991b1fdaa56ee907acd562a163dcc52284719b61188f823f2b0f6dc6f436",
      "capabilities": [
        "Auto Pilot"
      ],
      "channel_name": "stable",
      "creation_date": "2022-11-29T13:31:43.247000+00:00",
      "csv_description": "FUJITSU Enterprise Postgres 14 delivers an enterprise-grade PostgreSQL on OpenShift Container Platform.\n\nThis solution provides the flexibility of a hybrid cloud solution while delivering an enhanced distribution of PostgreSQL to support enterprise-level workloads and provide improved deployment and management, availability, performance, data governance and security.  \n\nAvailable as a multi-architecture container built for both amd64, s390x and ppc64le.\n\nThe download and Use of the Product is strictly subject to the terms of the End User License Agreement with Fujitsu Limited found at https://www.fast.fujitsu.com/fujitsu-enterprise-postgres-license-agreements. Where the Product that has been embedded as a whole or part into a third party program, only Authorised Customers may download and use the Product.\n",
      "csv_display_name": "FUJITSU Enterprise Postgres Operator",
      "csv_metadata_description": "OpenShift Operator for Fujitsu Enterprise Postgres",
      "csv_name": "fujitsu-enterprise-postgres-operator.v4.1.9",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": true,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-11-29T13:31:43.247000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.10",
      "organization": "certified-operators",
      "package": "fujitsu-enterprise-postgres-operator",
      "provided_apis": [
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPExporter",
          "plural": "fepexporters",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPAction",
          "plural": "fepactions",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPUser",
          "plural": "fepusers",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPVolume",
          "plural": "fepvolumes",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPBackup",
          "plural": "fepbackups",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPCert",
          "plural": "fepcerts",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPLogging",
          "plural": "feploggings",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPPgpool2",
          "plural": "feppgpool2s",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPUpgrade",
          "plural": "fepupgrades",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPAutoscale",
          "plural": "fepautoscales",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPCluster",
          "plural": "fepclusters",
          "version": "v2"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPConfig",
          "plural": "fepconfigs",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPRestore",
          "plural": "feprestores",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPPgpool2Cert",
          "plural": "feppgpool2certs",
          "version": "v1"
        }
      ],
      "provider": "Fujitsu",
      "related_images": [
        {
          "digest": "sha256:a07987c0b4d47c6ec53c2127ead56bb420ea493e9b5d1bb79064b5f1a1f57d91",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-14-server@sha256:a07987c0b4d47c6ec53c2127ead56bb420ea493e9b5d1bb79064b5f1a1f57d91",
          "name": "fep-14"
        },
        {
          "digest": "sha256:8690a50c3c900686fd5e8a65b04fee0125e4cf500c44845f3c4392a1ea1ef873",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-14-backup@sha256:8690a50c3c900686fd5e8a65b04fee0125e4cf500c44845f3c4392a1ea1ef873",
          "name": "backup-14"
        },
        {
          "digest": "sha256:3759bd7513228b49733b23db24fdeb2c6a31b3f93c7ac5f84ab0d8b9939e07ce",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-14-restore@sha256:3759bd7513228b49733b23db24fdeb2c6a31b3f93c7ac5f84ab0d8b9939e07ce",
          "name": "restore-14"
        },
        {
          "digest": "sha256:37729f8332f647eba5f8209ea9ce95aaa9ab6674774afc2931c4aa59dc51ae90",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-14-pgpool2@sha256:37729f8332f647eba5f8209ea9ce95aaa9ab6674774afc2931c4aa59dc51ae90",
          "name": "pgpool2-14"
        },
        {
          "digest": "sha256:f9e28fe00b2caee1b15e4867e86dad39c8e6913f137fe689fd30c18b9f19881c",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-exporter@sha256:f9e28fe00b2caee1b15e4867e86dad39c8e6913f137fe689fd30c18b9f19881c",
          "name": "exporter-14"
        },
        {
          "digest": "sha256:fb8bffe2c1833b7061a4856ecabda811ed16ff661d38b8ea6a5bc826bc0ccbc3",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-fluentd@sha256:fb8bffe2c1833b7061a4856ecabda811ed16ff661d38b8ea6a5bc826bc0ccbc3",
          "name": "fluentd-14"
        },
        {
          "digest": "sha256:326005d0cfb049f36759e6ef022c67029dbac1865810be3c850c1bc1cf604e8d",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-fluentbit@sha256:326005d0cfb049f36759e6ef022c67029dbac1865810be3c850c1bc1cf604e8d",
          "name": "fluentbit-14"
        },
        {
          "digest": "sha256:4f75b63a09ad3da34684db497814bfa8eae8356e43963ce67fbfcb48e4f350ac",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-cronjob@sha256:4f75b63a09ad3da34684db497814bfa8eae8356e43963ce67fbfcb48e4f350ac",
          "name": "cronjob-14"
        },
        {
          "digest": "sha256:793a06eefe93a7613cfdf1a89febf5603244f453a1e804760d93e6f182cc522a",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-14-upgrade@sha256:793a06eefe93a7613cfdf1a89febf5603244f453a1e804760d93e6f182cc522a",
          "name": "upgrade-14"
        },
        {
          "digest": "sha256:80f5d30c66844eb05de5d67999198d9ea3d457f8fbe81358de0b37db0f15f0e3",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-13-server@sha256:80f5d30c66844eb05de5d67999198d9ea3d457f8fbe81358de0b37db0f15f0e3",
          "name": "fep-13"
        },
        {
          "digest": "sha256:c27c4b52361e73dd460eac5d8fe4765f582d0ffd4f4fc072bff4523b785f47e5",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-13-backup@sha256:c27c4b52361e73dd460eac5d8fe4765f582d0ffd4f4fc072bff4523b785f47e5",
          "name": "backup-13"
        },
        {
          "digest": "sha256:99fd4033d9f5e536911915410abfc636b55f09e63f88cf32dbe9f58f79124963",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-13-restore@sha256:99fd4033d9f5e536911915410abfc636b55f09e63f88cf32dbe9f58f79124963",
          "name": "restore-13"
        },
        {
          "digest": "sha256:2f23389e16635ff3e75774dac430c35dc4b314f291d9303d9296852eb0e436bc",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-13-pgpool2@sha256:2f23389e16635ff3e75774dac430c35dc4b314f291d9303d9296852eb0e436bc",
          "name": "pgpool2-13"
        },
        {
          "digest": "sha256:b8c93f9dc83332f547682e7c1bc9a691d20f91793a27cabe1798bd9ae8c96cb2",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-13-exporter@sha256:b8c93f9dc83332f547682e7c1bc9a691d20f91793a27cabe1798bd9ae8c96cb2",
          "name": "fepexporter-13"
        },
        {
          "digest": "sha256:61592bcccccee2e72521d0640f005d1902730319ad697aa0f4fc5485bf22989d",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-12-server@sha256:61592bcccccee2e72521d0640f005d1902730319ad697aa0f4fc5485bf22989d",
          "name": "fep-12"
        },
        {
          "digest": "sha256:f126c128c1301afa684bdf3258c78d1ec68c08aa6e5c085c5f939940947e5c94",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-12-backup@sha256:f126c128c1301afa684bdf3258c78d1ec68c08aa6e5c085c5f939940947e5c94",
          "name": "backup-12"
        },
        {
          "digest": "sha256:240da72ba4ef71031c1587a1f44b7631697346bb7699be52dcebd64e9f93d66e",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-12-restore@sha256:240da72ba4ef71031c1587a1f44b7631697346bb7699be52dcebd64e9f93d66e",
          "name": "restore-12"
        },
        {
          "digest": "sha256:6d58e56e6e7ab0c07d6095b1cab429c47ea3d294bf1656bd0a7d6efed098cb02",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-12-pgpool2@sha256:6d58e56e6e7ab0c07d6095b1cab429c47ea3d294bf1656bd0a7d6efed098cb02",
          "name": "pgpool2-12"
        },
        {
          "digest": "sha256:ecf7b692793b5b7ad2e848f335456742901ef0ef6b0b171d4771f0642e76714e",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-operator@sha256:ecf7b692793b5b7ad2e848f335456742901ef0ef6b0b171d4771f0642e76714e",
          "name": "fujitsu-enterprise-postgres-operator-ecf7b692793b5b7ad2e848f335456742901ef0ef6b0b171d4771f0642e76714e-annotation"
        },
        {
          "digest": "sha256:ecf7b692793b5b7ad2e848f335456742901ef0ef6b0b171d4771f0642e76714e",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-operator@sha256:ecf7b692793b5b7ad2e848f335456742901ef0ef6b0b171d4771f0642e76714e",
          "name": "fep-ansible-operator"
        },
        {
          "digest": "sha256:a07987c0b4d47c6ec53c2127ead56bb420ea493e9b5d1bb79064b5f1a1f57d91",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-14-server@sha256:a07987c0b4d47c6ec53c2127ead56bb420ea493e9b5d1bb79064b5f1a1f57d91",
          "name": "fep"
        },
        {
          "digest": "sha256:8690a50c3c900686fd5e8a65b04fee0125e4cf500c44845f3c4392a1ea1ef873",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-14-backup@sha256:8690a50c3c900686fd5e8a65b04fee0125e4cf500c44845f3c4392a1ea1ef873",
          "name": "backup"
        },
        {
          "digest": "sha256:3759bd7513228b49733b23db24fdeb2c6a31b3f93c7ac5f84ab0d8b9939e07ce",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-14-restore@sha256:3759bd7513228b49733b23db24fdeb2c6a31b3f93c7ac5f84ab0d8b9939e07ce",
          "name": "restore"
        },
        {
          "digest": "sha256:37729f8332f647eba5f8209ea9ce95aaa9ab6674774afc2931c4aa59dc51ae90",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-14-pgpool2@sha256:37729f8332f647eba5f8209ea9ce95aaa9ab6674774afc2931c4aa59dc51ae90",
          "name": "pgpool2"
        },
        {
          "digest": "sha256:f9e28fe00b2caee1b15e4867e86dad39c8e6913f137fe689fd30c18b9f19881c",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-exporter@sha256:f9e28fe00b2caee1b15e4867e86dad39c8e6913f137fe689fd30c18b9f19881c",
          "name": "fepexporter"
        },
        {
          "digest": "sha256:fb8bffe2c1833b7061a4856ecabda811ed16ff661d38b8ea6a5bc826bc0ccbc3",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-fluentd@sha256:fb8bffe2c1833b7061a4856ecabda811ed16ff661d38b8ea6a5bc826bc0ccbc3",
          "name": "feplogging"
        },
        {
          "digest": "sha256:326005d0cfb049f36759e6ef022c67029dbac1865810be3c850c1bc1cf604e8d",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-fluentbit@sha256:326005d0cfb049f36759e6ef022c67029dbac1865810be3c850c1bc1cf604e8d",
          "name": "feplogging_fluentbit"
        },
        {
          "digest": "sha256:4f75b63a09ad3da34684db497814bfa8eae8356e43963ce67fbfcb48e4f350ac",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-cronjob@sha256:4f75b63a09ad3da34684db497814bfa8eae8356e43963ce67fbfcb48e4f350ac",
          "name": "cronjob"
        },
        {
          "digest": "sha256:793a06eefe93a7613cfdf1a89febf5603244f453a1e804760d93e6f182cc522a",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-14-upgrade@sha256:793a06eefe93a7613cfdf1a89febf5603244f453a1e804760d93e6f182cc522a",
          "name": "upgrade"
        },
        {
          "digest": "sha256:a07987c0b4d47c6ec53c2127ead56bb420ea493e9b5d1bb79064b5f1a1f57d91",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-14-server@sha256:a07987c0b4d47c6ec53c2127ead56bb420ea493e9b5d1bb79064b5f1a1f57d91",
          "name": "fep_14"
        },
        {
          "digest": "sha256:8690a50c3c900686fd5e8a65b04fee0125e4cf500c44845f3c4392a1ea1ef873",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-14-backup@sha256:8690a50c3c900686fd5e8a65b04fee0125e4cf500c44845f3c4392a1ea1ef873",
          "name": "backup_14"
        },
        {
          "digest": "sha256:3759bd7513228b49733b23db24fdeb2c6a31b3f93c7ac5f84ab0d8b9939e07ce",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-14-restore@sha256:3759bd7513228b49733b23db24fdeb2c6a31b3f93c7ac5f84ab0d8b9939e07ce",
          "name": "restore_14"
        },
        {
          "digest": "sha256:37729f8332f647eba5f8209ea9ce95aaa9ab6674774afc2931c4aa59dc51ae90",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-14-pgpool2@sha256:37729f8332f647eba5f8209ea9ce95aaa9ab6674774afc2931c4aa59dc51ae90",
          "name": "pgpool2_14"
        },
        {
          "digest": "sha256:f9e28fe00b2caee1b15e4867e86dad39c8e6913f137fe689fd30c18b9f19881c",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-exporter@sha256:f9e28fe00b2caee1b15e4867e86dad39c8e6913f137fe689fd30c18b9f19881c",
          "name": "fepexporter_14"
        },
        {
          "digest": "sha256:fb8bffe2c1833b7061a4856ecabda811ed16ff661d38b8ea6a5bc826bc0ccbc3",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-fluentd@sha256:fb8bffe2c1833b7061a4856ecabda811ed16ff661d38b8ea6a5bc826bc0ccbc3",
          "name": "feplogging_14"
        },
        {
          "digest": "sha256:326005d0cfb049f36759e6ef022c67029dbac1865810be3c850c1bc1cf604e8d",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-fluentbit@sha256:326005d0cfb049f36759e6ef022c67029dbac1865810be3c850c1bc1cf604e8d",
          "name": "feplogging_fluentbit_14"
        },
        {
          "digest": "sha256:4f75b63a09ad3da34684db497814bfa8eae8356e43963ce67fbfcb48e4f350ac",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-cronjob@sha256:4f75b63a09ad3da34684db497814bfa8eae8356e43963ce67fbfcb48e4f350ac",
          "name": "cronjob_14"
        },
        {
          "digest": "sha256:793a06eefe93a7613cfdf1a89febf5603244f453a1e804760d93e6f182cc522a",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-14-upgrade@sha256:793a06eefe93a7613cfdf1a89febf5603244f453a1e804760d93e6f182cc522a",
          "name": "upgrade_14"
        },
        {
          "digest": "sha256:80f5d30c66844eb05de5d67999198d9ea3d457f8fbe81358de0b37db0f15f0e3",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-13-server@sha256:80f5d30c66844eb05de5d67999198d9ea3d457f8fbe81358de0b37db0f15f0e3",
          "name": "fep_13"
        },
        {
          "digest": "sha256:c27c4b52361e73dd460eac5d8fe4765f582d0ffd4f4fc072bff4523b785f47e5",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-13-backup@sha256:c27c4b52361e73dd460eac5d8fe4765f582d0ffd4f4fc072bff4523b785f47e5",
          "name": "backup_13"
        },
        {
          "digest": "sha256:99fd4033d9f5e536911915410abfc636b55f09e63f88cf32dbe9f58f79124963",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-13-restore@sha256:99fd4033d9f5e536911915410abfc636b55f09e63f88cf32dbe9f58f79124963",
          "name": "restore_13"
        },
        {
          "digest": "sha256:2f23389e16635ff3e75774dac430c35dc4b314f291d9303d9296852eb0e436bc",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-13-pgpool2@sha256:2f23389e16635ff3e75774dac430c35dc4b314f291d9303d9296852eb0e436bc",
          "name": "pgpool2_13"
        },
        {
          "digest": "sha256:b8c93f9dc83332f547682e7c1bc9a691d20f91793a27cabe1798bd9ae8c96cb2",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-13-exporter@sha256:b8c93f9dc83332f547682e7c1bc9a691d20f91793a27cabe1798bd9ae8c96cb2",
          "name": "fepexporter_13"
        },
        {
          "digest": "sha256:61592bcccccee2e72521d0640f005d1902730319ad697aa0f4fc5485bf22989d",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-12-server@sha256:61592bcccccee2e72521d0640f005d1902730319ad697aa0f4fc5485bf22989d",
          "name": "fep_12"
        },
        {
          "digest": "sha256:f126c128c1301afa684bdf3258c78d1ec68c08aa6e5c085c5f939940947e5c94",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-12-backup@sha256:f126c128c1301afa684bdf3258c78d1ec68c08aa6e5c085c5f939940947e5c94",
          "name": "backup_12"
        },
        {
          "digest": "sha256:240da72ba4ef71031c1587a1f44b7631697346bb7699be52dcebd64e9f93d66e",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-12-restore@sha256:240da72ba4ef71031c1587a1f44b7631697346bb7699be52dcebd64e9f93d66e",
          "name": "restore_12"
        },
        {
          "digest": "sha256:6d58e56e6e7ab0c07d6095b1cab429c47ea3d294bf1656bd0a7d6efed098cb02",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-12-pgpool2@sha256:6d58e56e6e7ab0c07d6095b1cab429c47ea3d294bf1656bd0a7d6efed098cb02",
          "name": "pgpool2_12"
        }
      ],
      "replaces": null,
      "skip_range": "<4.1.9",
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.10",
      "version": "4.1.9",
      "version_original": "4.1.9"
    },
    {
      "_id": "638609eabca4ee9052d9b6be",
      "alm_examples": [
        {
          "api_version": "fep.fujitsu.io/v1",
          "kind": "FEPAction",
          "metadata": {
            "name": "new-fep-action"
          },
          "spec": {
            "fepAction": {
              "args": [
                "new-fep-sts-0",
                "new-fep-sts-1"
              ],
              "type": "reload"
            },
            "sysExtraLogging": false,
            "targetClusterName": "new-fep"
          }
        },
        {
          "api_version": "fep.fujitsu.io/v1",
          "kind": "FEPExporter",
          "metadata": {
            "name": "new-fep-exporter"
          },
          "spec": {
            "fepExporter": {
              "exporterLogLevel": "error",
              "fepClusterList": [
                "new-fep1"
              ],
              "image": {
                "pullPolicy": "IfNotPresent"
              },
              "mcSpec": {
                "limits": {
                  "cpu": "500m",
                  "memory": "700Mi"
                },
                "requests": {
                  "cpu": "200m",
                  "memory": "512Mi"
                }
              },
              "restartRequired": false,
              "sysExtraLogging": false,
              "userCustomQueries": "usr_example:\n  query: \"SELECT EXTRACT(EPOCH FROM (now() - pg_last_xact_replay_timestamp())) as lag\"\n  master: true\n  metrics:\n    - lag:\n        usage: \"GAUGE\"\n        description: \"Replication lag behind master in seconds\""
            }
          }
        },
        {
          "api_version": "fep.fujitsu.io/v1",
          "kind": "FEPLogging",
          "metadata": {
            "name": "new-fep-logging"
          },
          "spec": {
            "fepLogging": {
              "image": {
                "pullPolicy": "IfNotPresent"
              },
              "mcSpec": {
                "limits": {
                  "cpu": "500m",
                  "memory": "700Mi"
                },
                "requests": {
                  "cpu": "200m",
                  "memory": "512Mi"
                }
              },
              "restartRequired": false,
              "sysExtraLogging": false
            }
          }
        },
        {
          "api_version": "fep.fujitsu.io/v1",
          "kind": "FEPPgpool2",
          "metadata": {
            "name": "new-fep-pgpool2"
          },
          "spec": {
            "count": 2,
            "customhba": "local   all         all                               trust\nhost    all         all         127.0.0.1/32          trust\nhost    all         all         ::1/128               trust\n",
            "customlogsize": "128Mi",
            "customparams": "listen_addresses = '*'\npcp_listen_addresses = '*'\nnum_init_children = 32\nreserved_connections = 0\nenable_pool_hba = off\nallow_clear_text_frontend_auth = off\nauthentication_timeout = 80\nbackend_weight0 = 1\nbackend_weight1 = 1\nbackend_flag0 = 'ALWAYS_PRIMARY'\nbackend_flag1 = 'DISALLOW_TO_FAILOVER'\nconnection_cache = on\nmax_pool = 4\nlisten_backlog_multiplier = 2\nserialize_accept = off\nchild_life_time = 300\nclient_idle_limit = 0\nchild_max_connections = 0\nconnection_life_time = 0\nreset_query_list = 'ABORT; DISCARD ALL'\nclient_min_messages = info\nlog_min_messages = debug1\nlog_statement = on\nlog_per_node_statement = on\nlog_client_messages = on\nlog_hostname = on\nlog_connections = on\nlog_line_prefix = '%t: pid %p: '\nload_balance_mode = on\nignore_leading_white_space = on\nwhite_function_list = ''\nblack_function_list = 'currval,lastval,nextval,setval'\nblack_query_pattern_list = ''\ndatabase_redirect_preference_list = ''\napp_name_redirect_preference_list = ''\nallow_sql_comments = off\ndisable_load_balance_on_write = 'transaction'\nstatement_level_load_balance = on\nsr_check_period = 0\nsr_check_user = 'postgres'\ndelay_threshold = 0\nlog_standby_delay = 'none'\nssl = on\nssl_ciphers = 'HIGH:MEDIUM:+3DES:!aNULL'\nssl_prefer_server_ciphers = off\nssl_ecdh_curve = 'prime256v1'\nssl_dh_params_file = ''\nrelcache_expire = 0\nrelcache_size = 256\ncheck_temp_table = catalog\ncheck_unlogged_table = on\nenable_shared_relcache = on\nrelcache_query_target = primary\nwd_port0 = 9000\nfailover_on_backend_error = off\n",
            "custompcp": "none",
            "customsslcacert": "none",
            "customsslcert": "-----BEGIN CERTIFICATE-----\nMIIDTzCCAjegAwIBAgIUYssQ8I74US5g+1+Z7CHuaDgkZnEwDQYJKoZIhvcNAQEL\nBQAwNzEQMA4GA1UECgwHRnVqaXRzdTEjMCEGA1UEAwwaRkVQIFJvb3QgQ0EgZm9y\nIEt1YmVybmV0ZXMwHhcNMjEwMjA2MDM1MjI4WhcNMzEwMjA0MDM1MjI4WjA3MRAw\nDgYDVQQKDAdGdWppdHN1MSMwIQYDVQQDDBpGRVAgUm9vdCBDQSBmb3IgS3ViZXJu\nZXRlczCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBAMs97gUF0xkUzCgL\n7MiiDju9ySr/ziwjvcYU7jA9ML+SLmftMs3HtcYbAmSntqI+MDBSR/FAJTOoytuT\npV+mCFcGj2YAjDpliHPeNcUpbryy4YMChF3+MovkIwGCksxo5rhiWhGmoBYpA48P\n4Xe8SPlzqMzhFvNeKzyiUhvjutS2Y1Ss38lsTaurFPx64vQ2PaC54XzdwMptXtpb\ntYmWSzCpJWwxZ6lF3vitdA2w0tnBWNyctAd0+RIM/fvArxiIqseAux9t0uogm5to\nlRIhvekuxOpXBPEqtIYQ4j9XUW2JH8vUDnzPkPvjrq+A3Ug8OyyfGVrW7+VYXozu\nc4aP7P0CAwEAAaNTMFEwHQYDVR0OBBYEFBzCutQ7S74WEhS5V2sNEJBGyLpmMB8G\nA1UdIwQYMBaAFBzCutQ7S74WEhS5V2sNEJBGyLpmMA8GA1UdEwEB/wQFMAMBAf8w\nDQYJKoZIhvcNAQELBQADggEBAMDwD85RAaWEBptFgLzKw+9xEUy1vcZaonAuA1qc\nT342XTueyAugxkC11HwdCGgGS34VyctfMGqj4AW6pA2ez4tLrbOps4DmV4sw8uBL\n8pgRDgfly3ob9FEg2wa0hmrwX9jH5Bt4vySUE2785uPAqaspT2UNtTBxS85BUi1T\nsKId2Rtil6an281Z81wyWVI6Jm2D4MG0mbsiGcTPlCtdg/UljvDYymXlAvd4vNhl\nk9hDa13TgDqJKgKdTIcmZoNQdpEVgFcO0h9AEUy5AuLqxHq60dLfZ6ESGPlMI7Lm\ni4PzYbCnBmOe+7TnHcPSyrnehs66Ik+oifRd82eYS7vKjFw=\n-----END CERTIFICATE-----",
            "customsslkey": "none",
            "fepclustername": "new-fep",
            "imagePullPolicy": "IfNotPresent",
            "limits": {
              "cpu": "400m",
              "memory": "512Mi"
            },
            "requests": {
              "cpu": "200m",
              "memory": "256Mi"
            },
            "serviceport": 9999,
            "statusport": 9898
          }
        },
        {
          "api_version": "fep.fujitsu.io/v1",
          "kind": "FEPRestore",
          "metadata": {
            "name": "new-fep-restore"
          },
          "spec": {
            "fromFEPcluster": "new-fep",
            "imagePullPolicy": "IfNotPresent",
            "mcSpec": {
              "limits": {
                "cpu": "200m",
                "memory": "300Mi"
              },
              "requests": {
                "cpu": "100m",
                "memory": "200Mi"
              }
            },
            "restoretype": "latest",
            "sysExtraLogging": false,
            "toFEPcluster": "new-fep-2"
          }
        },
        {
          "api_version": "fep.fujitsu.io/v2",
          "kind": "FEPCluster",
          "metadata": {
            "name": "new-fep"
          },
          "spec": {
            "fep": {
              "customAnnotations": {
                "allDeployments": {}
              },
              "forceSsl": true,
              "image": {
                "pullPolicy": "IfNotPresent"
              },
              "instances": 1,
              "mcSpec": {
                "limits": {
                  "cpu": "500m",
                  "memory": "700Mi"
                },
                "requests": {
                  "cpu": "200m",
                  "memory": "512Mi"
                }
              },
              "podAntiAffinity": false,
              "podDisruptionBudget": false,
              "servicePort": 27500,
              "syncMode": "off",
              "sysExtraLogging": false
            },
            "fepChildCrVal": {
              "backup": {
                "image": {
                  "pullPolicy": "IfNotPresent"
                },
                "mcSpec": {
                  "limits": {
                    "cpu": 0.2,
                    "memory": "300Mi"
                  },
                  "requests": {
                    "cpu": 0.1,
                    "memory": "200Mi"
                  }
                },
                "pgbackrestParams": "# if log volume is not defined, log_directory should be\n# changed to '/database/userdata/data/log'\n[global]\nrepo1-retention-full=7\nrepo1-retention-full-type=time\nlog-path=/database/log/backup\n",
                "postScript": " ",
                "preScript": " ",
                "schedule": {
                  "num": 2
                },
                "schedule1": {
                  "schedule": "15 0 * * 0",
                  "type": "full"
                },
                "schedule2": {
                  "schedule": "15 0 * * 1-6",
                  "type": "incr"
                },
                "schedule3": {
                  "schedule": " ",
                  "type": " "
                },
                "schedule4": {
                  "schedule": " ",
                  "type": " "
                },
                "schedule5": {
                  "schedule": " ",
                  "type": " "
                }
              },
              "customPgAudit": "# define pg audit custom params here to override defaults.\n# if log volume is not defined, log_directory should be\n# changed to '/database/userdata/data/log'\n[output]\nlogger = 'auditlog'\nlog_directory = '/database/log/audit'\nlog_truncate_on_rotation = on\nlog_filename = 'pgaudit-%a.log'\nlog_rotation_age = 1d\nlog_rotation_size = 0\n[rule]\n",
              "customPgHba": "# define pg_hba custom rules here to be merged with default rules.\n# TYPE     DATABASE        USER        ADDRESS        METHOD\n",
              "customPgParams": "# define custom postgresql.conf parameters below to override defaults.\n# Current values are as per default FEP deployment\nshared_preload_libraries='pgx_datamasking,pg_prewarm,pg_stat_statements'\nsession_preload_libraries='pg_prewarm'\nmax_prepared_transactions = 100\nmax_worker_processes = 30\nmax_connections = 100\nwork_mem = 1MB\nmaintenance_work_mem = 12MB\nshared_buffers = 128MB\neffective_cache_size = 384MB\ncheckpoint_completion_target = 0.8\n# tcp parameters\ntcp_keepalives_idle = 30\ntcp_keepalives_interval = 10\ntcp_keepalives_count = 3\n# logging parameters in default fep installation\n# if log volume is not defined, log_directory should be\n# changed to '/database/userdata/data/log'\nlog_directory = '/database/log'\nlog_filename = 'logfile-%a.log'\nlog_file_mode = 0600\nlog_truncate_on_rotation = on\nlog_rotation_age = 1d\nlog_rotation_size = 0\nlog_checkpoints = on\nlog_line_prefix = '%e %t [%p]: [%l-1] user=%u,db=%d,app=%a,client=%h'\nlog_lock_waits = on\nlog_autovacuum_min_duration = 60s\nlogging_collector = on\npgaudit.config_file='/opt/app-root/src/pgaudit-cfg/pgaudit.conf'\nlog_replication_commands = on\nlog_min_messages = WARNING\nlog_destination = csvlog\n# wal_archive parameters in default fep installation\narchive_mode = on\narchive_command = 'pgbackrest --stanza=backupstanza --config=/database/userdata/pgbackrest.conf archive-push %p'\nwal_level = replica\nmax_wal_senders = 12\nwal_keep_segments = 64\ntrack_activities = on\ntrack_counts = on\npassword_encryption = 'md5'\n",
              "storage": {
                "archivewalVol": {
                  "size": "1Gi"
                },
                "backupVol": {
                  "size": "2Gi"
                },
                "dataVol": {
                  "size": "2Gi"
                },
                "logVol": {
                  "size": "1Gi"
                },
                "tablespaceVol": {
                  "size": "512Mi"
                },
                "walVol": {
                  "size": "1200Mi"
                }
              },
              "sysUsers": {
                "pgAdminPassword": "admin-password",
                "pgRewindPassword": "rewind_password",
                "pgRewindUser": "rewind_user",
                "pgdb": "mydb",
                "pgpassword": "mydbpassword",
                "pgreplpassword": "repluserpwd",
                "pgrepluser": "repluser",
                "pguser": "mydbuser",
                "tdepassphrase": "tde-passphrase"
              },
              "systemCertificates": {
                "cacrt": "-----BEGIN CERTIFICATE-----\nMIIDTzCCAjegAwIBAgIUYssQ8I74US5g+1+Z7CHuaDgkZnEwDQYJKoZIhvcNAQEL\nBQAwNzEQMA4GA1UECgwHRnVqaXRzdTEjMCEGA1UEAwwaRkVQIFJvb3QgQ0EgZm9y\nIEt1YmVybmV0ZXMwHhcNMjEwMjA2MDM1MjI4WhcNMzEwMjA0MDM1MjI4WjA3MRAw\nDgYDVQQKDAdGdWppdHN1MSMwIQYDVQQDDBpGRVAgUm9vdCBDQSBmb3IgS3ViZXJu\nZXRlczCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBAMs97gUF0xkUzCgL\n7MiiDju9ySr/ziwjvcYU7jA9ML+SLmftMs3HtcYbAmSntqI+MDBSR/FAJTOoytuT\npV+mCFcGj2YAjDpliHPeNcUpbryy4YMChF3+MovkIwGCksxo5rhiWhGmoBYpA48P\n4Xe8SPlzqMzhFvNeKzyiUhvjutS2Y1Ss38lsTaurFPx64vQ2PaC54XzdwMptXtpb\ntYmWSzCpJWwxZ6lF3vitdA2w0tnBWNyctAd0+RIM/fvArxiIqseAux9t0uogm5to\nlRIhvekuxOpXBPEqtIYQ4j9XUW2JH8vUDnzPkPvjrq+A3Ug8OyyfGVrW7+VYXozu\nc4aP7P0CAwEAAaNTMFEwHQYDVR0OBBYEFBzCutQ7S74WEhS5V2sNEJBGyLpmMB8G\nA1UdIwQYMBaAFBzCutQ7S74WEhS5V2sNEJBGyLpmMA8GA1UdEwEB/wQFMAMBAf8w\nDQYJKoZIhvcNAQELBQADggEBAMDwD85RAaWEBptFgLzKw+9xEUy1vcZaonAuA1qc\nT342XTueyAugxkC11HwdCGgGS34VyctfMGqj4AW6pA2ez4tLrbOps4DmV4sw8uBL\n8pgRDgfly3ob9FEg2wa0hmrwX9jH5Bt4vySUE2785uPAqaspT2UNtTBxS85BUi1T\nsKId2Rtil6an281Z81wyWVI6Jm2D4MG0mbsiGcTPlCtdg/UljvDYymXlAvd4vNhl\nk9hDa13TgDqJKgKdTIcmZoNQdpEVgFcO0h9AEUy5AuLqxHq60dLfZ6ESGPlMI7Lm\ni4PzYbCnBmOe+7TnHcPSyrnehs66Ik+oifRd82eYS7vKjFw=\n-----END CERTIFICATE-----",
                "crt": "-----BEGIN CERTIFICATE-----\nMIIDUTCCAjmgAwIBAgIRAMocW3qMoHrD6qRvMPppMkMwDQYJKoZIhvcNAQELBQAw\nNzEQMA4GA1UECgwHRnVqaXRzdTEjMCEGA1UEAwwaRkVQIFJvb3QgQ0EgZm9yIEt1\nYmVybmV0ZXMwHhcNMjEwMjA2MDQzMjM2WhcNMjYwMjA1MDQzMjM2WjA/MRAwDgYD\nVQQKEwdGdWppdHN1MSswKQYDVQQDEyJGVUpJVFNVIEVudGVycHJpc2UgUG9zdGdy\nZXMgU2VydmVyMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEA4AI33yvH\nZws+jta6qpV6wzJqF8odIfTIpCfbrVcUUtLFKJ1I2e4SceTKi6O3C/I1XuvWlpng\n5IO65+fQQLO06z1/AuQT78YUn/Wlm9x1aHVsv4ANB5JWWqDOjrRT3o7nRPGXfila\nbP0rGE2mJJcVR9nExJ3IeaktgT3sb8YlXvtchyYpmjdbfxabTz07ig0+6/cwKoRR\nxOK8Uf7f5euE0cI/490J6r5Rs4lgD8sIQNCUFlTFYvmAH7gcdssSFBt8NPlUATHE\nsoFmlW0DKCJWNhTLOht+s6L/1zwTHLjPG2pdkG6Wdgmu5H2pDml8CDNLDv98Aj7i\n+I5SRKKcVPlnuQIDAQABo1AwTjAdBgNVHSUEFjAUBggrBgEFBQcDAQYIKwYBBQUH\nAwIwDAYDVR0TAQH/BAIwADAfBgNVHSMEGDAWgBQcwrrUO0u+FhIUuVdrDRCQRsi6\nZjANBgkqhkiG9w0BAQsFAAOCAQEAm5dxBoI9pScOCvRAchg4CprdRDSJb9K6yB3O\nnCAxnM47iHeXnY3WlnI388kHu8DU7O4ba1tJbGs3KY9KzioPk43pU12jWkO1onoF\n+mTDjx/Ef1cYWA9r5q/LtgTa6Q2sxV4O2x67QW82aAnaxO34dV5zWCPIvAoovZBV\nHRT+BgCg3r2vD1RGKK2nl1aYJtWhO1SZubam+VttdZ/vbM9oOJctxmImsEtBXjkY\nKteePdQtLL5o03JhyXWyRshCq+HMmKf2KgyY8gvydGcP4eLQdBWcW40LcnVq6UjT\n0kJycJEKngMVademq1ZWHGaiYB7hyT6GhgIcHUJ2cKrPgbEh1Q==\n-----END CERTIFICATE-----",
                "key": "-----BEGIN RSA PRIVATE KEY-----\nMIIEowIBAAKCAQEA4AI33yvHZws+jta6qpV6wzJqF8odIfTIpCfbrVcUUtLFKJ1I\n2e4SceTKi6O3C/I1XuvWlpng5IO65+fQQLO06z1/AuQT78YUn/Wlm9x1aHVsv4AN\nB5JWWqDOjrRT3o7nRPGXfilabP0rGE2mJJcVR9nExJ3IeaktgT3sb8YlXvtchyYp\nmjdbfxabTz07ig0+6/cwKoRRxOK8Uf7f5euE0cI/490J6r5Rs4lgD8sIQNCUFlTF\nYvmAH7gcdssSFBt8NPlUATHEsoFmlW0DKCJWNhTLOht+s6L/1zwTHLjPG2pdkG6W\ndgmu5H2pDml8CDNLDv98Aj7i+I5SRKKcVPlnuQIDAQABAoIBAFPQYKlOzw/+BA0b\nyMIUpdctIMb/54CR/xR0mVw1DbSjigNVPjHUQvB8Y1B2FAITQObgJO06bAv0QdWN\nRb0/v/yYiNJDFjaLjaIAHlO/2+oWrXbFaZqgpVDJhB+e1xaZr2x7XGxm+p925k30\nl6pvIRY+I8JRKvZiV1VZHwL/R3JOtPr++xMZtLVjVOI+f+ySqJ+TZHuAjm49EKxj\ncEmmJ28b7QcziXsvKy00f+zbqLIBKXQdZAFU5eEr1BsDRXdRW+Kf0XIvftuy4BJZ\nvoKT+VGhEvF/qysswL4+6IAO6tpuYnnM0Y2d3sOGoWPkTcQK0MekYKzL/WmtCjNs\n9hodJtECgYEA5EWyhEOf4uOKe5TDp697UCUvXLoOR58FDe/S8XNvScn29jjOkqIg\nOMoqo9xAkJTNTzqn5UUdt1x/pgM2NxlPLFijrc0zQlX3SoOO2ryDd9WNi7YKtN16\nKJqa536WeZu2OEbuAZ+S3GALVy1RPeTNPnUOmKnF06DjDUGzLNCZy10CgYEA+zfw\n952DWuz1U0Z4wvAEqqcgUKXPKrkTXV/iUnjkDkrLYVr0ZofDNTXrdHl+UedFmaOC\ncieZn6DNhcdz5tKtyysGMH3g/qs9PfoGUngvcXsy0Egk04l3x1jc8TTCLqXZXYaQ\nHMsx51n+R58oncPtzYSUOr9qQ6PbC2CstTbFJA0CgYEAjGEsUliAB/jknfEzjXjG\nPdhQUxb8VyE864Az2lah9t/kJzFyIAziAeqZ5GE7t247AGFTBRTHHI8e1Qoemi3P\nWbc9GVIbFs1lIYbcIDpUIyrKPEP8O5QEXtoNLxXTFgAjRGKiVY87spjCAJ+W2ZhO\ne/1it5GYXfgQCYQA2yuBmOUCgYANRkR2YR1axaCk+NlSu6oTdmdPu6M5x7PNQE7O\nOtMaKjua9lppvIzFGAdMDUtueoEEAE7ZR1xnwfB6PDLUpJdIYAqgr1YfPt8qkjaZ\nTv56yZ7CwL0pbF8m6nwqRrZoDp1wwraEvvvxFKFKGY/k3kCHlpTakdjEoDjn3gDi\nRnWeVQKBgCEneMSzucei5LRppRtRaJw/Btll8qlPMlX3W7dxQ3cLwpmLOn0m51Fp\nPIZ44zYK8R6fu4+/sSrlfaIg86Ugeufp6YNxyNROKxUGza5vDIu5OftwWtBeg+UK\nZ8lLWNdX6pp7WMujmF3H1DrkBbauYMUKZ4UxUYtelgHERMePIxwb\n-----END RSA PRIVATE KEY-----"
              }
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [
        "amd64",
        "ppc64le",
        "s390x"
      ],
      "bundle_path": "registry.connect.redhat.com/fujitsu-postgres/fujitsu-enterprise-postgres-operator-bundle@sha256:94fa991b1fdaa56ee907acd562a163dcc52284719b61188f823f2b0f6dc6f436",
      "bundle_path_digest": "sha256:94fa991b1fdaa56ee907acd562a163dcc52284719b61188f823f2b0f6dc6f436",
      "capabilities": [
        "Auto Pilot"
      ],
      "channel_name": "stable",
      "creation_date": "2022-11-29T13:32:26.691000+00:00",
      "csv_description": "FUJITSU Enterprise Postgres 14 delivers an enterprise-grade PostgreSQL on OpenShift Container Platform.\n\nThis solution provides the flexibility of a hybrid cloud solution while delivering an enhanced distribution of PostgreSQL to support enterprise-level workloads and provide improved deployment and management, availability, performance, data governance and security.  \n\nAvailable as a multi-architecture container built for both amd64, s390x and ppc64le.\n\nThe download and Use of the Product is strictly subject to the terms of the End User License Agreement with Fujitsu Limited found at https://www.fast.fujitsu.com/fujitsu-enterprise-postgres-license-agreements. Where the Product that has been embedded as a whole or part into a third party program, only Authorised Customers may download and use the Product.\n",
      "csv_display_name": "FUJITSU Enterprise Postgres Operator",
      "csv_metadata_description": "OpenShift Operator for Fujitsu Enterprise Postgres",
      "csv_name": "fujitsu-enterprise-postgres-operator.v4.1.9",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": true,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-11-29T13:32:26.691000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.9",
      "organization": "certified-operators",
      "package": "fujitsu-enterprise-postgres-operator",
      "provided_apis": [
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPRestore",
          "plural": "feprestores",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPUpgrade",
          "plural": "fepupgrades",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPAutoscale",
          "plural": "fepautoscales",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPPgpool2",
          "plural": "feppgpool2s",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPVolume",
          "plural": "fepvolumes",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPBackup",
          "plural": "fepbackups",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPConfig",
          "plural": "fepconfigs",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPExporter",
          "plural": "fepexporters",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPCluster",
          "plural": "fepclusters",
          "version": "v2"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPAction",
          "plural": "fepactions",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPPgpool2Cert",
          "plural": "feppgpool2certs",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPLogging",
          "plural": "feploggings",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPCert",
          "plural": "fepcerts",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPUser",
          "plural": "fepusers",
          "version": "v1"
        }
      ],
      "provider": "Fujitsu",
      "related_images": [
        {
          "digest": "sha256:a07987c0b4d47c6ec53c2127ead56bb420ea493e9b5d1bb79064b5f1a1f57d91",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-14-server@sha256:a07987c0b4d47c6ec53c2127ead56bb420ea493e9b5d1bb79064b5f1a1f57d91",
          "name": "fep-14"
        },
        {
          "digest": "sha256:8690a50c3c900686fd5e8a65b04fee0125e4cf500c44845f3c4392a1ea1ef873",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-14-backup@sha256:8690a50c3c900686fd5e8a65b04fee0125e4cf500c44845f3c4392a1ea1ef873",
          "name": "backup-14"
        },
        {
          "digest": "sha256:3759bd7513228b49733b23db24fdeb2c6a31b3f93c7ac5f84ab0d8b9939e07ce",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-14-restore@sha256:3759bd7513228b49733b23db24fdeb2c6a31b3f93c7ac5f84ab0d8b9939e07ce",
          "name": "restore-14"
        },
        {
          "digest": "sha256:37729f8332f647eba5f8209ea9ce95aaa9ab6674774afc2931c4aa59dc51ae90",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-14-pgpool2@sha256:37729f8332f647eba5f8209ea9ce95aaa9ab6674774afc2931c4aa59dc51ae90",
          "name": "pgpool2-14"
        },
        {
          "digest": "sha256:f9e28fe00b2caee1b15e4867e86dad39c8e6913f137fe689fd30c18b9f19881c",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-exporter@sha256:f9e28fe00b2caee1b15e4867e86dad39c8e6913f137fe689fd30c18b9f19881c",
          "name": "exporter-14"
        },
        {
          "digest": "sha256:fb8bffe2c1833b7061a4856ecabda811ed16ff661d38b8ea6a5bc826bc0ccbc3",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-fluentd@sha256:fb8bffe2c1833b7061a4856ecabda811ed16ff661d38b8ea6a5bc826bc0ccbc3",
          "name": "fluentd-14"
        },
        {
          "digest": "sha256:326005d0cfb049f36759e6ef022c67029dbac1865810be3c850c1bc1cf604e8d",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-fluentbit@sha256:326005d0cfb049f36759e6ef022c67029dbac1865810be3c850c1bc1cf604e8d",
          "name": "fluentbit-14"
        },
        {
          "digest": "sha256:4f75b63a09ad3da34684db497814bfa8eae8356e43963ce67fbfcb48e4f350ac",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-cronjob@sha256:4f75b63a09ad3da34684db497814bfa8eae8356e43963ce67fbfcb48e4f350ac",
          "name": "cronjob-14"
        },
        {
          "digest": "sha256:793a06eefe93a7613cfdf1a89febf5603244f453a1e804760d93e6f182cc522a",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-14-upgrade@sha256:793a06eefe93a7613cfdf1a89febf5603244f453a1e804760d93e6f182cc522a",
          "name": "upgrade-14"
        },
        {
          "digest": "sha256:80f5d30c66844eb05de5d67999198d9ea3d457f8fbe81358de0b37db0f15f0e3",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-13-server@sha256:80f5d30c66844eb05de5d67999198d9ea3d457f8fbe81358de0b37db0f15f0e3",
          "name": "fep-13"
        },
        {
          "digest": "sha256:c27c4b52361e73dd460eac5d8fe4765f582d0ffd4f4fc072bff4523b785f47e5",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-13-backup@sha256:c27c4b52361e73dd460eac5d8fe4765f582d0ffd4f4fc072bff4523b785f47e5",
          "name": "backup-13"
        },
        {
          "digest": "sha256:99fd4033d9f5e536911915410abfc636b55f09e63f88cf32dbe9f58f79124963",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-13-restore@sha256:99fd4033d9f5e536911915410abfc636b55f09e63f88cf32dbe9f58f79124963",
          "name": "restore-13"
        },
        {
          "digest": "sha256:2f23389e16635ff3e75774dac430c35dc4b314f291d9303d9296852eb0e436bc",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-13-pgpool2@sha256:2f23389e16635ff3e75774dac430c35dc4b314f291d9303d9296852eb0e436bc",
          "name": "pgpool2-13"
        },
        {
          "digest": "sha256:b8c93f9dc83332f547682e7c1bc9a691d20f91793a27cabe1798bd9ae8c96cb2",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-13-exporter@sha256:b8c93f9dc83332f547682e7c1bc9a691d20f91793a27cabe1798bd9ae8c96cb2",
          "name": "fepexporter-13"
        },
        {
          "digest": "sha256:61592bcccccee2e72521d0640f005d1902730319ad697aa0f4fc5485bf22989d",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-12-server@sha256:61592bcccccee2e72521d0640f005d1902730319ad697aa0f4fc5485bf22989d",
          "name": "fep-12"
        },
        {
          "digest": "sha256:f126c128c1301afa684bdf3258c78d1ec68c08aa6e5c085c5f939940947e5c94",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-12-backup@sha256:f126c128c1301afa684bdf3258c78d1ec68c08aa6e5c085c5f939940947e5c94",
          "name": "backup-12"
        },
        {
          "digest": "sha256:240da72ba4ef71031c1587a1f44b7631697346bb7699be52dcebd64e9f93d66e",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-12-restore@sha256:240da72ba4ef71031c1587a1f44b7631697346bb7699be52dcebd64e9f93d66e",
          "name": "restore-12"
        },
        {
          "digest": "sha256:6d58e56e6e7ab0c07d6095b1cab429c47ea3d294bf1656bd0a7d6efed098cb02",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-12-pgpool2@sha256:6d58e56e6e7ab0c07d6095b1cab429c47ea3d294bf1656bd0a7d6efed098cb02",
          "name": "pgpool2-12"
        },
        {
          "digest": "sha256:ecf7b692793b5b7ad2e848f335456742901ef0ef6b0b171d4771f0642e76714e",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-operator@sha256:ecf7b692793b5b7ad2e848f335456742901ef0ef6b0b171d4771f0642e76714e",
          "name": "fujitsu-enterprise-postgres-operator-ecf7b692793b5b7ad2e848f335456742901ef0ef6b0b171d4771f0642e76714e-annotation"
        },
        {
          "digest": "sha256:ecf7b692793b5b7ad2e848f335456742901ef0ef6b0b171d4771f0642e76714e",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-operator@sha256:ecf7b692793b5b7ad2e848f335456742901ef0ef6b0b171d4771f0642e76714e",
          "name": "fep-ansible-operator"
        },
        {
          "digest": "sha256:a07987c0b4d47c6ec53c2127ead56bb420ea493e9b5d1bb79064b5f1a1f57d91",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-14-server@sha256:a07987c0b4d47c6ec53c2127ead56bb420ea493e9b5d1bb79064b5f1a1f57d91",
          "name": "fep"
        },
        {
          "digest": "sha256:8690a50c3c900686fd5e8a65b04fee0125e4cf500c44845f3c4392a1ea1ef873",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-14-backup@sha256:8690a50c3c900686fd5e8a65b04fee0125e4cf500c44845f3c4392a1ea1ef873",
          "name": "backup"
        },
        {
          "digest": "sha256:3759bd7513228b49733b23db24fdeb2c6a31b3f93c7ac5f84ab0d8b9939e07ce",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-14-restore@sha256:3759bd7513228b49733b23db24fdeb2c6a31b3f93c7ac5f84ab0d8b9939e07ce",
          "name": "restore"
        },
        {
          "digest": "sha256:37729f8332f647eba5f8209ea9ce95aaa9ab6674774afc2931c4aa59dc51ae90",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-14-pgpool2@sha256:37729f8332f647eba5f8209ea9ce95aaa9ab6674774afc2931c4aa59dc51ae90",
          "name": "pgpool2"
        },
        {
          "digest": "sha256:f9e28fe00b2caee1b15e4867e86dad39c8e6913f137fe689fd30c18b9f19881c",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-exporter@sha256:f9e28fe00b2caee1b15e4867e86dad39c8e6913f137fe689fd30c18b9f19881c",
          "name": "fepexporter"
        },
        {
          "digest": "sha256:fb8bffe2c1833b7061a4856ecabda811ed16ff661d38b8ea6a5bc826bc0ccbc3",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-fluentd@sha256:fb8bffe2c1833b7061a4856ecabda811ed16ff661d38b8ea6a5bc826bc0ccbc3",
          "name": "feplogging"
        },
        {
          "digest": "sha256:326005d0cfb049f36759e6ef022c67029dbac1865810be3c850c1bc1cf604e8d",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-fluentbit@sha256:326005d0cfb049f36759e6ef022c67029dbac1865810be3c850c1bc1cf604e8d",
          "name": "feplogging_fluentbit"
        },
        {
          "digest": "sha256:4f75b63a09ad3da34684db497814bfa8eae8356e43963ce67fbfcb48e4f350ac",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-cronjob@sha256:4f75b63a09ad3da34684db497814bfa8eae8356e43963ce67fbfcb48e4f350ac",
          "name": "cronjob"
        },
        {
          "digest": "sha256:793a06eefe93a7613cfdf1a89febf5603244f453a1e804760d93e6f182cc522a",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-14-upgrade@sha256:793a06eefe93a7613cfdf1a89febf5603244f453a1e804760d93e6f182cc522a",
          "name": "upgrade"
        },
        {
          "digest": "sha256:a07987c0b4d47c6ec53c2127ead56bb420ea493e9b5d1bb79064b5f1a1f57d91",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-14-server@sha256:a07987c0b4d47c6ec53c2127ead56bb420ea493e9b5d1bb79064b5f1a1f57d91",
          "name": "fep_14"
        },
        {
          "digest": "sha256:8690a50c3c900686fd5e8a65b04fee0125e4cf500c44845f3c4392a1ea1ef873",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-14-backup@sha256:8690a50c3c900686fd5e8a65b04fee0125e4cf500c44845f3c4392a1ea1ef873",
          "name": "backup_14"
        },
        {
          "digest": "sha256:3759bd7513228b49733b23db24fdeb2c6a31b3f93c7ac5f84ab0d8b9939e07ce",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-14-restore@sha256:3759bd7513228b49733b23db24fdeb2c6a31b3f93c7ac5f84ab0d8b9939e07ce",
          "name": "restore_14"
        },
        {
          "digest": "sha256:37729f8332f647eba5f8209ea9ce95aaa9ab6674774afc2931c4aa59dc51ae90",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-14-pgpool2@sha256:37729f8332f647eba5f8209ea9ce95aaa9ab6674774afc2931c4aa59dc51ae90",
          "name": "pgpool2_14"
        },
        {
          "digest": "sha256:f9e28fe00b2caee1b15e4867e86dad39c8e6913f137fe689fd30c18b9f19881c",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-exporter@sha256:f9e28fe00b2caee1b15e4867e86dad39c8e6913f137fe689fd30c18b9f19881c",
          "name": "fepexporter_14"
        },
        {
          "digest": "sha256:fb8bffe2c1833b7061a4856ecabda811ed16ff661d38b8ea6a5bc826bc0ccbc3",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-fluentd@sha256:fb8bffe2c1833b7061a4856ecabda811ed16ff661d38b8ea6a5bc826bc0ccbc3",
          "name": "feplogging_14"
        },
        {
          "digest": "sha256:326005d0cfb049f36759e6ef022c67029dbac1865810be3c850c1bc1cf604e8d",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-fluentbit@sha256:326005d0cfb049f36759e6ef022c67029dbac1865810be3c850c1bc1cf604e8d",
          "name": "feplogging_fluentbit_14"
        },
        {
          "digest": "sha256:4f75b63a09ad3da34684db497814bfa8eae8356e43963ce67fbfcb48e4f350ac",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-cronjob@sha256:4f75b63a09ad3da34684db497814bfa8eae8356e43963ce67fbfcb48e4f350ac",
          "name": "cronjob_14"
        },
        {
          "digest": "sha256:793a06eefe93a7613cfdf1a89febf5603244f453a1e804760d93e6f182cc522a",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-14-upgrade@sha256:793a06eefe93a7613cfdf1a89febf5603244f453a1e804760d93e6f182cc522a",
          "name": "upgrade_14"
        },
        {
          "digest": "sha256:80f5d30c66844eb05de5d67999198d9ea3d457f8fbe81358de0b37db0f15f0e3",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-13-server@sha256:80f5d30c66844eb05de5d67999198d9ea3d457f8fbe81358de0b37db0f15f0e3",
          "name": "fep_13"
        },
        {
          "digest": "sha256:c27c4b52361e73dd460eac5d8fe4765f582d0ffd4f4fc072bff4523b785f47e5",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-13-backup@sha256:c27c4b52361e73dd460eac5d8fe4765f582d0ffd4f4fc072bff4523b785f47e5",
          "name": "backup_13"
        },
        {
          "digest": "sha256:99fd4033d9f5e536911915410abfc636b55f09e63f88cf32dbe9f58f79124963",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-13-restore@sha256:99fd4033d9f5e536911915410abfc636b55f09e63f88cf32dbe9f58f79124963",
          "name": "restore_13"
        },
        {
          "digest": "sha256:2f23389e16635ff3e75774dac430c35dc4b314f291d9303d9296852eb0e436bc",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-13-pgpool2@sha256:2f23389e16635ff3e75774dac430c35dc4b314f291d9303d9296852eb0e436bc",
          "name": "pgpool2_13"
        },
        {
          "digest": "sha256:b8c93f9dc83332f547682e7c1bc9a691d20f91793a27cabe1798bd9ae8c96cb2",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-13-exporter@sha256:b8c93f9dc83332f547682e7c1bc9a691d20f91793a27cabe1798bd9ae8c96cb2",
          "name": "fepexporter_13"
        },
        {
          "digest": "sha256:61592bcccccee2e72521d0640f005d1902730319ad697aa0f4fc5485bf22989d",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-12-server@sha256:61592bcccccee2e72521d0640f005d1902730319ad697aa0f4fc5485bf22989d",
          "name": "fep_12"
        },
        {
          "digest": "sha256:f126c128c1301afa684bdf3258c78d1ec68c08aa6e5c085c5f939940947e5c94",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-12-backup@sha256:f126c128c1301afa684bdf3258c78d1ec68c08aa6e5c085c5f939940947e5c94",
          "name": "backup_12"
        },
        {
          "digest": "sha256:240da72ba4ef71031c1587a1f44b7631697346bb7699be52dcebd64e9f93d66e",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-12-restore@sha256:240da72ba4ef71031c1587a1f44b7631697346bb7699be52dcebd64e9f93d66e",
          "name": "restore_12"
        },
        {
          "digest": "sha256:6d58e56e6e7ab0c07d6095b1cab429c47ea3d294bf1656bd0a7d6efed098cb02",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-12-pgpool2@sha256:6d58e56e6e7ab0c07d6095b1cab429c47ea3d294bf1656bd0a7d6efed098cb02",
          "name": "pgpool2_12"
        }
      ],
      "replaces": null,
      "skip_range": "<4.1.9",
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.9",
      "version": "4.1.9",
      "version_original": "4.1.9"
    },
    {
      "_id": "63860c51bca4ee9052d9c800",
      "alm_examples": [
        {
          "api_version": "fep.fujitsu.io/v1",
          "kind": "FEPAction",
          "metadata": {
            "name": "new-fep-action"
          },
          "spec": {
            "fepAction": {
              "args": [
                "new-fep-sts-0",
                "new-fep-sts-1"
              ],
              "type": "reload"
            },
            "sysExtraLogging": false,
            "targetClusterName": "new-fep"
          }
        },
        {
          "api_version": "fep.fujitsu.io/v1",
          "kind": "FEPExporter",
          "metadata": {
            "name": "new-fep-exporter"
          },
          "spec": {
            "fepExporter": {
              "exporterLogLevel": "error",
              "fepClusterList": [
                "new-fep1"
              ],
              "image": {
                "pullPolicy": "IfNotPresent"
              },
              "mcSpec": {
                "limits": {
                  "cpu": "500m",
                  "memory": "700Mi"
                },
                "requests": {
                  "cpu": "200m",
                  "memory": "512Mi"
                }
              },
              "restartRequired": false,
              "sysExtraLogging": false,
              "userCustomQueries": "usr_example:\n  query: \"SELECT EXTRACT(EPOCH FROM (now() - pg_last_xact_replay_timestamp())) as lag\"\n  master: true\n  metrics:\n    - lag:\n        usage: \"GAUGE\"\n        description: \"Replication lag behind master in seconds\""
            }
          }
        },
        {
          "api_version": "fep.fujitsu.io/v1",
          "kind": "FEPLogging",
          "metadata": {
            "name": "new-fep-logging"
          },
          "spec": {
            "fepLogging": {
              "image": {
                "pullPolicy": "IfNotPresent"
              },
              "mcSpec": {
                "limits": {
                  "cpu": "500m",
                  "memory": "700Mi"
                },
                "requests": {
                  "cpu": "200m",
                  "memory": "512Mi"
                }
              },
              "restartRequired": false,
              "sysExtraLogging": false
            }
          }
        },
        {
          "api_version": "fep.fujitsu.io/v1",
          "kind": "FEPPgpool2",
          "metadata": {
            "name": "new-fep-pgpool2"
          },
          "spec": {
            "count": 2,
            "customhba": "local   all         all                               trust\nhost    all         all         127.0.0.1/32          trust\nhost    all         all         ::1/128               trust\n",
            "customlogsize": "128Mi",
            "customparams": "listen_addresses = '*'\npcp_listen_addresses = '*'\nnum_init_children = 32\nreserved_connections = 0\nenable_pool_hba = off\nallow_clear_text_frontend_auth = off\nauthentication_timeout = 80\nbackend_weight0 = 1\nbackend_weight1 = 1\nbackend_flag0 = 'ALWAYS_PRIMARY'\nbackend_flag1 = 'DISALLOW_TO_FAILOVER'\nconnection_cache = on\nmax_pool = 4\nlisten_backlog_multiplier = 2\nserialize_accept = off\nchild_life_time = 300\nclient_idle_limit = 0\nchild_max_connections = 0\nconnection_life_time = 0\nreset_query_list = 'ABORT; DISCARD ALL'\nclient_min_messages = info\nlog_min_messages = debug1\nlog_statement = on\nlog_per_node_statement = on\nlog_client_messages = on\nlog_hostname = on\nlog_connections = on\nlog_line_prefix = '%t: pid %p: '\nload_balance_mode = on\nignore_leading_white_space = on\nwhite_function_list = ''\nblack_function_list = 'currval,lastval,nextval,setval'\nblack_query_pattern_list = ''\ndatabase_redirect_preference_list = ''\napp_name_redirect_preference_list = ''\nallow_sql_comments = off\ndisable_load_balance_on_write = 'transaction'\nstatement_level_load_balance = on\nsr_check_period = 0\nsr_check_user = 'postgres'\ndelay_threshold = 0\nlog_standby_delay = 'none'\nssl = on\nssl_ciphers = 'HIGH:MEDIUM:+3DES:!aNULL'\nssl_prefer_server_ciphers = off\nssl_ecdh_curve = 'prime256v1'\nssl_dh_params_file = ''\nrelcache_expire = 0\nrelcache_size = 256\ncheck_temp_table = catalog\ncheck_unlogged_table = on\nenable_shared_relcache = on\nrelcache_query_target = primary\nwd_port0 = 9000\nfailover_on_backend_error = off\n",
            "custompcp": "none",
            "customsslcacert": "none",
            "customsslcert": "-----BEGIN CERTIFICATE-----\nMIIDTzCCAjegAwIBAgIUYssQ8I74US5g+1+Z7CHuaDgkZnEwDQYJKoZIhvcNAQEL\nBQAwNzEQMA4GA1UECgwHRnVqaXRzdTEjMCEGA1UEAwwaRkVQIFJvb3QgQ0EgZm9y\nIEt1YmVybmV0ZXMwHhcNMjEwMjA2MDM1MjI4WhcNMzEwMjA0MDM1MjI4WjA3MRAw\nDgYDVQQKDAdGdWppdHN1MSMwIQYDVQQDDBpGRVAgUm9vdCBDQSBmb3IgS3ViZXJu\nZXRlczCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBAMs97gUF0xkUzCgL\n7MiiDju9ySr/ziwjvcYU7jA9ML+SLmftMs3HtcYbAmSntqI+MDBSR/FAJTOoytuT\npV+mCFcGj2YAjDpliHPeNcUpbryy4YMChF3+MovkIwGCksxo5rhiWhGmoBYpA48P\n4Xe8SPlzqMzhFvNeKzyiUhvjutS2Y1Ss38lsTaurFPx64vQ2PaC54XzdwMptXtpb\ntYmWSzCpJWwxZ6lF3vitdA2w0tnBWNyctAd0+RIM/fvArxiIqseAux9t0uogm5to\nlRIhvekuxOpXBPEqtIYQ4j9XUW2JH8vUDnzPkPvjrq+A3Ug8OyyfGVrW7+VYXozu\nc4aP7P0CAwEAAaNTMFEwHQYDVR0OBBYEFBzCutQ7S74WEhS5V2sNEJBGyLpmMB8G\nA1UdIwQYMBaAFBzCutQ7S74WEhS5V2sNEJBGyLpmMA8GA1UdEwEB/wQFMAMBAf8w\nDQYJKoZIhvcNAQELBQADggEBAMDwD85RAaWEBptFgLzKw+9xEUy1vcZaonAuA1qc\nT342XTueyAugxkC11HwdCGgGS34VyctfMGqj4AW6pA2ez4tLrbOps4DmV4sw8uBL\n8pgRDgfly3ob9FEg2wa0hmrwX9jH5Bt4vySUE2785uPAqaspT2UNtTBxS85BUi1T\nsKId2Rtil6an281Z81wyWVI6Jm2D4MG0mbsiGcTPlCtdg/UljvDYymXlAvd4vNhl\nk9hDa13TgDqJKgKdTIcmZoNQdpEVgFcO0h9AEUy5AuLqxHq60dLfZ6ESGPlMI7Lm\ni4PzYbCnBmOe+7TnHcPSyrnehs66Ik+oifRd82eYS7vKjFw=\n-----END CERTIFICATE-----",
            "customsslkey": "none",
            "fepclustername": "new-fep",
            "imagePullPolicy": "IfNotPresent",
            "limits": {
              "cpu": "400m",
              "memory": "512Mi"
            },
            "requests": {
              "cpu": "200m",
              "memory": "256Mi"
            },
            "serviceport": 9999,
            "statusport": 9898
          }
        },
        {
          "api_version": "fep.fujitsu.io/v1",
          "kind": "FEPRestore",
          "metadata": {
            "name": "new-fep-restore"
          },
          "spec": {
            "fromFEPcluster": "new-fep",
            "imagePullPolicy": "IfNotPresent",
            "mcSpec": {
              "limits": {
                "cpu": "200m",
                "memory": "300Mi"
              },
              "requests": {
                "cpu": "100m",
                "memory": "200Mi"
              }
            },
            "restoretype": "latest",
            "sysExtraLogging": false,
            "toFEPcluster": "new-fep-2"
          }
        },
        {
          "api_version": "fep.fujitsu.io/v2",
          "kind": "FEPCluster",
          "metadata": {
            "name": "new-fep"
          },
          "spec": {
            "fep": {
              "customAnnotations": {
                "allDeployments": {}
              },
              "forceSsl": true,
              "image": {
                "pullPolicy": "IfNotPresent"
              },
              "instances": 1,
              "mcSpec": {
                "limits": {
                  "cpu": "500m",
                  "memory": "700Mi"
                },
                "requests": {
                  "cpu": "200m",
                  "memory": "512Mi"
                }
              },
              "podAntiAffinity": false,
              "podDisruptionBudget": false,
              "servicePort": 27500,
              "syncMode": "off",
              "sysExtraLogging": false
            },
            "fepChildCrVal": {
              "backup": {
                "image": {
                  "pullPolicy": "IfNotPresent"
                },
                "mcSpec": {
                  "limits": {
                    "cpu": 0.2,
                    "memory": "300Mi"
                  },
                  "requests": {
                    "cpu": 0.1,
                    "memory": "200Mi"
                  }
                },
                "pgbackrestParams": "# if log volume is not defined, log_directory should be\n# changed to '/database/userdata/data/log'\n[global]\nrepo1-retention-full=7\nrepo1-retention-full-type=time\nlog-path=/database/log/backup\n",
                "postScript": " ",
                "preScript": " ",
                "schedule": {
                  "num": 2
                },
                "schedule1": {
                  "schedule": "15 0 * * 0",
                  "type": "full"
                },
                "schedule2": {
                  "schedule": "15 0 * * 1-6",
                  "type": "incr"
                },
                "schedule3": {
                  "schedule": " ",
                  "type": " "
                },
                "schedule4": {
                  "schedule": " ",
                  "type": " "
                },
                "schedule5": {
                  "schedule": " ",
                  "type": " "
                }
              },
              "customPgAudit": "# define pg audit custom params here to override defaults.\n# if log volume is not defined, log_directory should be\n# changed to '/database/userdata/data/log'\n[output]\nlogger = 'auditlog'\nlog_directory = '/database/log/audit'\nlog_truncate_on_rotation = on\nlog_filename = 'pgaudit-%a.log'\nlog_rotation_age = 1d\nlog_rotation_size = 0\n[rule]\n",
              "customPgHba": "# define pg_hba custom rules here to be merged with default rules.\n# TYPE     DATABASE        USER        ADDRESS        METHOD\n",
              "customPgParams": "# define custom postgresql.conf parameters below to override defaults.\n# Current values are as per default FEP deployment\nshared_preload_libraries='pgx_datamasking,pg_prewarm,pg_stat_statements'\nsession_preload_libraries='pg_prewarm'\nmax_prepared_transactions = 100\nmax_worker_processes = 30\nmax_connections = 100\nwork_mem = 1MB\nmaintenance_work_mem = 12MB\nshared_buffers = 128MB\neffective_cache_size = 384MB\ncheckpoint_completion_target = 0.8\n# tcp parameters\ntcp_keepalives_idle = 30\ntcp_keepalives_interval = 10\ntcp_keepalives_count = 3\n# logging parameters in default fep installation\n# if log volume is not defined, log_directory should be\n# changed to '/database/userdata/data/log'\nlog_directory = '/database/log'\nlog_filename = 'logfile-%a.log'\nlog_file_mode = 0600\nlog_truncate_on_rotation = on\nlog_rotation_age = 1d\nlog_rotation_size = 0\nlog_checkpoints = on\nlog_line_prefix = '%e %t [%p]: [%l-1] user=%u,db=%d,app=%a,client=%h'\nlog_lock_waits = on\nlog_autovacuum_min_duration = 60s\nlogging_collector = on\npgaudit.config_file='/opt/app-root/src/pgaudit-cfg/pgaudit.conf'\nlog_replication_commands = on\nlog_min_messages = WARNING\nlog_destination = csvlog\n# wal_archive parameters in default fep installation\narchive_mode = on\narchive_command = 'pgbackrest --stanza=backupstanza --config=/database/userdata/pgbackrest.conf archive-push %p'\nwal_level = replica\nmax_wal_senders = 12\nwal_keep_segments = 64\ntrack_activities = on\ntrack_counts = on\npassword_encryption = 'md5'\n",
              "storage": {
                "archivewalVol": {
                  "size": "1Gi"
                },
                "backupVol": {
                  "size": "2Gi"
                },
                "dataVol": {
                  "size": "2Gi"
                },
                "logVol": {
                  "size": "1Gi"
                },
                "tablespaceVol": {
                  "size": "512Mi"
                },
                "walVol": {
                  "size": "1200Mi"
                }
              },
              "sysUsers": {
                "pgAdminPassword": "admin-password",
                "pgRewindPassword": "rewind_password",
                "pgRewindUser": "rewind_user",
                "pgdb": "mydb",
                "pgpassword": "mydbpassword",
                "pgreplpassword": "repluserpwd",
                "pgrepluser": "repluser",
                "pguser": "mydbuser",
                "tdepassphrase": "tde-passphrase"
              },
              "systemCertificates": {
                "cacrt": "-----BEGIN CERTIFICATE-----\nMIIDTzCCAjegAwIBAgIUYssQ8I74US5g+1+Z7CHuaDgkZnEwDQYJKoZIhvcNAQEL\nBQAwNzEQMA4GA1UECgwHRnVqaXRzdTEjMCEGA1UEAwwaRkVQIFJvb3QgQ0EgZm9y\nIEt1YmVybmV0ZXMwHhcNMjEwMjA2MDM1MjI4WhcNMzEwMjA0MDM1MjI4WjA3MRAw\nDgYDVQQKDAdGdWppdHN1MSMwIQYDVQQDDBpGRVAgUm9vdCBDQSBmb3IgS3ViZXJu\nZXRlczCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBAMs97gUF0xkUzCgL\n7MiiDju9ySr/ziwjvcYU7jA9ML+SLmftMs3HtcYbAmSntqI+MDBSR/FAJTOoytuT\npV+mCFcGj2YAjDpliHPeNcUpbryy4YMChF3+MovkIwGCksxo5rhiWhGmoBYpA48P\n4Xe8SPlzqMzhFvNeKzyiUhvjutS2Y1Ss38lsTaurFPx64vQ2PaC54XzdwMptXtpb\ntYmWSzCpJWwxZ6lF3vitdA2w0tnBWNyctAd0+RIM/fvArxiIqseAux9t0uogm5to\nlRIhvekuxOpXBPEqtIYQ4j9XUW2JH8vUDnzPkPvjrq+A3Ug8OyyfGVrW7+VYXozu\nc4aP7P0CAwEAAaNTMFEwHQYDVR0OBBYEFBzCutQ7S74WEhS5V2sNEJBGyLpmMB8G\nA1UdIwQYMBaAFBzCutQ7S74WEhS5V2sNEJBGyLpmMA8GA1UdEwEB/wQFMAMBAf8w\nDQYJKoZIhvcNAQELBQADggEBAMDwD85RAaWEBptFgLzKw+9xEUy1vcZaonAuA1qc\nT342XTueyAugxkC11HwdCGgGS34VyctfMGqj4AW6pA2ez4tLrbOps4DmV4sw8uBL\n8pgRDgfly3ob9FEg2wa0hmrwX9jH5Bt4vySUE2785uPAqaspT2UNtTBxS85BUi1T\nsKId2Rtil6an281Z81wyWVI6Jm2D4MG0mbsiGcTPlCtdg/UljvDYymXlAvd4vNhl\nk9hDa13TgDqJKgKdTIcmZoNQdpEVgFcO0h9AEUy5AuLqxHq60dLfZ6ESGPlMI7Lm\ni4PzYbCnBmOe+7TnHcPSyrnehs66Ik+oifRd82eYS7vKjFw=\n-----END CERTIFICATE-----",
                "crt": "-----BEGIN CERTIFICATE-----\nMIIDUTCCAjmgAwIBAgIRAMocW3qMoHrD6qRvMPppMkMwDQYJKoZIhvcNAQELBQAw\nNzEQMA4GA1UECgwHRnVqaXRzdTEjMCEGA1UEAwwaRkVQIFJvb3QgQ0EgZm9yIEt1\nYmVybmV0ZXMwHhcNMjEwMjA2MDQzMjM2WhcNMjYwMjA1MDQzMjM2WjA/MRAwDgYD\nVQQKEwdGdWppdHN1MSswKQYDVQQDEyJGVUpJVFNVIEVudGVycHJpc2UgUG9zdGdy\nZXMgU2VydmVyMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEA4AI33yvH\nZws+jta6qpV6wzJqF8odIfTIpCfbrVcUUtLFKJ1I2e4SceTKi6O3C/I1XuvWlpng\n5IO65+fQQLO06z1/AuQT78YUn/Wlm9x1aHVsv4ANB5JWWqDOjrRT3o7nRPGXfila\nbP0rGE2mJJcVR9nExJ3IeaktgT3sb8YlXvtchyYpmjdbfxabTz07ig0+6/cwKoRR\nxOK8Uf7f5euE0cI/490J6r5Rs4lgD8sIQNCUFlTFYvmAH7gcdssSFBt8NPlUATHE\nsoFmlW0DKCJWNhTLOht+s6L/1zwTHLjPG2pdkG6Wdgmu5H2pDml8CDNLDv98Aj7i\n+I5SRKKcVPlnuQIDAQABo1AwTjAdBgNVHSUEFjAUBggrBgEFBQcDAQYIKwYBBQUH\nAwIwDAYDVR0TAQH/BAIwADAfBgNVHSMEGDAWgBQcwrrUO0u+FhIUuVdrDRCQRsi6\nZjANBgkqhkiG9w0BAQsFAAOCAQEAm5dxBoI9pScOCvRAchg4CprdRDSJb9K6yB3O\nnCAxnM47iHeXnY3WlnI388kHu8DU7O4ba1tJbGs3KY9KzioPk43pU12jWkO1onoF\n+mTDjx/Ef1cYWA9r5q/LtgTa6Q2sxV4O2x67QW82aAnaxO34dV5zWCPIvAoovZBV\nHRT+BgCg3r2vD1RGKK2nl1aYJtWhO1SZubam+VttdZ/vbM9oOJctxmImsEtBXjkY\nKteePdQtLL5o03JhyXWyRshCq+HMmKf2KgyY8gvydGcP4eLQdBWcW40LcnVq6UjT\n0kJycJEKngMVademq1ZWHGaiYB7hyT6GhgIcHUJ2cKrPgbEh1Q==\n-----END CERTIFICATE-----",
                "key": "-----BEGIN RSA PRIVATE KEY-----\nMIIEowIBAAKCAQEA4AI33yvHZws+jta6qpV6wzJqF8odIfTIpCfbrVcUUtLFKJ1I\n2e4SceTKi6O3C/I1XuvWlpng5IO65+fQQLO06z1/AuQT78YUn/Wlm9x1aHVsv4AN\nB5JWWqDOjrRT3o7nRPGXfilabP0rGE2mJJcVR9nExJ3IeaktgT3sb8YlXvtchyYp\nmjdbfxabTz07ig0+6/cwKoRRxOK8Uf7f5euE0cI/490J6r5Rs4lgD8sIQNCUFlTF\nYvmAH7gcdssSFBt8NPlUATHEsoFmlW0DKCJWNhTLOht+s6L/1zwTHLjPG2pdkG6W\ndgmu5H2pDml8CDNLDv98Aj7i+I5SRKKcVPlnuQIDAQABAoIBAFPQYKlOzw/+BA0b\nyMIUpdctIMb/54CR/xR0mVw1DbSjigNVPjHUQvB8Y1B2FAITQObgJO06bAv0QdWN\nRb0/v/yYiNJDFjaLjaIAHlO/2+oWrXbFaZqgpVDJhB+e1xaZr2x7XGxm+p925k30\nl6pvIRY+I8JRKvZiV1VZHwL/R3JOtPr++xMZtLVjVOI+f+ySqJ+TZHuAjm49EKxj\ncEmmJ28b7QcziXsvKy00f+zbqLIBKXQdZAFU5eEr1BsDRXdRW+Kf0XIvftuy4BJZ\nvoKT+VGhEvF/qysswL4+6IAO6tpuYnnM0Y2d3sOGoWPkTcQK0MekYKzL/WmtCjNs\n9hodJtECgYEA5EWyhEOf4uOKe5TDp697UCUvXLoOR58FDe/S8XNvScn29jjOkqIg\nOMoqo9xAkJTNTzqn5UUdt1x/pgM2NxlPLFijrc0zQlX3SoOO2ryDd9WNi7YKtN16\nKJqa536WeZu2OEbuAZ+S3GALVy1RPeTNPnUOmKnF06DjDUGzLNCZy10CgYEA+zfw\n952DWuz1U0Z4wvAEqqcgUKXPKrkTXV/iUnjkDkrLYVr0ZofDNTXrdHl+UedFmaOC\ncieZn6DNhcdz5tKtyysGMH3g/qs9PfoGUngvcXsy0Egk04l3x1jc8TTCLqXZXYaQ\nHMsx51n+R58oncPtzYSUOr9qQ6PbC2CstTbFJA0CgYEAjGEsUliAB/jknfEzjXjG\nPdhQUxb8VyE864Az2lah9t/kJzFyIAziAeqZ5GE7t247AGFTBRTHHI8e1Qoemi3P\nWbc9GVIbFs1lIYbcIDpUIyrKPEP8O5QEXtoNLxXTFgAjRGKiVY87spjCAJ+W2ZhO\ne/1it5GYXfgQCYQA2yuBmOUCgYANRkR2YR1axaCk+NlSu6oTdmdPu6M5x7PNQE7O\nOtMaKjua9lppvIzFGAdMDUtueoEEAE7ZR1xnwfB6PDLUpJdIYAqgr1YfPt8qkjaZ\nTv56yZ7CwL0pbF8m6nwqRrZoDp1wwraEvvvxFKFKGY/k3kCHlpTakdjEoDjn3gDi\nRnWeVQKBgCEneMSzucei5LRppRtRaJw/Btll8qlPMlX3W7dxQ3cLwpmLOn0m51Fp\nPIZ44zYK8R6fu4+/sSrlfaIg86Ugeufp6YNxyNROKxUGza5vDIu5OftwWtBeg+UK\nZ8lLWNdX6pp7WMujmF3H1DrkBbauYMUKZ4UxUYtelgHERMePIxwb\n-----END RSA PRIVATE KEY-----"
              }
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [
        "amd64",
        "ppc64le",
        "s390x"
      ],
      "bundle_path": "registry.connect.redhat.com/fujitsu-postgres/fujitsu-enterprise-postgres-operator-bundle@sha256:94fa991b1fdaa56ee907acd562a163dcc52284719b61188f823f2b0f6dc6f436",
      "bundle_path_digest": "sha256:94fa991b1fdaa56ee907acd562a163dcc52284719b61188f823f2b0f6dc6f436",
      "capabilities": [
        "Auto Pilot"
      ],
      "channel_name": "stable",
      "creation_date": "2022-11-29T13:42:41.570000+00:00",
      "csv_description": "FUJITSU Enterprise Postgres 14 delivers an enterprise-grade PostgreSQL on OpenShift Container Platform.\n\nThis solution provides the flexibility of a hybrid cloud solution while delivering an enhanced distribution of PostgreSQL to support enterprise-level workloads and provide improved deployment and management, availability, performance, data governance and security.  \n\nAvailable as a multi-architecture container built for both amd64, s390x and ppc64le.\n\nThe download and Use of the Product is strictly subject to the terms of the End User License Agreement with Fujitsu Limited found at https://www.fast.fujitsu.com/fujitsu-enterprise-postgres-license-agreements. Where the Product that has been embedded as a whole or part into a third party program, only Authorised Customers may download and use the Product.\n",
      "csv_display_name": "FUJITSU Enterprise Postgres Operator",
      "csv_metadata_description": "OpenShift Operator for Fujitsu Enterprise Postgres",
      "csv_name": "fujitsu-enterprise-postgres-operator.v4.1.9",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": true,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-11-29T13:42:41.570000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.8",
      "organization": "certified-operators",
      "package": "fujitsu-enterprise-postgres-operator",
      "provided_apis": [
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPUser",
          "plural": "fepusers",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPConfig",
          "plural": "fepconfigs",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPAutoscale",
          "plural": "fepautoscales",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPUpgrade",
          "plural": "fepupgrades",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPBackup",
          "plural": "fepbackups",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPExporter",
          "plural": "fepexporters",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPAction",
          "plural": "fepactions",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPPgpool2",
          "plural": "feppgpool2s",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPCluster",
          "plural": "fepclusters",
          "version": "v2"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPRestore",
          "plural": "feprestores",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPPgpool2Cert",
          "plural": "feppgpool2certs",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPVolume",
          "plural": "fepvolumes",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPCert",
          "plural": "fepcerts",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPLogging",
          "plural": "feploggings",
          "version": "v1"
        }
      ],
      "provider": "Fujitsu",
      "related_images": [
        {
          "digest": "sha256:a07987c0b4d47c6ec53c2127ead56bb420ea493e9b5d1bb79064b5f1a1f57d91",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-14-server@sha256:a07987c0b4d47c6ec53c2127ead56bb420ea493e9b5d1bb79064b5f1a1f57d91",
          "name": "fep-14"
        },
        {
          "digest": "sha256:8690a50c3c900686fd5e8a65b04fee0125e4cf500c44845f3c4392a1ea1ef873",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-14-backup@sha256:8690a50c3c900686fd5e8a65b04fee0125e4cf500c44845f3c4392a1ea1ef873",
          "name": "backup-14"
        },
        {
          "digest": "sha256:3759bd7513228b49733b23db24fdeb2c6a31b3f93c7ac5f84ab0d8b9939e07ce",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-14-restore@sha256:3759bd7513228b49733b23db24fdeb2c6a31b3f93c7ac5f84ab0d8b9939e07ce",
          "name": "restore-14"
        },
        {
          "digest": "sha256:37729f8332f647eba5f8209ea9ce95aaa9ab6674774afc2931c4aa59dc51ae90",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-14-pgpool2@sha256:37729f8332f647eba5f8209ea9ce95aaa9ab6674774afc2931c4aa59dc51ae90",
          "name": "pgpool2-14"
        },
        {
          "digest": "sha256:f9e28fe00b2caee1b15e4867e86dad39c8e6913f137fe689fd30c18b9f19881c",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-exporter@sha256:f9e28fe00b2caee1b15e4867e86dad39c8e6913f137fe689fd30c18b9f19881c",
          "name": "exporter-14"
        },
        {
          "digest": "sha256:fb8bffe2c1833b7061a4856ecabda811ed16ff661d38b8ea6a5bc826bc0ccbc3",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-fluentd@sha256:fb8bffe2c1833b7061a4856ecabda811ed16ff661d38b8ea6a5bc826bc0ccbc3",
          "name": "fluentd-14"
        },
        {
          "digest": "sha256:326005d0cfb049f36759e6ef022c67029dbac1865810be3c850c1bc1cf604e8d",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-fluentbit@sha256:326005d0cfb049f36759e6ef022c67029dbac1865810be3c850c1bc1cf604e8d",
          "name": "fluentbit-14"
        },
        {
          "digest": "sha256:4f75b63a09ad3da34684db497814bfa8eae8356e43963ce67fbfcb48e4f350ac",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-cronjob@sha256:4f75b63a09ad3da34684db497814bfa8eae8356e43963ce67fbfcb48e4f350ac",
          "name": "cronjob-14"
        },
        {
          "digest": "sha256:793a06eefe93a7613cfdf1a89febf5603244f453a1e804760d93e6f182cc522a",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-14-upgrade@sha256:793a06eefe93a7613cfdf1a89febf5603244f453a1e804760d93e6f182cc522a",
          "name": "upgrade-14"
        },
        {
          "digest": "sha256:80f5d30c66844eb05de5d67999198d9ea3d457f8fbe81358de0b37db0f15f0e3",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-13-server@sha256:80f5d30c66844eb05de5d67999198d9ea3d457f8fbe81358de0b37db0f15f0e3",
          "name": "fep-13"
        },
        {
          "digest": "sha256:c27c4b52361e73dd460eac5d8fe4765f582d0ffd4f4fc072bff4523b785f47e5",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-13-backup@sha256:c27c4b52361e73dd460eac5d8fe4765f582d0ffd4f4fc072bff4523b785f47e5",
          "name": "backup-13"
        },
        {
          "digest": "sha256:99fd4033d9f5e536911915410abfc636b55f09e63f88cf32dbe9f58f79124963",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-13-restore@sha256:99fd4033d9f5e536911915410abfc636b55f09e63f88cf32dbe9f58f79124963",
          "name": "restore-13"
        },
        {
          "digest": "sha256:2f23389e16635ff3e75774dac430c35dc4b314f291d9303d9296852eb0e436bc",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-13-pgpool2@sha256:2f23389e16635ff3e75774dac430c35dc4b314f291d9303d9296852eb0e436bc",
          "name": "pgpool2-13"
        },
        {
          "digest": "sha256:b8c93f9dc83332f547682e7c1bc9a691d20f91793a27cabe1798bd9ae8c96cb2",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-13-exporter@sha256:b8c93f9dc83332f547682e7c1bc9a691d20f91793a27cabe1798bd9ae8c96cb2",
          "name": "fepexporter-13"
        },
        {
          "digest": "sha256:61592bcccccee2e72521d0640f005d1902730319ad697aa0f4fc5485bf22989d",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-12-server@sha256:61592bcccccee2e72521d0640f005d1902730319ad697aa0f4fc5485bf22989d",
          "name": "fep-12"
        },
        {
          "digest": "sha256:f126c128c1301afa684bdf3258c78d1ec68c08aa6e5c085c5f939940947e5c94",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-12-backup@sha256:f126c128c1301afa684bdf3258c78d1ec68c08aa6e5c085c5f939940947e5c94",
          "name": "backup-12"
        },
        {
          "digest": "sha256:240da72ba4ef71031c1587a1f44b7631697346bb7699be52dcebd64e9f93d66e",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-12-restore@sha256:240da72ba4ef71031c1587a1f44b7631697346bb7699be52dcebd64e9f93d66e",
          "name": "restore-12"
        },
        {
          "digest": "sha256:6d58e56e6e7ab0c07d6095b1cab429c47ea3d294bf1656bd0a7d6efed098cb02",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-12-pgpool2@sha256:6d58e56e6e7ab0c07d6095b1cab429c47ea3d294bf1656bd0a7d6efed098cb02",
          "name": "pgpool2-12"
        },
        {
          "digest": "sha256:ecf7b692793b5b7ad2e848f335456742901ef0ef6b0b171d4771f0642e76714e",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-operator@sha256:ecf7b692793b5b7ad2e848f335456742901ef0ef6b0b171d4771f0642e76714e",
          "name": "fujitsu-enterprise-postgres-operator-ecf7b692793b5b7ad2e848f335456742901ef0ef6b0b171d4771f0642e76714e-annotation"
        },
        {
          "digest": "sha256:ecf7b692793b5b7ad2e848f335456742901ef0ef6b0b171d4771f0642e76714e",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-operator@sha256:ecf7b692793b5b7ad2e848f335456742901ef0ef6b0b171d4771f0642e76714e",
          "name": "fep-ansible-operator"
        },
        {
          "digest": "sha256:a07987c0b4d47c6ec53c2127ead56bb420ea493e9b5d1bb79064b5f1a1f57d91",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-14-server@sha256:a07987c0b4d47c6ec53c2127ead56bb420ea493e9b5d1bb79064b5f1a1f57d91",
          "name": "fep"
        },
        {
          "digest": "sha256:8690a50c3c900686fd5e8a65b04fee0125e4cf500c44845f3c4392a1ea1ef873",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-14-backup@sha256:8690a50c3c900686fd5e8a65b04fee0125e4cf500c44845f3c4392a1ea1ef873",
          "name": "backup"
        },
        {
          "digest": "sha256:3759bd7513228b49733b23db24fdeb2c6a31b3f93c7ac5f84ab0d8b9939e07ce",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-14-restore@sha256:3759bd7513228b49733b23db24fdeb2c6a31b3f93c7ac5f84ab0d8b9939e07ce",
          "name": "restore"
        },
        {
          "digest": "sha256:37729f8332f647eba5f8209ea9ce95aaa9ab6674774afc2931c4aa59dc51ae90",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-14-pgpool2@sha256:37729f8332f647eba5f8209ea9ce95aaa9ab6674774afc2931c4aa59dc51ae90",
          "name": "pgpool2"
        },
        {
          "digest": "sha256:f9e28fe00b2caee1b15e4867e86dad39c8e6913f137fe689fd30c18b9f19881c",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-exporter@sha256:f9e28fe00b2caee1b15e4867e86dad39c8e6913f137fe689fd30c18b9f19881c",
          "name": "fepexporter"
        },
        {
          "digest": "sha256:fb8bffe2c1833b7061a4856ecabda811ed16ff661d38b8ea6a5bc826bc0ccbc3",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-fluentd@sha256:fb8bffe2c1833b7061a4856ecabda811ed16ff661d38b8ea6a5bc826bc0ccbc3",
          "name": "feplogging"
        },
        {
          "digest": "sha256:326005d0cfb049f36759e6ef022c67029dbac1865810be3c850c1bc1cf604e8d",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-fluentbit@sha256:326005d0cfb049f36759e6ef022c67029dbac1865810be3c850c1bc1cf604e8d",
          "name": "feplogging_fluentbit"
        },
        {
          "digest": "sha256:4f75b63a09ad3da34684db497814bfa8eae8356e43963ce67fbfcb48e4f350ac",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-cronjob@sha256:4f75b63a09ad3da34684db497814bfa8eae8356e43963ce67fbfcb48e4f350ac",
          "name": "cronjob"
        },
        {
          "digest": "sha256:793a06eefe93a7613cfdf1a89febf5603244f453a1e804760d93e6f182cc522a",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-14-upgrade@sha256:793a06eefe93a7613cfdf1a89febf5603244f453a1e804760d93e6f182cc522a",
          "name": "upgrade"
        },
        {
          "digest": "sha256:a07987c0b4d47c6ec53c2127ead56bb420ea493e9b5d1bb79064b5f1a1f57d91",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-14-server@sha256:a07987c0b4d47c6ec53c2127ead56bb420ea493e9b5d1bb79064b5f1a1f57d91",
          "name": "fep_14"
        },
        {
          "digest": "sha256:8690a50c3c900686fd5e8a65b04fee0125e4cf500c44845f3c4392a1ea1ef873",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-14-backup@sha256:8690a50c3c900686fd5e8a65b04fee0125e4cf500c44845f3c4392a1ea1ef873",
          "name": "backup_14"
        },
        {
          "digest": "sha256:3759bd7513228b49733b23db24fdeb2c6a31b3f93c7ac5f84ab0d8b9939e07ce",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-14-restore@sha256:3759bd7513228b49733b23db24fdeb2c6a31b3f93c7ac5f84ab0d8b9939e07ce",
          "name": "restore_14"
        },
        {
          "digest": "sha256:37729f8332f647eba5f8209ea9ce95aaa9ab6674774afc2931c4aa59dc51ae90",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-14-pgpool2@sha256:37729f8332f647eba5f8209ea9ce95aaa9ab6674774afc2931c4aa59dc51ae90",
          "name": "pgpool2_14"
        },
        {
          "digest": "sha256:f9e28fe00b2caee1b15e4867e86dad39c8e6913f137fe689fd30c18b9f19881c",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-exporter@sha256:f9e28fe00b2caee1b15e4867e86dad39c8e6913f137fe689fd30c18b9f19881c",
          "name": "fepexporter_14"
        },
        {
          "digest": "sha256:fb8bffe2c1833b7061a4856ecabda811ed16ff661d38b8ea6a5bc826bc0ccbc3",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-fluentd@sha256:fb8bffe2c1833b7061a4856ecabda811ed16ff661d38b8ea6a5bc826bc0ccbc3",
          "name": "feplogging_14"
        },
        {
          "digest": "sha256:326005d0cfb049f36759e6ef022c67029dbac1865810be3c850c1bc1cf604e8d",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-fluentbit@sha256:326005d0cfb049f36759e6ef022c67029dbac1865810be3c850c1bc1cf604e8d",
          "name": "feplogging_fluentbit_14"
        },
        {
          "digest": "sha256:4f75b63a09ad3da34684db497814bfa8eae8356e43963ce67fbfcb48e4f350ac",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-cronjob@sha256:4f75b63a09ad3da34684db497814bfa8eae8356e43963ce67fbfcb48e4f350ac",
          "name": "cronjob_14"
        },
        {
          "digest": "sha256:793a06eefe93a7613cfdf1a89febf5603244f453a1e804760d93e6f182cc522a",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-14-upgrade@sha256:793a06eefe93a7613cfdf1a89febf5603244f453a1e804760d93e6f182cc522a",
          "name": "upgrade_14"
        },
        {
          "digest": "sha256:80f5d30c66844eb05de5d67999198d9ea3d457f8fbe81358de0b37db0f15f0e3",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-13-server@sha256:80f5d30c66844eb05de5d67999198d9ea3d457f8fbe81358de0b37db0f15f0e3",
          "name": "fep_13"
        },
        {
          "digest": "sha256:c27c4b52361e73dd460eac5d8fe4765f582d0ffd4f4fc072bff4523b785f47e5",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-13-backup@sha256:c27c4b52361e73dd460eac5d8fe4765f582d0ffd4f4fc072bff4523b785f47e5",
          "name": "backup_13"
        },
        {
          "digest": "sha256:99fd4033d9f5e536911915410abfc636b55f09e63f88cf32dbe9f58f79124963",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-13-restore@sha256:99fd4033d9f5e536911915410abfc636b55f09e63f88cf32dbe9f58f79124963",
          "name": "restore_13"
        },
        {
          "digest": "sha256:2f23389e16635ff3e75774dac430c35dc4b314f291d9303d9296852eb0e436bc",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-13-pgpool2@sha256:2f23389e16635ff3e75774dac430c35dc4b314f291d9303d9296852eb0e436bc",
          "name": "pgpool2_13"
        },
        {
          "digest": "sha256:b8c93f9dc83332f547682e7c1bc9a691d20f91793a27cabe1798bd9ae8c96cb2",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-13-exporter@sha256:b8c93f9dc83332f547682e7c1bc9a691d20f91793a27cabe1798bd9ae8c96cb2",
          "name": "fepexporter_13"
        },
        {
          "digest": "sha256:61592bcccccee2e72521d0640f005d1902730319ad697aa0f4fc5485bf22989d",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-12-server@sha256:61592bcccccee2e72521d0640f005d1902730319ad697aa0f4fc5485bf22989d",
          "name": "fep_12"
        },
        {
          "digest": "sha256:f126c128c1301afa684bdf3258c78d1ec68c08aa6e5c085c5f939940947e5c94",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-12-backup@sha256:f126c128c1301afa684bdf3258c78d1ec68c08aa6e5c085c5f939940947e5c94",
          "name": "backup_12"
        },
        {
          "digest": "sha256:240da72ba4ef71031c1587a1f44b7631697346bb7699be52dcebd64e9f93d66e",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-12-restore@sha256:240da72ba4ef71031c1587a1f44b7631697346bb7699be52dcebd64e9f93d66e",
          "name": "restore_12"
        },
        {
          "digest": "sha256:6d58e56e6e7ab0c07d6095b1cab429c47ea3d294bf1656bd0a7d6efed098cb02",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-12-pgpool2@sha256:6d58e56e6e7ab0c07d6095b1cab429c47ea3d294bf1656bd0a7d6efed098cb02",
          "name": "pgpool2_12"
        }
      ],
      "replaces": null,
      "skip_range": "<4.1.9",
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.8",
      "version": "4.1.9",
      "version_original": "4.1.9"
    },
    {
      "_id": "638627c897ddd97b02451c37",
      "alm_examples": [
        {
          "api_version": "compute.functionmesh.io/v1alpha1",
          "kind": "Function",
          "metadata": {
            "name": "java-function-sample",
            "namespace": "default"
          },
          "spec": {
            "autoAck": true,
            "className": "org.apache.pulsar.functions.api.examples.ExclamationFunction",
            "clusterName": "test-pulsar",
            "forwardSourceMessageProperty": true,
            "input": {
              "topics": [
                "persistent://public/default/java-function-input-topic"
              ],
              "typeClassName": "java.lang.String"
            },
            "java": {
              "extraDependenciesDir": "random-dir/",
              "jar": "pulsar-functions-api-examples.jar",
              "jarLocation": "public/default/nlu-test-java-function"
            },
            "logTopic": "persistent://public/default/logging-function-logs",
            "maxPendingAsyncRequests": 1000,
            "maxReplicas": 5,
            "output": {
              "topic": "persistent://public/default/java-function-output-topic",
              "typeClassName": "java.lang.String"
            },
            "pod": {
              "annotations": {
                "managed-function": "true"
              },
              "env": [
                {
                  "name": "EXAMPLE_VARIABLE",
                  "value": "exampleValue"
                }
              ],
              "imagePullSecrets": [
                {
                  "name": "regcred"
                }
              ],
              "initContainers": [
                {
                  "command": [
                    "sh",
                    "-c",
                    "echo The app is running! && sleep 30"
                  ],
                  "image": "busybox:1.28",
                  "name": "init-function"
                }
              ],
              "labels": {
                "locaction": "mtv"
              },
              "sidecars": [
                {
                  "command": [
                    "sh",
                    "-c",
                    "echo The app is running! && sleep 30000"
                  ],
                  "image": "busybox:1.28",
                  "name": "sidecar-function"
                }
              ],
              "volumes": [
                {
                  "emptyDir": {},
                  "name": "cache-volume"
                }
              ]
            },
            "pulsar": {
              "authSecret": "test-auth",
              "pulsarConfig": "test-pulsar",
              "tlsSecret": "test-tls"
            },
            "replicas": 1,
            "resources": {
              "limits": {
                "cpu": "0.2",
                "memory": "1.1G"
              },
              "requests": {
                "cpu": "0.1",
                "memory": "1G"
              }
            },
            "secretsMap": {
              "name": {
                "key": "username",
                "path": "test-secret"
              },
              "pwd": {
                "key": "password",
                "path": "test-secret"
              }
            },
            "volumeMounts": [
              {
                "mountPath": "/cache",
                "name": "cache-volume"
              }
            ]
          }
        },
        {
          "api_version": "compute.functionmesh.io/v1alpha1",
          "kind": "FunctionMesh",
          "metadata": {
            "name": "functionmesh-sample"
          },
          "spec": {
            "functions": [
              {
                "autoAck": true,
                "className": "org.apache.pulsar.functions.api.examples.ExclamationFunction",
                "clusterName": "test-pulsar",
                "forwardSourceMessageProperty": true,
                "input": {
                  "topics": [
                    "persistent://public/default/functionmesh-input-topic"
                  ],
                  "typeClassName": "java.lang.String"
                },
                "java": {
                  "jar": "pulsar-functions-api-examples.jar",
                  "jarLocation": "public/default/nlu-test-functionmesh-ex1"
                },
                "logTopic": "persistent://public/default/logging-function-log",
                "maxReplicas": 1,
                "name": "ex1",
                "output": {
                  "topic": "persistent://public/default/mid-topic",
                  "typeClassName": "java.lang.String"
                },
                "pulsar": {
                  "pulsarConfig": "mesh-test-pulsar"
                },
                "replicas": 1,
                "resources": {
                  "limits": {
                    "cpu": "0.2",
                    "memory": "1.1G"
                  },
                  "requests": {
                    "cpu": "0.1",
                    "memory": "1G"
                  }
                }
              },
              {
                "autoAck": true,
                "className": "org.apache.pulsar.functions.api.examples.ExclamationFunction",
                "clusterName": "test-pulsar",
                "forwardSourceMessageProperty": true,
                "input": {
                  "topics": [
                    "persistent://public/default/mid-topic"
                  ],
                  "typeClassName": "java.lang.String"
                },
                "java": {
                  "jar": "pulsar-functions-api-examples.jar",
                  "jarLocation": "public/default/nlu-test-functionmesh-ex2"
                },
                "logTopic": "persistent://public/default/logging-function-logs",
                "maxReplicas": 1,
                "name": "ex2",
                "output": {
                  "topic": "persistent://public/default/functionmesh-output-topic",
                  "typeClassName": "java.lang.String"
                },
                "pulsar": {
                  "pulsarConfig": "mesh-test-pulsar"
                },
                "replicas": 1,
                "resources": {
                  "limits": {
                    "cpu": "0.2",
                    "memory": "1.1G"
                  },
                  "requests": {
                    "cpu": "0.1",
                    "memory": "1G"
                  }
                }
              }
            ]
          }
        },
        {
          "api_version": "compute.functionmesh.io/v1alpha1",
          "kind": "Sink",
          "metadata": {
            "name": "sink-sample"
          },
          "spec": {
            "autoAck": true,
            "className": "org.apache.pulsar.io.elasticsearch.ElasticSearchSink",
            "clusterName": "test-pulsar",
            "image": "streamnative/pulsar-io-elastic-search:2.10.0.0-rc10",
            "input": {
              "topics": [
                "persistent://public/default/input"
              ],
              "typeClassName": "[B"
            },
            "java": {
              "jar": "connectors/pulsar-io-elastic-search-2.10.0.0-rc10.nar",
              "jarLocation": ""
            },
            "maxReplicas": 1,
            "pulsar": {
              "pulsarConfig": "test-sink"
            },
            "replicas": 1,
            "resources": {
              "limits": {
                "cpu": "0.2",
                "memory": "1.1G"
              },
              "requests": {
                "cpu": "0.1",
                "memory": "1G"
              }
            },
            "sinkConfig": {
              "elasticSearchUrl": "http://quickstart-es-http.default.svc.cluster.local:9200",
              "indexName": "my_index",
              "password": "wJ757TmoXEd941kXm07Z2GW3",
              "typeName": "doc",
              "username": "elastic"
            }
          }
        },
        {
          "api_version": "compute.functionmesh.io/v1alpha1",
          "kind": "Source",
          "metadata": {
            "name": "source-sample"
          },
          "spec": {
            "className": "org.apache.pulsar.io.debezium.mongodb.DebeziumMongoDbSource",
            "clusterName": "test-pulsar",
            "forwardSourceMessageProperty": true,
            "image": "streamnative/pulsar-io-debezium-mongodb:2.10.0.0-rc10",
            "java": {
              "jar": "connectors/pulsar-io-debezium-mongodb-2.10.0.0-rc10.nar",
              "jarLocation": ""
            },
            "maxReplicas": 1,
            "output": {
              "producerConf": {
                "maxPendingMessages": 1000,
                "maxPendingMessagesAcrossPartitions": 50000,
                "useThreadLocalProducers": true
              },
              "topic": "persistent://public/default/destination",
              "typeClassName": "org.apache.pulsar.common.schema.KeyValue"
            },
            "pulsar": {
              "pulsarConfig": "test-source"
            },
            "replicas": 1,
            "resources": {
              "limits": {
                "cpu": "0.2",
                "memory": "1.1G"
              },
              "requests": {
                "cpu": "0.1",
                "memory": "1G"
              }
            },
            "sourceConfig": {
              "database.whitelist": "inventory",
              "mongodb.hosts": "rs0/mongo-dbz-0.mongo.default.svc.cluster.local:27017,rs0/mongo-dbz-1.mongo.default.svc.cluster.local:27017,rs0/mongo-dbz-2.mongo.default.svc.cluster.local:27017",
              "mongodb.name": "dbserver1",
              "mongodb.password": "dbz",
              "mongodb.task.id": "1",
              "mongodb.user": "debezium",
              "pulsar.service.url": "pulsar://test-pulsar-broker.default.svc.cluster.local:6650"
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/streamnative/function-mesh-bundle@sha256:949785d93c6890b1a3aee05706e7871edbfda4a0f396aabe9e1f26a41aa49197",
      "bundle_path_digest": "sha256:949785d93c6890b1a3aee05706e7871edbfda4a0f396aabe9e1f26a41aa49197",
      "capabilities": [
        "Full Lifecycle"
      ],
      "channel_name": "alpha",
      "creation_date": "2022-11-29T15:39:52.637000+00:00",
      "csv_description": "[Function Mesh](https://functionmesh.io/) is a serverless framework purpose-built for stream processing applications. It brings powerful event-streaming capabilities to your applications by orchestrating multiple [Pulsar Functions](http://pulsar.apache.org/docs/en/next/functions-overview/) and [Pulsar IO connectors](http://pulsar.apache.org/docs/en/next/io-overview/) for complex stream processing jobs on Kubernetes.\nPrerequisites:\n- Install cert-manager operator first.\n",
      "csv_display_name": "Function Mesh Operator",
      "csv_metadata_description": "The Function Mesh Operator manages the Pulsar Functions and Connectors deployed on a Kubernetes cluster.",
      "csv_name": "function-mesh.v0.8.0",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": false,
          "type": "OwnNamespace"
        },
        {
          "supported": false,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-11-29T15:39:52.637000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.11",
      "organization": "certified-operators",
      "package": "function-mesh",
      "provided_apis": [
        {
          "group": "compute.functionmesh.io",
          "kind": "Function",
          "version": "v1alpha1"
        },
        {
          "group": "compute.functionmesh.io",
          "kind": "FunctionMesh",
          "version": "v1alpha1"
        },
        {
          "group": "compute.functionmesh.io",
          "kind": "Sink",
          "version": "v1alpha1"
        },
        {
          "group": "compute.functionmesh.io",
          "kind": "Source",
          "version": "v1alpha1"
        }
      ],
      "provider": "StreamNative",
      "related_images": [
        {
          "digest": "sha256:279535d1118604037058e1d31dc79c94bfa6a5bce57c24862edad0fa0fd1c579",
          "image": "registry.connect.redhat.com/streamnative/function-mesh@sha256:279535d1118604037058e1d31dc79c94bfa6a5bce57c24862edad0fa0fd1c579",
          "name": "function-mesh"
        },
        {
          "digest": "sha256:67ecb332573384515406ebd71816781366b70adb0eb66345e5980e92603373e1",
          "image": "docker.cloudsmith.io/streamnative/mirrors/gcr.io/kubebuilder/kube-rbac-proxy@sha256:67ecb332573384515406ebd71816781366b70adb0eb66345e5980e92603373e1",
          "name": "kube-rbac-proxy"
        },
        {
          "digest": "sha256:279535d1118604037058e1d31dc79c94bfa6a5bce57c24862edad0fa0fd1c579",
          "image": "registry.connect.redhat.com/streamnative/function-mesh@sha256:279535d1118604037058e1d31dc79c94bfa6a5bce57c24862edad0fa0fd1c579",
          "name": "function-mesh-279535d1118604037058e1d31dc79c94bfa6a5bce57c24862edad0fa0fd1c579-annotation"
        },
        {
          "digest": "sha256:279535d1118604037058e1d31dc79c94bfa6a5bce57c24862edad0fa0fd1c579",
          "image": "registry.connect.redhat.com/streamnative/function-mesh@sha256:279535d1118604037058e1d31dc79c94bfa6a5bce57c24862edad0fa0fd1c579",
          "name": "manager"
        }
      ],
      "replaces": null,
      "skip_range": "<0.8.0",
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.11",
      "version": "0.8.0",
      "version_original": "0.8.0"
    },
    {
      "_id": "638628701ac39ec05a05865d",
      "alm_examples": [
        {
          "api_version": "compute.functionmesh.io/v1alpha1",
          "kind": "Function",
          "metadata": {
            "name": "java-function-sample",
            "namespace": "default"
          },
          "spec": {
            "autoAck": true,
            "className": "org.apache.pulsar.functions.api.examples.ExclamationFunction",
            "clusterName": "test-pulsar",
            "forwardSourceMessageProperty": true,
            "input": {
              "topics": [
                "persistent://public/default/java-function-input-topic"
              ],
              "typeClassName": "java.lang.String"
            },
            "java": {
              "extraDependenciesDir": "random-dir/",
              "jar": "pulsar-functions-api-examples.jar",
              "jarLocation": "public/default/nlu-test-java-function"
            },
            "logTopic": "persistent://public/default/logging-function-logs",
            "maxPendingAsyncRequests": 1000,
            "maxReplicas": 5,
            "output": {
              "topic": "persistent://public/default/java-function-output-topic",
              "typeClassName": "java.lang.String"
            },
            "pod": {
              "annotations": {
                "managed-function": "true"
              },
              "env": [
                {
                  "name": "EXAMPLE_VARIABLE",
                  "value": "exampleValue"
                }
              ],
              "imagePullSecrets": [
                {
                  "name": "regcred"
                }
              ],
              "initContainers": [
                {
                  "command": [
                    "sh",
                    "-c",
                    "echo The app is running! && sleep 30"
                  ],
                  "image": "busybox:1.28",
                  "name": "init-function"
                }
              ],
              "labels": {
                "locaction": "mtv"
              },
              "sidecars": [
                {
                  "command": [
                    "sh",
                    "-c",
                    "echo The app is running! && sleep 30000"
                  ],
                  "image": "busybox:1.28",
                  "name": "sidecar-function"
                }
              ],
              "volumes": [
                {
                  "emptyDir": {},
                  "name": "cache-volume"
                }
              ]
            },
            "pulsar": {
              "authSecret": "test-auth",
              "pulsarConfig": "test-pulsar",
              "tlsSecret": "test-tls"
            },
            "replicas": 1,
            "resources": {
              "limits": {
                "cpu": "0.2",
                "memory": "1.1G"
              },
              "requests": {
                "cpu": "0.1",
                "memory": "1G"
              }
            },
            "secretsMap": {
              "name": {
                "key": "username",
                "path": "test-secret"
              },
              "pwd": {
                "key": "password",
                "path": "test-secret"
              }
            },
            "volumeMounts": [
              {
                "mountPath": "/cache",
                "name": "cache-volume"
              }
            ]
          }
        },
        {
          "api_version": "compute.functionmesh.io/v1alpha1",
          "kind": "FunctionMesh",
          "metadata": {
            "name": "functionmesh-sample"
          },
          "spec": {
            "functions": [
              {
                "autoAck": true,
                "className": "org.apache.pulsar.functions.api.examples.ExclamationFunction",
                "clusterName": "test-pulsar",
                "forwardSourceMessageProperty": true,
                "input": {
                  "topics": [
                    "persistent://public/default/functionmesh-input-topic"
                  ],
                  "typeClassName": "java.lang.String"
                },
                "java": {
                  "jar": "pulsar-functions-api-examples.jar",
                  "jarLocation": "public/default/nlu-test-functionmesh-ex1"
                },
                "logTopic": "persistent://public/default/logging-function-log",
                "maxReplicas": 1,
                "name": "ex1",
                "output": {
                  "topic": "persistent://public/default/mid-topic",
                  "typeClassName": "java.lang.String"
                },
                "pulsar": {
                  "pulsarConfig": "mesh-test-pulsar"
                },
                "replicas": 1,
                "resources": {
                  "limits": {
                    "cpu": "0.2",
                    "memory": "1.1G"
                  },
                  "requests": {
                    "cpu": "0.1",
                    "memory": "1G"
                  }
                }
              },
              {
                "autoAck": true,
                "className": "org.apache.pulsar.functions.api.examples.ExclamationFunction",
                "clusterName": "test-pulsar",
                "forwardSourceMessageProperty": true,
                "input": {
                  "topics": [
                    "persistent://public/default/mid-topic"
                  ],
                  "typeClassName": "java.lang.String"
                },
                "java": {
                  "jar": "pulsar-functions-api-examples.jar",
                  "jarLocation": "public/default/nlu-test-functionmesh-ex2"
                },
                "logTopic": "persistent://public/default/logging-function-logs",
                "maxReplicas": 1,
                "name": "ex2",
                "output": {
                  "topic": "persistent://public/default/functionmesh-output-topic",
                  "typeClassName": "java.lang.String"
                },
                "pulsar": {
                  "pulsarConfig": "mesh-test-pulsar"
                },
                "replicas": 1,
                "resources": {
                  "limits": {
                    "cpu": "0.2",
                    "memory": "1.1G"
                  },
                  "requests": {
                    "cpu": "0.1",
                    "memory": "1G"
                  }
                }
              }
            ]
          }
        },
        {
          "api_version": "compute.functionmesh.io/v1alpha1",
          "kind": "Sink",
          "metadata": {
            "name": "sink-sample"
          },
          "spec": {
            "autoAck": true,
            "className": "org.apache.pulsar.io.elasticsearch.ElasticSearchSink",
            "clusterName": "test-pulsar",
            "image": "streamnative/pulsar-io-elastic-search:2.10.0.0-rc10",
            "input": {
              "topics": [
                "persistent://public/default/input"
              ],
              "typeClassName": "[B"
            },
            "java": {
              "jar": "connectors/pulsar-io-elastic-search-2.10.0.0-rc10.nar",
              "jarLocation": ""
            },
            "maxReplicas": 1,
            "pulsar": {
              "pulsarConfig": "test-sink"
            },
            "replicas": 1,
            "resources": {
              "limits": {
                "cpu": "0.2",
                "memory": "1.1G"
              },
              "requests": {
                "cpu": "0.1",
                "memory": "1G"
              }
            },
            "sinkConfig": {
              "elasticSearchUrl": "http://quickstart-es-http.default.svc.cluster.local:9200",
              "indexName": "my_index",
              "password": "wJ757TmoXEd941kXm07Z2GW3",
              "typeName": "doc",
              "username": "elastic"
            }
          }
        },
        {
          "api_version": "compute.functionmesh.io/v1alpha1",
          "kind": "Source",
          "metadata": {
            "name": "source-sample"
          },
          "spec": {
            "className": "org.apache.pulsar.io.debezium.mongodb.DebeziumMongoDbSource",
            "clusterName": "test-pulsar",
            "forwardSourceMessageProperty": true,
            "image": "streamnative/pulsar-io-debezium-mongodb:2.10.0.0-rc10",
            "java": {
              "jar": "connectors/pulsar-io-debezium-mongodb-2.10.0.0-rc10.nar",
              "jarLocation": ""
            },
            "maxReplicas": 1,
            "output": {
              "producerConf": {
                "maxPendingMessages": 1000,
                "maxPendingMessagesAcrossPartitions": 50000,
                "useThreadLocalProducers": true
              },
              "topic": "persistent://public/default/destination",
              "typeClassName": "org.apache.pulsar.common.schema.KeyValue"
            },
            "pulsar": {
              "pulsarConfig": "test-source"
            },
            "replicas": 1,
            "resources": {
              "limits": {
                "cpu": "0.2",
                "memory": "1.1G"
              },
              "requests": {
                "cpu": "0.1",
                "memory": "1G"
              }
            },
            "sourceConfig": {
              "database.whitelist": "inventory",
              "mongodb.hosts": "rs0/mongo-dbz-0.mongo.default.svc.cluster.local:27017,rs0/mongo-dbz-1.mongo.default.svc.cluster.local:27017,rs0/mongo-dbz-2.mongo.default.svc.cluster.local:27017",
              "mongodb.name": "dbserver1",
              "mongodb.password": "dbz",
              "mongodb.task.id": "1",
              "mongodb.user": "debezium",
              "pulsar.service.url": "pulsar://test-pulsar-broker.default.svc.cluster.local:6650"
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/streamnative/function-mesh-bundle@sha256:949785d93c6890b1a3aee05706e7871edbfda4a0f396aabe9e1f26a41aa49197",
      "bundle_path_digest": "sha256:949785d93c6890b1a3aee05706e7871edbfda4a0f396aabe9e1f26a41aa49197",
      "capabilities": [
        "Full Lifecycle"
      ],
      "channel_name": "alpha",
      "creation_date": "2022-11-29T15:42:40.786000+00:00",
      "csv_description": "[Function Mesh](https://functionmesh.io/) is a serverless framework purpose-built for stream processing applications. It brings powerful event-streaming capabilities to your applications by orchestrating multiple [Pulsar Functions](http://pulsar.apache.org/docs/en/next/functions-overview/) and [Pulsar IO connectors](http://pulsar.apache.org/docs/en/next/io-overview/) for complex stream processing jobs on Kubernetes.\nPrerequisites:\n- Install cert-manager operator first.\n",
      "csv_display_name": "Function Mesh Operator",
      "csv_metadata_description": "The Function Mesh Operator manages the Pulsar Functions and Connectors deployed on a Kubernetes cluster.",
      "csv_name": "function-mesh.v0.8.0",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": false,
          "type": "OwnNamespace"
        },
        {
          "supported": false,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-11-29T15:42:40.786000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.10",
      "organization": "certified-operators",
      "package": "function-mesh",
      "provided_apis": [
        {
          "group": "compute.functionmesh.io",
          "kind": "FunctionMesh",
          "plural": "functionmeshes",
          "version": "v1alpha1"
        },
        {
          "group": "compute.functionmesh.io",
          "kind": "Function",
          "plural": "functions",
          "version": "v1alpha1"
        },
        {
          "group": "compute.functionmesh.io",
          "kind": "Sink",
          "plural": "sinks",
          "version": "v1alpha1"
        },
        {
          "group": "compute.functionmesh.io",
          "kind": "Source",
          "plural": "sources",
          "version": "v1alpha1"
        }
      ],
      "provider": "StreamNative",
      "related_images": [
        {
          "digest": "sha256:279535d1118604037058e1d31dc79c94bfa6a5bce57c24862edad0fa0fd1c579",
          "image": "registry.connect.redhat.com/streamnative/function-mesh@sha256:279535d1118604037058e1d31dc79c94bfa6a5bce57c24862edad0fa0fd1c579",
          "name": "function-mesh"
        },
        {
          "digest": "sha256:67ecb332573384515406ebd71816781366b70adb0eb66345e5980e92603373e1",
          "image": "docker.cloudsmith.io/streamnative/mirrors/gcr.io/kubebuilder/kube-rbac-proxy@sha256:67ecb332573384515406ebd71816781366b70adb0eb66345e5980e92603373e1",
          "name": "kube-rbac-proxy"
        },
        {
          "digest": "sha256:279535d1118604037058e1d31dc79c94bfa6a5bce57c24862edad0fa0fd1c579",
          "image": "registry.connect.redhat.com/streamnative/function-mesh@sha256:279535d1118604037058e1d31dc79c94bfa6a5bce57c24862edad0fa0fd1c579",
          "name": "function-mesh-279535d1118604037058e1d31dc79c94bfa6a5bce57c24862edad0fa0fd1c579-annotation"
        },
        {
          "digest": "sha256:279535d1118604037058e1d31dc79c94bfa6a5bce57c24862edad0fa0fd1c579",
          "image": "registry.connect.redhat.com/streamnative/function-mesh@sha256:279535d1118604037058e1d31dc79c94bfa6a5bce57c24862edad0fa0fd1c579",
          "name": "manager"
        }
      ],
      "replaces": null,
      "skip_range": "<0.8.0",
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.10",
      "version": "0.8.0",
      "version_original": "0.8.0"
    },
    {
      "_id": "638628e9ce133156fb8acbd8",
      "alm_examples": [
        {
          "api_version": "compute.functionmesh.io/v1alpha1",
          "kind": "Function",
          "metadata": {
            "name": "java-function-sample",
            "namespace": "default"
          },
          "spec": {
            "autoAck": true,
            "className": "org.apache.pulsar.functions.api.examples.ExclamationFunction",
            "clusterName": "test-pulsar",
            "forwardSourceMessageProperty": true,
            "input": {
              "topics": [
                "persistent://public/default/java-function-input-topic"
              ],
              "typeClassName": "java.lang.String"
            },
            "java": {
              "extraDependenciesDir": "random-dir/",
              "jar": "pulsar-functions-api-examples.jar",
              "jarLocation": "public/default/nlu-test-java-function"
            },
            "logTopic": "persistent://public/default/logging-function-logs",
            "maxPendingAsyncRequests": 1000,
            "maxReplicas": 5,
            "output": {
              "topic": "persistent://public/default/java-function-output-topic",
              "typeClassName": "java.lang.String"
            },
            "pod": {
              "annotations": {
                "managed-function": "true"
              },
              "env": [
                {
                  "name": "EXAMPLE_VARIABLE",
                  "value": "exampleValue"
                }
              ],
              "imagePullSecrets": [
                {
                  "name": "regcred"
                }
              ],
              "initContainers": [
                {
                  "command": [
                    "sh",
                    "-c",
                    "echo The app is running! && sleep 30"
                  ],
                  "image": "busybox:1.28",
                  "name": "init-function"
                }
              ],
              "labels": {
                "locaction": "mtv"
              },
              "sidecars": [
                {
                  "command": [
                    "sh",
                    "-c",
                    "echo The app is running! && sleep 30000"
                  ],
                  "image": "busybox:1.28",
                  "name": "sidecar-function"
                }
              ],
              "volumes": [
                {
                  "emptyDir": {},
                  "name": "cache-volume"
                }
              ]
            },
            "pulsar": {
              "authSecret": "test-auth",
              "pulsarConfig": "test-pulsar",
              "tlsSecret": "test-tls"
            },
            "replicas": 1,
            "resources": {
              "limits": {
                "cpu": "0.2",
                "memory": "1.1G"
              },
              "requests": {
                "cpu": "0.1",
                "memory": "1G"
              }
            },
            "secretsMap": {
              "name": {
                "key": "username",
                "path": "test-secret"
              },
              "pwd": {
                "key": "password",
                "path": "test-secret"
              }
            },
            "volumeMounts": [
              {
                "mountPath": "/cache",
                "name": "cache-volume"
              }
            ]
          }
        },
        {
          "api_version": "compute.functionmesh.io/v1alpha1",
          "kind": "FunctionMesh",
          "metadata": {
            "name": "functionmesh-sample"
          },
          "spec": {
            "functions": [
              {
                "autoAck": true,
                "className": "org.apache.pulsar.functions.api.examples.ExclamationFunction",
                "clusterName": "test-pulsar",
                "forwardSourceMessageProperty": true,
                "input": {
                  "topics": [
                    "persistent://public/default/functionmesh-input-topic"
                  ],
                  "typeClassName": "java.lang.String"
                },
                "java": {
                  "jar": "pulsar-functions-api-examples.jar",
                  "jarLocation": "public/default/nlu-test-functionmesh-ex1"
                },
                "logTopic": "persistent://public/default/logging-function-log",
                "maxReplicas": 1,
                "name": "ex1",
                "output": {
                  "topic": "persistent://public/default/mid-topic",
                  "typeClassName": "java.lang.String"
                },
                "pulsar": {
                  "pulsarConfig": "mesh-test-pulsar"
                },
                "replicas": 1,
                "resources": {
                  "limits": {
                    "cpu": "0.2",
                    "memory": "1.1G"
                  },
                  "requests": {
                    "cpu": "0.1",
                    "memory": "1G"
                  }
                }
              },
              {
                "autoAck": true,
                "className": "org.apache.pulsar.functions.api.examples.ExclamationFunction",
                "clusterName": "test-pulsar",
                "forwardSourceMessageProperty": true,
                "input": {
                  "topics": [
                    "persistent://public/default/mid-topic"
                  ],
                  "typeClassName": "java.lang.String"
                },
                "java": {
                  "jar": "pulsar-functions-api-examples.jar",
                  "jarLocation": "public/default/nlu-test-functionmesh-ex2"
                },
                "logTopic": "persistent://public/default/logging-function-logs",
                "maxReplicas": 1,
                "name": "ex2",
                "output": {
                  "topic": "persistent://public/default/functionmesh-output-topic",
                  "typeClassName": "java.lang.String"
                },
                "pulsar": {
                  "pulsarConfig": "mesh-test-pulsar"
                },
                "replicas": 1,
                "resources": {
                  "limits": {
                    "cpu": "0.2",
                    "memory": "1.1G"
                  },
                  "requests": {
                    "cpu": "0.1",
                    "memory": "1G"
                  }
                }
              }
            ]
          }
        },
        {
          "api_version": "compute.functionmesh.io/v1alpha1",
          "kind": "Sink",
          "metadata": {
            "name": "sink-sample"
          },
          "spec": {
            "autoAck": true,
            "className": "org.apache.pulsar.io.elasticsearch.ElasticSearchSink",
            "clusterName": "test-pulsar",
            "image": "streamnative/pulsar-io-elastic-search:2.10.0.0-rc10",
            "input": {
              "topics": [
                "persistent://public/default/input"
              ],
              "typeClassName": "[B"
            },
            "java": {
              "jar": "connectors/pulsar-io-elastic-search-2.10.0.0-rc10.nar",
              "jarLocation": ""
            },
            "maxReplicas": 1,
            "pulsar": {
              "pulsarConfig": "test-sink"
            },
            "replicas": 1,
            "resources": {
              "limits": {
                "cpu": "0.2",
                "memory": "1.1G"
              },
              "requests": {
                "cpu": "0.1",
                "memory": "1G"
              }
            },
            "sinkConfig": {
              "elasticSearchUrl": "http://quickstart-es-http.default.svc.cluster.local:9200",
              "indexName": "my_index",
              "password": "wJ757TmoXEd941kXm07Z2GW3",
              "typeName": "doc",
              "username": "elastic"
            }
          }
        },
        {
          "api_version": "compute.functionmesh.io/v1alpha1",
          "kind": "Source",
          "metadata": {
            "name": "source-sample"
          },
          "spec": {
            "className": "org.apache.pulsar.io.debezium.mongodb.DebeziumMongoDbSource",
            "clusterName": "test-pulsar",
            "forwardSourceMessageProperty": true,
            "image": "streamnative/pulsar-io-debezium-mongodb:2.10.0.0-rc10",
            "java": {
              "jar": "connectors/pulsar-io-debezium-mongodb-2.10.0.0-rc10.nar",
              "jarLocation": ""
            },
            "maxReplicas": 1,
            "output": {
              "producerConf": {
                "maxPendingMessages": 1000,
                "maxPendingMessagesAcrossPartitions": 50000,
                "useThreadLocalProducers": true
              },
              "topic": "persistent://public/default/destination",
              "typeClassName": "org.apache.pulsar.common.schema.KeyValue"
            },
            "pulsar": {
              "pulsarConfig": "test-source"
            },
            "replicas": 1,
            "resources": {
              "limits": {
                "cpu": "0.2",
                "memory": "1.1G"
              },
              "requests": {
                "cpu": "0.1",
                "memory": "1G"
              }
            },
            "sourceConfig": {
              "database.whitelist": "inventory",
              "mongodb.hosts": "rs0/mongo-dbz-0.mongo.default.svc.cluster.local:27017,rs0/mongo-dbz-1.mongo.default.svc.cluster.local:27017,rs0/mongo-dbz-2.mongo.default.svc.cluster.local:27017",
              "mongodb.name": "dbserver1",
              "mongodb.password": "dbz",
              "mongodb.task.id": "1",
              "mongodb.user": "debezium",
              "pulsar.service.url": "pulsar://test-pulsar-broker.default.svc.cluster.local:6650"
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/streamnative/function-mesh-bundle@sha256:949785d93c6890b1a3aee05706e7871edbfda4a0f396aabe9e1f26a41aa49197",
      "bundle_path_digest": "sha256:949785d93c6890b1a3aee05706e7871edbfda4a0f396aabe9e1f26a41aa49197",
      "capabilities": [
        "Full Lifecycle"
      ],
      "channel_name": "alpha",
      "creation_date": "2022-11-29T15:44:41.852000+00:00",
      "csv_description": "[Function Mesh](https://functionmesh.io/) is a serverless framework purpose-built for stream processing applications. It brings powerful event-streaming capabilities to your applications by orchestrating multiple [Pulsar Functions](http://pulsar.apache.org/docs/en/next/functions-overview/) and [Pulsar IO connectors](http://pulsar.apache.org/docs/en/next/io-overview/) for complex stream processing jobs on Kubernetes.\nPrerequisites:\n- Install cert-manager operator first.\n",
      "csv_display_name": "Function Mesh Operator",
      "csv_metadata_description": "The Function Mesh Operator manages the Pulsar Functions and Connectors deployed on a Kubernetes cluster.",
      "csv_name": "function-mesh.v0.8.0",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": false,
          "type": "OwnNamespace"
        },
        {
          "supported": false,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-11-29T15:44:41.852000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.9",
      "organization": "certified-operators",
      "package": "function-mesh",
      "provided_apis": [
        {
          "group": "compute.functionmesh.io",
          "kind": "Function",
          "plural": "functions",
          "version": "v1alpha1"
        },
        {
          "group": "compute.functionmesh.io",
          "kind": "Sink",
          "plural": "sinks",
          "version": "v1alpha1"
        },
        {
          "group": "compute.functionmesh.io",
          "kind": "Source",
          "plural": "sources",
          "version": "v1alpha1"
        },
        {
          "group": "compute.functionmesh.io",
          "kind": "FunctionMesh",
          "plural": "functionmeshes",
          "version": "v1alpha1"
        }
      ],
      "provider": "StreamNative",
      "related_images": [
        {
          "digest": "sha256:279535d1118604037058e1d31dc79c94bfa6a5bce57c24862edad0fa0fd1c579",
          "image": "registry.connect.redhat.com/streamnative/function-mesh@sha256:279535d1118604037058e1d31dc79c94bfa6a5bce57c24862edad0fa0fd1c579",
          "name": "function-mesh"
        },
        {
          "digest": "sha256:67ecb332573384515406ebd71816781366b70adb0eb66345e5980e92603373e1",
          "image": "docker.cloudsmith.io/streamnative/mirrors/gcr.io/kubebuilder/kube-rbac-proxy@sha256:67ecb332573384515406ebd71816781366b70adb0eb66345e5980e92603373e1",
          "name": "kube-rbac-proxy"
        },
        {
          "digest": "sha256:279535d1118604037058e1d31dc79c94bfa6a5bce57c24862edad0fa0fd1c579",
          "image": "registry.connect.redhat.com/streamnative/function-mesh@sha256:279535d1118604037058e1d31dc79c94bfa6a5bce57c24862edad0fa0fd1c579",
          "name": "function-mesh-279535d1118604037058e1d31dc79c94bfa6a5bce57c24862edad0fa0fd1c579-annotation"
        },
        {
          "digest": "sha256:279535d1118604037058e1d31dc79c94bfa6a5bce57c24862edad0fa0fd1c579",
          "image": "registry.connect.redhat.com/streamnative/function-mesh@sha256:279535d1118604037058e1d31dc79c94bfa6a5bce57c24862edad0fa0fd1c579",
          "name": "manager"
        }
      ],
      "replaces": null,
      "skip_range": "<0.8.0",
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.9",
      "version": "0.8.0",
      "version_original": "0.8.0"
    },
    {
      "_id": "63862a49682aff0f29ba51fc",
      "alm_examples": [
        {
          "api_version": "compute.functionmesh.io/v1alpha1",
          "kind": "Function",
          "metadata": {
            "name": "java-function-sample",
            "namespace": "default"
          },
          "spec": {
            "autoAck": true,
            "className": "org.apache.pulsar.functions.api.examples.ExclamationFunction",
            "clusterName": "test-pulsar",
            "forwardSourceMessageProperty": true,
            "input": {
              "topics": [
                "persistent://public/default/java-function-input-topic"
              ],
              "typeClassName": "java.lang.String"
            },
            "java": {
              "extraDependenciesDir": "random-dir/",
              "jar": "pulsar-functions-api-examples.jar",
              "jarLocation": "public/default/nlu-test-java-function"
            },
            "logTopic": "persistent://public/default/logging-function-logs",
            "maxPendingAsyncRequests": 1000,
            "maxReplicas": 5,
            "output": {
              "topic": "persistent://public/default/java-function-output-topic",
              "typeClassName": "java.lang.String"
            },
            "pod": {
              "annotations": {
                "managed-function": "true"
              },
              "env": [
                {
                  "name": "EXAMPLE_VARIABLE",
                  "value": "exampleValue"
                }
              ],
              "imagePullSecrets": [
                {
                  "name": "regcred"
                }
              ],
              "initContainers": [
                {
                  "command": [
                    "sh",
                    "-c",
                    "echo The app is running! && sleep 30"
                  ],
                  "image": "busybox:1.28",
                  "name": "init-function"
                }
              ],
              "labels": {
                "locaction": "mtv"
              },
              "sidecars": [
                {
                  "command": [
                    "sh",
                    "-c",
                    "echo The app is running! && sleep 30000"
                  ],
                  "image": "busybox:1.28",
                  "name": "sidecar-function"
                }
              ],
              "volumes": [
                {
                  "emptyDir": {},
                  "name": "cache-volume"
                }
              ]
            },
            "pulsar": {
              "authSecret": "test-auth",
              "pulsarConfig": "test-pulsar",
              "tlsSecret": "test-tls"
            },
            "replicas": 1,
            "resources": {
              "limits": {
                "cpu": "0.2",
                "memory": "1.1G"
              },
              "requests": {
                "cpu": "0.1",
                "memory": "1G"
              }
            },
            "secretsMap": {
              "name": {
                "key": "username",
                "path": "test-secret"
              },
              "pwd": {
                "key": "password",
                "path": "test-secret"
              }
            },
            "volumeMounts": [
              {
                "mountPath": "/cache",
                "name": "cache-volume"
              }
            ]
          }
        },
        {
          "api_version": "compute.functionmesh.io/v1alpha1",
          "kind": "FunctionMesh",
          "metadata": {
            "name": "functionmesh-sample"
          },
          "spec": {
            "functions": [
              {
                "autoAck": true,
                "className": "org.apache.pulsar.functions.api.examples.ExclamationFunction",
                "clusterName": "test-pulsar",
                "forwardSourceMessageProperty": true,
                "input": {
                  "topics": [
                    "persistent://public/default/functionmesh-input-topic"
                  ],
                  "typeClassName": "java.lang.String"
                },
                "java": {
                  "jar": "pulsar-functions-api-examples.jar",
                  "jarLocation": "public/default/nlu-test-functionmesh-ex1"
                },
                "logTopic": "persistent://public/default/logging-function-log",
                "maxReplicas": 1,
                "name": "ex1",
                "output": {
                  "topic": "persistent://public/default/mid-topic",
                  "typeClassName": "java.lang.String"
                },
                "pulsar": {
                  "pulsarConfig": "mesh-test-pulsar"
                },
                "replicas": 1,
                "resources": {
                  "limits": {
                    "cpu": "0.2",
                    "memory": "1.1G"
                  },
                  "requests": {
                    "cpu": "0.1",
                    "memory": "1G"
                  }
                }
              },
              {
                "autoAck": true,
                "className": "org.apache.pulsar.functions.api.examples.ExclamationFunction",
                "clusterName": "test-pulsar",
                "forwardSourceMessageProperty": true,
                "input": {
                  "topics": [
                    "persistent://public/default/mid-topic"
                  ],
                  "typeClassName": "java.lang.String"
                },
                "java": {
                  "jar": "pulsar-functions-api-examples.jar",
                  "jarLocation": "public/default/nlu-test-functionmesh-ex2"
                },
                "logTopic": "persistent://public/default/logging-function-logs",
                "maxReplicas": 1,
                "name": "ex2",
                "output": {
                  "topic": "persistent://public/default/functionmesh-output-topic",
                  "typeClassName": "java.lang.String"
                },
                "pulsar": {
                  "pulsarConfig": "mesh-test-pulsar"
                },
                "replicas": 1,
                "resources": {
                  "limits": {
                    "cpu": "0.2",
                    "memory": "1.1G"
                  },
                  "requests": {
                    "cpu": "0.1",
                    "memory": "1G"
                  }
                }
              }
            ]
          }
        },
        {
          "api_version": "compute.functionmesh.io/v1alpha1",
          "kind": "Sink",
          "metadata": {
            "name": "sink-sample"
          },
          "spec": {
            "autoAck": true,
            "className": "org.apache.pulsar.io.elasticsearch.ElasticSearchSink",
            "clusterName": "test-pulsar",
            "image": "streamnative/pulsar-io-elastic-search:2.10.0.0-rc10",
            "input": {
              "topics": [
                "persistent://public/default/input"
              ],
              "typeClassName": "[B"
            },
            "java": {
              "jar": "connectors/pulsar-io-elastic-search-2.10.0.0-rc10.nar",
              "jarLocation": ""
            },
            "maxReplicas": 1,
            "pulsar": {
              "pulsarConfig": "test-sink"
            },
            "replicas": 1,
            "resources": {
              "limits": {
                "cpu": "0.2",
                "memory": "1.1G"
              },
              "requests": {
                "cpu": "0.1",
                "memory": "1G"
              }
            },
            "sinkConfig": {
              "elasticSearchUrl": "http://quickstart-es-http.default.svc.cluster.local:9200",
              "indexName": "my_index",
              "password": "wJ757TmoXEd941kXm07Z2GW3",
              "typeName": "doc",
              "username": "elastic"
            }
          }
        },
        {
          "api_version": "compute.functionmesh.io/v1alpha1",
          "kind": "Source",
          "metadata": {
            "name": "source-sample"
          },
          "spec": {
            "className": "org.apache.pulsar.io.debezium.mongodb.DebeziumMongoDbSource",
            "clusterName": "test-pulsar",
            "forwardSourceMessageProperty": true,
            "image": "streamnative/pulsar-io-debezium-mongodb:2.10.0.0-rc10",
            "java": {
              "jar": "connectors/pulsar-io-debezium-mongodb-2.10.0.0-rc10.nar",
              "jarLocation": ""
            },
            "maxReplicas": 1,
            "output": {
              "producerConf": {
                "maxPendingMessages": 1000,
                "maxPendingMessagesAcrossPartitions": 50000,
                "useThreadLocalProducers": true
              },
              "topic": "persistent://public/default/destination",
              "typeClassName": "org.apache.pulsar.common.schema.KeyValue"
            },
            "pulsar": {
              "pulsarConfig": "test-source"
            },
            "replicas": 1,
            "resources": {
              "limits": {
                "cpu": "0.2",
                "memory": "1.1G"
              },
              "requests": {
                "cpu": "0.1",
                "memory": "1G"
              }
            },
            "sourceConfig": {
              "database.whitelist": "inventory",
              "mongodb.hosts": "rs0/mongo-dbz-0.mongo.default.svc.cluster.local:27017,rs0/mongo-dbz-1.mongo.default.svc.cluster.local:27017,rs0/mongo-dbz-2.mongo.default.svc.cluster.local:27017",
              "mongodb.name": "dbserver1",
              "mongodb.password": "dbz",
              "mongodb.task.id": "1",
              "mongodb.user": "debezium",
              "pulsar.service.url": "pulsar://test-pulsar-broker.default.svc.cluster.local:6650"
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/streamnative/function-mesh-bundle@sha256:949785d93c6890b1a3aee05706e7871edbfda4a0f396aabe9e1f26a41aa49197",
      "bundle_path_digest": "sha256:949785d93c6890b1a3aee05706e7871edbfda4a0f396aabe9e1f26a41aa49197",
      "capabilities": [
        "Full Lifecycle"
      ],
      "channel_name": "alpha",
      "creation_date": "2022-11-29T15:50:33.049000+00:00",
      "csv_description": "[Function Mesh](https://functionmesh.io/) is a serverless framework purpose-built for stream processing applications. It brings powerful event-streaming capabilities to your applications by orchestrating multiple [Pulsar Functions](http://pulsar.apache.org/docs/en/next/functions-overview/) and [Pulsar IO connectors](http://pulsar.apache.org/docs/en/next/io-overview/) for complex stream processing jobs on Kubernetes.\nPrerequisites:\n- Install cert-manager operator first.\n",
      "csv_display_name": "Function Mesh Operator",
      "csv_metadata_description": "The Function Mesh Operator manages the Pulsar Functions and Connectors deployed on a Kubernetes cluster.",
      "csv_name": "function-mesh.v0.8.0",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": false,
          "type": "OwnNamespace"
        },
        {
          "supported": false,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-11-29T15:50:33.049000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.8",
      "organization": "certified-operators",
      "package": "function-mesh",
      "provided_apis": [
        {
          "group": "compute.functionmesh.io",
          "kind": "FunctionMesh",
          "plural": "functionmeshes",
          "version": "v1alpha1"
        },
        {
          "group": "compute.functionmesh.io",
          "kind": "Function",
          "plural": "functions",
          "version": "v1alpha1"
        },
        {
          "group": "compute.functionmesh.io",
          "kind": "Sink",
          "plural": "sinks",
          "version": "v1alpha1"
        },
        {
          "group": "compute.functionmesh.io",
          "kind": "Source",
          "plural": "sources",
          "version": "v1alpha1"
        }
      ],
      "provider": "StreamNative",
      "related_images": [
        {
          "digest": "sha256:279535d1118604037058e1d31dc79c94bfa6a5bce57c24862edad0fa0fd1c579",
          "image": "registry.connect.redhat.com/streamnative/function-mesh@sha256:279535d1118604037058e1d31dc79c94bfa6a5bce57c24862edad0fa0fd1c579",
          "name": "function-mesh"
        },
        {
          "digest": "sha256:67ecb332573384515406ebd71816781366b70adb0eb66345e5980e92603373e1",
          "image": "docker.cloudsmith.io/streamnative/mirrors/gcr.io/kubebuilder/kube-rbac-proxy@sha256:67ecb332573384515406ebd71816781366b70adb0eb66345e5980e92603373e1",
          "name": "kube-rbac-proxy"
        },
        {
          "digest": "sha256:279535d1118604037058e1d31dc79c94bfa6a5bce57c24862edad0fa0fd1c579",
          "image": "registry.connect.redhat.com/streamnative/function-mesh@sha256:279535d1118604037058e1d31dc79c94bfa6a5bce57c24862edad0fa0fd1c579",
          "name": "function-mesh-279535d1118604037058e1d31dc79c94bfa6a5bce57c24862edad0fa0fd1c579-annotation"
        },
        {
          "digest": "sha256:279535d1118604037058e1d31dc79c94bfa6a5bce57c24862edad0fa0fd1c579",
          "image": "registry.connect.redhat.com/streamnative/function-mesh@sha256:279535d1118604037058e1d31dc79c94bfa6a5bce57c24862edad0fa0fd1c579",
          "name": "manager"
        }
      ],
      "replaces": null,
      "skip_range": "<0.8.0",
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.8",
      "version": "0.8.0",
      "version_original": "0.8.0"
    },
    {
      "_id": "638795d55512e040e862f263",
      "alm_examples": [
        {
          "api_version": "dynatrace.com/v1beta1",
          "kind": "DynaKube",
          "metadata": {
            "name": "dynakube",
            "namespace": "dynatrace"
          },
          "spec": {
            "activeGate": {
              "capabilities": [
                "routing",
                "kubernetes-monitoring",
                "dynatrace-api"
              ],
              "image": "",
              "resources": {
                "limits": {
                  "cpu": "1000m",
                  "memory": "1.5Gi"
                },
                "requests": {
                  "cpu": "500m",
                  "memory": "512Mi"
                }
              }
            },
            "apiUrl": "https://ENVIRONMENTID.live.dynatrace.com/api",
            "oneAgent": {
              "classicFullStack": {
                "tolerations": [
                  {
                    "effect": "NoSchedule",
                    "key": "node-role.kubernetes.io/master",
                    "operator": "Exists"
                  },
                  {
                    "effect": "NoSchedule",
                    "key": "node-role.kubernetes.io/control-plane",
                    "operator": "Exists"
                  }
                ]
              }
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/dynatrace/dynatrace-operator-bundle@sha256:1d2a6c647bd52e99be8cb03449d9d0f73126bb600e45b510d4d060fa2d8c0931",
      "bundle_path_digest": "sha256:1d2a6c647bd52e99be8cb03449d9d0f73126bb600e45b510d4d060fa2d8c0931",
      "capabilities": [
        "Deep Insights"
      ],
      "channel_name": "alpha",
      "creation_date": "2022-11-30T17:41:41.064000+00:00",
      "csv_description": "The Dynatrace Operator supports rollout and lifecycle management of various Dynatrace components in Kubernetes and OpenShift.\n\nCurrently the Dynatrace Operator supports the following capabilities:\n\n### OneAgent\n  * `classicFullStack` rolls out one OneAgent pod per node to monitor its pods and the node itself\n  * `applicationMonitoring` is a webhook based injection mechanism for automatic app-only injection\n  * `hostMonitoring` monitors only the hosts, i.e., the nodes, in the cluster without app-only injection\n### ActiveGate\n  * `routing` routes OneAgent traffic through the ActiveGate\n  * `kubernetes-monitoring` allows monitoring the Kubernetes API\n  * `metrics-ingest` routes enriched metrics through an ActiveGate\n\nFor more information please refer to [our DynaKube Custom Resource examples](https://dt-url.net/dynakube-samples).\n\n### Installation\nOnce you've installed the Dynatrace Operator, you can create a DynaKube custom resource.\n\nFirst, please add a Secret within the Project you've deployed the Dynatrace Operator to, which would contain your API and PaaS tokens. Create tokens of type *Dynatrace API* (`API_TOKEN`) and *Platform as a Service* (`PAAS_TOKEN`) and use their values in the following commands respectively.\n\nFor assistance please refer to [Create user-generated access tokens](https://www.dynatrace.com/support/help/shortlink/token#create-user-generated-access-tokens).\n\n``` $ oc -n <project> create secret generic dynakube --from-literal=\"apiToken=API_TOKEN\" --from-literal=\"paasToken=PAAS_TOKEN\" ```\n\nYou may update this Secret at any time to rotate the tokens.\n\nAfter creation of the secret add the DynaKube object in the project where the Dynatrace Operator has been deployed, configured to your needs.\n\n### Required Parameters\n* `apiUrl` - provide the URL to the API of your Dynatrace environment. In Dynatrace SaaS it will look like `https://<ENVIRONMENTID>.live.dynatrace.com/api` . In Dynatrace Managed like `https://<YourDynatraceServerURL>/e/<ENVIRONMENTID>/api` .\n\n### Advanced Options\n* **Disable Certificate Checking** - disable any certificate validation that may interact poorly with proxies with in your cluster\n* **Image Override** - use a copy of the ActiveGate container image from a registry other than Docker's or Red Hat's\n\nFor a complete list of supported parameters please consult the [Operator Deploy Guide](https://www.dynatrace.com/support/help/shortlink/openshift-deploy).\n\n### Help\nYou can find more about our instructions in our [documentation](https://www.dynatrace.com/support/help/shortlink/openshift-deploy#install-dynatrace-operator).\n",
      "csv_display_name": "Dynatrace Operator",
      "csv_metadata_description": "",
      "csv_name": "dynatrace-operator.v0.10.0",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": false,
          "type": "OwnNamespace"
        },
        {
          "supported": false,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-11-30T17:41:41.064000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.11",
      "organization": "certified-operators",
      "package": "dynatrace-operator",
      "provided_apis": [
        {
          "group": "dynatrace.com",
          "kind": "DynaKube",
          "version": "v1alpha1"
        },
        {
          "group": "dynatrace.com",
          "kind": "DynaKube",
          "version": "v1beta1"
        }
      ],
      "provider": "Dynatrace LLC",
      "related_images": [
        {
          "digest": "sha256:2ee69ec8e8eeae6e026985bc799871142a8f83a60338480bfd11cc993768e708",
          "image": "registry.connect.redhat.com/dynatrace/dynatrace-operator@sha256:2ee69ec8e8eeae6e026985bc799871142a8f83a60338480bfd11cc993768e708",
          "name": "dynatrace-operator"
        },
        {
          "digest": "sha256:2ee69ec8e8eeae6e026985bc799871142a8f83a60338480bfd11cc993768e708",
          "image": "registry.connect.redhat.com/dynatrace/dynatrace-operator@sha256:2ee69ec8e8eeae6e026985bc799871142a8f83a60338480bfd11cc993768e708",
          "name": "dynatrace-operator-2ee69ec8e8eeae6e026985bc799871142a8f83a60338480bfd11cc993768e708-annotation"
        },
        {
          "digest": "sha256:2ee69ec8e8eeae6e026985bc799871142a8f83a60338480bfd11cc993768e708",
          "image": "registry.connect.redhat.com/dynatrace/dynatrace-operator@sha256:2ee69ec8e8eeae6e026985bc799871142a8f83a60338480bfd11cc993768e708",
          "name": "webhook"
        }
      ],
      "replaces": null,
      "skip_range": "<=v0.9.1",
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.11",
      "version": "0.10.0",
      "version_original": "0.10.0"
    },
    {
      "_id": "6387963dce133156fb918d40",
      "alm_examples": [
        {
          "api_version": "dynatrace.com/v1beta1",
          "kind": "DynaKube",
          "metadata": {
            "name": "dynakube",
            "namespace": "dynatrace"
          },
          "spec": {
            "activeGate": {
              "capabilities": [
                "routing",
                "kubernetes-monitoring",
                "dynatrace-api"
              ],
              "image": "",
              "resources": {
                "limits": {
                  "cpu": "1000m",
                  "memory": "1.5Gi"
                },
                "requests": {
                  "cpu": "500m",
                  "memory": "512Mi"
                }
              }
            },
            "apiUrl": "https://ENVIRONMENTID.live.dynatrace.com/api",
            "oneAgent": {
              "classicFullStack": {
                "tolerations": [
                  {
                    "effect": "NoSchedule",
                    "key": "node-role.kubernetes.io/master",
                    "operator": "Exists"
                  },
                  {
                    "effect": "NoSchedule",
                    "key": "node-role.kubernetes.io/control-plane",
                    "operator": "Exists"
                  }
                ]
              }
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/dynatrace/dynatrace-operator-bundle@sha256:1d2a6c647bd52e99be8cb03449d9d0f73126bb600e45b510d4d060fa2d8c0931",
      "bundle_path_digest": "sha256:1d2a6c647bd52e99be8cb03449d9d0f73126bb600e45b510d4d060fa2d8c0931",
      "capabilities": [
        "Deep Insights"
      ],
      "channel_name": "alpha",
      "creation_date": "2022-11-30T17:43:25.882000+00:00",
      "csv_description": "The Dynatrace Operator supports rollout and lifecycle management of various Dynatrace components in Kubernetes and OpenShift.\n\nCurrently the Dynatrace Operator supports the following capabilities:\n\n### OneAgent\n  * `classicFullStack` rolls out one OneAgent pod per node to monitor its pods and the node itself\n  * `applicationMonitoring` is a webhook based injection mechanism for automatic app-only injection\n  * `hostMonitoring` monitors only the hosts, i.e., the nodes, in the cluster without app-only injection\n### ActiveGate\n  * `routing` routes OneAgent traffic through the ActiveGate\n  * `kubernetes-monitoring` allows monitoring the Kubernetes API\n  * `metrics-ingest` routes enriched metrics through an ActiveGate\n\nFor more information please refer to [our DynaKube Custom Resource examples](https://dt-url.net/dynakube-samples).\n\n### Installation\nOnce you've installed the Dynatrace Operator, you can create a DynaKube custom resource.\n\nFirst, please add a Secret within the Project you've deployed the Dynatrace Operator to, which would contain your API and PaaS tokens. Create tokens of type *Dynatrace API* (`API_TOKEN`) and *Platform as a Service* (`PAAS_TOKEN`) and use their values in the following commands respectively.\n\nFor assistance please refer to [Create user-generated access tokens](https://www.dynatrace.com/support/help/shortlink/token#create-user-generated-access-tokens).\n\n``` $ oc -n <project> create secret generic dynakube --from-literal=\"apiToken=API_TOKEN\" --from-literal=\"paasToken=PAAS_TOKEN\" ```\n\nYou may update this Secret at any time to rotate the tokens.\n\nAfter creation of the secret add the DynaKube object in the project where the Dynatrace Operator has been deployed, configured to your needs.\n\n### Required Parameters\n* `apiUrl` - provide the URL to the API of your Dynatrace environment. In Dynatrace SaaS it will look like `https://<ENVIRONMENTID>.live.dynatrace.com/api` . In Dynatrace Managed like `https://<YourDynatraceServerURL>/e/<ENVIRONMENTID>/api` .\n\n### Advanced Options\n* **Disable Certificate Checking** - disable any certificate validation that may interact poorly with proxies with in your cluster\n* **Image Override** - use a copy of the ActiveGate container image from a registry other than Docker's or Red Hat's\n\nFor a complete list of supported parameters please consult the [Operator Deploy Guide](https://www.dynatrace.com/support/help/shortlink/openshift-deploy).\n\n### Help\nYou can find more about our instructions in our [documentation](https://www.dynatrace.com/support/help/shortlink/openshift-deploy#install-dynatrace-operator).\n",
      "csv_display_name": "Dynatrace Operator",
      "csv_metadata_description": "",
      "csv_name": "dynatrace-operator.v0.10.0",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": false,
          "type": "OwnNamespace"
        },
        {
          "supported": false,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-11-30T17:43:25.882000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.10",
      "organization": "certified-operators",
      "package": "dynatrace-operator",
      "provided_apis": [
        {
          "group": "dynatrace.com",
          "kind": "DynaKube",
          "plural": "dynakubes",
          "version": "v1alpha1"
        },
        {
          "group": "dynatrace.com",
          "kind": "DynaKube",
          "plural": "dynakubes",
          "version": "v1beta1"
        }
      ],
      "provider": "Dynatrace LLC",
      "related_images": [
        {
          "digest": "sha256:2ee69ec8e8eeae6e026985bc799871142a8f83a60338480bfd11cc993768e708",
          "image": "registry.connect.redhat.com/dynatrace/dynatrace-operator@sha256:2ee69ec8e8eeae6e026985bc799871142a8f83a60338480bfd11cc993768e708",
          "name": "dynatrace-operator"
        },
        {
          "digest": "sha256:2ee69ec8e8eeae6e026985bc799871142a8f83a60338480bfd11cc993768e708",
          "image": "registry.connect.redhat.com/dynatrace/dynatrace-operator@sha256:2ee69ec8e8eeae6e026985bc799871142a8f83a60338480bfd11cc993768e708",
          "name": "dynatrace-operator-2ee69ec8e8eeae6e026985bc799871142a8f83a60338480bfd11cc993768e708-annotation"
        },
        {
          "digest": "sha256:2ee69ec8e8eeae6e026985bc799871142a8f83a60338480bfd11cc993768e708",
          "image": "registry.connect.redhat.com/dynatrace/dynatrace-operator@sha256:2ee69ec8e8eeae6e026985bc799871142a8f83a60338480bfd11cc993768e708",
          "name": "webhook"
        }
      ],
      "replaces": null,
      "skip_range": "<=v0.9.1",
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.10",
      "version": "0.10.0",
      "version_original": "0.10.0"
    },
    {
      "_id": "638796685512e040e862f4a7",
      "alm_examples": [
        {
          "api_version": "dynatrace.com/v1beta1",
          "kind": "DynaKube",
          "metadata": {
            "name": "dynakube",
            "namespace": "dynatrace"
          },
          "spec": {
            "activeGate": {
              "capabilities": [
                "routing",
                "kubernetes-monitoring",
                "dynatrace-api"
              ],
              "image": "",
              "resources": {
                "limits": {
                  "cpu": "1000m",
                  "memory": "1.5Gi"
                },
                "requests": {
                  "cpu": "500m",
                  "memory": "512Mi"
                }
              }
            },
            "apiUrl": "https://ENVIRONMENTID.live.dynatrace.com/api",
            "oneAgent": {
              "classicFullStack": {
                "tolerations": [
                  {
                    "effect": "NoSchedule",
                    "key": "node-role.kubernetes.io/master",
                    "operator": "Exists"
                  },
                  {
                    "effect": "NoSchedule",
                    "key": "node-role.kubernetes.io/control-plane",
                    "operator": "Exists"
                  }
                ]
              }
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/dynatrace/dynatrace-operator-bundle@sha256:1d2a6c647bd52e99be8cb03449d9d0f73126bb600e45b510d4d060fa2d8c0931",
      "bundle_path_digest": "sha256:1d2a6c647bd52e99be8cb03449d9d0f73126bb600e45b510d4d060fa2d8c0931",
      "capabilities": [
        "Deep Insights"
      ],
      "channel_name": "alpha",
      "creation_date": "2022-11-30T17:44:08.370000+00:00",
      "csv_description": "The Dynatrace Operator supports rollout and lifecycle management of various Dynatrace components in Kubernetes and OpenShift.\n\nCurrently the Dynatrace Operator supports the following capabilities:\n\n### OneAgent\n  * `classicFullStack` rolls out one OneAgent pod per node to monitor its pods and the node itself\n  * `applicationMonitoring` is a webhook based injection mechanism for automatic app-only injection\n  * `hostMonitoring` monitors only the hosts, i.e., the nodes, in the cluster without app-only injection\n### ActiveGate\n  * `routing` routes OneAgent traffic through the ActiveGate\n  * `kubernetes-monitoring` allows monitoring the Kubernetes API\n  * `metrics-ingest` routes enriched metrics through an ActiveGate\n\nFor more information please refer to [our DynaKube Custom Resource examples](https://dt-url.net/dynakube-samples).\n\n### Installation\nOnce you've installed the Dynatrace Operator, you can create a DynaKube custom resource.\n\nFirst, please add a Secret within the Project you've deployed the Dynatrace Operator to, which would contain your API and PaaS tokens. Create tokens of type *Dynatrace API* (`API_TOKEN`) and *Platform as a Service* (`PAAS_TOKEN`) and use their values in the following commands respectively.\n\nFor assistance please refer to [Create user-generated access tokens](https://www.dynatrace.com/support/help/shortlink/token#create-user-generated-access-tokens).\n\n``` $ oc -n <project> create secret generic dynakube --from-literal=\"apiToken=API_TOKEN\" --from-literal=\"paasToken=PAAS_TOKEN\" ```\n\nYou may update this Secret at any time to rotate the tokens.\n\nAfter creation of the secret add the DynaKube object in the project where the Dynatrace Operator has been deployed, configured to your needs.\n\n### Required Parameters\n* `apiUrl` - provide the URL to the API of your Dynatrace environment. In Dynatrace SaaS it will look like `https://<ENVIRONMENTID>.live.dynatrace.com/api` . In Dynatrace Managed like `https://<YourDynatraceServerURL>/e/<ENVIRONMENTID>/api` .\n\n### Advanced Options\n* **Disable Certificate Checking** - disable any certificate validation that may interact poorly with proxies with in your cluster\n* **Image Override** - use a copy of the ActiveGate container image from a registry other than Docker's or Red Hat's\n\nFor a complete list of supported parameters please consult the [Operator Deploy Guide](https://www.dynatrace.com/support/help/shortlink/openshift-deploy).\n\n### Help\nYou can find more about our instructions in our [documentation](https://www.dynatrace.com/support/help/shortlink/openshift-deploy#install-dynatrace-operator).\n",
      "csv_display_name": "Dynatrace Operator",
      "csv_metadata_description": "",
      "csv_name": "dynatrace-operator.v0.10.0",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": false,
          "type": "OwnNamespace"
        },
        {
          "supported": false,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-11-30T17:44:08.370000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.9",
      "organization": "certified-operators",
      "package": "dynatrace-operator",
      "provided_apis": [
        {
          "group": "dynatrace.com",
          "kind": "DynaKube",
          "plural": "dynakubes",
          "version": "v1alpha1"
        },
        {
          "group": "dynatrace.com",
          "kind": "DynaKube",
          "plural": "dynakubes",
          "version": "v1beta1"
        }
      ],
      "provider": "Dynatrace LLC",
      "related_images": [
        {
          "digest": "sha256:2ee69ec8e8eeae6e026985bc799871142a8f83a60338480bfd11cc993768e708",
          "image": "registry.connect.redhat.com/dynatrace/dynatrace-operator@sha256:2ee69ec8e8eeae6e026985bc799871142a8f83a60338480bfd11cc993768e708",
          "name": "dynatrace-operator"
        },
        {
          "digest": "sha256:2ee69ec8e8eeae6e026985bc799871142a8f83a60338480bfd11cc993768e708",
          "image": "registry.connect.redhat.com/dynatrace/dynatrace-operator@sha256:2ee69ec8e8eeae6e026985bc799871142a8f83a60338480bfd11cc993768e708",
          "name": "dynatrace-operator-2ee69ec8e8eeae6e026985bc799871142a8f83a60338480bfd11cc993768e708-annotation"
        },
        {
          "digest": "sha256:2ee69ec8e8eeae6e026985bc799871142a8f83a60338480bfd11cc993768e708",
          "image": "registry.connect.redhat.com/dynatrace/dynatrace-operator@sha256:2ee69ec8e8eeae6e026985bc799871142a8f83a60338480bfd11cc993768e708",
          "name": "webhook"
        }
      ],
      "replaces": null,
      "skip_range": "<=v0.9.1",
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.9",
      "version": "0.10.0",
      "version_original": "0.10.0"
    },
    {
      "_id": "63879b1b1c3e504e225a24df",
      "alm_examples": [
        {
          "api_version": "dynatrace.com/v1beta1",
          "kind": "DynaKube",
          "metadata": {
            "name": "dynakube",
            "namespace": "dynatrace"
          },
          "spec": {
            "activeGate": {
              "capabilities": [
                "routing",
                "kubernetes-monitoring",
                "dynatrace-api"
              ],
              "image": "",
              "resources": {
                "limits": {
                  "cpu": "1000m",
                  "memory": "1.5Gi"
                },
                "requests": {
                  "cpu": "500m",
                  "memory": "512Mi"
                }
              }
            },
            "apiUrl": "https://ENVIRONMENTID.live.dynatrace.com/api",
            "oneAgent": {
              "classicFullStack": {
                "tolerations": [
                  {
                    "effect": "NoSchedule",
                    "key": "node-role.kubernetes.io/master",
                    "operator": "Exists"
                  },
                  {
                    "effect": "NoSchedule",
                    "key": "node-role.kubernetes.io/control-plane",
                    "operator": "Exists"
                  }
                ]
              }
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/dynatrace/dynatrace-operator-bundle@sha256:1d2a6c647bd52e99be8cb03449d9d0f73126bb600e45b510d4d060fa2d8c0931",
      "bundle_path_digest": "sha256:1d2a6c647bd52e99be8cb03449d9d0f73126bb600e45b510d4d060fa2d8c0931",
      "capabilities": [
        "Deep Insights"
      ],
      "channel_name": "alpha",
      "creation_date": "2022-11-30T18:04:11.506000+00:00",
      "csv_description": "The Dynatrace Operator supports rollout and lifecycle management of various Dynatrace components in Kubernetes and OpenShift.\n\nCurrently the Dynatrace Operator supports the following capabilities:\n\n### OneAgent\n  * `classicFullStack` rolls out one OneAgent pod per node to monitor its pods and the node itself\n  * `applicationMonitoring` is a webhook based injection mechanism for automatic app-only injection\n  * `hostMonitoring` monitors only the hosts, i.e., the nodes, in the cluster without app-only injection\n### ActiveGate\n  * `routing` routes OneAgent traffic through the ActiveGate\n  * `kubernetes-monitoring` allows monitoring the Kubernetes API\n  * `metrics-ingest` routes enriched metrics through an ActiveGate\n\nFor more information please refer to [our DynaKube Custom Resource examples](https://dt-url.net/dynakube-samples).\n\n### Installation\nOnce you've installed the Dynatrace Operator, you can create a DynaKube custom resource.\n\nFirst, please add a Secret within the Project you've deployed the Dynatrace Operator to, which would contain your API and PaaS tokens. Create tokens of type *Dynatrace API* (`API_TOKEN`) and *Platform as a Service* (`PAAS_TOKEN`) and use their values in the following commands respectively.\n\nFor assistance please refer to [Create user-generated access tokens](https://www.dynatrace.com/support/help/shortlink/token#create-user-generated-access-tokens).\n\n``` $ oc -n <project> create secret generic dynakube --from-literal=\"apiToken=API_TOKEN\" --from-literal=\"paasToken=PAAS_TOKEN\" ```\n\nYou may update this Secret at any time to rotate the tokens.\n\nAfter creation of the secret add the DynaKube object in the project where the Dynatrace Operator has been deployed, configured to your needs.\n\n### Required Parameters\n* `apiUrl` - provide the URL to the API of your Dynatrace environment. In Dynatrace SaaS it will look like `https://<ENVIRONMENTID>.live.dynatrace.com/api` . In Dynatrace Managed like `https://<YourDynatraceServerURL>/e/<ENVIRONMENTID>/api` .\n\n### Advanced Options\n* **Disable Certificate Checking** - disable any certificate validation that may interact poorly with proxies with in your cluster\n* **Image Override** - use a copy of the ActiveGate container image from a registry other than Docker's or Red Hat's\n\nFor a complete list of supported parameters please consult the [Operator Deploy Guide](https://www.dynatrace.com/support/help/shortlink/openshift-deploy).\n\n### Help\nYou can find more about our instructions in our [documentation](https://www.dynatrace.com/support/help/shortlink/openshift-deploy#install-dynatrace-operator).\n",
      "csv_display_name": "Dynatrace Operator",
      "csv_metadata_description": "",
      "csv_name": "dynatrace-operator.v0.10.0",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": false,
          "type": "OwnNamespace"
        },
        {
          "supported": false,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-11-30T18:04:11.506000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.8",
      "organization": "certified-operators",
      "package": "dynatrace-operator",
      "provided_apis": [
        {
          "group": "dynatrace.com",
          "kind": "DynaKube",
          "plural": "dynakubes",
          "version": "v1beta1"
        },
        {
          "group": "dynatrace.com",
          "kind": "DynaKube",
          "plural": "dynakubes",
          "version": "v1alpha1"
        }
      ],
      "provider": "Dynatrace LLC",
      "related_images": [
        {
          "digest": "sha256:2ee69ec8e8eeae6e026985bc799871142a8f83a60338480bfd11cc993768e708",
          "image": "registry.connect.redhat.com/dynatrace/dynatrace-operator@sha256:2ee69ec8e8eeae6e026985bc799871142a8f83a60338480bfd11cc993768e708",
          "name": "dynatrace-operator"
        },
        {
          "digest": "sha256:2ee69ec8e8eeae6e026985bc799871142a8f83a60338480bfd11cc993768e708",
          "image": "registry.connect.redhat.com/dynatrace/dynatrace-operator@sha256:2ee69ec8e8eeae6e026985bc799871142a8f83a60338480bfd11cc993768e708",
          "name": "dynatrace-operator-2ee69ec8e8eeae6e026985bc799871142a8f83a60338480bfd11cc993768e708-annotation"
        },
        {
          "digest": "sha256:2ee69ec8e8eeae6e026985bc799871142a8f83a60338480bfd11cc993768e708",
          "image": "registry.connect.redhat.com/dynatrace/dynatrace-operator@sha256:2ee69ec8e8eeae6e026985bc799871142a8f83a60338480bfd11cc993768e708",
          "name": "webhook"
        }
      ],
      "replaces": null,
      "skip_range": "<=v0.9.1",
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.8",
      "version": "0.10.0",
      "version_original": "0.10.0"
    },
    {
      "_id": "638836dbd5abe2935b88b394",
      "alm_examples": [
        {
          "api_version": "vlic.veritas.com/v1",
          "kind": "License",
          "metadata": {
            "name": "license-dev"
          },
          "spec": {
            "licenseEdition": "Developer"
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/veritas-technologies/infoscale-licensing-operator-bundle@sha256:6ed2ec1be3156926f7ac384fc6285058e97b6c4140437b58d616c7bc2dea3274",
      "bundle_path_digest": "sha256:6ed2ec1be3156926f7ac384fc6285058e97b6c4140437b58d616c7bc2dea3274",
      "capabilities": [
        "Seamless Upgrades"
      ],
      "channel_name": "stable",
      "creation_date": "2022-12-01T05:08:43.204000+00:00",
      "csv_description": "InfoScale\u2122 Licensing Operator provides license to use Veritas Products.",
      "csv_display_name": "InfoScale\u2122 Licensing Operator",
      "csv_metadata_description": "InfoScale\u2122 Licensing Operator manages the lifecycle of the Licensing for InfoScale\u2122 cluster",
      "csv_name": "infoscale-licensing-operator.v8.0.200",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": false,
          "type": "OwnNamespace"
        },
        {
          "supported": false,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-12-01T05:08:43.204000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.10",
      "organization": "certified-operators",
      "package": "infoscale-licensing-operator",
      "provided_apis": [
        {
          "group": "vlic.veritas.com",
          "kind": "License",
          "plural": "licenses",
          "version": "v1"
        }
      ],
      "provider": "Veritas Technologies LLC",
      "related_images": [
        {
          "digest": "sha256:38c01336cb39b7a731efbe74583d4823acb2b93858326d18e6a26c2186dc3370",
          "image": "registry.connect.redhat.com/veritas-technologies/infoscale-licensing-operator@sha256:38c01336cb39b7a731efbe74583d4823acb2b93858326d18e6a26c2186dc3370",
          "name": "manager"
        },
        {
          "digest": "sha256:38c01336cb39b7a731efbe74583d4823acb2b93858326d18e6a26c2186dc3370",
          "image": "registry.connect.redhat.com/veritas-technologies/infoscale-licensing-operator@sha256:38c01336cb39b7a731efbe74583d4823acb2b93858326d18e6a26c2186dc3370",
          "name": "pre-manager"
        },
        {
          "digest": "sha256:38c01336cb39b7a731efbe74583d4823acb2b93858326d18e6a26c2186dc3370",
          "image": "registry.connect.redhat.com/veritas-technologies/infoscale-licensing-operator@sha256:38c01336cb39b7a731efbe74583d4823acb2b93858326d18e6a26c2186dc3370",
          "name": "infoscale-licensing-operator-38c01336cb39b7a731efbe74583d4823acb2b93858326d18e6a26c2186dc3370-annotation"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.10",
      "version": "8.0.200",
      "version_original": "8.0.200"
    },
    {
      "_id": "638836f56f67ca6711733ba6",
      "alm_examples": [
        {
          "api_version": "vlic.veritas.com/v1",
          "kind": "License",
          "metadata": {
            "name": "license-dev"
          },
          "spec": {
            "licenseEdition": "Developer"
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/veritas-technologies/infoscale-licensing-operator-bundle@sha256:6ed2ec1be3156926f7ac384fc6285058e97b6c4140437b58d616c7bc2dea3274",
      "bundle_path_digest": "sha256:6ed2ec1be3156926f7ac384fc6285058e97b6c4140437b58d616c7bc2dea3274",
      "capabilities": [
        "Seamless Upgrades"
      ],
      "channel_name": "stable",
      "creation_date": "2022-12-01T05:09:09.126000+00:00",
      "csv_description": "InfoScale\u2122 Licensing Operator provides license to use Veritas Products.",
      "csv_display_name": "InfoScale\u2122 Licensing Operator",
      "csv_metadata_description": "InfoScale\u2122 Licensing Operator manages the lifecycle of the Licensing for InfoScale\u2122 cluster",
      "csv_name": "infoscale-licensing-operator.v8.0.200",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": false,
          "type": "OwnNamespace"
        },
        {
          "supported": false,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-12-01T05:09:09.126000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.9",
      "organization": "certified-operators",
      "package": "infoscale-licensing-operator",
      "provided_apis": [
        {
          "group": "vlic.veritas.com",
          "kind": "License",
          "plural": "licenses",
          "version": "v1"
        }
      ],
      "provider": "Veritas Technologies LLC",
      "related_images": [
        {
          "digest": "sha256:38c01336cb39b7a731efbe74583d4823acb2b93858326d18e6a26c2186dc3370",
          "image": "registry.connect.redhat.com/veritas-technologies/infoscale-licensing-operator@sha256:38c01336cb39b7a731efbe74583d4823acb2b93858326d18e6a26c2186dc3370",
          "name": "manager"
        },
        {
          "digest": "sha256:38c01336cb39b7a731efbe74583d4823acb2b93858326d18e6a26c2186dc3370",
          "image": "registry.connect.redhat.com/veritas-technologies/infoscale-licensing-operator@sha256:38c01336cb39b7a731efbe74583d4823acb2b93858326d18e6a26c2186dc3370",
          "name": "pre-manager"
        },
        {
          "digest": "sha256:38c01336cb39b7a731efbe74583d4823acb2b93858326d18e6a26c2186dc3370",
          "image": "registry.connect.redhat.com/veritas-technologies/infoscale-licensing-operator@sha256:38c01336cb39b7a731efbe74583d4823acb2b93858326d18e6a26c2186dc3370",
          "name": "infoscale-licensing-operator-38c01336cb39b7a731efbe74583d4823acb2b93858326d18e6a26c2186dc3370-annotation"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.9",
      "version": "8.0.200",
      "version_original": "8.0.200"
    },
    {
      "_id": "638837057476f7ea6ac674a6",
      "alm_examples": [
        {
          "api_version": "vlic.veritas.com/v1",
          "kind": "License",
          "metadata": {
            "name": "license-dev"
          },
          "spec": {
            "licenseEdition": "Developer"
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/veritas-technologies/infoscale-licensing-operator-bundle@sha256:6ed2ec1be3156926f7ac384fc6285058e97b6c4140437b58d616c7bc2dea3274",
      "bundle_path_digest": "sha256:6ed2ec1be3156926f7ac384fc6285058e97b6c4140437b58d616c7bc2dea3274",
      "capabilities": [
        "Seamless Upgrades"
      ],
      "channel_name": "stable",
      "creation_date": "2022-12-01T05:09:25.654000+00:00",
      "csv_description": "InfoScale\u2122 Licensing Operator provides license to use Veritas Products.",
      "csv_display_name": "InfoScale\u2122 Licensing Operator",
      "csv_metadata_description": "InfoScale\u2122 Licensing Operator manages the lifecycle of the Licensing for InfoScale\u2122 cluster",
      "csv_name": "infoscale-licensing-operator.v8.0.200",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": false,
          "type": "OwnNamespace"
        },
        {
          "supported": false,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-12-01T05:09:25.654000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.11",
      "organization": "certified-operators",
      "package": "infoscale-licensing-operator",
      "provided_apis": [
        {
          "group": "vlic.veritas.com",
          "kind": "License",
          "version": "v1"
        }
      ],
      "provider": "Veritas Technologies LLC",
      "related_images": [
        {
          "digest": "sha256:38c01336cb39b7a731efbe74583d4823acb2b93858326d18e6a26c2186dc3370",
          "image": "registry.connect.redhat.com/veritas-technologies/infoscale-licensing-operator@sha256:38c01336cb39b7a731efbe74583d4823acb2b93858326d18e6a26c2186dc3370",
          "name": "manager"
        },
        {
          "digest": "sha256:38c01336cb39b7a731efbe74583d4823acb2b93858326d18e6a26c2186dc3370",
          "image": "registry.connect.redhat.com/veritas-technologies/infoscale-licensing-operator@sha256:38c01336cb39b7a731efbe74583d4823acb2b93858326d18e6a26c2186dc3370",
          "name": "pre-manager"
        },
        {
          "digest": "sha256:38c01336cb39b7a731efbe74583d4823acb2b93858326d18e6a26c2186dc3370",
          "image": "registry.connect.redhat.com/veritas-technologies/infoscale-licensing-operator@sha256:38c01336cb39b7a731efbe74583d4823acb2b93858326d18e6a26c2186dc3370",
          "name": "infoscale-licensing-operator-38c01336cb39b7a731efbe74583d4823acb2b93858326d18e6a26c2186dc3370-annotation"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.11",
      "version": "8.0.200",
      "version_original": "8.0.200"
    },
    {
      "_id": "63883a08b857338562fc7cec",
      "alm_examples": [
        {
          "api_version": "infoscale.veritas.com/v1",
          "kind": "InfoScaleCluster",
          "metadata": {
            "name": "infoscalecluster-dev"
          },
          "spec": {
            "clusterInfo": [
              {},
              {}
            ],
            "version": "8.0.200"
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/veritas-technologies/infoscale-sds-operator-bundle@sha256:8376d68c646fc665a714d1e702543163ed3268dc3a7fc61b8e9bdd57d65a7b27",
      "bundle_path_digest": "sha256:8376d68c646fc665a714d1e702543163ed3268dc3a7fc61b8e9bdd57d65a7b27",
      "capabilities": [
        "Seamless Upgrades"
      ],
      "channel_name": "stable",
      "creation_date": "2022-12-01T05:22:16.566000+00:00",
      "csv_description": "## InfoScale\u2122 SDS Operator\n\nInfoScale\u2122 SDS Operator manages the lifecycle of the InfoScale\u2122 cluster\n\n## Overview\n\n- Veritas InfoScale\u2122 delivers Infrastructure resiliency and persistent storage for your critical containerized applications for OpenShift\u00ae and Kubernetes Native deployments\n- Engineered to support stateful workloads generated for mission-critical containerized applications.\n\n---\n\n## Data Services & Benefits\n\n**1. Software-Defined Persistent Storage Volumes:** Enables customers to support multiple container application requirements leveraging existing SAN or DAS storage\n\n**2. CSI API Driver:** Facilitates static and dynamic provisioning for applications with RWX, RWO and ROX access modes\n\n**3. Life Cycle Management:** Enables automated deployment, configuration and upgrades of InfoScale Software-defined container images. Certified and Integrated with Red Hat OpenShift for a single-click deployment\n\n**4. Availability:** Provides scaling, mounting and/or movement of InfoScale persistent storage volumes on cluster nodes with minimal disruption\n\n**5. Data Integrity:** Prevents data corruption by allowing only the active cluster nodes to write to the volume. The I/O fencing feature recovers from cluster disruptions quickly by ensuring that application pods are moved to another node to continue normal operations\n\n**6. Point-in-Time Data Copies:** Create snapshots of Persistent Volumes for backup products, data analytics or forensic discovery and analysis\n\n**7. Disaster Recovery (DR) Tech Preview:** This DR feature provides the ability to test and validate disaster recovery capabilities by migrating Kubernetes cluster metadata and application components to peer cluster in case of a local or remote disaster\n\n---\n\n## Pre-requisites\n\n- [Please refer to pre-requisite section from official documentation](https://www.veritas.com/support/en_US/doc/151215298-151215302-0)\n\n\n## InfoScale Cluster custom resource\n\n```\nkind: InfoScaleCluster\nmetadata:\n  name:  < Infoscale Cluster Name >\n\nspec:\n  version: \"8.0.200\"\n\n  clusterInfo:\n  - nodeName: <Name of the first node>\n  ip:\n  - <Optional - First IP address of the first node >\n  - <Optional - Second IP address of the first node>\n  excludeDevice:\n  - <Optional - Device path of the disk on the node that you want to exclude from Infoscale disk group.>\n\n  - nodeName: <Name of the second node>\n  ip:\n  - <Optional - First IP address of the second node >\n  - <Optional - Second IP address of the second node>\n  excludeDevice:\n  - <Optional - Device path of the disk on the node that you want to exclude from Infoscale disk group.>\n\n  # You can add up to 16 nodes.\n\n  customImageRegistry: < Optional \u2013 Registry for Infoscale Container images>\n```\n\n#### Note\n\nYou can specify up to 16 worker nodes in CR. Although cluster configuration is allowed even with one Network Interface Card,\nVeritas recommends a minimum of two physical links for performance and High Availability (HA). Number of links for each network link must be same on all\nnodes. Optionally, you can enter node level IP addresses. If IP addresses are not provided, IP addresses of OpenShift cluster nodes are used.\n",
      "csv_display_name": "InfoScale\u2122 SDS Operator",
      "csv_metadata_description": "InfoScale\u2122 SDS Operator manages the lifecycle of the InfoScale\u2122 cluster",
      "csv_name": "infoscale-sds-operator.v8.0.200",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": false,
          "type": "OwnNamespace"
        },
        {
          "supported": false,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-12-02T15:37:42.821000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.12",
      "organization": "certified-operators",
      "package": "infoscale-sds-operator",
      "provided_apis": [
        {
          "group": "infoscale.veritas.com",
          "kind": "InfoScaleCluster",
          "version": "v1"
        },
        {
          "group": "infoscale.veritas.com",
          "kind": "InfoScaleStoragePool",
          "version": "v1"
        }
      ],
      "provider": "Veritas Technologies LLC",
      "related_images": [
        {
          "digest": "sha256:26573b98743245c0b607abbe0d981abce6ce9fb1c5c0cd9fe735351b7136f4e8",
          "image": "registry.connect.redhat.com/veritas-technologies/infoscale-sds-operator@sha256:26573b98743245c0b607abbe0d981abce6ce9fb1c5c0cd9fe735351b7136f4e8",
          "name": "manager"
        },
        {
          "digest": "sha256:26573b98743245c0b607abbe0d981abce6ce9fb1c5c0cd9fe735351b7136f4e8",
          "image": "registry.connect.redhat.com/veritas-technologies/infoscale-sds-operator@sha256:26573b98743245c0b607abbe0d981abce6ce9fb1c5c0cd9fe735351b7136f4e8",
          "name": "pre-manager"
        },
        {
          "digest": "sha256:26573b98743245c0b607abbe0d981abce6ce9fb1c5c0cd9fe735351b7136f4e8",
          "image": "registry.connect.redhat.com/veritas-technologies/infoscale-sds-operator@sha256:26573b98743245c0b607abbe0d981abce6ce9fb1c5c0cd9fe735351b7136f4e8",
          "name": "infoscale-sds-operator-26573b98743245c0b607abbe0d981abce6ce9fb1c5c0cd9fe735351b7136f4e8-annotation"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.12",
      "version": "8.0.200",
      "version_original": "8.0.200"
    },
    {
      "_id": "63883a4dac1d5bea0b4cf2f5",
      "alm_examples": [
        {
          "api_version": "infoscale.veritas.com/v1",
          "kind": "InfoScaleCluster",
          "metadata": {
            "name": "infoscalecluster-dev"
          },
          "spec": {
            "clusterInfo": [
              {},
              {}
            ],
            "version": "8.0.200"
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/veritas-technologies/infoscale-sds-operator-bundle@sha256:8376d68c646fc665a714d1e702543163ed3268dc3a7fc61b8e9bdd57d65a7b27",
      "bundle_path_digest": "sha256:8376d68c646fc665a714d1e702543163ed3268dc3a7fc61b8e9bdd57d65a7b27",
      "capabilities": [
        "Seamless Upgrades"
      ],
      "channel_name": "stable",
      "creation_date": "2022-12-01T05:23:25.508000+00:00",
      "csv_description": "## InfoScale\u2122 SDS Operator\n\nInfoScale\u2122 SDS Operator manages the lifecycle of the InfoScale\u2122 cluster\n\n## Overview\n\n- Veritas InfoScale\u2122 delivers Infrastructure resiliency and persistent storage for your critical containerized applications for OpenShift\u00ae and Kubernetes Native deployments\n- Engineered to support stateful workloads generated for mission-critical containerized applications.\n\n---\n\n## Data Services & Benefits\n\n**1. Software-Defined Persistent Storage Volumes:** Enables customers to support multiple container application requirements leveraging existing SAN or DAS storage\n\n**2. CSI API Driver:** Facilitates static and dynamic provisioning for applications with RWX, RWO and ROX access modes\n\n**3. Life Cycle Management:** Enables automated deployment, configuration and upgrades of InfoScale Software-defined container images. Certified and Integrated with Red Hat OpenShift for a single-click deployment\n\n**4. Availability:** Provides scaling, mounting and/or movement of InfoScale persistent storage volumes on cluster nodes with minimal disruption\n\n**5. Data Integrity:** Prevents data corruption by allowing only the active cluster nodes to write to the volume. The I/O fencing feature recovers from cluster disruptions quickly by ensuring that application pods are moved to another node to continue normal operations\n\n**6. Point-in-Time Data Copies:** Create snapshots of Persistent Volumes for backup products, data analytics or forensic discovery and analysis\n\n**7. Disaster Recovery (DR) Tech Preview:** This DR feature provides the ability to test and validate disaster recovery capabilities by migrating Kubernetes cluster metadata and application components to peer cluster in case of a local or remote disaster\n\n---\n\n## Pre-requisites\n\n- [Please refer to pre-requisite section from official documentation](https://www.veritas.com/support/en_US/doc/151215298-151215302-0)\n\n\n## InfoScale Cluster custom resource\n\n```\nkind: InfoScaleCluster\nmetadata:\n  name:  < Infoscale Cluster Name >\n\nspec:\n  version: \"8.0.200\"\n\n  clusterInfo:\n  - nodeName: <Name of the first node>\n  ip:\n  - <Optional - First IP address of the first node >\n  - <Optional - Second IP address of the first node>\n  excludeDevice:\n  - <Optional - Device path of the disk on the node that you want to exclude from Infoscale disk group.>\n\n  - nodeName: <Name of the second node>\n  ip:\n  - <Optional - First IP address of the second node >\n  - <Optional - Second IP address of the second node>\n  excludeDevice:\n  - <Optional - Device path of the disk on the node that you want to exclude from Infoscale disk group.>\n\n  # You can add up to 16 nodes.\n\n  customImageRegistry: < Optional \u2013 Registry for Infoscale Container images>\n```\n\n#### Note\n\nYou can specify up to 16 worker nodes in CR. Although cluster configuration is allowed even with one Network Interface Card,\nVeritas recommends a minimum of two physical links for performance and High Availability (HA). Number of links for each network link must be same on all\nnodes. Optionally, you can enter node level IP addresses. If IP addresses are not provided, IP addresses of OpenShift cluster nodes are used.\n",
      "csv_display_name": "InfoScale\u2122 SDS Operator",
      "csv_metadata_description": "InfoScale\u2122 SDS Operator manages the lifecycle of the InfoScale\u2122 cluster",
      "csv_name": "infoscale-sds-operator.v8.0.200",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": false,
          "type": "OwnNamespace"
        },
        {
          "supported": false,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-12-02T15:38:58.018000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.10",
      "organization": "certified-operators",
      "package": "infoscale-sds-operator",
      "provided_apis": [
        {
          "group": "infoscale.veritas.com",
          "kind": "InfoScaleStoragePool",
          "plural": "infoscalestoragepools",
          "version": "v1"
        },
        {
          "group": "infoscale.veritas.com",
          "kind": "InfoScaleCluster",
          "plural": "infoscaleclusters",
          "version": "v1"
        }
      ],
      "provider": "Veritas Technologies LLC",
      "related_images": [
        {
          "digest": "sha256:26573b98743245c0b607abbe0d981abce6ce9fb1c5c0cd9fe735351b7136f4e8",
          "image": "registry.connect.redhat.com/veritas-technologies/infoscale-sds-operator@sha256:26573b98743245c0b607abbe0d981abce6ce9fb1c5c0cd9fe735351b7136f4e8",
          "name": "manager"
        },
        {
          "digest": "sha256:26573b98743245c0b607abbe0d981abce6ce9fb1c5c0cd9fe735351b7136f4e8",
          "image": "registry.connect.redhat.com/veritas-technologies/infoscale-sds-operator@sha256:26573b98743245c0b607abbe0d981abce6ce9fb1c5c0cd9fe735351b7136f4e8",
          "name": "pre-manager"
        },
        {
          "digest": "sha256:26573b98743245c0b607abbe0d981abce6ce9fb1c5c0cd9fe735351b7136f4e8",
          "image": "registry.connect.redhat.com/veritas-technologies/infoscale-sds-operator@sha256:26573b98743245c0b607abbe0d981abce6ce9fb1c5c0cd9fe735351b7136f4e8",
          "name": "infoscale-sds-operator-26573b98743245c0b607abbe0d981abce6ce9fb1c5c0cd9fe735351b7136f4e8-annotation"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.10",
      "version": "8.0.200",
      "version_original": "8.0.200"
    },
    {
      "_id": "63883a73b0e069685e162573",
      "alm_examples": [
        {
          "api_version": "infoscale.veritas.com/v1",
          "kind": "InfoScaleCluster",
          "metadata": {
            "name": "infoscalecluster-dev"
          },
          "spec": {
            "clusterInfo": [
              {},
              {}
            ],
            "version": "8.0.200"
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/veritas-technologies/infoscale-sds-operator-bundle@sha256:8376d68c646fc665a714d1e702543163ed3268dc3a7fc61b8e9bdd57d65a7b27",
      "bundle_path_digest": "sha256:8376d68c646fc665a714d1e702543163ed3268dc3a7fc61b8e9bdd57d65a7b27",
      "capabilities": [
        "Seamless Upgrades"
      ],
      "channel_name": "stable",
      "creation_date": "2022-12-01T05:24:03.755000+00:00",
      "csv_description": "## InfoScale\u2122 SDS Operator\n\nInfoScale\u2122 SDS Operator manages the lifecycle of the InfoScale\u2122 cluster\n\n## Overview\n\n- Veritas InfoScale\u2122 delivers Infrastructure resiliency and persistent storage for your critical containerized applications for OpenShift\u00ae and Kubernetes Native deployments\n- Engineered to support stateful workloads generated for mission-critical containerized applications.\n\n---\n\n## Data Services & Benefits\n\n**1. Software-Defined Persistent Storage Volumes:** Enables customers to support multiple container application requirements leveraging existing SAN or DAS storage\n\n**2. CSI API Driver:** Facilitates static and dynamic provisioning for applications with RWX, RWO and ROX access modes\n\n**3. Life Cycle Management:** Enables automated deployment, configuration and upgrades of InfoScale Software-defined container images. Certified and Integrated with Red Hat OpenShift for a single-click deployment\n\n**4. Availability:** Provides scaling, mounting and/or movement of InfoScale persistent storage volumes on cluster nodes with minimal disruption\n\n**5. Data Integrity:** Prevents data corruption by allowing only the active cluster nodes to write to the volume. The I/O fencing feature recovers from cluster disruptions quickly by ensuring that application pods are moved to another node to continue normal operations\n\n**6. Point-in-Time Data Copies:** Create snapshots of Persistent Volumes for backup products, data analytics or forensic discovery and analysis\n\n**7. Disaster Recovery (DR) Tech Preview:** This DR feature provides the ability to test and validate disaster recovery capabilities by migrating Kubernetes cluster metadata and application components to peer cluster in case of a local or remote disaster\n\n---\n\n## Pre-requisites\n\n- [Please refer to pre-requisite section from official documentation](https://www.veritas.com/support/en_US/doc/151215298-151215302-0)\n\n\n## InfoScale Cluster custom resource\n\n```\nkind: InfoScaleCluster\nmetadata:\n  name:  < Infoscale Cluster Name >\n\nspec:\n  version: \"8.0.200\"\n\n  clusterInfo:\n  - nodeName: <Name of the first node>\n  ip:\n  - <Optional - First IP address of the first node >\n  - <Optional - Second IP address of the first node>\n  excludeDevice:\n  - <Optional - Device path of the disk on the node that you want to exclude from Infoscale disk group.>\n\n  - nodeName: <Name of the second node>\n  ip:\n  - <Optional - First IP address of the second node >\n  - <Optional - Second IP address of the second node>\n  excludeDevice:\n  - <Optional - Device path of the disk on the node that you want to exclude from Infoscale disk group.>\n\n  # You can add up to 16 nodes.\n\n  customImageRegistry: < Optional \u2013 Registry for Infoscale Container images>\n```\n\n#### Note\n\nYou can specify up to 16 worker nodes in CR. Although cluster configuration is allowed even with one Network Interface Card,\nVeritas recommends a minimum of two physical links for performance and High Availability (HA). Number of links for each network link must be same on all\nnodes. Optionally, you can enter node level IP addresses. If IP addresses are not provided, IP addresses of OpenShift cluster nodes are used.\n",
      "csv_display_name": "InfoScale\u2122 SDS Operator",
      "csv_metadata_description": "InfoScale\u2122 SDS Operator manages the lifecycle of the InfoScale\u2122 cluster",
      "csv_name": "infoscale-sds-operator.v8.0.200",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": false,
          "type": "OwnNamespace"
        },
        {
          "supported": false,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-12-02T15:34:56.017000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.9",
      "organization": "certified-operators",
      "package": "infoscale-sds-operator",
      "provided_apis": [
        {
          "group": "infoscale.veritas.com",
          "kind": "InfoScaleCluster",
          "plural": "infoscaleclusters",
          "version": "v1"
        },
        {
          "group": "infoscale.veritas.com",
          "kind": "InfoScaleStoragePool",
          "plural": "infoscalestoragepools",
          "version": "v1"
        }
      ],
      "provider": "Veritas Technologies LLC",
      "related_images": [
        {
          "digest": "sha256:26573b98743245c0b607abbe0d981abce6ce9fb1c5c0cd9fe735351b7136f4e8",
          "image": "registry.connect.redhat.com/veritas-technologies/infoscale-sds-operator@sha256:26573b98743245c0b607abbe0d981abce6ce9fb1c5c0cd9fe735351b7136f4e8",
          "name": "manager"
        },
        {
          "digest": "sha256:26573b98743245c0b607abbe0d981abce6ce9fb1c5c0cd9fe735351b7136f4e8",
          "image": "registry.connect.redhat.com/veritas-technologies/infoscale-sds-operator@sha256:26573b98743245c0b607abbe0d981abce6ce9fb1c5c0cd9fe735351b7136f4e8",
          "name": "pre-manager"
        },
        {
          "digest": "sha256:26573b98743245c0b607abbe0d981abce6ce9fb1c5c0cd9fe735351b7136f4e8",
          "image": "registry.connect.redhat.com/veritas-technologies/infoscale-sds-operator@sha256:26573b98743245c0b607abbe0d981abce6ce9fb1c5c0cd9fe735351b7136f4e8",
          "name": "infoscale-sds-operator-26573b98743245c0b607abbe0d981abce6ce9fb1c5c0cd9fe735351b7136f4e8-annotation"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.9",
      "version": "8.0.200",
      "version_original": "8.0.200"
    },
    {
      "_id": "638912deb5dcf804fdcc0e85",
      "alm_examples": [
        {
          "api_version": "sonatype.com/v1alpha1",
          "kind": "NexusRepo",
          "metadata": {
            "name": "example-nexusrepo"
          },
          "spec": {
            "config": {
              "enabled": false,
              "mountPath": "/sonatype-nexus-conf"
            },
            "deployment": {
              "annotations": {},
              "postStart": {},
              "preStart": {},
              "terminationGracePeriodSeconds": 120
            },
            "deploymentStrategy": {
              "type": "Recreate"
            },
            "ingress": {
              "annotations": {},
              "enabled": false,
              "path": "/",
              "tls": {
                "enabled": true,
                "secretName": "nexus-tls"
              }
            },
            "nexus": {
              "dockerPort": 5003,
              "env": [
                {
                  "name": "INSTALL4J_ADD_VM_PARAMS",
                  "value": "-Xms2703M -Xmx2703M -XX:MaxDirectMemorySize=2703M -XX:+UnlockExperimentalVMOptions -XX:+UseCGroupMemoryLimitForHeap"
                },
                {
                  "name": "NEXUS_SECURITY_RANDOMPASSWORD",
                  "value": "false"
                }
              ],
              "hostAliases": [],
              "imageName": "registry.connect.redhat.com/sonatype/nexus-repository-manager@sha256:0a537bf191cc09fec0787300c7b4de7713afcb00f62774b7a4e5578be6b6ff0e",
              "imagePullPolicy": "IfNotPresent",
              "imagePullSecret": "",
              "livenessProbe": {
                "failureThreshold": 6,
                "initialDelaySeconds": 30,
                "path": "/",
                "periodSeconds": 30
              },
              "nexusPort": 8081,
              "podAnnotations": {},
              "readinessProbe": {
                "failureThreshold": 6,
                "initialDelaySeconds": 30,
                "path": "/",
                "periodSeconds": 30
              },
              "resources": {},
              "securityContext": {},
              "service": {
                "type": "NodePort"
              }
            },
            "nexusProxyRoute": {
              "enabled": false
            },
            "persistence": {
              "accessMode": "ReadWriteOnce",
              "enabled": true,
              "storageSize": "8Gi"
            },
            "replicaCount": 1,
            "route": {
              "enabled": false,
              "name": "docker",
              "portName": "docker"
            },
            "secret": {
              "enabled": false,
              "mountPath": "/etc/secret-volume",
              "readOnly": true
            },
            "service": {
              "annotations": {},
              "enabled": false,
              "labels": {},
              "ports": [
                {
                  "name": "nexus-service",
                  "port": 80,
                  "targetPort": 80
                }
              ]
            },
            "statefulset": {
              "enabled": false
            },
            "tolerations": []
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/sonatype/nxrm-operator-bundle@sha256:4a56fb008bf13835a8575c23c31c0804dc7c6c12d2bcb3dea1035344bd3f61fa",
      "bundle_path_digest": "sha256:4a56fb008bf13835a8575c23c31c0804dc7c6c12d2bcb3dea1035344bd3f61fa",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "stable",
      "creation_date": "2022-12-01T20:47:26.043000+00:00",
      "csv_description": "Nexus Repository is the central source of control to efficiently manage all binaries\nand build artifacts across your DevOps pipeline.\nThe flow of open source and third-party components into and through an organization\ncreates a complex software supply chain.\nNexus Repository delivers speed, efficiency, and quality to the governance\nand management of all dependencies, libraries, and applications for your DevOps teams.\n\n## Core Capabilities\n\n* **Dependency Management**:\n  Improves reliability with repeatable, fast access to secure dependencies\n* **Developer Productivity**:\n  Streamline developer workflows by enabling the sharing of components and applications across teams\n* **Supply Chain Performance**:\n  Improve speed-to-market and reduced build times with release advanced staging and component tagging\n* **CI/CD Integrations**:\n  Increase DevOps scalability with integrations to the most popular build and deployment tools\n\nVersion control systems and package registries do not scale when managing proprietary,\nopen source, and third-party components.\nOrganizations need a central binary and build artifact repository to manage dependencies\nacross the entire software supply chain.\n\n## Limitations\n\nHigh Availability Clustering (HA-C) is not supported for Nexus Repository Pro for OpenShift.\n\nThis operator will be released on a quarterly basis.\n\n## Controlling Automatic vs Manual Update\n\nIf you use the default configuration for the Nexus Repository Operator installation,\nplease notice that on any new operator release, the corresponding deployments are\nalso updated without user intervention, resulting in unscheduled downtime.\n\nIf you want to avoid this unscheduled downtime, we recommend installing the operator\ninto **its own namespace** with **manual approval** for updates.\n\n## Usage\n\nOnce the server instance is created by the operator and running,\nyou'll want to expose the service as you see fit:\n1. Create a Route to that service for nexus.port (8081).\n\nBy default, the Nexus Repository starts up in OSS mode until a license is installed.\n\nThe Nexus Repository can be further configured via the NexusRepo custom resource definition:\n\n| Parameter                                   | Description                         | Default                                 |\n| ------------------------------------------  | ----------------------------------  | ----------------------------------------|\n| `statefulset.enabled`                       | Use statefulset instead of deployment | `false` |\n| `deploymentStrategy.type`                   | Deployment Strategy     |  `Recreate` |\n| `nexus.env`                                 | Nexus environment variables         | `See example.` |\n| `nexus.resources`                           | Nexus resource requests and limits  | `{}`                                    |\n| `nexus.dockerPort`                          | Port to access docker               | `5003`                                  |\n| `nexus.nexusPort`                           | Internal port for Nexus service     | `8081`                                  |\n| `nexus.service.type`                        | Service for Nexus                   |`NodePort`                                |\n| `nexus.service.clusterIp`                   | Specific cluster IP when service type is cluster IP. Use None for headless service |`nil`   |\n| `nexus.securityContext`                     | Security Context |\n| `nexus.labels`                              | Service labels                      | `{}`                                    |\n| `nexus.podAnnotations`                      | Pod Annotations                     | `{}`\n| `nexus.livenessProbe.initialDelaySeconds`   | LivenessProbe initial delay         | 30                                      |\n| `nexus.livenessProbe.periodSeconds`         | Seconds between polls               | 30                                      |\n| `nexus.livenessProbe.failureThreshold`      | Number of attempts before failure   | 6                                       |\n| `nexus.livenessProbe.timeoutSeconds`        | Time in seconds after liveness probe times out    | `nil`                     |\n| `nexus.livenessProbe.path`                  | Path for LivenessProbe              | /                                       |\n| `nexus.readinessProbe.initialDelaySeconds`  | ReadinessProbe initial delay        | 30                                      |\n| `nexus.readinessProbe.periodSeconds`        | Seconds between polls               | 30                                      |\n| `nexus.readinessProbe.failureThreshold`     | Number of attempts before failure   | 6                                       |\n| `nexus.readinessProbe.timeoutSeconds`       | Time in seconds after readiness probe times out    | `nil`                    |\n| `nexus.readinessProbe.path`                 | Path for ReadinessProbe             | /                                       |\n| `nexus.hostAliases`                         | Aliases for IPs in /etc/hosts       | []                                      |\n| `ingress.enabled`                           | Create an ingress for Nexus         | `true`                                  |\n| `ingress.annotations`                       | Annotations to enhance ingress configuration  | `{}`                          |\n| `ingress.tls.enabled`                       | Enable TLS                          | `true`                                 |\n| `ingress.tls.secretName`                    | Name of the secret storing TLS cert, `false` to use the Ingress' default certificate | `nexus-tls`                             |\n| `ingress.path`                              | Path for ingress rules. GCP users should set to `/*` | `/`                    |\n| `tolerations`                               | tolerations list                    | `[]`                                    |\n| `config.enabled`                            | Enable configmap                    | `false`                                 |\n| `config.mountPath`                          | Path to mount the config            | `/sonatype-nexus-conf`                  |\n| `config.data`                               | Configmap data                      | `nil`                                   |\n| `deployment.terminationGracePeriodSeconds`  | Time to allow for clean shutdown    | 120                                     |\n| `deployment.annotations`                    | Annotations to enhance deployment configuration  | `{}`                       |\n| `deployment.initContainers`                 | Init containers to run before main containers  | `nil`                        |\n| `deployment.postStart.command`              | Command to run after starting the nexus container  | `nil`                    |\n| `deployment.preStart.command`               | Command to run before starting the nexus container  | `nil`                   |\n| `deployment.additionalContainers`           | Add additional Container         | `nil`                                      |\n| `deployment.additionalVolumes`              | Add additional Volumes           | `nil`                                      |\n| `deployment.additionalVolumeMounts`         | Add additional Volume mounts     | `nil`                                      |\n| `secret.enabled`                            | Enable secret                    | `false`                                    |\n| `secret.mountPath`                          | Path to mount the secret         | `/etc/secret-volume`                       |\n| `secret.readOnly`                           | Secret readonly state            | `true`                                     |\n| `secret.data`                               | Secret data                      | `nil`                                      |\n| `service.enabled`                           | Enable additional service        | `nil`                                      |\n| `service.name`                              | Service name                     | `nil`                                      |\n| `service.portName`                          | Service port name                | `nil`                                      |\n| `service.labels`                            | Service labels                   | `nil`                                      |\n| `service.annotations`                       | Service annotations              | `nil`                                      |\n| `service.loadBalancerSourceRanges`          | Service LoadBalancer source IP whitelist | `nil`                              |\n| `service.targetPort`                        | Service port                     | `nil`                                      |\n| `service.port`                              | Port for exposing service        | `nil`                                      |\n| `route.enabled`         | Set to true to create route for additional service | `false` |\n| `route.name`            | Name of route                                      | `docker` |\n| `route.portName`        | Target port name of service                        | `docker` |\n| `route.labels`          | Labels to be added to route                        | `{}` |\n| `route.annotations`     | Annotations to be added to route                   | `{}` |\n| `route.path`            | Host name of Route e.g jenkins.example.com         | nil |",
      "csv_display_name": "Nexus Repository Operator",
      "csv_metadata_description": "Nexus Repository is the central source of control to efficiently manage all binaries\nand build artifacts across your DevOps pipeline.",
      "csv_name": "nxrm-operator-certified.v3.43.0-3",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-12-01T20:47:26.043000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.10",
      "organization": "certified-operators",
      "package": "nxrm-operator-certified",
      "provided_apis": [
        {
          "group": "sonatype.com",
          "kind": "NexusRepo",
          "plural": "nexusrepos",
          "version": "v1alpha1"
        }
      ],
      "provider": "Sonatype",
      "related_images": [
        {
          "digest": "sha256:6c07c254e166fc21a442ddb6509384aac008ea9723fb0042006bbc824b7f3d66",
          "image": "registry.connect.redhat.com/sonatype/nxrm-operator-certified@sha256:6c07c254e166fc21a442ddb6509384aac008ea9723fb0042006bbc824b7f3d66",
          "name": "nxrm-operator-certified-6c07c254e166fc21a442ddb6509384aac008ea9723fb0042006bbc824b7f3d66-annotation"
        },
        {
          "digest": "sha256:6c07c254e166fc21a442ddb6509384aac008ea9723fb0042006bbc824b7f3d66",
          "image": "registry.connect.redhat.com/sonatype/nxrm-operator-certified@sha256:6c07c254e166fc21a442ddb6509384aac008ea9723fb0042006bbc824b7f3d66",
          "name": "nxrm-operator-certified"
        },
        {
          "digest": "sha256:0a537bf191cc09fec0787300c7b4de7713afcb00f62774b7a4e5578be6b6ff0e",
          "image": "registry.connect.redhat.com/sonatype/nexus-repository-manager@sha256:0a537bf191cc09fec0787300c7b4de7713afcb00f62774b7a4e5578be6b6ff0e",
          "name": "nexus"
        },
        {
          "digest": "sha256:0a537bf191cc09fec0787300c7b4de7713afcb00f62774b7a4e5578be6b6ff0e",
          "image": "registry.connect.redhat.com/sonatype/nexus-repository-manager@sha256:0a537bf191cc09fec0787300c7b4de7713afcb00f62774b7a4e5578be6b6ff0e",
          "name": "nexus-repository-manager-0a537bf191cc09fec0787300c7b4de7713afcb00f62774b7a4e5578be6b6ff0e-annotation"
        }
      ],
      "replaces": null,
      "skip_range": "<3.43.0-3",
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.10",
      "version": "3.43.0-3",
      "version_original": "3.43.0-3"
    },
    {
      "_id": "638913f8b85733856200a827",
      "alm_examples": [
        {
          "api_version": "sonatype.com/v1alpha1",
          "kind": "NexusRepo",
          "metadata": {
            "name": "example-nexusrepo"
          },
          "spec": {
            "config": {
              "enabled": false,
              "mountPath": "/sonatype-nexus-conf"
            },
            "deployment": {
              "annotations": {},
              "postStart": {},
              "preStart": {},
              "terminationGracePeriodSeconds": 120
            },
            "deploymentStrategy": {
              "type": "Recreate"
            },
            "ingress": {
              "annotations": {},
              "enabled": false,
              "path": "/",
              "tls": {
                "enabled": true,
                "secretName": "nexus-tls"
              }
            },
            "nexus": {
              "dockerPort": 5003,
              "env": [
                {
                  "name": "INSTALL4J_ADD_VM_PARAMS",
                  "value": "-Xms2703M -Xmx2703M -XX:MaxDirectMemorySize=2703M -XX:+UnlockExperimentalVMOptions -XX:+UseCGroupMemoryLimitForHeap"
                },
                {
                  "name": "NEXUS_SECURITY_RANDOMPASSWORD",
                  "value": "false"
                }
              ],
              "hostAliases": [],
              "imageName": "registry.connect.redhat.com/sonatype/nexus-repository-manager@sha256:0a537bf191cc09fec0787300c7b4de7713afcb00f62774b7a4e5578be6b6ff0e",
              "imagePullPolicy": "IfNotPresent",
              "imagePullSecret": "",
              "livenessProbe": {
                "failureThreshold": 6,
                "initialDelaySeconds": 30,
                "path": "/",
                "periodSeconds": 30
              },
              "nexusPort": 8081,
              "podAnnotations": {},
              "readinessProbe": {
                "failureThreshold": 6,
                "initialDelaySeconds": 30,
                "path": "/",
                "periodSeconds": 30
              },
              "resources": {},
              "securityContext": {},
              "service": {
                "type": "NodePort"
              }
            },
            "nexusProxyRoute": {
              "enabled": false
            },
            "persistence": {
              "accessMode": "ReadWriteOnce",
              "enabled": true,
              "storageSize": "8Gi"
            },
            "replicaCount": 1,
            "route": {
              "enabled": false,
              "name": "docker",
              "portName": "docker"
            },
            "secret": {
              "enabled": false,
              "mountPath": "/etc/secret-volume",
              "readOnly": true
            },
            "service": {
              "annotations": {},
              "enabled": false,
              "labels": {},
              "ports": [
                {
                  "name": "nexus-service",
                  "port": 80,
                  "targetPort": 80
                }
              ]
            },
            "statefulset": {
              "enabled": false
            },
            "tolerations": []
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/sonatype/nxrm-operator-bundle@sha256:4a56fb008bf13835a8575c23c31c0804dc7c6c12d2bcb3dea1035344bd3f61fa",
      "bundle_path_digest": "sha256:4a56fb008bf13835a8575c23c31c0804dc7c6c12d2bcb3dea1035344bd3f61fa",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "stable",
      "creation_date": "2022-12-01T20:52:08.672000+00:00",
      "csv_description": "Nexus Repository is the central source of control to efficiently manage all binaries\nand build artifacts across your DevOps pipeline.\nThe flow of open source and third-party components into and through an organization\ncreates a complex software supply chain.\nNexus Repository delivers speed, efficiency, and quality to the governance\nand management of all dependencies, libraries, and applications for your DevOps teams.\n\n## Core Capabilities\n\n* **Dependency Management**:\n  Improves reliability with repeatable, fast access to secure dependencies\n* **Developer Productivity**:\n  Streamline developer workflows by enabling the sharing of components and applications across teams\n* **Supply Chain Performance**:\n  Improve speed-to-market and reduced build times with release advanced staging and component tagging\n* **CI/CD Integrations**:\n  Increase DevOps scalability with integrations to the most popular build and deployment tools\n\nVersion control systems and package registries do not scale when managing proprietary,\nopen source, and third-party components.\nOrganizations need a central binary and build artifact repository to manage dependencies\nacross the entire software supply chain.\n\n## Limitations\n\nHigh Availability Clustering (HA-C) is not supported for Nexus Repository Pro for OpenShift.\n\nThis operator will be released on a quarterly basis.\n\n## Controlling Automatic vs Manual Update\n\nIf you use the default configuration for the Nexus Repository Operator installation,\nplease notice that on any new operator release, the corresponding deployments are\nalso updated without user intervention, resulting in unscheduled downtime.\n\nIf you want to avoid this unscheduled downtime, we recommend installing the operator\ninto **its own namespace** with **manual approval** for updates.\n\n## Usage\n\nOnce the server instance is created by the operator and running,\nyou'll want to expose the service as you see fit:\n1. Create a Route to that service for nexus.port (8081).\n\nBy default, the Nexus Repository starts up in OSS mode until a license is installed.\n\nThe Nexus Repository can be further configured via the NexusRepo custom resource definition:\n\n| Parameter                                   | Description                         | Default                                 |\n| ------------------------------------------  | ----------------------------------  | ----------------------------------------|\n| `statefulset.enabled`                       | Use statefulset instead of deployment | `false` |\n| `deploymentStrategy.type`                   | Deployment Strategy     |  `Recreate` |\n| `nexus.env`                                 | Nexus environment variables         | `See example.` |\n| `nexus.resources`                           | Nexus resource requests and limits  | `{}`                                    |\n| `nexus.dockerPort`                          | Port to access docker               | `5003`                                  |\n| `nexus.nexusPort`                           | Internal port for Nexus service     | `8081`                                  |\n| `nexus.service.type`                        | Service for Nexus                   |`NodePort`                                |\n| `nexus.service.clusterIp`                   | Specific cluster IP when service type is cluster IP. Use None for headless service |`nil`   |\n| `nexus.securityContext`                     | Security Context |\n| `nexus.labels`                              | Service labels                      | `{}`                                    |\n| `nexus.podAnnotations`                      | Pod Annotations                     | `{}`\n| `nexus.livenessProbe.initialDelaySeconds`   | LivenessProbe initial delay         | 30                                      |\n| `nexus.livenessProbe.periodSeconds`         | Seconds between polls               | 30                                      |\n| `nexus.livenessProbe.failureThreshold`      | Number of attempts before failure   | 6                                       |\n| `nexus.livenessProbe.timeoutSeconds`        | Time in seconds after liveness probe times out    | `nil`                     |\n| `nexus.livenessProbe.path`                  | Path for LivenessProbe              | /                                       |\n| `nexus.readinessProbe.initialDelaySeconds`  | ReadinessProbe initial delay        | 30                                      |\n| `nexus.readinessProbe.periodSeconds`        | Seconds between polls               | 30                                      |\n| `nexus.readinessProbe.failureThreshold`     | Number of attempts before failure   | 6                                       |\n| `nexus.readinessProbe.timeoutSeconds`       | Time in seconds after readiness probe times out    | `nil`                    |\n| `nexus.readinessProbe.path`                 | Path for ReadinessProbe             | /                                       |\n| `nexus.hostAliases`                         | Aliases for IPs in /etc/hosts       | []                                      |\n| `ingress.enabled`                           | Create an ingress for Nexus         | `true`                                  |\n| `ingress.annotations`                       | Annotations to enhance ingress configuration  | `{}`                          |\n| `ingress.tls.enabled`                       | Enable TLS                          | `true`                                 |\n| `ingress.tls.secretName`                    | Name of the secret storing TLS cert, `false` to use the Ingress' default certificate | `nexus-tls`                             |\n| `ingress.path`                              | Path for ingress rules. GCP users should set to `/*` | `/`                    |\n| `tolerations`                               | tolerations list                    | `[]`                                    |\n| `config.enabled`                            | Enable configmap                    | `false`                                 |\n| `config.mountPath`                          | Path to mount the config            | `/sonatype-nexus-conf`                  |\n| `config.data`                               | Configmap data                      | `nil`                                   |\n| `deployment.terminationGracePeriodSeconds`  | Time to allow for clean shutdown    | 120                                     |\n| `deployment.annotations`                    | Annotations to enhance deployment configuration  | `{}`                       |\n| `deployment.initContainers`                 | Init containers to run before main containers  | `nil`                        |\n| `deployment.postStart.command`              | Command to run after starting the nexus container  | `nil`                    |\n| `deployment.preStart.command`               | Command to run before starting the nexus container  | `nil`                   |\n| `deployment.additionalContainers`           | Add additional Container         | `nil`                                      |\n| `deployment.additionalVolumes`              | Add additional Volumes           | `nil`                                      |\n| `deployment.additionalVolumeMounts`         | Add additional Volume mounts     | `nil`                                      |\n| `secret.enabled`                            | Enable secret                    | `false`                                    |\n| `secret.mountPath`                          | Path to mount the secret         | `/etc/secret-volume`                       |\n| `secret.readOnly`                           | Secret readonly state            | `true`                                     |\n| `secret.data`                               | Secret data                      | `nil`                                      |\n| `service.enabled`                           | Enable additional service        | `nil`                                      |\n| `service.name`                              | Service name                     | `nil`                                      |\n| `service.portName`                          | Service port name                | `nil`                                      |\n| `service.labels`                            | Service labels                   | `nil`                                      |\n| `service.annotations`                       | Service annotations              | `nil`                                      |\n| `service.loadBalancerSourceRanges`          | Service LoadBalancer source IP whitelist | `nil`                              |\n| `service.targetPort`                        | Service port                     | `nil`                                      |\n| `service.port`                              | Port for exposing service        | `nil`                                      |\n| `route.enabled`         | Set to true to create route for additional service | `false` |\n| `route.name`            | Name of route                                      | `docker` |\n| `route.portName`        | Target port name of service                        | `docker` |\n| `route.labels`          | Labels to be added to route                        | `{}` |\n| `route.annotations`     | Annotations to be added to route                   | `{}` |\n| `route.path`            | Host name of Route e.g jenkins.example.com         | nil |",
      "csv_display_name": "Nexus Repository Operator",
      "csv_metadata_description": "Nexus Repository is the central source of control to efficiently manage all binaries\nand build artifacts across your DevOps pipeline.",
      "csv_name": "nxrm-operator-certified.v3.43.0-3",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-12-01T20:52:08.672000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.12",
      "organization": "certified-operators",
      "package": "nxrm-operator-certified",
      "provided_apis": [
        {
          "group": "sonatype.com",
          "kind": "NexusRepo",
          "version": "v1alpha1"
        }
      ],
      "provider": "Sonatype",
      "related_images": [
        {
          "digest": "sha256:6c07c254e166fc21a442ddb6509384aac008ea9723fb0042006bbc824b7f3d66",
          "image": "registry.connect.redhat.com/sonatype/nxrm-operator-certified@sha256:6c07c254e166fc21a442ddb6509384aac008ea9723fb0042006bbc824b7f3d66",
          "name": "nxrm-operator-certified-6c07c254e166fc21a442ddb6509384aac008ea9723fb0042006bbc824b7f3d66-annotation"
        },
        {
          "digest": "sha256:6c07c254e166fc21a442ddb6509384aac008ea9723fb0042006bbc824b7f3d66",
          "image": "registry.connect.redhat.com/sonatype/nxrm-operator-certified@sha256:6c07c254e166fc21a442ddb6509384aac008ea9723fb0042006bbc824b7f3d66",
          "name": "nxrm-operator-certified"
        },
        {
          "digest": "sha256:0a537bf191cc09fec0787300c7b4de7713afcb00f62774b7a4e5578be6b6ff0e",
          "image": "registry.connect.redhat.com/sonatype/nexus-repository-manager@sha256:0a537bf191cc09fec0787300c7b4de7713afcb00f62774b7a4e5578be6b6ff0e",
          "name": "nexus"
        },
        {
          "digest": "sha256:0a537bf191cc09fec0787300c7b4de7713afcb00f62774b7a4e5578be6b6ff0e",
          "image": "registry.connect.redhat.com/sonatype/nexus-repository-manager@sha256:0a537bf191cc09fec0787300c7b4de7713afcb00f62774b7a4e5578be6b6ff0e",
          "name": "nexus-repository-manager-0a537bf191cc09fec0787300c7b4de7713afcb00f62774b7a4e5578be6b6ff0e-annotation"
        }
      ],
      "replaces": null,
      "skip_range": "<3.43.0-3",
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.12",
      "version": "3.43.0-3",
      "version_original": "3.43.0-3"
    },
    {
      "_id": "638914f1ccb9ae7e6c718b8b",
      "alm_examples": [
        {
          "api_version": "sonatype.com/v1alpha1",
          "kind": "NexusRepo",
          "metadata": {
            "name": "example-nexusrepo"
          },
          "spec": {
            "config": {
              "enabled": false,
              "mountPath": "/sonatype-nexus-conf"
            },
            "deployment": {
              "annotations": {},
              "postStart": {},
              "preStart": {},
              "terminationGracePeriodSeconds": 120
            },
            "deploymentStrategy": {
              "type": "Recreate"
            },
            "ingress": {
              "annotations": {},
              "enabled": false,
              "path": "/",
              "tls": {
                "enabled": true,
                "secretName": "nexus-tls"
              }
            },
            "nexus": {
              "dockerPort": 5003,
              "env": [
                {
                  "name": "INSTALL4J_ADD_VM_PARAMS",
                  "value": "-Xms2703M -Xmx2703M -XX:MaxDirectMemorySize=2703M -XX:+UnlockExperimentalVMOptions -XX:+UseCGroupMemoryLimitForHeap"
                },
                {
                  "name": "NEXUS_SECURITY_RANDOMPASSWORD",
                  "value": "false"
                }
              ],
              "hostAliases": [],
              "imageName": "registry.connect.redhat.com/sonatype/nexus-repository-manager@sha256:0a537bf191cc09fec0787300c7b4de7713afcb00f62774b7a4e5578be6b6ff0e",
              "imagePullPolicy": "IfNotPresent",
              "imagePullSecret": "",
              "livenessProbe": {
                "failureThreshold": 6,
                "initialDelaySeconds": 30,
                "path": "/",
                "periodSeconds": 30
              },
              "nexusPort": 8081,
              "podAnnotations": {},
              "readinessProbe": {
                "failureThreshold": 6,
                "initialDelaySeconds": 30,
                "path": "/",
                "periodSeconds": 30
              },
              "resources": {},
              "securityContext": {},
              "service": {
                "type": "NodePort"
              }
            },
            "nexusProxyRoute": {
              "enabled": false
            },
            "persistence": {
              "accessMode": "ReadWriteOnce",
              "enabled": true,
              "storageSize": "8Gi"
            },
            "replicaCount": 1,
            "route": {
              "enabled": false,
              "name": "docker",
              "portName": "docker"
            },
            "secret": {
              "enabled": false,
              "mountPath": "/etc/secret-volume",
              "readOnly": true
            },
            "service": {
              "annotations": {},
              "enabled": false,
              "labels": {},
              "ports": [
                {
                  "name": "nexus-service",
                  "port": 80,
                  "targetPort": 80
                }
              ]
            },
            "statefulset": {
              "enabled": false
            },
            "tolerations": []
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/sonatype/nxrm-operator-bundle@sha256:4a56fb008bf13835a8575c23c31c0804dc7c6c12d2bcb3dea1035344bd3f61fa",
      "bundle_path_digest": "sha256:4a56fb008bf13835a8575c23c31c0804dc7c6c12d2bcb3dea1035344bd3f61fa",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "stable",
      "creation_date": "2022-12-01T20:56:17.625000+00:00",
      "csv_description": "Nexus Repository is the central source of control to efficiently manage all binaries\nand build artifacts across your DevOps pipeline.\nThe flow of open source and third-party components into and through an organization\ncreates a complex software supply chain.\nNexus Repository delivers speed, efficiency, and quality to the governance\nand management of all dependencies, libraries, and applications for your DevOps teams.\n\n## Core Capabilities\n\n* **Dependency Management**:\n  Improves reliability with repeatable, fast access to secure dependencies\n* **Developer Productivity**:\n  Streamline developer workflows by enabling the sharing of components and applications across teams\n* **Supply Chain Performance**:\n  Improve speed-to-market and reduced build times with release advanced staging and component tagging\n* **CI/CD Integrations**:\n  Increase DevOps scalability with integrations to the most popular build and deployment tools\n\nVersion control systems and package registries do not scale when managing proprietary,\nopen source, and third-party components.\nOrganizations need a central binary and build artifact repository to manage dependencies\nacross the entire software supply chain.\n\n## Limitations\n\nHigh Availability Clustering (HA-C) is not supported for Nexus Repository Pro for OpenShift.\n\nThis operator will be released on a quarterly basis.\n\n## Controlling Automatic vs Manual Update\n\nIf you use the default configuration for the Nexus Repository Operator installation,\nplease notice that on any new operator release, the corresponding deployments are\nalso updated without user intervention, resulting in unscheduled downtime.\n\nIf you want to avoid this unscheduled downtime, we recommend installing the operator\ninto **its own namespace** with **manual approval** for updates.\n\n## Usage\n\nOnce the server instance is created by the operator and running,\nyou'll want to expose the service as you see fit:\n1. Create a Route to that service for nexus.port (8081).\n\nBy default, the Nexus Repository starts up in OSS mode until a license is installed.\n\nThe Nexus Repository can be further configured via the NexusRepo custom resource definition:\n\n| Parameter                                   | Description                         | Default                                 |\n| ------------------------------------------  | ----------------------------------  | ----------------------------------------|\n| `statefulset.enabled`                       | Use statefulset instead of deployment | `false` |\n| `deploymentStrategy.type`                   | Deployment Strategy     |  `Recreate` |\n| `nexus.env`                                 | Nexus environment variables         | `See example.` |\n| `nexus.resources`                           | Nexus resource requests and limits  | `{}`                                    |\n| `nexus.dockerPort`                          | Port to access docker               | `5003`                                  |\n| `nexus.nexusPort`                           | Internal port for Nexus service     | `8081`                                  |\n| `nexus.service.type`                        | Service for Nexus                   |`NodePort`                                |\n| `nexus.service.clusterIp`                   | Specific cluster IP when service type is cluster IP. Use None for headless service |`nil`   |\n| `nexus.securityContext`                     | Security Context |\n| `nexus.labels`                              | Service labels                      | `{}`                                    |\n| `nexus.podAnnotations`                      | Pod Annotations                     | `{}`\n| `nexus.livenessProbe.initialDelaySeconds`   | LivenessProbe initial delay         | 30                                      |\n| `nexus.livenessProbe.periodSeconds`         | Seconds between polls               | 30                                      |\n| `nexus.livenessProbe.failureThreshold`      | Number of attempts before failure   | 6                                       |\n| `nexus.livenessProbe.timeoutSeconds`        | Time in seconds after liveness probe times out    | `nil`                     |\n| `nexus.livenessProbe.path`                  | Path for LivenessProbe              | /                                       |\n| `nexus.readinessProbe.initialDelaySeconds`  | ReadinessProbe initial delay        | 30                                      |\n| `nexus.readinessProbe.periodSeconds`        | Seconds between polls               | 30                                      |\n| `nexus.readinessProbe.failureThreshold`     | Number of attempts before failure   | 6                                       |\n| `nexus.readinessProbe.timeoutSeconds`       | Time in seconds after readiness probe times out    | `nil`                    |\n| `nexus.readinessProbe.path`                 | Path for ReadinessProbe             | /                                       |\n| `nexus.hostAliases`                         | Aliases for IPs in /etc/hosts       | []                                      |\n| `ingress.enabled`                           | Create an ingress for Nexus         | `true`                                  |\n| `ingress.annotations`                       | Annotations to enhance ingress configuration  | `{}`                          |\n| `ingress.tls.enabled`                       | Enable TLS                          | `true`                                 |\n| `ingress.tls.secretName`                    | Name of the secret storing TLS cert, `false` to use the Ingress' default certificate | `nexus-tls`                             |\n| `ingress.path`                              | Path for ingress rules. GCP users should set to `/*` | `/`                    |\n| `tolerations`                               | tolerations list                    | `[]`                                    |\n| `config.enabled`                            | Enable configmap                    | `false`                                 |\n| `config.mountPath`                          | Path to mount the config            | `/sonatype-nexus-conf`                  |\n| `config.data`                               | Configmap data                      | `nil`                                   |\n| `deployment.terminationGracePeriodSeconds`  | Time to allow for clean shutdown    | 120                                     |\n| `deployment.annotations`                    | Annotations to enhance deployment configuration  | `{}`                       |\n| `deployment.initContainers`                 | Init containers to run before main containers  | `nil`                        |\n| `deployment.postStart.command`              | Command to run after starting the nexus container  | `nil`                    |\n| `deployment.preStart.command`               | Command to run before starting the nexus container  | `nil`                   |\n| `deployment.additionalContainers`           | Add additional Container         | `nil`                                      |\n| `deployment.additionalVolumes`              | Add additional Volumes           | `nil`                                      |\n| `deployment.additionalVolumeMounts`         | Add additional Volume mounts     | `nil`                                      |\n| `secret.enabled`                            | Enable secret                    | `false`                                    |\n| `secret.mountPath`                          | Path to mount the secret         | `/etc/secret-volume`                       |\n| `secret.readOnly`                           | Secret readonly state            | `true`                                     |\n| `secret.data`                               | Secret data                      | `nil`                                      |\n| `service.enabled`                           | Enable additional service        | `nil`                                      |\n| `service.name`                              | Service name                     | `nil`                                      |\n| `service.portName`                          | Service port name                | `nil`                                      |\n| `service.labels`                            | Service labels                   | `nil`                                      |\n| `service.annotations`                       | Service annotations              | `nil`                                      |\n| `service.loadBalancerSourceRanges`          | Service LoadBalancer source IP whitelist | `nil`                              |\n| `service.targetPort`                        | Service port                     | `nil`                                      |\n| `service.port`                              | Port for exposing service        | `nil`                                      |\n| `route.enabled`         | Set to true to create route for additional service | `false` |\n| `route.name`            | Name of route                                      | `docker` |\n| `route.portName`        | Target port name of service                        | `docker` |\n| `route.labels`          | Labels to be added to route                        | `{}` |\n| `route.annotations`     | Annotations to be added to route                   | `{}` |\n| `route.path`            | Host name of Route e.g jenkins.example.com         | nil |",
      "csv_display_name": "Nexus Repository Operator",
      "csv_metadata_description": "Nexus Repository is the central source of control to efficiently manage all binaries\nand build artifacts across your DevOps pipeline.",
      "csv_name": "nxrm-operator-certified.v3.43.0-3",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-12-01T20:56:17.625000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.11",
      "organization": "certified-operators",
      "package": "nxrm-operator-certified",
      "provided_apis": [
        {
          "group": "sonatype.com",
          "kind": "NexusRepo",
          "version": "v1alpha1"
        }
      ],
      "provider": "Sonatype",
      "related_images": [
        {
          "digest": "sha256:6c07c254e166fc21a442ddb6509384aac008ea9723fb0042006bbc824b7f3d66",
          "image": "registry.connect.redhat.com/sonatype/nxrm-operator-certified@sha256:6c07c254e166fc21a442ddb6509384aac008ea9723fb0042006bbc824b7f3d66",
          "name": "nxrm-operator-certified-6c07c254e166fc21a442ddb6509384aac008ea9723fb0042006bbc824b7f3d66-annotation"
        },
        {
          "digest": "sha256:6c07c254e166fc21a442ddb6509384aac008ea9723fb0042006bbc824b7f3d66",
          "image": "registry.connect.redhat.com/sonatype/nxrm-operator-certified@sha256:6c07c254e166fc21a442ddb6509384aac008ea9723fb0042006bbc824b7f3d66",
          "name": "nxrm-operator-certified"
        },
        {
          "digest": "sha256:0a537bf191cc09fec0787300c7b4de7713afcb00f62774b7a4e5578be6b6ff0e",
          "image": "registry.connect.redhat.com/sonatype/nexus-repository-manager@sha256:0a537bf191cc09fec0787300c7b4de7713afcb00f62774b7a4e5578be6b6ff0e",
          "name": "nexus"
        },
        {
          "digest": "sha256:0a537bf191cc09fec0787300c7b4de7713afcb00f62774b7a4e5578be6b6ff0e",
          "image": "registry.connect.redhat.com/sonatype/nexus-repository-manager@sha256:0a537bf191cc09fec0787300c7b4de7713afcb00f62774b7a4e5578be6b6ff0e",
          "name": "nexus-repository-manager-0a537bf191cc09fec0787300c7b4de7713afcb00f62774b7a4e5578be6b6ff0e-annotation"
        }
      ],
      "replaces": null,
      "skip_range": "<3.43.0-3",
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.11",
      "version": "3.43.0-3",
      "version_original": "3.43.0-3"
    },
    {
      "_id": "63892a1ba9e0e29d92ff2fed",
      "alm_examples": [
        {
          "api_version": "infoscale.veritas.com/v1",
          "kind": "InfoScaleCluster",
          "metadata": {
            "name": "infoscalecluster-dev"
          },
          "spec": {
            "clusterInfo": [
              {},
              {}
            ],
            "version": "8.0.200"
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/veritas-technologies/infoscale-sds-operator-bundle@sha256:8376d68c646fc665a714d1e702543163ed3268dc3a7fc61b8e9bdd57d65a7b27",
      "bundle_path_digest": "sha256:8376d68c646fc665a714d1e702543163ed3268dc3a7fc61b8e9bdd57d65a7b27",
      "capabilities": [
        "Seamless Upgrades"
      ],
      "channel_name": "stable",
      "creation_date": "2022-12-01T22:26:35.103000+00:00",
      "csv_description": "## InfoScale\u2122 SDS Operator\n\nInfoScale\u2122 SDS Operator manages the lifecycle of the InfoScale\u2122 cluster\n\n## Overview\n\n- Veritas InfoScale\u2122 delivers Infrastructure resiliency and persistent storage for your critical containerized applications for OpenShift\u00ae and Kubernetes Native deployments\n- Engineered to support stateful workloads generated for mission-critical containerized applications.\n\n---\n\n## Data Services & Benefits\n\n**1. Software-Defined Persistent Storage Volumes:** Enables customers to support multiple container application requirements leveraging existing SAN or DAS storage\n\n**2. CSI API Driver:** Facilitates static and dynamic provisioning for applications with RWX, RWO and ROX access modes\n\n**3. Life Cycle Management:** Enables automated deployment, configuration and upgrades of InfoScale Software-defined container images. Certified and Integrated with Red Hat OpenShift for a single-click deployment\n\n**4. Availability:** Provides scaling, mounting and/or movement of InfoScale persistent storage volumes on cluster nodes with minimal disruption\n\n**5. Data Integrity:** Prevents data corruption by allowing only the active cluster nodes to write to the volume. The I/O fencing feature recovers from cluster disruptions quickly by ensuring that application pods are moved to another node to continue normal operations\n\n**6. Point-in-Time Data Copies:** Create snapshots of Persistent Volumes for backup products, data analytics or forensic discovery and analysis\n\n**7. Disaster Recovery (DR) Tech Preview:** This DR feature provides the ability to test and validate disaster recovery capabilities by migrating Kubernetes cluster metadata and application components to peer cluster in case of a local or remote disaster\n\n---\n\n## Pre-requisites\n\n- [Please refer to pre-requisite section from official documentation](https://www.veritas.com/support/en_US/doc/151215298-151215302-0)\n\n\n## InfoScale Cluster custom resource\n\n```\nkind: InfoScaleCluster\nmetadata:\n  name:  < Infoscale Cluster Name >\n\nspec:\n  version: \"8.0.200\"\n\n  clusterInfo:\n  - nodeName: <Name of the first node>\n  ip:\n  - <Optional - First IP address of the first node >\n  - <Optional - Second IP address of the first node>\n  excludeDevice:\n  - <Optional - Device path of the disk on the node that you want to exclude from Infoscale disk group.>\n\n  - nodeName: <Name of the second node>\n  ip:\n  - <Optional - First IP address of the second node >\n  - <Optional - Second IP address of the second node>\n  excludeDevice:\n  - <Optional - Device path of the disk on the node that you want to exclude from Infoscale disk group.>\n\n  # You can add up to 16 nodes.\n\n  customImageRegistry: < Optional \u2013 Registry for Infoscale Container images>\n```\n\n#### Note\n\nYou can specify up to 16 worker nodes in CR. Although cluster configuration is allowed even with one Network Interface Card,\nVeritas recommends a minimum of two physical links for performance and High Availability (HA). Number of links for each network link must be same on all\nnodes. Optionally, you can enter node level IP addresses. If IP addresses are not provided, IP addresses of OpenShift cluster nodes are used.\n",
      "csv_display_name": "InfoScale\u2122 SDS Operator",
      "csv_metadata_description": "InfoScale\u2122 SDS Operator manages the lifecycle of the InfoScale\u2122 cluster",
      "csv_name": "infoscale-sds-operator.v8.0.200",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": false,
          "type": "OwnNamespace"
        },
        {
          "supported": false,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-12-02T15:38:49.451000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.11",
      "organization": "certified-operators",
      "package": "infoscale-sds-operator",
      "provided_apis": [
        {
          "group": "infoscale.veritas.com",
          "kind": "InfoScaleCluster",
          "version": "v1"
        },
        {
          "group": "infoscale.veritas.com",
          "kind": "InfoScaleStoragePool",
          "version": "v1"
        }
      ],
      "provider": "Veritas Technologies LLC",
      "related_images": [
        {
          "digest": "sha256:26573b98743245c0b607abbe0d981abce6ce9fb1c5c0cd9fe735351b7136f4e8",
          "image": "registry.connect.redhat.com/veritas-technologies/infoscale-sds-operator@sha256:26573b98743245c0b607abbe0d981abce6ce9fb1c5c0cd9fe735351b7136f4e8",
          "name": "manager"
        },
        {
          "digest": "sha256:26573b98743245c0b607abbe0d981abce6ce9fb1c5c0cd9fe735351b7136f4e8",
          "image": "registry.connect.redhat.com/veritas-technologies/infoscale-sds-operator@sha256:26573b98743245c0b607abbe0d981abce6ce9fb1c5c0cd9fe735351b7136f4e8",
          "name": "pre-manager"
        },
        {
          "digest": "sha256:26573b98743245c0b607abbe0d981abce6ce9fb1c5c0cd9fe735351b7136f4e8",
          "image": "registry.connect.redhat.com/veritas-technologies/infoscale-sds-operator@sha256:26573b98743245c0b607abbe0d981abce6ce9fb1c5c0cd9fe735351b7136f4e8",
          "name": "infoscale-sds-operator-26573b98743245c0b607abbe0d981abce6ce9fb1c5c0cd9fe735351b7136f4e8-annotation"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.11",
      "version": "8.0.200",
      "version_original": "8.0.200"
    },
    {
      "_id": "638934b83df2cad99bd60206",
      "alm_examples": [
        {
          "api_version": "enterprise.splunk.com/v4",
          "kind": "ClusterManager",
          "metadata": {
            "name": "clustermanager-sample"
          },
          "spec": {}
        },
        {
          "api_version": "enterprise.splunk.com/v4",
          "kind": "IndexerCluster",
          "metadata": {
            "name": "indexercluster-sample"
          },
          "spec": {}
        },
        {
          "api_version": "enterprise.splunk.com/v4",
          "kind": "LicenseManager",
          "metadata": {
            "name": "licensemanager-sample"
          },
          "spec": {}
        },
        {
          "api_version": "enterprise.splunk.com/v4",
          "kind": "MonitoringConsole",
          "metadata": {
            "name": "monitoringconsole-sample"
          },
          "spec": {}
        },
        {
          "api_version": "enterprise.splunk.com/v4",
          "kind": "SearchHeadCluster",
          "metadata": {
            "name": "searchheadcluster-sample"
          },
          "spec": {}
        },
        {
          "api_version": "enterprise.splunk.com/v4",
          "kind": "Standalone",
          "metadata": {
            "name": "standalone-sample"
          },
          "spec": {}
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/splunk/splunk-operator-bundle@sha256:f9d86f5ff414ae425f9343243831cd4ad6c8b4ee2e4c9f4910942a2e56c07689",
      "bundle_path_digest": "sha256:f9d86f5ff414ae425f9343243831cd4ad6c8b4ee2e4c9f4910942a2e56c07689",
      "capabilities": [
        "Seamless Upgrades"
      ],
      "channel_name": "stable",
      "creation_date": "2022-12-01T23:11:52.049000+00:00",
      "csv_description": "# Getting Started with the Splunk Operator for Kubernetes The Splunk Operator for Kubernetes enables you to quickly and easily deploy Splunk Enterprise on your choice of private or public cloud provider. The Operator simplifies scaling and management of Splunk Enterprise by automating administrative workflows using Kubernetes best practices. The Splunk Operator runs as a container, and uses the Kubernetes [operator pattern](https://kubernetes.io/docs/concepts/extend-kubernetes/operator/) and [custom resources](https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/) objects to create and manage a scalable and sustainable Splunk Enterprise environment. This guide is intended to help new users get up and running with the Splunk Operator for Kubernetes. It is divided into the following sections: * [Known Issues for the Splunk Operator](#known-issues-for-the-splunk-operator) * [Prerequisites for the Splunk Operator](#prerequisites-for-the-splunk-operator) * [Installing the Splunk Operator](#installing-the-splunk-operator) * [Creating Splunk Enterprise Deployments](#creating-a-splunk-enterprise-deployment) * [Securing Splunk Deployments in Kubernetes](Security.md) * [Contacting Support](#contacting-support) ## Support Resources SPLUNK SUPPORTED: The Splunk Operator for Kubernetes is a supported method for deploying distributed Splunk Enterprise environments using containers. The Splunk Operator is categorized as an Extension and subject to the support terms found [here](https://www.splunk.com/en_us/legal/splunk-software-support-policy.html). Splunk Enterprise deployed using the Splunk Operator is subject to the applicable support level offered [here](https://www.splunk.com/en_us/customer-success/support-programs.html). COMMUNITY DEVELOPED: Splunk Operator for Kubernetes is an open source product developed by Splunkers with contributions from the community of partners and customers. The primary reason why Splunk is taking this approach is to push product development closer to those that use and depend upon it. This direct connection will help us all be more successful and move at a rapid pace. If you're interested in contributing to the SOK open source project, review the [Contributing to the Project](CONTRIBUTING.md) page. **Community Support & Discussions on [Slack](https://splunk-usergroups.slack.com)** channel #splunk-operator-for-kubernetes **File Issues or Enhancements in [GitHub](https://github.com/splunk/splunk-operator/issues)** splunk/splunk-operator ## Known Issues for the Splunk Operator Review the [Change Log](ChangeLog.md) page for a history of changes in each release. ## Prerequisites for the Splunk Operator ### Supported Kubernetes Versions - Kubernetes, version 1.21.0+ and later (x86 64-bit only). The Splunk Operator should work with any [CNCF certified distribution](https://www.cncf.io/certification/software-conformance/) of Kubernetes. We do not have platform recommendations, but this is a table of platforms that our developers, customers, and partners have used successfully with the Splunk Operator. <table> <tr><td> Splunk Development & Testing Platforms </td><td> Amazon Elastic Kubernetes Service (EKS), Google Kubernetes Engine (GKE) </td></tr> <tr><td> Customer Reported Platforms </td><td> Microsoft Azure Kubernetes Service (AKS), Red Hat OpenShift </td></tr> <tr><td> Partner Tested Platforms</td><td> HPE Ezmeral</td></tr> <tr><td> Other Platforms </td><td>CNCF certified distribution</td></tr> </table> ### Splunk Enterprise Compatibility Each Splunk Operator release has specific Splunk Enterprise compatibility requirements. Before installing or upgrading the Splunk Operator, review the [Change Log](ChangeLog.md) to verify version compatibility with Splunk Enterprise releases. ### Splunk Apps Installation Apps and add-ons can be installed using the Splunk Operator by following the instructions given at [Installing Splunk Apps](Examples.md#installing-splunk-apps).  Premium apps such as Enterprise Security and IT Service Intelligence are currently not supported. ### Docker requirements The Splunk Operator requires these docker images to be present or available to your Kubernetes cluster: * `splunk/splunk-operator`: The Splunk Operator image built by this repository or the [official release](https://hub.docker.com/r/splunk/splunk-operator) (2.1.0 or later) * `splunk/splunk:<version>`: The [Splunk Enterprise image](https://github.com/splunk/docker-splunk) (9.0.2 or later) All of the Splunk Enterprise images are publicly available on [Docker Hub](https://hub.docker.com/). If your cluster does not have access to pull from Docker Hub, see the [Required Images Documentation](Images.md) page. Review the [Change Log](ChangeLog.md) page for a history of changes and Splunk Enterprise compatibility for each release. ### Hardware Resources Requirements The resource guidelines for running production Splunk Enterprise instances in pods through the Splunk Operator are the same as running Splunk Enterprise natively on a supported operating system and file system. Refer to the Splunk Enterprise [Reference Hardware documentation](https://docs.splunk.com/Documentation/Splunk/latest/Capacity/Referencehardware) for additional details.  We would also recommend following the same guidance on [Splunk Enterprise for disabling Transparent Huge Pages (THP)](https://docs.splunk.com/Documentation/Splunk/latest/ReleaseNotes/SplunkandTHP) for the nodes in your Kubernetes cluster.  Please be aware that this may impact performance of other non-Splunk workloads. #### Minimum Reference Hardware Based on Splunk Enterprise [Reference Hardware documentation](https://docs.splunk.com/Documentation/Splunk/latest/Capacity/Referencehardware), a summary of the minimum reference hardware requirements is given below. | Standalone        | Search Head / Search Head Cluster | Indexer Cluster | | ---------- | ------- | ------- | | _Each Standalone Pod: 12 Physical CPU Cores or 24 vCPU at 2Ghz or greater per core, 12GB RAM._| _Each Search Head Pod: 16 Physical CPU Cores or 32 vCPU at 2Ghz or greater per core, 12GB RAM._| _Each Indexer Pod: 12 Physical CPU cores, or 24 vCPU at 2GHz or greater per core, 12GB RAM._ | #### _Using Kubernetes Quality of Service Classes_ In addition to the guidelines provided in the reference hardware, [Kubernetes Quality of Service Classes](https://kubernetes.io/docs/tasks/configure-pod-container/quality-service-pod/) can be used to configure CPU/Mem resources allocations that map to your _service level objectives_. For further information on utilizing Kubernetes Quality of Service (QoS) classes, see the table below: | QoS        | Summary| Description | | ---------- | ------- | ------- | | _Guaranteed_ | _CPU/Mem ```requests``` = CPU/Mem ```limits```_    | _When the CPU and memory  ```requests``` and ```limits``` values are equal, the pod is given a QoS class of Guaranteed. This level of service is recommended for Splunk Enterprise ___production environments___._ | | _Burstable_ | _CPU/Mem ```requests``` < CPU/Mem ```limits```_  | _When the CPU and memory ```requests``` value is set lower than the ```limits``` the pod is given a QoS class of Burstable. This level of service is useful in a user acceptance testing ___(UAT) environment___, where the pods run with minimum resources, and Kubernetes allocates additional resources depending on usage._| | _BestEffort_ | _No CPU/Mem ```requests``` or ```limits``` are set_ | _When the ```requests``` or ```limits``` values are not set, the pod is given a QoS class of BestEffort. This level of service is sufficient for ___testing, or a small development task___._ | Examples on how to implement these QoS are given at [Examples of Guaranteed and Burstable QoS](CustomResources.md#examples-of-guaranteed-and-burstable-qos) section. ### Storage guidelines The Splunk Operator uses Kubernetes [Persistent Volume Claims](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims) to store all of your Splunk Enterprise configuration (\"$SPLUNK_HOME/etc\" path) and event (\"$SPLUNK_HOME/var\" path) data. If one of the underlying machines fail, Kubernetes will automatically try to recover by restarting the Splunk Enterprise pods on another machine that is able to reuse the same data volumes. This minimizes the maintenance burden on your operations team by reducing the impact of common hardware failures to the equivalent of a service restart.  The use of Persistent Volume Claims requires that your cluster is configured to support one or more Kubernetes persistent [Storage Classes](https://kubernetes.io/docs/concepts/storage/storage-classes/). See the [Setting Up a Persistent Storage for Splunk](StorageClass.md) page for more information. ### What Storage Type To Use? The Kubernetes infrastructure must have access to storage that meets or exceeds the recommendations provided in the Splunk Enterprise storage type recommendations at [Reference Hardware documentation - what storage type to use for a given role?](https://docs.splunk.com/Documentation/Splunk/latest/Capacity/Referencehardware#What_storage_type_should_I_use_for_a_role.3F) In summary, Indexers with SmartStore need NVMe or SSD storage to provide the necessary IOPs for a successful Splunk Enterprise environment. ### Splunk SmartStore Required For production environments, we are requiring the use of Splunk SmartStore. As a Splunk Enterprise deployment's data volume increases, demand for storage typically outpaces demand for compute resources. [Splunk's SmartStore Feature](https://docs.splunk.com/Documentation/Splunk/latest/Indexer/AboutSmartStore) allows you to manage your indexer storage and compute resources in a ___cost-effective___ manner by scaling those resources separately. SmartStore utilizes a fast storage cache on each indexer node to keep recent data locally available for search and keep other data in a remote object store. Look into the [SmartStore Resource Guide](SmartStore.md) document for configuring and using SmartStore through operator. ## Installing the Splunk Operator A Kubernetes cluster administrator can install and start the Splunk Operator for specific namespace by running: ``` kubectl apply -f https://github.com/splunk/splunk-operator/releases/download/2.1.0/splunk-operator-namespace.yaml ``` A Kubernetes cluster administrator can install and start the Splunk Operator for cluster-wide by running: ``` kubectl apply -f https://github.com/splunk/splunk-operator/releases/download/2.1.0/splunk-operator-cluster.yaml ``` The [Advanced Installation Instructions](Install.md) page offers guidance for advanced configurations, including the use of private image registries, installation at cluster scope, and installing the Splunk Operator as a user who is not a Kubernetes administrator. Users of Red Hat OpenShift should review the [Red Hat OpenShift](OpenShift.md) page. The [Advanced Installation Instructions](Install.md) page offers guidance for advanced configurations, including the use of private image registries, installation at cluster scope, and installing the Splunk Operator as a user who is not a Kubernetes administrator. Users of Red Hat OpenShift should review the [Red Hat OpenShift](OpenShift.md) page. *Note: We recommended that the Splunk Enterprise Docker image is copied to a private registry, or directly onto your Kubernetes workers before creating large Splunk Enterprise deployments. See the [Required Images Documentation](Images.md) page, and the [Advanced Installation Instructions](Install.md) page for guidance on working with copies of the Docker images.* After the Splunk Operator starts, you'll see a single pod running within your current namespace: ``` $ kubectl get pods NAME                               READY   STATUS    RESTARTS   AGE splunk-operator-75f5d4d85b-8pshn   1/1     Running   0          5s ``` ### Installation using Helm charts Installing the Splunk Operator using Helm allows you to quickly deploy the operator and Splunk Enterprise in a Kubernetes cluster. The operator and custom resources are easily configurable allowing for advanced installations including support for Splunk Validated Architectures. Helm also provides a number of features to manage the operator and custom resource lifecycle. The [Installation using Helm](Helm.md) page will walk you through installing and configuring Splunk Enterprise deployments using Helm charts. ## Upgrading the Splunk Operator For information on upgrading the Splunk Operator, see the [How to upgrade Splunk Operator and Splunk Enterprise Deployments](SplunkOperatorUpgrade.md) page. ## Creating a Splunk Enterprise deployment The `Standalone` custom resource is used to create a single instance deployment of Splunk Enterprise. For example: 1. Run the command to create a deployment named \u201cs1\u201d: ```yaml cat <<EOF | kubectl apply -n splunk-operator -f - apiVersion: enterprise.splunk.com/v4 kind: Standalone metadata: name: s1 finalizers: - enterprise.splunk.com/delete-pvc EOF ``` **The `enterprise.splunk.com/delete-pvc` finalizer is optional, and tells the Splunk Operator to remove any Kubernetes [Persistent Volumes](https://kubernetes.io/docs/concepts/storage/persistent-volumes/) associated with the instance if you delete the custom resource(CR).** Within a few minutes, you'll see new pods running in your namespace: ``` $ kubectl get pods NAME                                   READY STATUS    RESTARTS   AGE splunk-operator-7c5599546c-wt4xl        1/1 Running   0          11h splunk-s1-standalone-0                  1/1 Running   0          45s ``` *Note: if your shell prints a `%` at the end, leave that out when you copy the output.* 2. You can use a simple network port forward to open port 8000 for Splunk Web access: ``` kubectl port-forward splunk-s1-standalone-0 8000 ``` 3. Get your passwords for the namespace. The Splunk Enterprise passwords used in the namespace are generated automatically. To learn how to find and read the passwords, see the [Reading global kubernetes secret object](Examples.md#reading-global-kubernetes-secret-object) page. 4. Log into Splunk Enterprise at http://localhost:8000 using the `admin` account with the password. 5. To delete your standalone deployment, run: ``` kubectl delete standalone s1 ``` The `Standalone` custom resource is just one of the resources the Splunk Operator provides. You can find more custom resources and the parameters they support on the [Custom Resource Guide](CustomResources.md) page. For additional deployment examples, including Splunk Enterprise clusters, see the  [Configuring Splunk Enterprise Deployments](Examples.md) page. For additional guidance on making Splunk Enterprise ports accessible outside of Kubernetes, see the [Configuring Ingress](Ingress.md) page. ## Contacting Support If you are a Splunk Enterprise customer with a valid support entitlement contract and have a Splunk-related question, you can open a support case on the https://www.splunk.com/ support portal.",
      "csv_display_name": "Splunk Operator",
      "csv_metadata_description": "The Splunk Operator for Kubernetes enables you to quickly and easily deploy Splunk Enterprise on your choice of private or public cloud provider. The Operator simplifies scaling and management of Splunk Enterprise by automating administrative workflows using Kubernetes best practices.",
      "csv_name": "splunk-operator.v2.1.0",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": true,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-12-01T23:11:52.049000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.11",
      "organization": "certified-operators",
      "package": "splunk-operator",
      "provided_apis": [
        {
          "group": "enterprise.splunk.com",
          "kind": "ClusterManager",
          "version": "v4"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "ClusterMaster",
          "version": "v1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "ClusterMaster",
          "version": "v2"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "ClusterMaster",
          "version": "v3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "IndexerCluster",
          "version": "v1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "IndexerCluster",
          "version": "v2"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "IndexerCluster",
          "version": "v3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "IndexerCluster",
          "version": "v4"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "LicenseManager",
          "version": "v4"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "LicenseMaster",
          "version": "v1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "LicenseMaster",
          "version": "v2"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "LicenseMaster",
          "version": "v3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "MonitoringConsole",
          "version": "v3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "MonitoringConsole",
          "version": "v4"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "SearchHeadCluster",
          "version": "v1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "SearchHeadCluster",
          "version": "v2"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "SearchHeadCluster",
          "version": "v3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "SearchHeadCluster",
          "version": "v4"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "Standalone",
          "version": "v1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "Standalone",
          "version": "v2"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "Standalone",
          "version": "v3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "Standalone",
          "version": "v4"
        }
      ],
      "provider": "Splunk Inc.",
      "related_images": [
        {
          "digest": "sha256:0be31e80a19326e83ff31f3b8d0b71c72f577b80157cf76cab4ab8cd64e7daa0",
          "image": "splunk/splunk@sha256:0be31e80a19326e83ff31f3b8d0b71c72f577b80157cf76cab4ab8cd64e7daa0",
          "name": "splunk-enterprise"
        },
        {
          "digest": "sha256:9764292790ac8f1537d4f031c3bee9cd349dca98206e4191cbf1bbe3d4389aa9",
          "image": "splunk/splunk-operator@sha256:9764292790ac8f1537d4f031c3bee9cd349dca98206e4191cbf1bbe3d4389aa9",
          "name": "splunk-operator-9764292790ac8f1537d4f031c3bee9cd349dca98206e4191cbf1bbe3d4389aa9-annotation"
        },
        {
          "digest": "sha256:d99a8d144816b951a67648c12c0b988936ccd25cf3754f3cd85ab8c01592248f",
          "image": "gcr.io/kubebuilder/kube-rbac-proxy@sha256:d99a8d144816b951a67648c12c0b988936ccd25cf3754f3cd85ab8c01592248f",
          "name": "kube-rbac-proxy"
        },
        {
          "digest": "sha256:9764292790ac8f1537d4f031c3bee9cd349dca98206e4191cbf1bbe3d4389aa9",
          "image": "splunk/splunk-operator@sha256:9764292790ac8f1537d4f031c3bee9cd349dca98206e4191cbf1bbe3d4389aa9",
          "name": "manager"
        },
        {
          "digest": "sha256:0be31e80a19326e83ff31f3b8d0b71c72f577b80157cf76cab4ab8cd64e7daa0",
          "image": "splunk/splunk@sha256:0be31e80a19326e83ff31f3b8d0b71c72f577b80157cf76cab4ab8cd64e7daa0",
          "name": "splunk_enterprise"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.11",
      "version": "2.1.0",
      "version_original": "2.1.0"
    },
    {
      "_id": "638934e2b5dcf804fdcc97dc",
      "alm_examples": [
        {
          "api_version": "enterprise.splunk.com/v4",
          "kind": "ClusterManager",
          "metadata": {
            "name": "clustermanager-sample"
          },
          "spec": {}
        },
        {
          "api_version": "enterprise.splunk.com/v4",
          "kind": "IndexerCluster",
          "metadata": {
            "name": "indexercluster-sample"
          },
          "spec": {}
        },
        {
          "api_version": "enterprise.splunk.com/v4",
          "kind": "LicenseManager",
          "metadata": {
            "name": "licensemanager-sample"
          },
          "spec": {}
        },
        {
          "api_version": "enterprise.splunk.com/v4",
          "kind": "MonitoringConsole",
          "metadata": {
            "name": "monitoringconsole-sample"
          },
          "spec": {}
        },
        {
          "api_version": "enterprise.splunk.com/v4",
          "kind": "SearchHeadCluster",
          "metadata": {
            "name": "searchheadcluster-sample"
          },
          "spec": {}
        },
        {
          "api_version": "enterprise.splunk.com/v4",
          "kind": "Standalone",
          "metadata": {
            "name": "standalone-sample"
          },
          "spec": {}
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/splunk/splunk-operator-bundle@sha256:f9d86f5ff414ae425f9343243831cd4ad6c8b4ee2e4c9f4910942a2e56c07689",
      "bundle_path_digest": "sha256:f9d86f5ff414ae425f9343243831cd4ad6c8b4ee2e4c9f4910942a2e56c07689",
      "capabilities": [
        "Seamless Upgrades"
      ],
      "channel_name": "stable",
      "creation_date": "2022-12-01T23:12:34.388000+00:00",
      "csv_description": "# Getting Started with the Splunk Operator for Kubernetes The Splunk Operator for Kubernetes enables you to quickly and easily deploy Splunk Enterprise on your choice of private or public cloud provider. The Operator simplifies scaling and management of Splunk Enterprise by automating administrative workflows using Kubernetes best practices. The Splunk Operator runs as a container, and uses the Kubernetes [operator pattern](https://kubernetes.io/docs/concepts/extend-kubernetes/operator/) and [custom resources](https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/) objects to create and manage a scalable and sustainable Splunk Enterprise environment. This guide is intended to help new users get up and running with the Splunk Operator for Kubernetes. It is divided into the following sections: * [Known Issues for the Splunk Operator](#known-issues-for-the-splunk-operator) * [Prerequisites for the Splunk Operator](#prerequisites-for-the-splunk-operator) * [Installing the Splunk Operator](#installing-the-splunk-operator) * [Creating Splunk Enterprise Deployments](#creating-a-splunk-enterprise-deployment) * [Securing Splunk Deployments in Kubernetes](Security.md) * [Contacting Support](#contacting-support) ## Support Resources SPLUNK SUPPORTED: The Splunk Operator for Kubernetes is a supported method for deploying distributed Splunk Enterprise environments using containers. The Splunk Operator is categorized as an Extension and subject to the support terms found [here](https://www.splunk.com/en_us/legal/splunk-software-support-policy.html). Splunk Enterprise deployed using the Splunk Operator is subject to the applicable support level offered [here](https://www.splunk.com/en_us/customer-success/support-programs.html). COMMUNITY DEVELOPED: Splunk Operator for Kubernetes is an open source product developed by Splunkers with contributions from the community of partners and customers. The primary reason why Splunk is taking this approach is to push product development closer to those that use and depend upon it. This direct connection will help us all be more successful and move at a rapid pace. If you're interested in contributing to the SOK open source project, review the [Contributing to the Project](CONTRIBUTING.md) page. **Community Support & Discussions on [Slack](https://splunk-usergroups.slack.com)** channel #splunk-operator-for-kubernetes **File Issues or Enhancements in [GitHub](https://github.com/splunk/splunk-operator/issues)** splunk/splunk-operator ## Known Issues for the Splunk Operator Review the [Change Log](ChangeLog.md) page for a history of changes in each release. ## Prerequisites for the Splunk Operator ### Supported Kubernetes Versions - Kubernetes, version 1.21.0+ and later (x86 64-bit only). The Splunk Operator should work with any [CNCF certified distribution](https://www.cncf.io/certification/software-conformance/) of Kubernetes. We do not have platform recommendations, but this is a table of platforms that our developers, customers, and partners have used successfully with the Splunk Operator. <table> <tr><td> Splunk Development & Testing Platforms </td><td> Amazon Elastic Kubernetes Service (EKS), Google Kubernetes Engine (GKE) </td></tr> <tr><td> Customer Reported Platforms </td><td> Microsoft Azure Kubernetes Service (AKS), Red Hat OpenShift </td></tr> <tr><td> Partner Tested Platforms</td><td> HPE Ezmeral</td></tr> <tr><td> Other Platforms </td><td>CNCF certified distribution</td></tr> </table> ### Splunk Enterprise Compatibility Each Splunk Operator release has specific Splunk Enterprise compatibility requirements. Before installing or upgrading the Splunk Operator, review the [Change Log](ChangeLog.md) to verify version compatibility with Splunk Enterprise releases. ### Splunk Apps Installation Apps and add-ons can be installed using the Splunk Operator by following the instructions given at [Installing Splunk Apps](Examples.md#installing-splunk-apps).  Premium apps such as Enterprise Security and IT Service Intelligence are currently not supported. ### Docker requirements The Splunk Operator requires these docker images to be present or available to your Kubernetes cluster: * `splunk/splunk-operator`: The Splunk Operator image built by this repository or the [official release](https://hub.docker.com/r/splunk/splunk-operator) (2.1.0 or later) * `splunk/splunk:<version>`: The [Splunk Enterprise image](https://github.com/splunk/docker-splunk) (9.0.2 or later) All of the Splunk Enterprise images are publicly available on [Docker Hub](https://hub.docker.com/). If your cluster does not have access to pull from Docker Hub, see the [Required Images Documentation](Images.md) page. Review the [Change Log](ChangeLog.md) page for a history of changes and Splunk Enterprise compatibility for each release. ### Hardware Resources Requirements The resource guidelines for running production Splunk Enterprise instances in pods through the Splunk Operator are the same as running Splunk Enterprise natively on a supported operating system and file system. Refer to the Splunk Enterprise [Reference Hardware documentation](https://docs.splunk.com/Documentation/Splunk/latest/Capacity/Referencehardware) for additional details.  We would also recommend following the same guidance on [Splunk Enterprise for disabling Transparent Huge Pages (THP)](https://docs.splunk.com/Documentation/Splunk/latest/ReleaseNotes/SplunkandTHP) for the nodes in your Kubernetes cluster.  Please be aware that this may impact performance of other non-Splunk workloads. #### Minimum Reference Hardware Based on Splunk Enterprise [Reference Hardware documentation](https://docs.splunk.com/Documentation/Splunk/latest/Capacity/Referencehardware), a summary of the minimum reference hardware requirements is given below. | Standalone        | Search Head / Search Head Cluster | Indexer Cluster | | ---------- | ------- | ------- | | _Each Standalone Pod: 12 Physical CPU Cores or 24 vCPU at 2Ghz or greater per core, 12GB RAM._| _Each Search Head Pod: 16 Physical CPU Cores or 32 vCPU at 2Ghz or greater per core, 12GB RAM._| _Each Indexer Pod: 12 Physical CPU cores, or 24 vCPU at 2GHz or greater per core, 12GB RAM._ | #### _Using Kubernetes Quality of Service Classes_ In addition to the guidelines provided in the reference hardware, [Kubernetes Quality of Service Classes](https://kubernetes.io/docs/tasks/configure-pod-container/quality-service-pod/) can be used to configure CPU/Mem resources allocations that map to your _service level objectives_. For further information on utilizing Kubernetes Quality of Service (QoS) classes, see the table below: | QoS        | Summary| Description | | ---------- | ------- | ------- | | _Guaranteed_ | _CPU/Mem ```requests``` = CPU/Mem ```limits```_    | _When the CPU and memory  ```requests``` and ```limits``` values are equal, the pod is given a QoS class of Guaranteed. This level of service is recommended for Splunk Enterprise ___production environments___._ | | _Burstable_ | _CPU/Mem ```requests``` < CPU/Mem ```limits```_  | _When the CPU and memory ```requests``` value is set lower than the ```limits``` the pod is given a QoS class of Burstable. This level of service is useful in a user acceptance testing ___(UAT) environment___, where the pods run with minimum resources, and Kubernetes allocates additional resources depending on usage._| | _BestEffort_ | _No CPU/Mem ```requests``` or ```limits``` are set_ | _When the ```requests``` or ```limits``` values are not set, the pod is given a QoS class of BestEffort. This level of service is sufficient for ___testing, or a small development task___._ | Examples on how to implement these QoS are given at [Examples of Guaranteed and Burstable QoS](CustomResources.md#examples-of-guaranteed-and-burstable-qos) section. ### Storage guidelines The Splunk Operator uses Kubernetes [Persistent Volume Claims](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims) to store all of your Splunk Enterprise configuration (\"$SPLUNK_HOME/etc\" path) and event (\"$SPLUNK_HOME/var\" path) data. If one of the underlying machines fail, Kubernetes will automatically try to recover by restarting the Splunk Enterprise pods on another machine that is able to reuse the same data volumes. This minimizes the maintenance burden on your operations team by reducing the impact of common hardware failures to the equivalent of a service restart.  The use of Persistent Volume Claims requires that your cluster is configured to support one or more Kubernetes persistent [Storage Classes](https://kubernetes.io/docs/concepts/storage/storage-classes/). See the [Setting Up a Persistent Storage for Splunk](StorageClass.md) page for more information. ### What Storage Type To Use? The Kubernetes infrastructure must have access to storage that meets or exceeds the recommendations provided in the Splunk Enterprise storage type recommendations at [Reference Hardware documentation - what storage type to use for a given role?](https://docs.splunk.com/Documentation/Splunk/latest/Capacity/Referencehardware#What_storage_type_should_I_use_for_a_role.3F) In summary, Indexers with SmartStore need NVMe or SSD storage to provide the necessary IOPs for a successful Splunk Enterprise environment. ### Splunk SmartStore Required For production environments, we are requiring the use of Splunk SmartStore. As a Splunk Enterprise deployment's data volume increases, demand for storage typically outpaces demand for compute resources. [Splunk's SmartStore Feature](https://docs.splunk.com/Documentation/Splunk/latest/Indexer/AboutSmartStore) allows you to manage your indexer storage and compute resources in a ___cost-effective___ manner by scaling those resources separately. SmartStore utilizes a fast storage cache on each indexer node to keep recent data locally available for search and keep other data in a remote object store. Look into the [SmartStore Resource Guide](SmartStore.md) document for configuring and using SmartStore through operator. ## Installing the Splunk Operator A Kubernetes cluster administrator can install and start the Splunk Operator for specific namespace by running: ``` kubectl apply -f https://github.com/splunk/splunk-operator/releases/download/2.1.0/splunk-operator-namespace.yaml ``` A Kubernetes cluster administrator can install and start the Splunk Operator for cluster-wide by running: ``` kubectl apply -f https://github.com/splunk/splunk-operator/releases/download/2.1.0/splunk-operator-cluster.yaml ``` The [Advanced Installation Instructions](Install.md) page offers guidance for advanced configurations, including the use of private image registries, installation at cluster scope, and installing the Splunk Operator as a user who is not a Kubernetes administrator. Users of Red Hat OpenShift should review the [Red Hat OpenShift](OpenShift.md) page. The [Advanced Installation Instructions](Install.md) page offers guidance for advanced configurations, including the use of private image registries, installation at cluster scope, and installing the Splunk Operator as a user who is not a Kubernetes administrator. Users of Red Hat OpenShift should review the [Red Hat OpenShift](OpenShift.md) page. *Note: We recommended that the Splunk Enterprise Docker image is copied to a private registry, or directly onto your Kubernetes workers before creating large Splunk Enterprise deployments. See the [Required Images Documentation](Images.md) page, and the [Advanced Installation Instructions](Install.md) page for guidance on working with copies of the Docker images.* After the Splunk Operator starts, you'll see a single pod running within your current namespace: ``` $ kubectl get pods NAME                               READY   STATUS    RESTARTS   AGE splunk-operator-75f5d4d85b-8pshn   1/1     Running   0          5s ``` ### Installation using Helm charts Installing the Splunk Operator using Helm allows you to quickly deploy the operator and Splunk Enterprise in a Kubernetes cluster. The operator and custom resources are easily configurable allowing for advanced installations including support for Splunk Validated Architectures. Helm also provides a number of features to manage the operator and custom resource lifecycle. The [Installation using Helm](Helm.md) page will walk you through installing and configuring Splunk Enterprise deployments using Helm charts. ## Upgrading the Splunk Operator For information on upgrading the Splunk Operator, see the [How to upgrade Splunk Operator and Splunk Enterprise Deployments](SplunkOperatorUpgrade.md) page. ## Creating a Splunk Enterprise deployment The `Standalone` custom resource is used to create a single instance deployment of Splunk Enterprise. For example: 1. Run the command to create a deployment named \u201cs1\u201d: ```yaml cat <<EOF | kubectl apply -n splunk-operator -f - apiVersion: enterprise.splunk.com/v4 kind: Standalone metadata: name: s1 finalizers: - enterprise.splunk.com/delete-pvc EOF ``` **The `enterprise.splunk.com/delete-pvc` finalizer is optional, and tells the Splunk Operator to remove any Kubernetes [Persistent Volumes](https://kubernetes.io/docs/concepts/storage/persistent-volumes/) associated with the instance if you delete the custom resource(CR).** Within a few minutes, you'll see new pods running in your namespace: ``` $ kubectl get pods NAME                                   READY STATUS    RESTARTS   AGE splunk-operator-7c5599546c-wt4xl        1/1 Running   0          11h splunk-s1-standalone-0                  1/1 Running   0          45s ``` *Note: if your shell prints a `%` at the end, leave that out when you copy the output.* 2. You can use a simple network port forward to open port 8000 for Splunk Web access: ``` kubectl port-forward splunk-s1-standalone-0 8000 ``` 3. Get your passwords for the namespace. The Splunk Enterprise passwords used in the namespace are generated automatically. To learn how to find and read the passwords, see the [Reading global kubernetes secret object](Examples.md#reading-global-kubernetes-secret-object) page. 4. Log into Splunk Enterprise at http://localhost:8000 using the `admin` account with the password. 5. To delete your standalone deployment, run: ``` kubectl delete standalone s1 ``` The `Standalone` custom resource is just one of the resources the Splunk Operator provides. You can find more custom resources and the parameters they support on the [Custom Resource Guide](CustomResources.md) page. For additional deployment examples, including Splunk Enterprise clusters, see the  [Configuring Splunk Enterprise Deployments](Examples.md) page. For additional guidance on making Splunk Enterprise ports accessible outside of Kubernetes, see the [Configuring Ingress](Ingress.md) page. ## Contacting Support If you are a Splunk Enterprise customer with a valid support entitlement contract and have a Splunk-related question, you can open a support case on the https://www.splunk.com/ support portal.",
      "csv_display_name": "Splunk Operator",
      "csv_metadata_description": "The Splunk Operator for Kubernetes enables you to quickly and easily deploy Splunk Enterprise on your choice of private or public cloud provider. The Operator simplifies scaling and management of Splunk Enterprise by automating administrative workflows using Kubernetes best practices.",
      "csv_name": "splunk-operator.v2.1.0",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": true,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-12-01T23:12:34.388000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.9",
      "organization": "certified-operators",
      "package": "splunk-operator",
      "provided_apis": [
        {
          "group": "enterprise.splunk.com",
          "kind": "Standalone",
          "plural": "standalones",
          "version": "v1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "SearchHeadCluster",
          "plural": "searchheadclusters",
          "version": "v3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "ClusterMaster",
          "plural": "clustermasters",
          "version": "v2"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "LicenseManager",
          "plural": "licensemanagers",
          "version": "v4"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "IndexerCluster",
          "plural": "indexerclusters",
          "version": "v2"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "MonitoringConsole",
          "plural": "monitoringconsoles",
          "version": "v4"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "LicenseMaster",
          "plural": "licensemasters",
          "version": "v3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "LicenseMaster",
          "plural": "licensemasters",
          "version": "v1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "Standalone",
          "plural": "standalones",
          "version": "v4"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "Standalone",
          "plural": "standalones",
          "version": "v3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "Standalone",
          "plural": "standalones",
          "version": "v2"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "SearchHeadCluster",
          "plural": "searchheadclusters",
          "version": "v1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "SearchHeadCluster",
          "plural": "searchheadclusters",
          "version": "v4"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "LicenseMaster",
          "plural": "licensemasters",
          "version": "v2"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "SearchHeadCluster",
          "plural": "searchheadclusters",
          "version": "v2"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "ClusterManager",
          "plural": "clustermanagers",
          "version": "v4"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "IndexerCluster",
          "plural": "indexerclusters",
          "version": "v3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "MonitoringConsole",
          "plural": "monitoringconsoles",
          "version": "v3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "ClusterMaster",
          "plural": "clustermasters",
          "version": "v3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "IndexerCluster",
          "plural": "indexerclusters",
          "version": "v4"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "IndexerCluster",
          "plural": "indexerclusters",
          "version": "v1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "ClusterMaster",
          "plural": "clustermasters",
          "version": "v1"
        }
      ],
      "provider": "Splunk Inc.",
      "related_images": [
        {
          "digest": "sha256:0be31e80a19326e83ff31f3b8d0b71c72f577b80157cf76cab4ab8cd64e7daa0",
          "image": "splunk/splunk@sha256:0be31e80a19326e83ff31f3b8d0b71c72f577b80157cf76cab4ab8cd64e7daa0",
          "name": "splunk-enterprise"
        },
        {
          "digest": "sha256:9764292790ac8f1537d4f031c3bee9cd349dca98206e4191cbf1bbe3d4389aa9",
          "image": "splunk/splunk-operator@sha256:9764292790ac8f1537d4f031c3bee9cd349dca98206e4191cbf1bbe3d4389aa9",
          "name": "splunk-operator-9764292790ac8f1537d4f031c3bee9cd349dca98206e4191cbf1bbe3d4389aa9-annotation"
        },
        {
          "digest": "sha256:d99a8d144816b951a67648c12c0b988936ccd25cf3754f3cd85ab8c01592248f",
          "image": "gcr.io/kubebuilder/kube-rbac-proxy@sha256:d99a8d144816b951a67648c12c0b988936ccd25cf3754f3cd85ab8c01592248f",
          "name": "kube-rbac-proxy"
        },
        {
          "digest": "sha256:9764292790ac8f1537d4f031c3bee9cd349dca98206e4191cbf1bbe3d4389aa9",
          "image": "splunk/splunk-operator@sha256:9764292790ac8f1537d4f031c3bee9cd349dca98206e4191cbf1bbe3d4389aa9",
          "name": "manager"
        },
        {
          "digest": "sha256:0be31e80a19326e83ff31f3b8d0b71c72f577b80157cf76cab4ab8cd64e7daa0",
          "image": "splunk/splunk@sha256:0be31e80a19326e83ff31f3b8d0b71c72f577b80157cf76cab4ab8cd64e7daa0",
          "name": "splunk_enterprise"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.9",
      "version": "2.1.0",
      "version_original": "2.1.0"
    },
    {
      "_id": "6389357b3df2cad99bd605e1",
      "alm_examples": [
        {
          "api_version": "enterprise.splunk.com/v4",
          "kind": "ClusterManager",
          "metadata": {
            "name": "clustermanager-sample"
          },
          "spec": {}
        },
        {
          "api_version": "enterprise.splunk.com/v4",
          "kind": "IndexerCluster",
          "metadata": {
            "name": "indexercluster-sample"
          },
          "spec": {}
        },
        {
          "api_version": "enterprise.splunk.com/v4",
          "kind": "LicenseManager",
          "metadata": {
            "name": "licensemanager-sample"
          },
          "spec": {}
        },
        {
          "api_version": "enterprise.splunk.com/v4",
          "kind": "MonitoringConsole",
          "metadata": {
            "name": "monitoringconsole-sample"
          },
          "spec": {}
        },
        {
          "api_version": "enterprise.splunk.com/v4",
          "kind": "SearchHeadCluster",
          "metadata": {
            "name": "searchheadcluster-sample"
          },
          "spec": {}
        },
        {
          "api_version": "enterprise.splunk.com/v4",
          "kind": "Standalone",
          "metadata": {
            "name": "standalone-sample"
          },
          "spec": {}
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/splunk/splunk-operator-bundle@sha256:f9d86f5ff414ae425f9343243831cd4ad6c8b4ee2e4c9f4910942a2e56c07689",
      "bundle_path_digest": "sha256:f9d86f5ff414ae425f9343243831cd4ad6c8b4ee2e4c9f4910942a2e56c07689",
      "capabilities": [
        "Seamless Upgrades"
      ],
      "channel_name": "stable",
      "creation_date": "2022-12-01T23:15:07.858000+00:00",
      "csv_description": "# Getting Started with the Splunk Operator for Kubernetes The Splunk Operator for Kubernetes enables you to quickly and easily deploy Splunk Enterprise on your choice of private or public cloud provider. The Operator simplifies scaling and management of Splunk Enterprise by automating administrative workflows using Kubernetes best practices. The Splunk Operator runs as a container, and uses the Kubernetes [operator pattern](https://kubernetes.io/docs/concepts/extend-kubernetes/operator/) and [custom resources](https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/) objects to create and manage a scalable and sustainable Splunk Enterprise environment. This guide is intended to help new users get up and running with the Splunk Operator for Kubernetes. It is divided into the following sections: * [Known Issues for the Splunk Operator](#known-issues-for-the-splunk-operator) * [Prerequisites for the Splunk Operator](#prerequisites-for-the-splunk-operator) * [Installing the Splunk Operator](#installing-the-splunk-operator) * [Creating Splunk Enterprise Deployments](#creating-a-splunk-enterprise-deployment) * [Securing Splunk Deployments in Kubernetes](Security.md) * [Contacting Support](#contacting-support) ## Support Resources SPLUNK SUPPORTED: The Splunk Operator for Kubernetes is a supported method for deploying distributed Splunk Enterprise environments using containers. The Splunk Operator is categorized as an Extension and subject to the support terms found [here](https://www.splunk.com/en_us/legal/splunk-software-support-policy.html). Splunk Enterprise deployed using the Splunk Operator is subject to the applicable support level offered [here](https://www.splunk.com/en_us/customer-success/support-programs.html). COMMUNITY DEVELOPED: Splunk Operator for Kubernetes is an open source product developed by Splunkers with contributions from the community of partners and customers. The primary reason why Splunk is taking this approach is to push product development closer to those that use and depend upon it. This direct connection will help us all be more successful and move at a rapid pace. If you're interested in contributing to the SOK open source project, review the [Contributing to the Project](CONTRIBUTING.md) page. **Community Support & Discussions on [Slack](https://splunk-usergroups.slack.com)** channel #splunk-operator-for-kubernetes **File Issues or Enhancements in [GitHub](https://github.com/splunk/splunk-operator/issues)** splunk/splunk-operator ## Known Issues for the Splunk Operator Review the [Change Log](ChangeLog.md) page for a history of changes in each release. ## Prerequisites for the Splunk Operator ### Supported Kubernetes Versions - Kubernetes, version 1.21.0+ and later (x86 64-bit only). The Splunk Operator should work with any [CNCF certified distribution](https://www.cncf.io/certification/software-conformance/) of Kubernetes. We do not have platform recommendations, but this is a table of platforms that our developers, customers, and partners have used successfully with the Splunk Operator. <table> <tr><td> Splunk Development & Testing Platforms </td><td> Amazon Elastic Kubernetes Service (EKS), Google Kubernetes Engine (GKE) </td></tr> <tr><td> Customer Reported Platforms </td><td> Microsoft Azure Kubernetes Service (AKS), Red Hat OpenShift </td></tr> <tr><td> Partner Tested Platforms</td><td> HPE Ezmeral</td></tr> <tr><td> Other Platforms </td><td>CNCF certified distribution</td></tr> </table> ### Splunk Enterprise Compatibility Each Splunk Operator release has specific Splunk Enterprise compatibility requirements. Before installing or upgrading the Splunk Operator, review the [Change Log](ChangeLog.md) to verify version compatibility with Splunk Enterprise releases. ### Splunk Apps Installation Apps and add-ons can be installed using the Splunk Operator by following the instructions given at [Installing Splunk Apps](Examples.md#installing-splunk-apps).  Premium apps such as Enterprise Security and IT Service Intelligence are currently not supported. ### Docker requirements The Splunk Operator requires these docker images to be present or available to your Kubernetes cluster: * `splunk/splunk-operator`: The Splunk Operator image built by this repository or the [official release](https://hub.docker.com/r/splunk/splunk-operator) (2.1.0 or later) * `splunk/splunk:<version>`: The [Splunk Enterprise image](https://github.com/splunk/docker-splunk) (9.0.2 or later) All of the Splunk Enterprise images are publicly available on [Docker Hub](https://hub.docker.com/). If your cluster does not have access to pull from Docker Hub, see the [Required Images Documentation](Images.md) page. Review the [Change Log](ChangeLog.md) page for a history of changes and Splunk Enterprise compatibility for each release. ### Hardware Resources Requirements The resource guidelines for running production Splunk Enterprise instances in pods through the Splunk Operator are the same as running Splunk Enterprise natively on a supported operating system and file system. Refer to the Splunk Enterprise [Reference Hardware documentation](https://docs.splunk.com/Documentation/Splunk/latest/Capacity/Referencehardware) for additional details.  We would also recommend following the same guidance on [Splunk Enterprise for disabling Transparent Huge Pages (THP)](https://docs.splunk.com/Documentation/Splunk/latest/ReleaseNotes/SplunkandTHP) for the nodes in your Kubernetes cluster.  Please be aware that this may impact performance of other non-Splunk workloads. #### Minimum Reference Hardware Based on Splunk Enterprise [Reference Hardware documentation](https://docs.splunk.com/Documentation/Splunk/latest/Capacity/Referencehardware), a summary of the minimum reference hardware requirements is given below. | Standalone        | Search Head / Search Head Cluster | Indexer Cluster | | ---------- | ------- | ------- | | _Each Standalone Pod: 12 Physical CPU Cores or 24 vCPU at 2Ghz or greater per core, 12GB RAM._| _Each Search Head Pod: 16 Physical CPU Cores or 32 vCPU at 2Ghz or greater per core, 12GB RAM._| _Each Indexer Pod: 12 Physical CPU cores, or 24 vCPU at 2GHz or greater per core, 12GB RAM._ | #### _Using Kubernetes Quality of Service Classes_ In addition to the guidelines provided in the reference hardware, [Kubernetes Quality of Service Classes](https://kubernetes.io/docs/tasks/configure-pod-container/quality-service-pod/) can be used to configure CPU/Mem resources allocations that map to your _service level objectives_. For further information on utilizing Kubernetes Quality of Service (QoS) classes, see the table below: | QoS        | Summary| Description | | ---------- | ------- | ------- | | _Guaranteed_ | _CPU/Mem ```requests``` = CPU/Mem ```limits```_    | _When the CPU and memory  ```requests``` and ```limits``` values are equal, the pod is given a QoS class of Guaranteed. This level of service is recommended for Splunk Enterprise ___production environments___._ | | _Burstable_ | _CPU/Mem ```requests``` < CPU/Mem ```limits```_  | _When the CPU and memory ```requests``` value is set lower than the ```limits``` the pod is given a QoS class of Burstable. This level of service is useful in a user acceptance testing ___(UAT) environment___, where the pods run with minimum resources, and Kubernetes allocates additional resources depending on usage._| | _BestEffort_ | _No CPU/Mem ```requests``` or ```limits``` are set_ | _When the ```requests``` or ```limits``` values are not set, the pod is given a QoS class of BestEffort. This level of service is sufficient for ___testing, or a small development task___._ | Examples on how to implement these QoS are given at [Examples of Guaranteed and Burstable QoS](CustomResources.md#examples-of-guaranteed-and-burstable-qos) section. ### Storage guidelines The Splunk Operator uses Kubernetes [Persistent Volume Claims](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims) to store all of your Splunk Enterprise configuration (\"$SPLUNK_HOME/etc\" path) and event (\"$SPLUNK_HOME/var\" path) data. If one of the underlying machines fail, Kubernetes will automatically try to recover by restarting the Splunk Enterprise pods on another machine that is able to reuse the same data volumes. This minimizes the maintenance burden on your operations team by reducing the impact of common hardware failures to the equivalent of a service restart.  The use of Persistent Volume Claims requires that your cluster is configured to support one or more Kubernetes persistent [Storage Classes](https://kubernetes.io/docs/concepts/storage/storage-classes/). See the [Setting Up a Persistent Storage for Splunk](StorageClass.md) page for more information. ### What Storage Type To Use? The Kubernetes infrastructure must have access to storage that meets or exceeds the recommendations provided in the Splunk Enterprise storage type recommendations at [Reference Hardware documentation - what storage type to use for a given role?](https://docs.splunk.com/Documentation/Splunk/latest/Capacity/Referencehardware#What_storage_type_should_I_use_for_a_role.3F) In summary, Indexers with SmartStore need NVMe or SSD storage to provide the necessary IOPs for a successful Splunk Enterprise environment. ### Splunk SmartStore Required For production environments, we are requiring the use of Splunk SmartStore. As a Splunk Enterprise deployment's data volume increases, demand for storage typically outpaces demand for compute resources. [Splunk's SmartStore Feature](https://docs.splunk.com/Documentation/Splunk/latest/Indexer/AboutSmartStore) allows you to manage your indexer storage and compute resources in a ___cost-effective___ manner by scaling those resources separately. SmartStore utilizes a fast storage cache on each indexer node to keep recent data locally available for search and keep other data in a remote object store. Look into the [SmartStore Resource Guide](SmartStore.md) document for configuring and using SmartStore through operator. ## Installing the Splunk Operator A Kubernetes cluster administrator can install and start the Splunk Operator for specific namespace by running: ``` kubectl apply -f https://github.com/splunk/splunk-operator/releases/download/2.1.0/splunk-operator-namespace.yaml ``` A Kubernetes cluster administrator can install and start the Splunk Operator for cluster-wide by running: ``` kubectl apply -f https://github.com/splunk/splunk-operator/releases/download/2.1.0/splunk-operator-cluster.yaml ``` The [Advanced Installation Instructions](Install.md) page offers guidance for advanced configurations, including the use of private image registries, installation at cluster scope, and installing the Splunk Operator as a user who is not a Kubernetes administrator. Users of Red Hat OpenShift should review the [Red Hat OpenShift](OpenShift.md) page. The [Advanced Installation Instructions](Install.md) page offers guidance for advanced configurations, including the use of private image registries, installation at cluster scope, and installing the Splunk Operator as a user who is not a Kubernetes administrator. Users of Red Hat OpenShift should review the [Red Hat OpenShift](OpenShift.md) page. *Note: We recommended that the Splunk Enterprise Docker image is copied to a private registry, or directly onto your Kubernetes workers before creating large Splunk Enterprise deployments. See the [Required Images Documentation](Images.md) page, and the [Advanced Installation Instructions](Install.md) page for guidance on working with copies of the Docker images.* After the Splunk Operator starts, you'll see a single pod running within your current namespace: ``` $ kubectl get pods NAME                               READY   STATUS    RESTARTS   AGE splunk-operator-75f5d4d85b-8pshn   1/1     Running   0          5s ``` ### Installation using Helm charts Installing the Splunk Operator using Helm allows you to quickly deploy the operator and Splunk Enterprise in a Kubernetes cluster. The operator and custom resources are easily configurable allowing for advanced installations including support for Splunk Validated Architectures. Helm also provides a number of features to manage the operator and custom resource lifecycle. The [Installation using Helm](Helm.md) page will walk you through installing and configuring Splunk Enterprise deployments using Helm charts. ## Upgrading the Splunk Operator For information on upgrading the Splunk Operator, see the [How to upgrade Splunk Operator and Splunk Enterprise Deployments](SplunkOperatorUpgrade.md) page. ## Creating a Splunk Enterprise deployment The `Standalone` custom resource is used to create a single instance deployment of Splunk Enterprise. For example: 1. Run the command to create a deployment named \u201cs1\u201d: ```yaml cat <<EOF | kubectl apply -n splunk-operator -f - apiVersion: enterprise.splunk.com/v4 kind: Standalone metadata: name: s1 finalizers: - enterprise.splunk.com/delete-pvc EOF ``` **The `enterprise.splunk.com/delete-pvc` finalizer is optional, and tells the Splunk Operator to remove any Kubernetes [Persistent Volumes](https://kubernetes.io/docs/concepts/storage/persistent-volumes/) associated with the instance if you delete the custom resource(CR).** Within a few minutes, you'll see new pods running in your namespace: ``` $ kubectl get pods NAME                                   READY STATUS    RESTARTS   AGE splunk-operator-7c5599546c-wt4xl        1/1 Running   0          11h splunk-s1-standalone-0                  1/1 Running   0          45s ``` *Note: if your shell prints a `%` at the end, leave that out when you copy the output.* 2. You can use a simple network port forward to open port 8000 for Splunk Web access: ``` kubectl port-forward splunk-s1-standalone-0 8000 ``` 3. Get your passwords for the namespace. The Splunk Enterprise passwords used in the namespace are generated automatically. To learn how to find and read the passwords, see the [Reading global kubernetes secret object](Examples.md#reading-global-kubernetes-secret-object) page. 4. Log into Splunk Enterprise at http://localhost:8000 using the `admin` account with the password. 5. To delete your standalone deployment, run: ``` kubectl delete standalone s1 ``` The `Standalone` custom resource is just one of the resources the Splunk Operator provides. You can find more custom resources and the parameters they support on the [Custom Resource Guide](CustomResources.md) page. For additional deployment examples, including Splunk Enterprise clusters, see the  [Configuring Splunk Enterprise Deployments](Examples.md) page. For additional guidance on making Splunk Enterprise ports accessible outside of Kubernetes, see the [Configuring Ingress](Ingress.md) page. ## Contacting Support If you are a Splunk Enterprise customer with a valid support entitlement contract and have a Splunk-related question, you can open a support case on the https://www.splunk.com/ support portal.",
      "csv_display_name": "Splunk Operator",
      "csv_metadata_description": "The Splunk Operator for Kubernetes enables you to quickly and easily deploy Splunk Enterprise on your choice of private or public cloud provider. The Operator simplifies scaling and management of Splunk Enterprise by automating administrative workflows using Kubernetes best practices.",
      "csv_name": "splunk-operator.v2.1.0",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": true,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-12-01T23:15:07.858000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.10",
      "organization": "certified-operators",
      "package": "splunk-operator",
      "provided_apis": [
        {
          "group": "enterprise.splunk.com",
          "kind": "ClusterMaster",
          "plural": "clustermasters",
          "version": "v1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "IndexerCluster",
          "plural": "indexerclusters",
          "version": "v4"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "LicenseMaster",
          "plural": "licensemasters",
          "version": "v3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "Standalone",
          "plural": "standalones",
          "version": "v2"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "SearchHeadCluster",
          "plural": "searchheadclusters",
          "version": "v3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "SearchHeadCluster",
          "plural": "searchheadclusters",
          "version": "v4"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "IndexerCluster",
          "plural": "indexerclusters",
          "version": "v2"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "SearchHeadCluster",
          "plural": "searchheadclusters",
          "version": "v2"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "Standalone",
          "plural": "standalones",
          "version": "v4"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "LicenseMaster",
          "plural": "licensemasters",
          "version": "v1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "MonitoringConsole",
          "plural": "monitoringconsoles",
          "version": "v3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "ClusterMaster",
          "plural": "clustermasters",
          "version": "v3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "Standalone",
          "plural": "standalones",
          "version": "v3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "MonitoringConsole",
          "plural": "monitoringconsoles",
          "version": "v4"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "IndexerCluster",
          "plural": "indexerclusters",
          "version": "v1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "Standalone",
          "plural": "standalones",
          "version": "v1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "LicenseManager",
          "plural": "licensemanagers",
          "version": "v4"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "ClusterMaster",
          "plural": "clustermasters",
          "version": "v2"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "SearchHeadCluster",
          "plural": "searchheadclusters",
          "version": "v1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "ClusterManager",
          "plural": "clustermanagers",
          "version": "v4"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "LicenseMaster",
          "plural": "licensemasters",
          "version": "v2"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "IndexerCluster",
          "plural": "indexerclusters",
          "version": "v3"
        }
      ],
      "provider": "Splunk Inc.",
      "related_images": [
        {
          "digest": "sha256:0be31e80a19326e83ff31f3b8d0b71c72f577b80157cf76cab4ab8cd64e7daa0",
          "image": "splunk/splunk@sha256:0be31e80a19326e83ff31f3b8d0b71c72f577b80157cf76cab4ab8cd64e7daa0",
          "name": "splunk-enterprise"
        },
        {
          "digest": "sha256:9764292790ac8f1537d4f031c3bee9cd349dca98206e4191cbf1bbe3d4389aa9",
          "image": "splunk/splunk-operator@sha256:9764292790ac8f1537d4f031c3bee9cd349dca98206e4191cbf1bbe3d4389aa9",
          "name": "splunk-operator-9764292790ac8f1537d4f031c3bee9cd349dca98206e4191cbf1bbe3d4389aa9-annotation"
        },
        {
          "digest": "sha256:d99a8d144816b951a67648c12c0b988936ccd25cf3754f3cd85ab8c01592248f",
          "image": "gcr.io/kubebuilder/kube-rbac-proxy@sha256:d99a8d144816b951a67648c12c0b988936ccd25cf3754f3cd85ab8c01592248f",
          "name": "kube-rbac-proxy"
        },
        {
          "digest": "sha256:9764292790ac8f1537d4f031c3bee9cd349dca98206e4191cbf1bbe3d4389aa9",
          "image": "splunk/splunk-operator@sha256:9764292790ac8f1537d4f031c3bee9cd349dca98206e4191cbf1bbe3d4389aa9",
          "name": "manager"
        },
        {
          "digest": "sha256:0be31e80a19326e83ff31f3b8d0b71c72f577b80157cf76cab4ab8cd64e7daa0",
          "image": "splunk/splunk@sha256:0be31e80a19326e83ff31f3b8d0b71c72f577b80157cf76cab4ab8cd64e7daa0",
          "name": "splunk_enterprise"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.10",
      "version": "2.1.0",
      "version_original": "2.1.0"
    },
    {
      "_id": "63894ff1366047db17b8f54c",
      "alm_examples": [
        {
          "api_version": "apps.gitlab.com/v1beta1",
          "kind": "GitLab",
          "metadata": {
            "name": "gitlab",
            "namespace": "gitlab-system"
          },
          "spec": {
            "chart": {
              "values": {
                "certmanager": {
                  "install": false
                },
                "global": {
                  "hosts": {
                    "domain": "example.com"
                  },
                  "ingress": {
                    "configureCertmanager": false,
                    "tls": {}
                  }
                }
              },
              "version": ""
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/gitlab/gitlab-operator-bundle@sha256:35fb9490e7775268920b574fcf013be898f76161bdf2b5332cbfb6f242b4bd30",
      "bundle_path_digest": "sha256:35fb9490e7775268920b574fcf013be898f76161bdf2b5332cbfb6f242b4bd30",
      "capabilities": [
        "Seamless Upgrades"
      ],
      "channel_name": "stable",
      "creation_date": "2022-12-02T01:08:01.091000+00:00",
      "csv_description": "# Overview\n\nThe GitLab operator is responsible for managing the full lifecycle of GitLab instances in your Kubernetes or Openshift container platforms.\n\n[Documentation](https://docs.gitlab.com/charts/installation/operator.html)\n\nThe operator, while new and still actively being developed, aims to:\n- ease installation and configuration of GitLab instances\n- offer seamless upgrades from version to version\n\n## GitLab\n\nGitLab is a complete open-source DevOps platform, delivered as a single application, fundamentally changing the way Development, Security, and Ops teams collaborate and build software. From idea to production, GitLab helps teams improve cycle time from weeks to minutes, reduce development process costs and decrease time to market while increasing developer productivity.\n\nBuilt on Open Source, GitLab delivers new innovations and features on the same day of every month by leveraging contributions from a passionate, global community of thousands of developers and millions of users. Over 100,000 of the world\u2019s most demanding organizations trust GitLab to deliver great software at new speeds.\n\nIf you would like to enable advanced DevOps capabilities and activate enterprise features such as security, risk, and compliance capabilities, please contact our sales team to purchase an enterprise license.\n\n# Prerequisites\n\nPlease visit [Prerequisites](https://docs.gitlab.com/charts/installation/operator.html#prerequisites) section of GitLab Operator Documentation.\n\n## IngressClass\n\nCluster-wide `IngressClass` should be created prior to Operator setup, as OLM does not currently support this object type:\n\n```yaml\napiVersion: networking.k8s.io/v1\nkind: IngressClass\nmetadata:\n  # Ensure this value matches `spec.chart.values.global.ingress.class`\n  # in the GitLab CR on the next step.\n  name: gitlab-nginx\nspec:\n  controller: k8s.io/ingress-nginx\n```\n",
      "csv_display_name": "GitLab",
      "csv_metadata_description": "The GitLab operator is responsible for managing the full lifecycle of GitLab instances in your Kubernetes or Openshift container platforms.",
      "csv_name": "gitlab-operator-kubernetes.v0.14.1",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": false,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-12-02T01:08:01.091000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.10",
      "organization": "certified-operators",
      "package": "gitlab-operator-kubernetes",
      "provided_apis": [
        {
          "group": "apps.gitlab.com",
          "kind": "GitLab",
          "plural": "gitlabs",
          "version": "v1beta1"
        }
      ],
      "provider": "GitLab Inc",
      "related_images": [
        {
          "digest": "sha256:6ed2063bc7c808f10d96633aeca707a2c0c577e162e1e922160f5e7023c0fc22",
          "image": "registry.gitlab.com/gitlab-org/cloud-native/gitlab-operator@sha256:6ed2063bc7c808f10d96633aeca707a2c0c577e162e1e922160f5e7023c0fc22",
          "name": "cloud-native/gitlab-operator-6ed2063bc7c808f10d96633aeca707a2c0c577e162e1e922160f5e7023c0fc22-annotation"
        },
        {
          "digest": "sha256:6ed2063bc7c808f10d96633aeca707a2c0c577e162e1e922160f5e7023c0fc22",
          "image": "registry.gitlab.com/gitlab-org/cloud-native/gitlab-operator@sha256:6ed2063bc7c808f10d96633aeca707a2c0c577e162e1e922160f5e7023c0fc22",
          "name": "manager"
        },
        {
          "digest": "sha256:e10d1d982dd653db74ca87a1d1ad017bc5ef1aeb651bdea089debf16485b080b",
          "image": "gcr.io/kubebuilder/kube-rbac-proxy@sha256:e10d1d982dd653db74ca87a1d1ad017bc5ef1aeb651bdea089debf16485b080b",
          "name": "kube-rbac-proxy"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.10",
      "version": "0.14.1",
      "version_original": "0.14.1"
    },
    {
      "_id": "63894ff1ac1d5bea0b51dd2a",
      "alm_examples": [
        {
          "api_version": "apps.gitlab.com/v1beta1",
          "kind": "GitLab",
          "metadata": {
            "name": "gitlab",
            "namespace": "gitlab-system"
          },
          "spec": {
            "chart": {
              "values": {
                "certmanager": {
                  "install": false
                },
                "global": {
                  "hosts": {
                    "domain": "example.com"
                  },
                  "ingress": {
                    "configureCertmanager": false,
                    "tls": {}
                  }
                }
              },
              "version": ""
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/gitlab/gitlab-operator-bundle@sha256:35fb9490e7775268920b574fcf013be898f76161bdf2b5332cbfb6f242b4bd30",
      "bundle_path_digest": "sha256:35fb9490e7775268920b574fcf013be898f76161bdf2b5332cbfb6f242b4bd30",
      "capabilities": [
        "Seamless Upgrades"
      ],
      "channel_name": "unstable",
      "creation_date": "2022-12-02T01:08:01.854000+00:00",
      "csv_description": "# Overview\n\nThe GitLab operator is responsible for managing the full lifecycle of GitLab instances in your Kubernetes or Openshift container platforms.\n\n[Documentation](https://docs.gitlab.com/charts/installation/operator.html)\n\nThe operator, while new and still actively being developed, aims to:\n- ease installation and configuration of GitLab instances\n- offer seamless upgrades from version to version\n\n## GitLab\n\nGitLab is a complete open-source DevOps platform, delivered as a single application, fundamentally changing the way Development, Security, and Ops teams collaborate and build software. From idea to production, GitLab helps teams improve cycle time from weeks to minutes, reduce development process costs and decrease time to market while increasing developer productivity.\n\nBuilt on Open Source, GitLab delivers new innovations and features on the same day of every month by leveraging contributions from a passionate, global community of thousands of developers and millions of users. Over 100,000 of the world\u2019s most demanding organizations trust GitLab to deliver great software at new speeds.\n\nIf you would like to enable advanced DevOps capabilities and activate enterprise features such as security, risk, and compliance capabilities, please contact our sales team to purchase an enterprise license.\n\n# Prerequisites\n\nPlease visit [Prerequisites](https://docs.gitlab.com/charts/installation/operator.html#prerequisites) section of GitLab Operator Documentation.\n\n## IngressClass\n\nCluster-wide `IngressClass` should be created prior to Operator setup, as OLM does not currently support this object type:\n\n```yaml\napiVersion: networking.k8s.io/v1\nkind: IngressClass\nmetadata:\n  # Ensure this value matches `spec.chart.values.global.ingress.class`\n  # in the GitLab CR on the next step.\n  name: gitlab-nginx\nspec:\n  controller: k8s.io/ingress-nginx\n```\n",
      "csv_display_name": "GitLab",
      "csv_metadata_description": "The GitLab operator is responsible for managing the full lifecycle of GitLab instances in your Kubernetes or Openshift container platforms.",
      "csv_name": "gitlab-operator-kubernetes.v0.14.1",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": false,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": false,
      "last_update_date": "2022-12-02T01:08:01.854000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.10",
      "organization": "certified-operators",
      "package": "gitlab-operator-kubernetes",
      "provided_apis": [
        {
          "group": "apps.gitlab.com",
          "kind": "GitLab",
          "plural": "gitlabs",
          "version": "v1beta1"
        }
      ],
      "provider": "GitLab Inc",
      "related_images": [
        {
          "digest": "sha256:6ed2063bc7c808f10d96633aeca707a2c0c577e162e1e922160f5e7023c0fc22",
          "image": "registry.gitlab.com/gitlab-org/cloud-native/gitlab-operator@sha256:6ed2063bc7c808f10d96633aeca707a2c0c577e162e1e922160f5e7023c0fc22",
          "name": "cloud-native/gitlab-operator-6ed2063bc7c808f10d96633aeca707a2c0c577e162e1e922160f5e7023c0fc22-annotation"
        },
        {
          "digest": "sha256:6ed2063bc7c808f10d96633aeca707a2c0c577e162e1e922160f5e7023c0fc22",
          "image": "registry.gitlab.com/gitlab-org/cloud-native/gitlab-operator@sha256:6ed2063bc7c808f10d96633aeca707a2c0c577e162e1e922160f5e7023c0fc22",
          "name": "manager"
        },
        {
          "digest": "sha256:e10d1d982dd653db74ca87a1d1ad017bc5ef1aeb651bdea089debf16485b080b",
          "image": "gcr.io/kubebuilder/kube-rbac-proxy@sha256:e10d1d982dd653db74ca87a1d1ad017bc5ef1aeb651bdea089debf16485b080b",
          "name": "kube-rbac-proxy"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.10",
      "version": "0.14.1",
      "version_original": "0.14.1"
    },
    {
      "_id": "6389505aa9e0e29d92ff95cb",
      "alm_examples": [
        {
          "api_version": "apps.gitlab.com/v1beta1",
          "kind": "GitLab",
          "metadata": {
            "name": "gitlab",
            "namespace": "gitlab-system"
          },
          "spec": {
            "chart": {
              "values": {
                "certmanager": {
                  "install": false
                },
                "global": {
                  "hosts": {
                    "domain": "example.com"
                  },
                  "ingress": {
                    "configureCertmanager": false,
                    "tls": {}
                  }
                }
              },
              "version": ""
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/gitlab/gitlab-operator-bundle@sha256:35fb9490e7775268920b574fcf013be898f76161bdf2b5332cbfb6f242b4bd30",
      "bundle_path_digest": "sha256:35fb9490e7775268920b574fcf013be898f76161bdf2b5332cbfb6f242b4bd30",
      "capabilities": [
        "Seamless Upgrades"
      ],
      "channel_name": "stable",
      "creation_date": "2022-12-02T01:09:46.832000+00:00",
      "csv_description": "# Overview\n\nThe GitLab operator is responsible for managing the full lifecycle of GitLab instances in your Kubernetes or Openshift container platforms.\n\n[Documentation](https://docs.gitlab.com/charts/installation/operator.html)\n\nThe operator, while new and still actively being developed, aims to:\n- ease installation and configuration of GitLab instances\n- offer seamless upgrades from version to version\n\n## GitLab\n\nGitLab is a complete open-source DevOps platform, delivered as a single application, fundamentally changing the way Development, Security, and Ops teams collaborate and build software. From idea to production, GitLab helps teams improve cycle time from weeks to minutes, reduce development process costs and decrease time to market while increasing developer productivity.\n\nBuilt on Open Source, GitLab delivers new innovations and features on the same day of every month by leveraging contributions from a passionate, global community of thousands of developers and millions of users. Over 100,000 of the world\u2019s most demanding organizations trust GitLab to deliver great software at new speeds.\n\nIf you would like to enable advanced DevOps capabilities and activate enterprise features such as security, risk, and compliance capabilities, please contact our sales team to purchase an enterprise license.\n\n# Prerequisites\n\nPlease visit [Prerequisites](https://docs.gitlab.com/charts/installation/operator.html#prerequisites) section of GitLab Operator Documentation.\n\n## IngressClass\n\nCluster-wide `IngressClass` should be created prior to Operator setup, as OLM does not currently support this object type:\n\n```yaml\napiVersion: networking.k8s.io/v1\nkind: IngressClass\nmetadata:\n  # Ensure this value matches `spec.chart.values.global.ingress.class`\n  # in the GitLab CR on the next step.\n  name: gitlab-nginx\nspec:\n  controller: k8s.io/ingress-nginx\n```\n",
      "csv_display_name": "GitLab",
      "csv_metadata_description": "The GitLab operator is responsible for managing the full lifecycle of GitLab instances in your Kubernetes or Openshift container platforms.",
      "csv_name": "gitlab-operator-kubernetes.v0.14.1",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": false,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-12-02T01:09:46.832000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.9",
      "organization": "certified-operators",
      "package": "gitlab-operator-kubernetes",
      "provided_apis": [
        {
          "group": "apps.gitlab.com",
          "kind": "GitLab",
          "plural": "gitlabs",
          "version": "v1beta1"
        }
      ],
      "provider": "GitLab Inc",
      "related_images": [
        {
          "digest": "sha256:6ed2063bc7c808f10d96633aeca707a2c0c577e162e1e922160f5e7023c0fc22",
          "image": "registry.gitlab.com/gitlab-org/cloud-native/gitlab-operator@sha256:6ed2063bc7c808f10d96633aeca707a2c0c577e162e1e922160f5e7023c0fc22",
          "name": "cloud-native/gitlab-operator-6ed2063bc7c808f10d96633aeca707a2c0c577e162e1e922160f5e7023c0fc22-annotation"
        },
        {
          "digest": "sha256:6ed2063bc7c808f10d96633aeca707a2c0c577e162e1e922160f5e7023c0fc22",
          "image": "registry.gitlab.com/gitlab-org/cloud-native/gitlab-operator@sha256:6ed2063bc7c808f10d96633aeca707a2c0c577e162e1e922160f5e7023c0fc22",
          "name": "manager"
        },
        {
          "digest": "sha256:e10d1d982dd653db74ca87a1d1ad017bc5ef1aeb651bdea089debf16485b080b",
          "image": "gcr.io/kubebuilder/kube-rbac-proxy@sha256:e10d1d982dd653db74ca87a1d1ad017bc5ef1aeb651bdea089debf16485b080b",
          "name": "kube-rbac-proxy"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.9",
      "version": "0.14.1",
      "version_original": "0.14.1"
    },
    {
      "_id": "6389505bb8573385620169c4",
      "alm_examples": [
        {
          "api_version": "apps.gitlab.com/v1beta1",
          "kind": "GitLab",
          "metadata": {
            "name": "gitlab",
            "namespace": "gitlab-system"
          },
          "spec": {
            "chart": {
              "values": {
                "certmanager": {
                  "install": false
                },
                "global": {
                  "hosts": {
                    "domain": "example.com"
                  },
                  "ingress": {
                    "configureCertmanager": false,
                    "tls": {}
                  }
                }
              },
              "version": ""
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/gitlab/gitlab-operator-bundle@sha256:35fb9490e7775268920b574fcf013be898f76161bdf2b5332cbfb6f242b4bd30",
      "bundle_path_digest": "sha256:35fb9490e7775268920b574fcf013be898f76161bdf2b5332cbfb6f242b4bd30",
      "capabilities": [
        "Seamless Upgrades"
      ],
      "channel_name": "unstable",
      "creation_date": "2022-12-02T01:09:47.564000+00:00",
      "csv_description": "# Overview\n\nThe GitLab operator is responsible for managing the full lifecycle of GitLab instances in your Kubernetes or Openshift container platforms.\n\n[Documentation](https://docs.gitlab.com/charts/installation/operator.html)\n\nThe operator, while new and still actively being developed, aims to:\n- ease installation and configuration of GitLab instances\n- offer seamless upgrades from version to version\n\n## GitLab\n\nGitLab is a complete open-source DevOps platform, delivered as a single application, fundamentally changing the way Development, Security, and Ops teams collaborate and build software. From idea to production, GitLab helps teams improve cycle time from weeks to minutes, reduce development process costs and decrease time to market while increasing developer productivity.\n\nBuilt on Open Source, GitLab delivers new innovations and features on the same day of every month by leveraging contributions from a passionate, global community of thousands of developers and millions of users. Over 100,000 of the world\u2019s most demanding organizations trust GitLab to deliver great software at new speeds.\n\nIf you would like to enable advanced DevOps capabilities and activate enterprise features such as security, risk, and compliance capabilities, please contact our sales team to purchase an enterprise license.\n\n# Prerequisites\n\nPlease visit [Prerequisites](https://docs.gitlab.com/charts/installation/operator.html#prerequisites) section of GitLab Operator Documentation.\n\n## IngressClass\n\nCluster-wide `IngressClass` should be created prior to Operator setup, as OLM does not currently support this object type:\n\n```yaml\napiVersion: networking.k8s.io/v1\nkind: IngressClass\nmetadata:\n  # Ensure this value matches `spec.chart.values.global.ingress.class`\n  # in the GitLab CR on the next step.\n  name: gitlab-nginx\nspec:\n  controller: k8s.io/ingress-nginx\n```\n",
      "csv_display_name": "GitLab",
      "csv_metadata_description": "The GitLab operator is responsible for managing the full lifecycle of GitLab instances in your Kubernetes or Openshift container platforms.",
      "csv_name": "gitlab-operator-kubernetes.v0.14.1",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": false,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": false,
      "last_update_date": "2022-12-02T01:09:47.564000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.9",
      "organization": "certified-operators",
      "package": "gitlab-operator-kubernetes",
      "provided_apis": [
        {
          "group": "apps.gitlab.com",
          "kind": "GitLab",
          "plural": "gitlabs",
          "version": "v1beta1"
        }
      ],
      "provider": "GitLab Inc",
      "related_images": [
        {
          "digest": "sha256:6ed2063bc7c808f10d96633aeca707a2c0c577e162e1e922160f5e7023c0fc22",
          "image": "registry.gitlab.com/gitlab-org/cloud-native/gitlab-operator@sha256:6ed2063bc7c808f10d96633aeca707a2c0c577e162e1e922160f5e7023c0fc22",
          "name": "cloud-native/gitlab-operator-6ed2063bc7c808f10d96633aeca707a2c0c577e162e1e922160f5e7023c0fc22-annotation"
        },
        {
          "digest": "sha256:6ed2063bc7c808f10d96633aeca707a2c0c577e162e1e922160f5e7023c0fc22",
          "image": "registry.gitlab.com/gitlab-org/cloud-native/gitlab-operator@sha256:6ed2063bc7c808f10d96633aeca707a2c0c577e162e1e922160f5e7023c0fc22",
          "name": "manager"
        },
        {
          "digest": "sha256:e10d1d982dd653db74ca87a1d1ad017bc5ef1aeb651bdea089debf16485b080b",
          "image": "gcr.io/kubebuilder/kube-rbac-proxy@sha256:e10d1d982dd653db74ca87a1d1ad017bc5ef1aeb651bdea089debf16485b080b",
          "name": "kube-rbac-proxy"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.9",
      "version": "0.14.1",
      "version_original": "0.14.1"
    },
    {
      "_id": "638950d17476f7ea6acb6678",
      "alm_examples": [
        {
          "api_version": "apps.gitlab.com/v1beta1",
          "kind": "GitLab",
          "metadata": {
            "name": "gitlab",
            "namespace": "gitlab-system"
          },
          "spec": {
            "chart": {
              "values": {
                "certmanager": {
                  "install": false
                },
                "global": {
                  "hosts": {
                    "domain": "example.com"
                  },
                  "ingress": {
                    "configureCertmanager": false,
                    "tls": {}
                  }
                }
              },
              "version": ""
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/gitlab/gitlab-operator-bundle@sha256:35fb9490e7775268920b574fcf013be898f76161bdf2b5332cbfb6f242b4bd30",
      "bundle_path_digest": "sha256:35fb9490e7775268920b574fcf013be898f76161bdf2b5332cbfb6f242b4bd30",
      "capabilities": [
        "Seamless Upgrades"
      ],
      "channel_name": "stable",
      "creation_date": "2022-12-02T01:11:45.771000+00:00",
      "csv_description": "# Overview\n\nThe GitLab operator is responsible for managing the full lifecycle of GitLab instances in your Kubernetes or Openshift container platforms.\n\n[Documentation](https://docs.gitlab.com/charts/installation/operator.html)\n\nThe operator, while new and still actively being developed, aims to:\n- ease installation and configuration of GitLab instances\n- offer seamless upgrades from version to version\n\n## GitLab\n\nGitLab is a complete open-source DevOps platform, delivered as a single application, fundamentally changing the way Development, Security, and Ops teams collaborate and build software. From idea to production, GitLab helps teams improve cycle time from weeks to minutes, reduce development process costs and decrease time to market while increasing developer productivity.\n\nBuilt on Open Source, GitLab delivers new innovations and features on the same day of every month by leveraging contributions from a passionate, global community of thousands of developers and millions of users. Over 100,000 of the world\u2019s most demanding organizations trust GitLab to deliver great software at new speeds.\n\nIf you would like to enable advanced DevOps capabilities and activate enterprise features such as security, risk, and compliance capabilities, please contact our sales team to purchase an enterprise license.\n\n# Prerequisites\n\nPlease visit [Prerequisites](https://docs.gitlab.com/charts/installation/operator.html#prerequisites) section of GitLab Operator Documentation.\n\n## IngressClass\n\nCluster-wide `IngressClass` should be created prior to Operator setup, as OLM does not currently support this object type:\n\n```yaml\napiVersion: networking.k8s.io/v1\nkind: IngressClass\nmetadata:\n  # Ensure this value matches `spec.chart.values.global.ingress.class`\n  # in the GitLab CR on the next step.\n  name: gitlab-nginx\nspec:\n  controller: k8s.io/ingress-nginx\n```\n",
      "csv_display_name": "GitLab",
      "csv_metadata_description": "The GitLab operator is responsible for managing the full lifecycle of GitLab instances in your Kubernetes or Openshift container platforms.",
      "csv_name": "gitlab-operator-kubernetes.v0.14.1",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": false,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-12-02T01:11:45.771000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.8",
      "organization": "certified-operators",
      "package": "gitlab-operator-kubernetes",
      "provided_apis": [
        {
          "group": "apps.gitlab.com",
          "kind": "GitLab",
          "plural": "gitlabs",
          "version": "v1beta1"
        }
      ],
      "provider": "GitLab Inc",
      "related_images": [
        {
          "digest": "sha256:6ed2063bc7c808f10d96633aeca707a2c0c577e162e1e922160f5e7023c0fc22",
          "image": "registry.gitlab.com/gitlab-org/cloud-native/gitlab-operator@sha256:6ed2063bc7c808f10d96633aeca707a2c0c577e162e1e922160f5e7023c0fc22",
          "name": "cloud-native/gitlab-operator-6ed2063bc7c808f10d96633aeca707a2c0c577e162e1e922160f5e7023c0fc22-annotation"
        },
        {
          "digest": "sha256:6ed2063bc7c808f10d96633aeca707a2c0c577e162e1e922160f5e7023c0fc22",
          "image": "registry.gitlab.com/gitlab-org/cloud-native/gitlab-operator@sha256:6ed2063bc7c808f10d96633aeca707a2c0c577e162e1e922160f5e7023c0fc22",
          "name": "manager"
        },
        {
          "digest": "sha256:e10d1d982dd653db74ca87a1d1ad017bc5ef1aeb651bdea089debf16485b080b",
          "image": "gcr.io/kubebuilder/kube-rbac-proxy@sha256:e10d1d982dd653db74ca87a1d1ad017bc5ef1aeb651bdea089debf16485b080b",
          "name": "kube-rbac-proxy"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.8",
      "version": "0.14.1",
      "version_original": "0.14.1"
    },
    {
      "_id": "638950d27476f7ea6acb667b",
      "alm_examples": [
        {
          "api_version": "apps.gitlab.com/v1beta1",
          "kind": "GitLab",
          "metadata": {
            "name": "gitlab",
            "namespace": "gitlab-system"
          },
          "spec": {
            "chart": {
              "values": {
                "certmanager": {
                  "install": false
                },
                "global": {
                  "hosts": {
                    "domain": "example.com"
                  },
                  "ingress": {
                    "configureCertmanager": false,
                    "tls": {}
                  }
                }
              },
              "version": ""
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/gitlab/gitlab-operator-bundle@sha256:35fb9490e7775268920b574fcf013be898f76161bdf2b5332cbfb6f242b4bd30",
      "bundle_path_digest": "sha256:35fb9490e7775268920b574fcf013be898f76161bdf2b5332cbfb6f242b4bd30",
      "capabilities": [
        "Seamless Upgrades"
      ],
      "channel_name": "unstable",
      "creation_date": "2022-12-02T01:11:46.487000+00:00",
      "csv_description": "# Overview\n\nThe GitLab operator is responsible for managing the full lifecycle of GitLab instances in your Kubernetes or Openshift container platforms.\n\n[Documentation](https://docs.gitlab.com/charts/installation/operator.html)\n\nThe operator, while new and still actively being developed, aims to:\n- ease installation and configuration of GitLab instances\n- offer seamless upgrades from version to version\n\n## GitLab\n\nGitLab is a complete open-source DevOps platform, delivered as a single application, fundamentally changing the way Development, Security, and Ops teams collaborate and build software. From idea to production, GitLab helps teams improve cycle time from weeks to minutes, reduce development process costs and decrease time to market while increasing developer productivity.\n\nBuilt on Open Source, GitLab delivers new innovations and features on the same day of every month by leveraging contributions from a passionate, global community of thousands of developers and millions of users. Over 100,000 of the world\u2019s most demanding organizations trust GitLab to deliver great software at new speeds.\n\nIf you would like to enable advanced DevOps capabilities and activate enterprise features such as security, risk, and compliance capabilities, please contact our sales team to purchase an enterprise license.\n\n# Prerequisites\n\nPlease visit [Prerequisites](https://docs.gitlab.com/charts/installation/operator.html#prerequisites) section of GitLab Operator Documentation.\n\n## IngressClass\n\nCluster-wide `IngressClass` should be created prior to Operator setup, as OLM does not currently support this object type:\n\n```yaml\napiVersion: networking.k8s.io/v1\nkind: IngressClass\nmetadata:\n  # Ensure this value matches `spec.chart.values.global.ingress.class`\n  # in the GitLab CR on the next step.\n  name: gitlab-nginx\nspec:\n  controller: k8s.io/ingress-nginx\n```\n",
      "csv_display_name": "GitLab",
      "csv_metadata_description": "The GitLab operator is responsible for managing the full lifecycle of GitLab instances in your Kubernetes or Openshift container platforms.",
      "csv_name": "gitlab-operator-kubernetes.v0.14.1",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": false,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": false,
      "last_update_date": "2022-12-02T01:11:46.487000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.8",
      "organization": "certified-operators",
      "package": "gitlab-operator-kubernetes",
      "provided_apis": [
        {
          "group": "apps.gitlab.com",
          "kind": "GitLab",
          "plural": "gitlabs",
          "version": "v1beta1"
        }
      ],
      "provider": "GitLab Inc",
      "related_images": [
        {
          "digest": "sha256:6ed2063bc7c808f10d96633aeca707a2c0c577e162e1e922160f5e7023c0fc22",
          "image": "registry.gitlab.com/gitlab-org/cloud-native/gitlab-operator@sha256:6ed2063bc7c808f10d96633aeca707a2c0c577e162e1e922160f5e7023c0fc22",
          "name": "cloud-native/gitlab-operator-6ed2063bc7c808f10d96633aeca707a2c0c577e162e1e922160f5e7023c0fc22-annotation"
        },
        {
          "digest": "sha256:6ed2063bc7c808f10d96633aeca707a2c0c577e162e1e922160f5e7023c0fc22",
          "image": "registry.gitlab.com/gitlab-org/cloud-native/gitlab-operator@sha256:6ed2063bc7c808f10d96633aeca707a2c0c577e162e1e922160f5e7023c0fc22",
          "name": "manager"
        },
        {
          "digest": "sha256:e10d1d982dd653db74ca87a1d1ad017bc5ef1aeb651bdea089debf16485b080b",
          "image": "gcr.io/kubebuilder/kube-rbac-proxy@sha256:e10d1d982dd653db74ca87a1d1ad017bc5ef1aeb651bdea089debf16485b080b",
          "name": "kube-rbac-proxy"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.8",
      "version": "0.14.1",
      "version_original": "0.14.1"
    },
    {
      "_id": "638a1b1f366047db17bab698",
      "alm_examples": [
        {
          "api_version": "infoscale.veritas.com/v1",
          "kind": "InfoScaleCluster",
          "metadata": {
            "name": "infoscalecluster-dev"
          },
          "spec": {
            "clusterInfo": [
              {},
              {}
            ],
            "version": "8.0.200"
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/veritas-technologies/infoscale-sds-operator-bundle@sha256:d466dbaa6d949f82f6bc4b25ea3ea7bf78cfaf0aa282c04e7c07d63d9249cf1b",
      "bundle_path_digest": "sha256:d466dbaa6d949f82f6bc4b25ea3ea7bf78cfaf0aa282c04e7c07d63d9249cf1b",
      "capabilities": [
        "Seamless Upgrades"
      ],
      "channel_name": "stable",
      "creation_date": "2022-12-02T15:34:55.261000+00:00",
      "csv_description": "## InfoScale\u2122 SDS Operator\n\nInfoScale\u2122 SDS Operator manages the lifecycle of the InfoScale\u2122 cluster\n\n## Overview\n\n- Veritas InfoScale\u2122 delivers Infrastructure resiliency and persistent storage for your critical containerized applications for OpenShift\u00ae and Kubernetes Native deployments\n- Engineered to support stateful workloads generated for mission-critical containerized applications.\n\n---\n\n## Data Services & Benefits\n\n**1. Software-Defined Persistent Storage Volumes:** Enables customers to support multiple container application requirements leveraging existing SAN or DAS storage\n\n**2. CSI API Driver:** Facilitates static and dynamic provisioning for applications with RWX, RWO and ROX access modes\n\n**3. Life Cycle Management:** Enables automated deployment, configuration and upgrades of InfoScale Software-defined container images. Certified and Integrated with Red Hat OpenShift for a single-click deployment\n\n**4. Availability:** Provides scaling, mounting and/or movement of InfoScale persistent storage volumes on cluster nodes with minimal disruption\n\n**5. Data Integrity:** Prevents data corruption by allowing only the active cluster nodes to write to the volume. The I/O fencing feature recovers from cluster disruptions quickly by ensuring that application pods are moved to another node to continue normal operations\n\n**6. Point-in-Time Data Copies:** Create snapshots of Persistent Volumes for backup products, data analytics or forensic discovery and analysis\n\n**7. Disaster Recovery (DR) Tech Preview:** This DR feature provides the ability to test and validate disaster recovery capabilities by migrating Kubernetes cluster metadata and application components to peer cluster in case of a local or remote disaster\n\n---\n\n## Pre-requisites\n\n- [Please refer to pre-requisite section from official documentation](https://www.veritas.com/support/en_US/doc/151215298-151215302-0)\n\n\n## InfoScale Cluster custom resource\n\n```\nkind: InfoScaleCluster\nmetadata:\n  name:  < Infoscale Cluster Name >\n\nspec:\n  version: \"8.0.200\"\n\n  clusterInfo:\n  - nodeName: <Name of the first node>\n  ip:\n  - <Optional - First IP address of the first node >\n  - <Optional - Second IP address of the first node>\n  excludeDevice:\n  - <Optional - Device path of the disk on the node that you want to exclude from Infoscale disk group.>\n\n  - nodeName: <Name of the second node>\n  ip:\n  - <Optional - First IP address of the second node >\n  - <Optional - Second IP address of the second node>\n  excludeDevice:\n  - <Optional - Device path of the disk on the node that you want to exclude from Infoscale disk group.>\n\n  # You can add up to 16 nodes.\n\n  customImageRegistry: < Optional \u2013 Registry for Infoscale Container images>\n```\n\n#### Note\n\nYou can specify up to 16 worker nodes in CR. Although cluster configuration is allowed even with one Network Interface Card,\nVeritas recommends a minimum of two physical links for performance and High Availability (HA). Number of links for each network link must be same on all\nnodes. Optionally, you can enter node level IP addresses. If IP addresses are not provided, IP addresses of OpenShift cluster nodes are used.\n",
      "csv_display_name": "InfoScale\u2122 SDS Operator",
      "csv_metadata_description": "InfoScale\u2122 SDS Operator manages the lifecycle of the InfoScale\u2122 cluster",
      "csv_name": "infoscale-sds-operator.v8.0.201",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": false,
          "type": "OwnNamespace"
        },
        {
          "supported": false,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-12-02T15:34:55.261000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.9",
      "organization": "certified-operators",
      "package": "infoscale-sds-operator",
      "provided_apis": [
        {
          "group": "infoscale.veritas.com",
          "kind": "InfoScaleStoragePool",
          "plural": "infoscalestoragepools",
          "version": "v1"
        },
        {
          "group": "infoscale.veritas.com",
          "kind": "InfoScaleCluster",
          "plural": "infoscaleclusters",
          "version": "v1"
        }
      ],
      "provider": "Veritas Technologies LLC",
      "related_images": [
        {
          "digest": "sha256:90ec4c17e9262a4df4b2155621183ec549e96bfbf8a2b7dc5f5dcbf8f29d4de0",
          "image": "registry.connect.redhat.com/veritas-technologies/infoscale-sds-operator@sha256:90ec4c17e9262a4df4b2155621183ec549e96bfbf8a2b7dc5f5dcbf8f29d4de0",
          "name": "manager"
        },
        {
          "digest": "sha256:90ec4c17e9262a4df4b2155621183ec549e96bfbf8a2b7dc5f5dcbf8f29d4de0",
          "image": "registry.connect.redhat.com/veritas-technologies/infoscale-sds-operator@sha256:90ec4c17e9262a4df4b2155621183ec549e96bfbf8a2b7dc5f5dcbf8f29d4de0",
          "name": "pre-manager"
        },
        {
          "digest": "sha256:90ec4c17e9262a4df4b2155621183ec549e96bfbf8a2b7dc5f5dcbf8f29d4de0",
          "image": "registry.connect.redhat.com/veritas-technologies/infoscale-sds-operator@sha256:90ec4c17e9262a4df4b2155621183ec549e96bfbf8a2b7dc5f5dcbf8f29d4de0",
          "name": "infoscale-sds-operator-90ec4c17e9262a4df4b2155621183ec549e96bfbf8a2b7dc5f5dcbf8f29d4de0-annotation"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.9",
      "version": "8.0.201",
      "version_original": "8.0.201"
    },
    {
      "_id": "638a1bc5366047db17bab811",
      "alm_examples": [
        {
          "api_version": "infoscale.veritas.com/v1",
          "kind": "InfoScaleCluster",
          "metadata": {
            "name": "infoscalecluster-dev"
          },
          "spec": {
            "clusterInfo": [
              {},
              {}
            ],
            "version": "8.0.200"
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/veritas-technologies/infoscale-sds-operator-bundle@sha256:d466dbaa6d949f82f6bc4b25ea3ea7bf78cfaf0aa282c04e7c07d63d9249cf1b",
      "bundle_path_digest": "sha256:d466dbaa6d949f82f6bc4b25ea3ea7bf78cfaf0aa282c04e7c07d63d9249cf1b",
      "capabilities": [
        "Seamless Upgrades"
      ],
      "channel_name": "stable",
      "creation_date": "2022-12-02T15:37:41.592000+00:00",
      "csv_description": "## InfoScale\u2122 SDS Operator\n\nInfoScale\u2122 SDS Operator manages the lifecycle of the InfoScale\u2122 cluster\n\n## Overview\n\n- Veritas InfoScale\u2122 delivers Infrastructure resiliency and persistent storage for your critical containerized applications for OpenShift\u00ae and Kubernetes Native deployments\n- Engineered to support stateful workloads generated for mission-critical containerized applications.\n\n---\n\n## Data Services & Benefits\n\n**1. Software-Defined Persistent Storage Volumes:** Enables customers to support multiple container application requirements leveraging existing SAN or DAS storage\n\n**2. CSI API Driver:** Facilitates static and dynamic provisioning for applications with RWX, RWO and ROX access modes\n\n**3. Life Cycle Management:** Enables automated deployment, configuration and upgrades of InfoScale Software-defined container images. Certified and Integrated with Red Hat OpenShift for a single-click deployment\n\n**4. Availability:** Provides scaling, mounting and/or movement of InfoScale persistent storage volumes on cluster nodes with minimal disruption\n\n**5. Data Integrity:** Prevents data corruption by allowing only the active cluster nodes to write to the volume. The I/O fencing feature recovers from cluster disruptions quickly by ensuring that application pods are moved to another node to continue normal operations\n\n**6. Point-in-Time Data Copies:** Create snapshots of Persistent Volumes for backup products, data analytics or forensic discovery and analysis\n\n**7. Disaster Recovery (DR) Tech Preview:** This DR feature provides the ability to test and validate disaster recovery capabilities by migrating Kubernetes cluster metadata and application components to peer cluster in case of a local or remote disaster\n\n---\n\n## Pre-requisites\n\n- [Please refer to pre-requisite section from official documentation](https://www.veritas.com/support/en_US/doc/151215298-151215302-0)\n\n\n## InfoScale Cluster custom resource\n\n```\nkind: InfoScaleCluster\nmetadata:\n  name:  < Infoscale Cluster Name >\n\nspec:\n  version: \"8.0.200\"\n\n  clusterInfo:\n  - nodeName: <Name of the first node>\n  ip:\n  - <Optional - First IP address of the first node >\n  - <Optional - Second IP address of the first node>\n  excludeDevice:\n  - <Optional - Device path of the disk on the node that you want to exclude from Infoscale disk group.>\n\n  - nodeName: <Name of the second node>\n  ip:\n  - <Optional - First IP address of the second node >\n  - <Optional - Second IP address of the second node>\n  excludeDevice:\n  - <Optional - Device path of the disk on the node that you want to exclude from Infoscale disk group.>\n\n  # You can add up to 16 nodes.\n\n  customImageRegistry: < Optional \u2013 Registry for Infoscale Container images>\n```\n\n#### Note\n\nYou can specify up to 16 worker nodes in CR. Although cluster configuration is allowed even with one Network Interface Card,\nVeritas recommends a minimum of two physical links for performance and High Availability (HA). Number of links for each network link must be same on all\nnodes. Optionally, you can enter node level IP addresses. If IP addresses are not provided, IP addresses of OpenShift cluster nodes are used.\n",
      "csv_display_name": "InfoScale\u2122 SDS Operator",
      "csv_metadata_description": "InfoScale\u2122 SDS Operator manages the lifecycle of the InfoScale\u2122 cluster",
      "csv_name": "infoscale-sds-operator.v8.0.201",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": false,
          "type": "OwnNamespace"
        },
        {
          "supported": false,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-12-02T15:37:41.592000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.12",
      "organization": "certified-operators",
      "package": "infoscale-sds-operator",
      "provided_apis": [
        {
          "group": "infoscale.veritas.com",
          "kind": "InfoScaleCluster",
          "version": "v1"
        },
        {
          "group": "infoscale.veritas.com",
          "kind": "InfoScaleStoragePool",
          "version": "v1"
        }
      ],
      "provider": "Veritas Technologies LLC",
      "related_images": [
        {
          "digest": "sha256:90ec4c17e9262a4df4b2155621183ec549e96bfbf8a2b7dc5f5dcbf8f29d4de0",
          "image": "registry.connect.redhat.com/veritas-technologies/infoscale-sds-operator@sha256:90ec4c17e9262a4df4b2155621183ec549e96bfbf8a2b7dc5f5dcbf8f29d4de0",
          "name": "manager"
        },
        {
          "digest": "sha256:90ec4c17e9262a4df4b2155621183ec549e96bfbf8a2b7dc5f5dcbf8f29d4de0",
          "image": "registry.connect.redhat.com/veritas-technologies/infoscale-sds-operator@sha256:90ec4c17e9262a4df4b2155621183ec549e96bfbf8a2b7dc5f5dcbf8f29d4de0",
          "name": "pre-manager"
        },
        {
          "digest": "sha256:90ec4c17e9262a4df4b2155621183ec549e96bfbf8a2b7dc5f5dcbf8f29d4de0",
          "image": "registry.connect.redhat.com/veritas-technologies/infoscale-sds-operator@sha256:90ec4c17e9262a4df4b2155621183ec549e96bfbf8a2b7dc5f5dcbf8f29d4de0",
          "name": "infoscale-sds-operator-90ec4c17e9262a4df4b2155621183ec549e96bfbf8a2b7dc5f5dcbf8f29d4de0-annotation"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.12",
      "version": "8.0.201",
      "version_original": "8.0.201"
    },
    {
      "_id": "638a1c08366047db17bab8b2",
      "alm_examples": [
        {
          "api_version": "infoscale.veritas.com/v1",
          "kind": "InfoScaleCluster",
          "metadata": {
            "name": "infoscalecluster-dev"
          },
          "spec": {
            "clusterInfo": [
              {},
              {}
            ],
            "version": "8.0.200"
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/veritas-technologies/infoscale-sds-operator-bundle@sha256:d466dbaa6d949f82f6bc4b25ea3ea7bf78cfaf0aa282c04e7c07d63d9249cf1b",
      "bundle_path_digest": "sha256:d466dbaa6d949f82f6bc4b25ea3ea7bf78cfaf0aa282c04e7c07d63d9249cf1b",
      "capabilities": [
        "Seamless Upgrades"
      ],
      "channel_name": "stable",
      "creation_date": "2022-12-02T15:38:48.149000+00:00",
      "csv_description": "## InfoScale\u2122 SDS Operator\n\nInfoScale\u2122 SDS Operator manages the lifecycle of the InfoScale\u2122 cluster\n\n## Overview\n\n- Veritas InfoScale\u2122 delivers Infrastructure resiliency and persistent storage for your critical containerized applications for OpenShift\u00ae and Kubernetes Native deployments\n- Engineered to support stateful workloads generated for mission-critical containerized applications.\n\n---\n\n## Data Services & Benefits\n\n**1. Software-Defined Persistent Storage Volumes:** Enables customers to support multiple container application requirements leveraging existing SAN or DAS storage\n\n**2. CSI API Driver:** Facilitates static and dynamic provisioning for applications with RWX, RWO and ROX access modes\n\n**3. Life Cycle Management:** Enables automated deployment, configuration and upgrades of InfoScale Software-defined container images. Certified and Integrated with Red Hat OpenShift for a single-click deployment\n\n**4. Availability:** Provides scaling, mounting and/or movement of InfoScale persistent storage volumes on cluster nodes with minimal disruption\n\n**5. Data Integrity:** Prevents data corruption by allowing only the active cluster nodes to write to the volume. The I/O fencing feature recovers from cluster disruptions quickly by ensuring that application pods are moved to another node to continue normal operations\n\n**6. Point-in-Time Data Copies:** Create snapshots of Persistent Volumes for backup products, data analytics or forensic discovery and analysis\n\n**7. Disaster Recovery (DR) Tech Preview:** This DR feature provides the ability to test and validate disaster recovery capabilities by migrating Kubernetes cluster metadata and application components to peer cluster in case of a local or remote disaster\n\n---\n\n## Pre-requisites\n\n- [Please refer to pre-requisite section from official documentation](https://www.veritas.com/support/en_US/doc/151215298-151215302-0)\n\n\n## InfoScale Cluster custom resource\n\n```\nkind: InfoScaleCluster\nmetadata:\n  name:  < Infoscale Cluster Name >\n\nspec:\n  version: \"8.0.200\"\n\n  clusterInfo:\n  - nodeName: <Name of the first node>\n  ip:\n  - <Optional - First IP address of the first node >\n  - <Optional - Second IP address of the first node>\n  excludeDevice:\n  - <Optional - Device path of the disk on the node that you want to exclude from Infoscale disk group.>\n\n  - nodeName: <Name of the second node>\n  ip:\n  - <Optional - First IP address of the second node >\n  - <Optional - Second IP address of the second node>\n  excludeDevice:\n  - <Optional - Device path of the disk on the node that you want to exclude from Infoscale disk group.>\n\n  # You can add up to 16 nodes.\n\n  customImageRegistry: < Optional \u2013 Registry for Infoscale Container images>\n```\n\n#### Note\n\nYou can specify up to 16 worker nodes in CR. Although cluster configuration is allowed even with one Network Interface Card,\nVeritas recommends a minimum of two physical links for performance and High Availability (HA). Number of links for each network link must be same on all\nnodes. Optionally, you can enter node level IP addresses. If IP addresses are not provided, IP addresses of OpenShift cluster nodes are used.\n",
      "csv_display_name": "InfoScale\u2122 SDS Operator",
      "csv_metadata_description": "InfoScale\u2122 SDS Operator manages the lifecycle of the InfoScale\u2122 cluster",
      "csv_name": "infoscale-sds-operator.v8.0.201",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": false,
          "type": "OwnNamespace"
        },
        {
          "supported": false,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-12-02T15:38:48.149000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.11",
      "organization": "certified-operators",
      "package": "infoscale-sds-operator",
      "provided_apis": [
        {
          "group": "infoscale.veritas.com",
          "kind": "InfoScaleCluster",
          "version": "v1"
        },
        {
          "group": "infoscale.veritas.com",
          "kind": "InfoScaleStoragePool",
          "version": "v1"
        }
      ],
      "provider": "Veritas Technologies LLC",
      "related_images": [
        {
          "digest": "sha256:90ec4c17e9262a4df4b2155621183ec549e96bfbf8a2b7dc5f5dcbf8f29d4de0",
          "image": "registry.connect.redhat.com/veritas-technologies/infoscale-sds-operator@sha256:90ec4c17e9262a4df4b2155621183ec549e96bfbf8a2b7dc5f5dcbf8f29d4de0",
          "name": "manager"
        },
        {
          "digest": "sha256:90ec4c17e9262a4df4b2155621183ec549e96bfbf8a2b7dc5f5dcbf8f29d4de0",
          "image": "registry.connect.redhat.com/veritas-technologies/infoscale-sds-operator@sha256:90ec4c17e9262a4df4b2155621183ec549e96bfbf8a2b7dc5f5dcbf8f29d4de0",
          "name": "pre-manager"
        },
        {
          "digest": "sha256:90ec4c17e9262a4df4b2155621183ec549e96bfbf8a2b7dc5f5dcbf8f29d4de0",
          "image": "registry.connect.redhat.com/veritas-technologies/infoscale-sds-operator@sha256:90ec4c17e9262a4df4b2155621183ec549e96bfbf8a2b7dc5f5dcbf8f29d4de0",
          "name": "infoscale-sds-operator-90ec4c17e9262a4df4b2155621183ec549e96bfbf8a2b7dc5f5dcbf8f29d4de0-annotation"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.11",
      "version": "8.0.201",
      "version_original": "8.0.201"
    },
    {
      "_id": "638a1c116f67ca671179f59d",
      "alm_examples": [
        {
          "api_version": "infoscale.veritas.com/v1",
          "kind": "InfoScaleCluster",
          "metadata": {
            "name": "infoscalecluster-dev"
          },
          "spec": {
            "clusterInfo": [
              {},
              {}
            ],
            "version": "8.0.200"
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/veritas-technologies/infoscale-sds-operator-bundle@sha256:d466dbaa6d949f82f6bc4b25ea3ea7bf78cfaf0aa282c04e7c07d63d9249cf1b",
      "bundle_path_digest": "sha256:d466dbaa6d949f82f6bc4b25ea3ea7bf78cfaf0aa282c04e7c07d63d9249cf1b",
      "capabilities": [
        "Seamless Upgrades"
      ],
      "channel_name": "stable",
      "creation_date": "2022-12-02T15:38:57.446000+00:00",
      "csv_description": "## InfoScale\u2122 SDS Operator\n\nInfoScale\u2122 SDS Operator manages the lifecycle of the InfoScale\u2122 cluster\n\n## Overview\n\n- Veritas InfoScale\u2122 delivers Infrastructure resiliency and persistent storage for your critical containerized applications for OpenShift\u00ae and Kubernetes Native deployments\n- Engineered to support stateful workloads generated for mission-critical containerized applications.\n\n---\n\n## Data Services & Benefits\n\n**1. Software-Defined Persistent Storage Volumes:** Enables customers to support multiple container application requirements leveraging existing SAN or DAS storage\n\n**2. CSI API Driver:** Facilitates static and dynamic provisioning for applications with RWX, RWO and ROX access modes\n\n**3. Life Cycle Management:** Enables automated deployment, configuration and upgrades of InfoScale Software-defined container images. Certified and Integrated with Red Hat OpenShift for a single-click deployment\n\n**4. Availability:** Provides scaling, mounting and/or movement of InfoScale persistent storage volumes on cluster nodes with minimal disruption\n\n**5. Data Integrity:** Prevents data corruption by allowing only the active cluster nodes to write to the volume. The I/O fencing feature recovers from cluster disruptions quickly by ensuring that application pods are moved to another node to continue normal operations\n\n**6. Point-in-Time Data Copies:** Create snapshots of Persistent Volumes for backup products, data analytics or forensic discovery and analysis\n\n**7. Disaster Recovery (DR) Tech Preview:** This DR feature provides the ability to test and validate disaster recovery capabilities by migrating Kubernetes cluster metadata and application components to peer cluster in case of a local or remote disaster\n\n---\n\n## Pre-requisites\n\n- [Please refer to pre-requisite section from official documentation](https://www.veritas.com/support/en_US/doc/151215298-151215302-0)\n\n\n## InfoScale Cluster custom resource\n\n```\nkind: InfoScaleCluster\nmetadata:\n  name:  < Infoscale Cluster Name >\n\nspec:\n  version: \"8.0.200\"\n\n  clusterInfo:\n  - nodeName: <Name of the first node>\n  ip:\n  - <Optional - First IP address of the first node >\n  - <Optional - Second IP address of the first node>\n  excludeDevice:\n  - <Optional - Device path of the disk on the node that you want to exclude from Infoscale disk group.>\n\n  - nodeName: <Name of the second node>\n  ip:\n  - <Optional - First IP address of the second node >\n  - <Optional - Second IP address of the second node>\n  excludeDevice:\n  - <Optional - Device path of the disk on the node that you want to exclude from Infoscale disk group.>\n\n  # You can add up to 16 nodes.\n\n  customImageRegistry: < Optional \u2013 Registry for Infoscale Container images>\n```\n\n#### Note\n\nYou can specify up to 16 worker nodes in CR. Although cluster configuration is allowed even with one Network Interface Card,\nVeritas recommends a minimum of two physical links for performance and High Availability (HA). Number of links for each network link must be same on all\nnodes. Optionally, you can enter node level IP addresses. If IP addresses are not provided, IP addresses of OpenShift cluster nodes are used.\n",
      "csv_display_name": "InfoScale\u2122 SDS Operator",
      "csv_metadata_description": "InfoScale\u2122 SDS Operator manages the lifecycle of the InfoScale\u2122 cluster",
      "csv_name": "infoscale-sds-operator.v8.0.201",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": false,
          "type": "OwnNamespace"
        },
        {
          "supported": false,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-12-02T15:38:57.446000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.10",
      "organization": "certified-operators",
      "package": "infoscale-sds-operator",
      "provided_apis": [
        {
          "group": "infoscale.veritas.com",
          "kind": "InfoScaleCluster",
          "plural": "infoscaleclusters",
          "version": "v1"
        },
        {
          "group": "infoscale.veritas.com",
          "kind": "InfoScaleStoragePool",
          "plural": "infoscalestoragepools",
          "version": "v1"
        }
      ],
      "provider": "Veritas Technologies LLC",
      "related_images": [
        {
          "digest": "sha256:90ec4c17e9262a4df4b2155621183ec549e96bfbf8a2b7dc5f5dcbf8f29d4de0",
          "image": "registry.connect.redhat.com/veritas-technologies/infoscale-sds-operator@sha256:90ec4c17e9262a4df4b2155621183ec549e96bfbf8a2b7dc5f5dcbf8f29d4de0",
          "name": "manager"
        },
        {
          "digest": "sha256:90ec4c17e9262a4df4b2155621183ec549e96bfbf8a2b7dc5f5dcbf8f29d4de0",
          "image": "registry.connect.redhat.com/veritas-technologies/infoscale-sds-operator@sha256:90ec4c17e9262a4df4b2155621183ec549e96bfbf8a2b7dc5f5dcbf8f29d4de0",
          "name": "pre-manager"
        },
        {
          "digest": "sha256:90ec4c17e9262a4df4b2155621183ec549e96bfbf8a2b7dc5f5dcbf8f29d4de0",
          "image": "registry.connect.redhat.com/veritas-technologies/infoscale-sds-operator@sha256:90ec4c17e9262a4df4b2155621183ec549e96bfbf8a2b7dc5f5dcbf8f29d4de0",
          "name": "infoscale-sds-operator-90ec4c17e9262a4df4b2155621183ec549e96bfbf8a2b7dc5f5dcbf8f29d4de0-annotation"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.10",
      "version": "8.0.201",
      "version_original": "8.0.201"
    },
    {
      "_id": "638a257e7476f7ea6acd41e5",
      "alm_examples": [
        {
          "api_version": "astra.netapp.io/v1",
          "kind": "AstraControlCenter",
          "metadata": {
            "name": "astra"
          },
          "spec": {
            "accountName": "Example",
            "additionalValues": {},
            "astraAddress": "astra.example.com",
            "astraResourcesScaler": "Default",
            "astraVersion": "22.11.0-82",
            "autoSupport": {
              "enrolled": true
            },
            "crds": {
              "externalCertManager": false,
              "externalTraefik": false
            },
            "email": "admin@example.com",
            "firstName": "SRE",
            "imageRegistry": {
              "name": "example.registry/astra",
              "secret": "astra-registry-cred"
            },
            "ingressType": "Generic",
            "lastName": "Admin",
            "storageClass": "ontap-gold",
            "volumeReclaimPolicy": "Retain"
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/netapp/acc-operator-bundle@sha256:b4aa651369a2b07710cbc2bb1cb6f5cf76e80e7b36b8646568dd5a09850219dc",
      "bundle_path_digest": "sha256:b4aa651369a2b07710cbc2bb1cb6f5cf76e80e7b36b8646568dd5a09850219dc",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "stable",
      "creation_date": "2022-12-02T16:19:10.809000+00:00",
      "csv_description": "Astra Control is an application-aware data management solution that manages, protects and moves data-rich Kubernetes workloads in both public clouds and on-premises.\n\nAstra Control enables data protection, disaster recovery, and migration for your Kubernetes workloads, leveraging NetApp's industry-leading data management technology\nfor snapshots, backups, replication and cloning.\n\n### How to deploy Astra Control\n\nRefer to [Installation Procedure](https://docs.netapp.com/us-en/astra-control-center/get-started/acc_operatorhub_install.html) to deploy Astra Control Center using\nthe Operator.\n\n### Documentation\n\nRefer to [Astra Control Center Documentation](https://docs.netapp.com/us-en/astra-control-center/index.html) to complete the setup and start managing applications.\n\nNOTE: The version listed under *Latest version* on this page might not reflect the actual version of NetApp Astra Control Center you are installing. The version in the file name of the Astra Control Center bundle that you download from the NetApp Support Site is the version of Astra Control Center that will be installed.\n",
      "csv_display_name": "netapp-acc-operator",
      "csv_metadata_description": "Install, configure and monitor Astra Control Center",
      "csv_name": "acc-operator.v22.11.0",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": false,
          "type": "OwnNamespace"
        },
        {
          "supported": false,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-12-02T16:19:10.809000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.9",
      "organization": "certified-operators",
      "package": "acc-operator",
      "provided_apis": [
        {
          "group": "astra.netapp.io",
          "kind": "AstraControlCenter",
          "plural": "astracontrolcenters",
          "version": "v1"
        }
      ],
      "provider": "NetApp",
      "related_images": [
        {
          "digest": "sha256:7a8b4b60345cff84089835a561b4b65c5eccb4585283314c90ce2108c063c87d",
          "image": "registry.redhat.io/openshift4/ose-kube-rbac-proxy@sha256:7a8b4b60345cff84089835a561b4b65c5eccb4585283314c90ce2108c063c87d",
          "name": "ose-kube-rbac-proxy"
        },
        {
          "digest": "sha256:0fd63647be4157975b94cf8623c8926ab4b29d11b7b661075df4d6f315d5cb49",
          "image": "registry.connect.redhat.com/netapp/acc-operator-22-8-0@sha256:0fd63647be4157975b94cf8623c8926ab4b29d11b7b661075df4d6f315d5cb49",
          "name": "acc-operator"
        },
        {
          "digest": "sha256:7a8b4b60345cff84089835a561b4b65c5eccb4585283314c90ce2108c063c87d",
          "image": "registry.redhat.io/openshift4/ose-kube-rbac-proxy@sha256:7a8b4b60345cff84089835a561b4b65c5eccb4585283314c90ce2108c063c87d",
          "name": "kube-rbac-proxy"
        },
        {
          "digest": "sha256:0fd63647be4157975b94cf8623c8926ab4b29d11b7b661075df4d6f315d5cb49",
          "image": "registry.connect.redhat.com/netapp/acc-operator-22-8-0@sha256:0fd63647be4157975b94cf8623c8926ab4b29d11b7b661075df4d6f315d5cb49",
          "name": "manager"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.9",
      "version": "22.11.0",
      "version_original": "22.11.0"
    },
    {
      "_id": "638a269a7476f7ea6acd43c1",
      "alm_examples": [
        {
          "api_version": "astra.netapp.io/v1",
          "kind": "AstraControlCenter",
          "metadata": {
            "name": "astra"
          },
          "spec": {
            "accountName": "Example",
            "additionalValues": {},
            "astraAddress": "astra.example.com",
            "astraResourcesScaler": "Default",
            "astraVersion": "22.11.0-82",
            "autoSupport": {
              "enrolled": true
            },
            "crds": {
              "externalCertManager": false,
              "externalTraefik": false
            },
            "email": "admin@example.com",
            "firstName": "SRE",
            "imageRegistry": {
              "name": "example.registry/astra",
              "secret": "astra-registry-cred"
            },
            "ingressType": "Generic",
            "lastName": "Admin",
            "storageClass": "ontap-gold",
            "volumeReclaimPolicy": "Retain"
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/netapp/acc-operator-bundle@sha256:b4aa651369a2b07710cbc2bb1cb6f5cf76e80e7b36b8646568dd5a09850219dc",
      "bundle_path_digest": "sha256:b4aa651369a2b07710cbc2bb1cb6f5cf76e80e7b36b8646568dd5a09850219dc",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "stable",
      "creation_date": "2022-12-02T16:23:54.450000+00:00",
      "csv_description": "Astra Control is an application-aware data management solution that manages, protects and moves data-rich Kubernetes workloads in both public clouds and on-premises.\n\nAstra Control enables data protection, disaster recovery, and migration for your Kubernetes workloads, leveraging NetApp's industry-leading data management technology\nfor snapshots, backups, replication and cloning.\n\n### How to deploy Astra Control\n\nRefer to [Installation Procedure](https://docs.netapp.com/us-en/astra-control-center/get-started/acc_operatorhub_install.html) to deploy Astra Control Center using\nthe Operator.\n\n### Documentation\n\nRefer to [Astra Control Center Documentation](https://docs.netapp.com/us-en/astra-control-center/index.html) to complete the setup and start managing applications.\n\nNOTE: The version listed under *Latest version* on this page might not reflect the actual version of NetApp Astra Control Center you are installing. The version in the file name of the Astra Control Center bundle that you download from the NetApp Support Site is the version of Astra Control Center that will be installed.\n",
      "csv_display_name": "netapp-acc-operator",
      "csv_metadata_description": "Install, configure and monitor Astra Control Center",
      "csv_name": "acc-operator.v22.11.0",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": false,
          "type": "OwnNamespace"
        },
        {
          "supported": false,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-12-02T16:23:54.450000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.11",
      "organization": "certified-operators",
      "package": "acc-operator",
      "provided_apis": [
        {
          "group": "astra.netapp.io",
          "kind": "AstraControlCenter",
          "version": "v1"
        }
      ],
      "provider": "NetApp",
      "related_images": [
        {
          "digest": "sha256:7a8b4b60345cff84089835a561b4b65c5eccb4585283314c90ce2108c063c87d",
          "image": "registry.redhat.io/openshift4/ose-kube-rbac-proxy@sha256:7a8b4b60345cff84089835a561b4b65c5eccb4585283314c90ce2108c063c87d",
          "name": "ose-kube-rbac-proxy"
        },
        {
          "digest": "sha256:0fd63647be4157975b94cf8623c8926ab4b29d11b7b661075df4d6f315d5cb49",
          "image": "registry.connect.redhat.com/netapp/acc-operator-22-8-0@sha256:0fd63647be4157975b94cf8623c8926ab4b29d11b7b661075df4d6f315d5cb49",
          "name": "acc-operator"
        },
        {
          "digest": "sha256:7a8b4b60345cff84089835a561b4b65c5eccb4585283314c90ce2108c063c87d",
          "image": "registry.redhat.io/openshift4/ose-kube-rbac-proxy@sha256:7a8b4b60345cff84089835a561b4b65c5eccb4585283314c90ce2108c063c87d",
          "name": "kube-rbac-proxy"
        },
        {
          "digest": "sha256:0fd63647be4157975b94cf8623c8926ab4b29d11b7b661075df4d6f315d5cb49",
          "image": "registry.connect.redhat.com/netapp/acc-operator-22-8-0@sha256:0fd63647be4157975b94cf8623c8926ab4b29d11b7b661075df4d6f315d5cb49",
          "name": "manager"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.11",
      "version": "22.11.0",
      "version_original": "22.11.0"
    },
    {
      "_id": "638a2761d6793f9ccb4ccb47",
      "alm_examples": [
        {
          "api_version": "astra.netapp.io/v1",
          "kind": "AstraControlCenter",
          "metadata": {
            "name": "astra"
          },
          "spec": {
            "accountName": "Example",
            "additionalValues": {},
            "astraAddress": "astra.example.com",
            "astraResourcesScaler": "Default",
            "astraVersion": "22.11.0-82",
            "autoSupport": {
              "enrolled": true
            },
            "crds": {
              "externalCertManager": false,
              "externalTraefik": false
            },
            "email": "admin@example.com",
            "firstName": "SRE",
            "imageRegistry": {
              "name": "example.registry/astra",
              "secret": "astra-registry-cred"
            },
            "ingressType": "Generic",
            "lastName": "Admin",
            "storageClass": "ontap-gold",
            "volumeReclaimPolicy": "Retain"
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/netapp/acc-operator-bundle@sha256:b4aa651369a2b07710cbc2bb1cb6f5cf76e80e7b36b8646568dd5a09850219dc",
      "bundle_path_digest": "sha256:b4aa651369a2b07710cbc2bb1cb6f5cf76e80e7b36b8646568dd5a09850219dc",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "stable",
      "creation_date": "2022-12-02T16:27:13.128000+00:00",
      "csv_description": "Astra Control is an application-aware data management solution that manages, protects and moves data-rich Kubernetes workloads in both public clouds and on-premises.\n\nAstra Control enables data protection, disaster recovery, and migration for your Kubernetes workloads, leveraging NetApp's industry-leading data management technology\nfor snapshots, backups, replication and cloning.\n\n### How to deploy Astra Control\n\nRefer to [Installation Procedure](https://docs.netapp.com/us-en/astra-control-center/get-started/acc_operatorhub_install.html) to deploy Astra Control Center using\nthe Operator.\n\n### Documentation\n\nRefer to [Astra Control Center Documentation](https://docs.netapp.com/us-en/astra-control-center/index.html) to complete the setup and start managing applications.\n\nNOTE: The version listed under *Latest version* on this page might not reflect the actual version of NetApp Astra Control Center you are installing. The version in the file name of the Astra Control Center bundle that you download from the NetApp Support Site is the version of Astra Control Center that will be installed.\n",
      "csv_display_name": "netapp-acc-operator",
      "csv_metadata_description": "Install, configure and monitor Astra Control Center",
      "csv_name": "acc-operator.v22.11.0",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": false,
          "type": "OwnNamespace"
        },
        {
          "supported": false,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-12-02T16:27:13.128000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.10",
      "organization": "certified-operators",
      "package": "acc-operator",
      "provided_apis": [
        {
          "group": "astra.netapp.io",
          "kind": "AstraControlCenter",
          "plural": "astracontrolcenters",
          "version": "v1"
        }
      ],
      "provider": "NetApp",
      "related_images": [
        {
          "digest": "sha256:7a8b4b60345cff84089835a561b4b65c5eccb4585283314c90ce2108c063c87d",
          "image": "registry.redhat.io/openshift4/ose-kube-rbac-proxy@sha256:7a8b4b60345cff84089835a561b4b65c5eccb4585283314c90ce2108c063c87d",
          "name": "ose-kube-rbac-proxy"
        },
        {
          "digest": "sha256:0fd63647be4157975b94cf8623c8926ab4b29d11b7b661075df4d6f315d5cb49",
          "image": "registry.connect.redhat.com/netapp/acc-operator-22-8-0@sha256:0fd63647be4157975b94cf8623c8926ab4b29d11b7b661075df4d6f315d5cb49",
          "name": "acc-operator"
        },
        {
          "digest": "sha256:7a8b4b60345cff84089835a561b4b65c5eccb4585283314c90ce2108c063c87d",
          "image": "registry.redhat.io/openshift4/ose-kube-rbac-proxy@sha256:7a8b4b60345cff84089835a561b4b65c5eccb4585283314c90ce2108c063c87d",
          "name": "kube-rbac-proxy"
        },
        {
          "digest": "sha256:0fd63647be4157975b94cf8623c8926ab4b29d11b7b661075df4d6f315d5cb49",
          "image": "registry.connect.redhat.com/netapp/acc-operator-22-8-0@sha256:0fd63647be4157975b94cf8623c8926ab4b29d11b7b661075df4d6f315d5cb49",
          "name": "manager"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.10",
      "version": "22.11.0",
      "version_original": "22.11.0"
    },
    {
      "_id": "638a27f7b0e069685e1ced09",
      "alm_examples": [
        {
          "api_version": "astra.netapp.io/v1",
          "kind": "AstraControlCenter",
          "metadata": {
            "name": "astra"
          },
          "spec": {
            "accountName": "Example",
            "additionalValues": {},
            "astraAddress": "astra.example.com",
            "astraResourcesScaler": "Default",
            "astraVersion": "22.11.0-82",
            "autoSupport": {
              "enrolled": true
            },
            "crds": {
              "externalCertManager": false,
              "externalTraefik": false
            },
            "email": "admin@example.com",
            "firstName": "SRE",
            "imageRegistry": {
              "name": "example.registry/astra",
              "secret": "astra-registry-cred"
            },
            "ingressType": "Generic",
            "lastName": "Admin",
            "storageClass": "ontap-gold",
            "volumeReclaimPolicy": "Retain"
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/netapp/acc-operator-bundle@sha256:b4aa651369a2b07710cbc2bb1cb6f5cf76e80e7b36b8646568dd5a09850219dc",
      "bundle_path_digest": "sha256:b4aa651369a2b07710cbc2bb1cb6f5cf76e80e7b36b8646568dd5a09850219dc",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "stable",
      "creation_date": "2022-12-02T16:29:43.420000+00:00",
      "csv_description": "Astra Control is an application-aware data management solution that manages, protects and moves data-rich Kubernetes workloads in both public clouds and on-premises.\n\nAstra Control enables data protection, disaster recovery, and migration for your Kubernetes workloads, leveraging NetApp's industry-leading data management technology\nfor snapshots, backups, replication and cloning.\n\n### How to deploy Astra Control\n\nRefer to [Installation Procedure](https://docs.netapp.com/us-en/astra-control-center/get-started/acc_operatorhub_install.html) to deploy Astra Control Center using\nthe Operator.\n\n### Documentation\n\nRefer to [Astra Control Center Documentation](https://docs.netapp.com/us-en/astra-control-center/index.html) to complete the setup and start managing applications.\n\nNOTE: The version listed under *Latest version* on this page might not reflect the actual version of NetApp Astra Control Center you are installing. The version in the file name of the Astra Control Center bundle that you download from the NetApp Support Site is the version of Astra Control Center that will be installed.\n",
      "csv_display_name": "netapp-acc-operator",
      "csv_metadata_description": "Install, configure and monitor Astra Control Center",
      "csv_name": "acc-operator.v22.11.0",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": false,
          "type": "OwnNamespace"
        },
        {
          "supported": false,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-12-02T16:29:43.420000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.8",
      "organization": "certified-operators",
      "package": "acc-operator",
      "provided_apis": [
        {
          "group": "astra.netapp.io",
          "kind": "AstraControlCenter",
          "plural": "astracontrolcenters",
          "version": "v1"
        }
      ],
      "provider": "NetApp",
      "related_images": [
        {
          "digest": "sha256:7a8b4b60345cff84089835a561b4b65c5eccb4585283314c90ce2108c063c87d",
          "image": "registry.redhat.io/openshift4/ose-kube-rbac-proxy@sha256:7a8b4b60345cff84089835a561b4b65c5eccb4585283314c90ce2108c063c87d",
          "name": "ose-kube-rbac-proxy"
        },
        {
          "digest": "sha256:0fd63647be4157975b94cf8623c8926ab4b29d11b7b661075df4d6f315d5cb49",
          "image": "registry.connect.redhat.com/netapp/acc-operator-22-8-0@sha256:0fd63647be4157975b94cf8623c8926ab4b29d11b7b661075df4d6f315d5cb49",
          "name": "acc-operator"
        },
        {
          "digest": "sha256:7a8b4b60345cff84089835a561b4b65c5eccb4585283314c90ce2108c063c87d",
          "image": "registry.redhat.io/openshift4/ose-kube-rbac-proxy@sha256:7a8b4b60345cff84089835a561b4b65c5eccb4585283314c90ce2108c063c87d",
          "name": "kube-rbac-proxy"
        },
        {
          "digest": "sha256:0fd63647be4157975b94cf8623c8926ab4b29d11b7b661075df4d6f315d5cb49",
          "image": "registry.connect.redhat.com/netapp/acc-operator-22-8-0@sha256:0fd63647be4157975b94cf8623c8926ab4b29d11b7b661075df4d6f315d5cb49",
          "name": "manager"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.8",
      "version": "22.11.0",
      "version_original": "22.11.0"
    },
    {
      "_id": "638db7d124d0beb3100d16d7",
      "alm_examples": [
        {
          "api_version": "app.redislabs.com/v1",
          "kind": "RedisEnterpriseCluster",
          "metadata": {
            "name": "rec"
          },
          "spec": {
            "bootstrapperImageSpec": {
              "repository": "registry.connect.redhat.com/redislabs/redis-enterprise-operator"
            },
            "nodes": 3,
            "persistentSpec": {
              "enabled": true
            },
            "redisEnterpriseImageSpec": {
              "imagePullPolicy": "IfNotPresent",
              "repository": "registry.connect.redhat.com/redislabs/redis-enterprise"
            },
            "redisEnterpriseNodeResources": {
              "limits": {
                "cpu": "4000m",
                "memory": "4Gi"
              },
              "requests": {
                "cpu": "4000m",
                "memory": "4Gi"
              }
            },
            "redisEnterpriseServicesRiggerImageSpec": {
              "repository": "registry.connect.redhat.com/redislabs/services-manager"
            },
            "uiServiceType": "ClusterIP",
            "username": "demo@redislabs.com"
          }
        },
        {
          "api_version": "app.redislabs.com/v1alpha1",
          "kind": "RedisEnterpriseDatabase",
          "metadata": {
            "name": "redb"
          },
          "spec": {
            "redisEnterpriseCluster": {
              "name": "rec"
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [
          "disconnected"
        ],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/redislabs/redis-enterprise-operator-bundle@sha256:b9fe65e0fd84903b1b9256395a204990dcc0acf8a1759e6b6694e7c450377837",
      "bundle_path_digest": "sha256:b9fe65e0fd84903b1b9256395a204990dcc0acf8a1759e6b6694e7c450377837",
      "capabilities": [
        "Deep Insights"
      ],
      "channel_name": "6.2.18",
      "creation_date": "2022-12-05T09:20:17.617000+00:00",
      "csv_description": "Redis Enterprise Software is enterprise grade, distributed, in-memory NoSQL database server, fully compatible with open source Redis by Redis.\nRedis Enterprise Software extends open source Redis and delivers stable high performance, zero-downtime linear scaling and high availability, with significant operational savings.\nRedis Enterprise provides a flexible and cost-effective data platform so developers can focus on rapid, high-quality development for sophisticated use cases that meet the needs of a modern digital enterprise. With Redis Enterprise, you can:\n* Enjoy high performance and record-setting low latencies with minimal operational overhead\n* Develop highly scalable microservices-based and Kubernetes-orchestrated applications\n* Use versatile data structures for a variety of use cases, such as high-speed transactions, user session management, real-time analytics, caching and many more\n* Leverage enterprise-grade operational controls for high availability, security and seamless scale\n* Automate operational best practices for deploying and managing the Redis Enterprise data platform with built-in Kubernetes Operator support\n* Deploy, manage and move applications to and from any cloud for seamless portability\n\n### Core capabilities\n* **Speed application development and time to market** - Unburden developers from the complexity of infrastructure operations and empower them to manage data with maximum flexibility. This helps them build modern applications quickly, make changes rapidly and support a variety of different data structures, models, relationships and use cases.\n* **Ensure business continuity with always-on data service** - Maintain service uptime and ensure failsafe high availability, instant failover and automatic recovery to protect your applications against unplanned downtime, outages and data loss.\n* **Design for performance and unmatched user experience** - Deliver the highest level of responsiveness and interactive customer experiences with best-in-class exceptional linear scaling high performance through a shared-nothing architecture and in-memory processing.\n* **Simplify operations with automated lifecycle management and layered orchestration** - Automate database provisioning, management and maintenance, and abstract away the complexities of high availability, seamless scale and zero-downtime upgrades with built-in support for Kubernetes and Operators.\n* **Enjoy multi-level robust security** - Provide granular control to meet self-imposed and regulatory compliance regulations and standards with built-in finegrained security controls and data encryption.\n* **Gain platform independence with flexible deployment options** - Ensure portability with multiple deployment options across any cloud of choice for multicloud and hybrid deployments and on-premises.\n* **Ensure success** - Leverage 24x7 enterprise-grade support backed by expertise in managing and scaling hundreds of thousands of Redis databases for thousands of enterprise customers worldwide.\n* **Future proof investments** - Use the most popular database amongst developers, built on open source innovation and entrenched firmly in the application landscape with over 1B downloads on Docker. Redis preserves your investment for years with easy extensibility and versatility through its Modules and support for over 50 different programming languages\n\n#### Prerequisites\nA minimum of 3 nodes which support the following requirements:\n* RAM - At least 3GB with 4GB recommended\n* Persistent Storage - At least 10GB of free space with 20GB recommended.\n* A kubernetes version of 1.9 or higher\n\n#### Recommendations/known issues\n*  Changing the name of the Redis Enterprise Cluster in the spec from the\n  default requires running commands to grant permissions to the pods:\n\n  Operator service account:\n    > *oc adm policy add-scc-to-user redis-enterprise-scc system:serviceaccount:my-project:redis-enterprise-operator*<br>\n\n    Redis Enterprise Cluster service account:\n    > *oc adm policy add-scc-to-user redis-enterprise-scc system:serviceaccount:my-project:<name of Redis Enterprise Cluster>*\n\n    Replace my-project with the actual name of the project/namespace. Replace <name of Redis Enterprise Cluster> with the name of the Redis Enterprise Cluster.\n\n* When creating a Redis Enterprise Cluster without specifying in its spec a `persistentSpec` with explicit `storageClassName`,\nthe default Storage Class of the Kubernetes cluster will be used to dynamically provision storage for the Redis Enterprise Cluster's nodes.\nHowever, if the Kubernetes cluster doesn't have a default Storage Class configured\nand no specific Storage Class was specified in the Redis Enterprise Cluster Spec, the installation of the cluster could fail.\nTo avoid such scenario in cases where a specific Storage Class isn't specified, make sure the Kubernetes Cluster has a default Storage Cluster configured.\nThis can be done by running a `kubectl get storageclass` command and see if one of the Storage Classes' names contains a `(default)` mark.",
      "csv_display_name": "Redis Enterprise Operator",
      "csv_metadata_description": "",
      "csv_name": "redis-enterprise-operator.v6.2.18-3a",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": false,
      "last_update_date": "2022-12-05T09:20:17.617000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.10",
      "organization": "certified-operators",
      "package": "redis-enterprise-operator-cert",
      "provided_apis": [
        {
          "group": "app.redislabs.com",
          "kind": "RedisEnterpriseCluster",
          "plural": "redisenterpriseclusters",
          "version": "v1"
        },
        {
          "group": "app.redislabs.com",
          "kind": "RedisEnterpriseCluster",
          "plural": "redisenterpriseclusters",
          "version": "v1alpha1"
        },
        {
          "group": "app.redislabs.com",
          "kind": "RedisEnterpriseDatabase",
          "plural": "redisenterprisedatabases",
          "version": "v1alpha1"
        }
      ],
      "provider": "Redis",
      "related_images": [
        {
          "digest": "sha256:afd2c3dd1e432c410af77202e9ce262fa69f4734357480c292db73f4608ec4ec",
          "image": "registry.connect.redhat.com/redislabs/redis-enterprise-operator@sha256:afd2c3dd1e432c410af77202e9ce262fa69f4734357480c292db73f4608ec4ec",
          "name": "redis-enterprise-operator"
        },
        {
          "digest": "sha256:afd2c3dd1e432c410af77202e9ce262fa69f4734357480c292db73f4608ec4ec",
          "image": "registry.connect.redhat.com/redislabs/redis-enterprise-operator@sha256:afd2c3dd1e432c410af77202e9ce262fa69f4734357480c292db73f4608ec4ec",
          "name": "admission-image"
        },
        {
          "digest": "sha256:afd2c3dd1e432c410af77202e9ce262fa69f4734357480c292db73f4608ec4ec",
          "image": "registry.connect.redhat.com/redislabs/redis-enterprise-operator@sha256:afd2c3dd1e432c410af77202e9ce262fa69f4734357480c292db73f4608ec4ec",
          "name": "bootstrapper-image"
        },
        {
          "digest": "sha256:1c20d9f1dc847db43aea55832caef6e497c1971d2dbcec4e8421092e469907e9",
          "image": "registry.connect.redhat.com/redislabs/services-manager@sha256:1c20d9f1dc847db43aea55832caef6e497c1971d2dbcec4e8421092e469907e9",
          "name": "k8s-controller-image"
        },
        {
          "digest": "sha256:5f83e568da6b9ae9ca8dcd91b799efd52d4afe94eeb5b0471e82914b2a987d25",
          "image": "registry.connect.redhat.com/redislabs/redis-enterprise@sha256:5f83e568da6b9ae9ca8dcd91b799efd52d4afe94eeb5b0471e82914b2a987d25",
          "name": "rs-image"
        },
        {
          "digest": "sha256:afd2c3dd1e432c410af77202e9ce262fa69f4734357480c292db73f4608ec4ec",
          "image": "registry.connect.redhat.com/redislabs/redis-enterprise-operator@sha256:afd2c3dd1e432c410af77202e9ce262fa69f4734357480c292db73f4608ec4ec",
          "name": "redis-enterprise-operator-afd2c3dd1e432c410af77202e9ce262fa69f4734357480c292db73f4608ec4ec-annotation"
        },
        {
          "digest": "sha256:afd2c3dd1e432c410af77202e9ce262fa69f4734357480c292db73f4608ec4ec",
          "image": "registry.connect.redhat.com/redislabs/redis-enterprise-operator@sha256:afd2c3dd1e432c410af77202e9ce262fa69f4734357480c292db73f4608ec4ec",
          "name": "admission"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.10",
      "version": "6.2.18-3a",
      "version_original": "6.2.18-3a"
    },
    {
      "_id": "638db7d3b0e069685e214f85",
      "alm_examples": [
        {
          "api_version": "app.redislabs.com/v1",
          "kind": "RedisEnterpriseCluster",
          "metadata": {
            "name": "rec"
          },
          "spec": {
            "bootstrapperImageSpec": {
              "repository": "registry.connect.redhat.com/redislabs/redis-enterprise-operator"
            },
            "nodes": 3,
            "persistentSpec": {
              "enabled": true
            },
            "redisEnterpriseImageSpec": {
              "imagePullPolicy": "IfNotPresent",
              "repository": "registry.connect.redhat.com/redislabs/redis-enterprise"
            },
            "redisEnterpriseNodeResources": {
              "limits": {
                "cpu": "4000m",
                "memory": "4Gi"
              },
              "requests": {
                "cpu": "4000m",
                "memory": "4Gi"
              }
            },
            "redisEnterpriseServicesRiggerImageSpec": {
              "repository": "registry.connect.redhat.com/redislabs/services-manager"
            },
            "uiServiceType": "ClusterIP",
            "username": "demo@redislabs.com"
          }
        },
        {
          "api_version": "app.redislabs.com/v1alpha1",
          "kind": "RedisEnterpriseDatabase",
          "metadata": {
            "name": "redb"
          },
          "spec": {
            "redisEnterpriseCluster": {
              "name": "rec"
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [
          "disconnected"
        ],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/redislabs/redis-enterprise-operator-bundle@sha256:b9fe65e0fd84903b1b9256395a204990dcc0acf8a1759e6b6694e7c450377837",
      "bundle_path_digest": "sha256:b9fe65e0fd84903b1b9256395a204990dcc0acf8a1759e6b6694e7c450377837",
      "capabilities": [
        "Deep Insights"
      ],
      "channel_name": "production",
      "creation_date": "2022-12-05T09:20:19.429000+00:00",
      "csv_description": "Redis Enterprise Software is enterprise grade, distributed, in-memory NoSQL database server, fully compatible with open source Redis by Redis.\nRedis Enterprise Software extends open source Redis and delivers stable high performance, zero-downtime linear scaling and high availability, with significant operational savings.\nRedis Enterprise provides a flexible and cost-effective data platform so developers can focus on rapid, high-quality development for sophisticated use cases that meet the needs of a modern digital enterprise. With Redis Enterprise, you can:\n* Enjoy high performance and record-setting low latencies with minimal operational overhead\n* Develop highly scalable microservices-based and Kubernetes-orchestrated applications\n* Use versatile data structures for a variety of use cases, such as high-speed transactions, user session management, real-time analytics, caching and many more\n* Leverage enterprise-grade operational controls for high availability, security and seamless scale\n* Automate operational best practices for deploying and managing the Redis Enterprise data platform with built-in Kubernetes Operator support\n* Deploy, manage and move applications to and from any cloud for seamless portability\n\n### Core capabilities\n* **Speed application development and time to market** - Unburden developers from the complexity of infrastructure operations and empower them to manage data with maximum flexibility. This helps them build modern applications quickly, make changes rapidly and support a variety of different data structures, models, relationships and use cases.\n* **Ensure business continuity with always-on data service** - Maintain service uptime and ensure failsafe high availability, instant failover and automatic recovery to protect your applications against unplanned downtime, outages and data loss.\n* **Design for performance and unmatched user experience** - Deliver the highest level of responsiveness and interactive customer experiences with best-in-class exceptional linear scaling high performance through a shared-nothing architecture and in-memory processing.\n* **Simplify operations with automated lifecycle management and layered orchestration** - Automate database provisioning, management and maintenance, and abstract away the complexities of high availability, seamless scale and zero-downtime upgrades with built-in support for Kubernetes and Operators.\n* **Enjoy multi-level robust security** - Provide granular control to meet self-imposed and regulatory compliance regulations and standards with built-in finegrained security controls and data encryption.\n* **Gain platform independence with flexible deployment options** - Ensure portability with multiple deployment options across any cloud of choice for multicloud and hybrid deployments and on-premises.\n* **Ensure success** - Leverage 24x7 enterprise-grade support backed by expertise in managing and scaling hundreds of thousands of Redis databases for thousands of enterprise customers worldwide.\n* **Future proof investments** - Use the most popular database amongst developers, built on open source innovation and entrenched firmly in the application landscape with over 1B downloads on Docker. Redis preserves your investment for years with easy extensibility and versatility through its Modules and support for over 50 different programming languages\n\n#### Prerequisites\nA minimum of 3 nodes which support the following requirements:\n* RAM - At least 3GB with 4GB recommended\n* Persistent Storage - At least 10GB of free space with 20GB recommended.\n* A kubernetes version of 1.9 or higher\n\n#### Recommendations/known issues\n*  Changing the name of the Redis Enterprise Cluster in the spec from the\n  default requires running commands to grant permissions to the pods:\n\n  Operator service account:\n    > *oc adm policy add-scc-to-user redis-enterprise-scc system:serviceaccount:my-project:redis-enterprise-operator*<br>\n\n    Redis Enterprise Cluster service account:\n    > *oc adm policy add-scc-to-user redis-enterprise-scc system:serviceaccount:my-project:<name of Redis Enterprise Cluster>*\n\n    Replace my-project with the actual name of the project/namespace. Replace <name of Redis Enterprise Cluster> with the name of the Redis Enterprise Cluster.\n\n* When creating a Redis Enterprise Cluster without specifying in its spec a `persistentSpec` with explicit `storageClassName`,\nthe default Storage Class of the Kubernetes cluster will be used to dynamically provision storage for the Redis Enterprise Cluster's nodes.\nHowever, if the Kubernetes cluster doesn't have a default Storage Class configured\nand no specific Storage Class was specified in the Redis Enterprise Cluster Spec, the installation of the cluster could fail.\nTo avoid such scenario in cases where a specific Storage Class isn't specified, make sure the Kubernetes Cluster has a default Storage Cluster configured.\nThis can be done by running a `kubectl get storageclass` command and see if one of the Storage Classes' names contains a `(default)` mark.",
      "csv_display_name": "Redis Enterprise Operator",
      "csv_metadata_description": "",
      "csv_name": "redis-enterprise-operator.v6.2.18-3a",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-12-05T09:20:19.429000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.10",
      "organization": "certified-operators",
      "package": "redis-enterprise-operator-cert",
      "provided_apis": [
        {
          "group": "app.redislabs.com",
          "kind": "RedisEnterpriseCluster",
          "plural": "redisenterpriseclusters",
          "version": "v1"
        },
        {
          "group": "app.redislabs.com",
          "kind": "RedisEnterpriseCluster",
          "plural": "redisenterpriseclusters",
          "version": "v1alpha1"
        },
        {
          "group": "app.redislabs.com",
          "kind": "RedisEnterpriseDatabase",
          "plural": "redisenterprisedatabases",
          "version": "v1alpha1"
        }
      ],
      "provider": "Redis",
      "related_images": [
        {
          "digest": "sha256:afd2c3dd1e432c410af77202e9ce262fa69f4734357480c292db73f4608ec4ec",
          "image": "registry.connect.redhat.com/redislabs/redis-enterprise-operator@sha256:afd2c3dd1e432c410af77202e9ce262fa69f4734357480c292db73f4608ec4ec",
          "name": "redis-enterprise-operator"
        },
        {
          "digest": "sha256:afd2c3dd1e432c410af77202e9ce262fa69f4734357480c292db73f4608ec4ec",
          "image": "registry.connect.redhat.com/redislabs/redis-enterprise-operator@sha256:afd2c3dd1e432c410af77202e9ce262fa69f4734357480c292db73f4608ec4ec",
          "name": "admission-image"
        },
        {
          "digest": "sha256:afd2c3dd1e432c410af77202e9ce262fa69f4734357480c292db73f4608ec4ec",
          "image": "registry.connect.redhat.com/redislabs/redis-enterprise-operator@sha256:afd2c3dd1e432c410af77202e9ce262fa69f4734357480c292db73f4608ec4ec",
          "name": "bootstrapper-image"
        },
        {
          "digest": "sha256:1c20d9f1dc847db43aea55832caef6e497c1971d2dbcec4e8421092e469907e9",
          "image": "registry.connect.redhat.com/redislabs/services-manager@sha256:1c20d9f1dc847db43aea55832caef6e497c1971d2dbcec4e8421092e469907e9",
          "name": "k8s-controller-image"
        },
        {
          "digest": "sha256:5f83e568da6b9ae9ca8dcd91b799efd52d4afe94eeb5b0471e82914b2a987d25",
          "image": "registry.connect.redhat.com/redislabs/redis-enterprise@sha256:5f83e568da6b9ae9ca8dcd91b799efd52d4afe94eeb5b0471e82914b2a987d25",
          "name": "rs-image"
        },
        {
          "digest": "sha256:afd2c3dd1e432c410af77202e9ce262fa69f4734357480c292db73f4608ec4ec",
          "image": "registry.connect.redhat.com/redislabs/redis-enterprise-operator@sha256:afd2c3dd1e432c410af77202e9ce262fa69f4734357480c292db73f4608ec4ec",
          "name": "redis-enterprise-operator-afd2c3dd1e432c410af77202e9ce262fa69f4734357480c292db73f4608ec4ec-annotation"
        },
        {
          "digest": "sha256:afd2c3dd1e432c410af77202e9ce262fa69f4734357480c292db73f4608ec4ec",
          "image": "registry.connect.redhat.com/redislabs/redis-enterprise-operator@sha256:afd2c3dd1e432c410af77202e9ce262fa69f4734357480c292db73f4608ec4ec",
          "name": "admission"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.10",
      "version": "6.2.18-3a",
      "version_original": "6.2.18-3a"
    },
    {
      "_id": "638db7e424d0beb3100d171b",
      "alm_examples": [
        {
          "api_version": "app.redislabs.com/v1",
          "kind": "RedisEnterpriseCluster",
          "metadata": {
            "name": "rec"
          },
          "spec": {
            "bootstrapperImageSpec": {
              "repository": "registry.connect.redhat.com/redislabs/redis-enterprise-operator"
            },
            "nodes": 3,
            "persistentSpec": {
              "enabled": true
            },
            "redisEnterpriseImageSpec": {
              "imagePullPolicy": "IfNotPresent",
              "repository": "registry.connect.redhat.com/redislabs/redis-enterprise"
            },
            "redisEnterpriseNodeResources": {
              "limits": {
                "cpu": "4000m",
                "memory": "4Gi"
              },
              "requests": {
                "cpu": "4000m",
                "memory": "4Gi"
              }
            },
            "redisEnterpriseServicesRiggerImageSpec": {
              "repository": "registry.connect.redhat.com/redislabs/services-manager"
            },
            "uiServiceType": "ClusterIP",
            "username": "demo@redislabs.com"
          }
        },
        {
          "api_version": "app.redislabs.com/v1alpha1",
          "kind": "RedisEnterpriseDatabase",
          "metadata": {
            "name": "redb"
          },
          "spec": {
            "redisEnterpriseCluster": {
              "name": "rec"
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [
          "disconnected"
        ],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/redislabs/redis-enterprise-operator-bundle@sha256:b9fe65e0fd84903b1b9256395a204990dcc0acf8a1759e6b6694e7c450377837",
      "bundle_path_digest": "sha256:b9fe65e0fd84903b1b9256395a204990dcc0acf8a1759e6b6694e7c450377837",
      "capabilities": [
        "Deep Insights"
      ],
      "channel_name": "6.2.18",
      "creation_date": "2022-12-05T09:20:36.629000+00:00",
      "csv_description": "Redis Enterprise Software is enterprise grade, distributed, in-memory NoSQL database server, fully compatible with open source Redis by Redis.\nRedis Enterprise Software extends open source Redis and delivers stable high performance, zero-downtime linear scaling and high availability, with significant operational savings.\nRedis Enterprise provides a flexible and cost-effective data platform so developers can focus on rapid, high-quality development for sophisticated use cases that meet the needs of a modern digital enterprise. With Redis Enterprise, you can:\n* Enjoy high performance and record-setting low latencies with minimal operational overhead\n* Develop highly scalable microservices-based and Kubernetes-orchestrated applications\n* Use versatile data structures for a variety of use cases, such as high-speed transactions, user session management, real-time analytics, caching and many more\n* Leverage enterprise-grade operational controls for high availability, security and seamless scale\n* Automate operational best practices for deploying and managing the Redis Enterprise data platform with built-in Kubernetes Operator support\n* Deploy, manage and move applications to and from any cloud for seamless portability\n\n### Core capabilities\n* **Speed application development and time to market** - Unburden developers from the complexity of infrastructure operations and empower them to manage data with maximum flexibility. This helps them build modern applications quickly, make changes rapidly and support a variety of different data structures, models, relationships and use cases.\n* **Ensure business continuity with always-on data service** - Maintain service uptime and ensure failsafe high availability, instant failover and automatic recovery to protect your applications against unplanned downtime, outages and data loss.\n* **Design for performance and unmatched user experience** - Deliver the highest level of responsiveness and interactive customer experiences with best-in-class exceptional linear scaling high performance through a shared-nothing architecture and in-memory processing.\n* **Simplify operations with automated lifecycle management and layered orchestration** - Automate database provisioning, management and maintenance, and abstract away the complexities of high availability, seamless scale and zero-downtime upgrades with built-in support for Kubernetes and Operators.\n* **Enjoy multi-level robust security** - Provide granular control to meet self-imposed and regulatory compliance regulations and standards with built-in finegrained security controls and data encryption.\n* **Gain platform independence with flexible deployment options** - Ensure portability with multiple deployment options across any cloud of choice for multicloud and hybrid deployments and on-premises.\n* **Ensure success** - Leverage 24x7 enterprise-grade support backed by expertise in managing and scaling hundreds of thousands of Redis databases for thousands of enterprise customers worldwide.\n* **Future proof investments** - Use the most popular database amongst developers, built on open source innovation and entrenched firmly in the application landscape with over 1B downloads on Docker. Redis preserves your investment for years with easy extensibility and versatility through its Modules and support for over 50 different programming languages\n\n#### Prerequisites\nA minimum of 3 nodes which support the following requirements:\n* RAM - At least 3GB with 4GB recommended\n* Persistent Storage - At least 10GB of free space with 20GB recommended.\n* A kubernetes version of 1.9 or higher\n\n#### Recommendations/known issues\n*  Changing the name of the Redis Enterprise Cluster in the spec from the\n  default requires running commands to grant permissions to the pods:\n\n  Operator service account:\n    > *oc adm policy add-scc-to-user redis-enterprise-scc system:serviceaccount:my-project:redis-enterprise-operator*<br>\n\n    Redis Enterprise Cluster service account:\n    > *oc adm policy add-scc-to-user redis-enterprise-scc system:serviceaccount:my-project:<name of Redis Enterprise Cluster>*\n\n    Replace my-project with the actual name of the project/namespace. Replace <name of Redis Enterprise Cluster> with the name of the Redis Enterprise Cluster.\n\n* When creating a Redis Enterprise Cluster without specifying in its spec a `persistentSpec` with explicit `storageClassName`,\nthe default Storage Class of the Kubernetes cluster will be used to dynamically provision storage for the Redis Enterprise Cluster's nodes.\nHowever, if the Kubernetes cluster doesn't have a default Storage Class configured\nand no specific Storage Class was specified in the Redis Enterprise Cluster Spec, the installation of the cluster could fail.\nTo avoid such scenario in cases where a specific Storage Class isn't specified, make sure the Kubernetes Cluster has a default Storage Cluster configured.\nThis can be done by running a `kubectl get storageclass` command and see if one of the Storage Classes' names contains a `(default)` mark.",
      "csv_display_name": "Redis Enterprise Operator",
      "csv_metadata_description": "",
      "csv_name": "redis-enterprise-operator.v6.2.18-3a",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": false,
      "last_update_date": "2022-12-05T09:20:36.629000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.9",
      "organization": "certified-operators",
      "package": "redis-enterprise-operator-cert",
      "provided_apis": [
        {
          "group": "app.redislabs.com",
          "kind": "RedisEnterpriseCluster",
          "plural": "redisenterpriseclusters",
          "version": "v1"
        },
        {
          "group": "app.redislabs.com",
          "kind": "RedisEnterpriseCluster",
          "plural": "redisenterpriseclusters",
          "version": "v1alpha1"
        },
        {
          "group": "app.redislabs.com",
          "kind": "RedisEnterpriseDatabase",
          "plural": "redisenterprisedatabases",
          "version": "v1alpha1"
        }
      ],
      "provider": "Redis",
      "related_images": [
        {
          "digest": "sha256:afd2c3dd1e432c410af77202e9ce262fa69f4734357480c292db73f4608ec4ec",
          "image": "registry.connect.redhat.com/redislabs/redis-enterprise-operator@sha256:afd2c3dd1e432c410af77202e9ce262fa69f4734357480c292db73f4608ec4ec",
          "name": "redis-enterprise-operator"
        },
        {
          "digest": "sha256:afd2c3dd1e432c410af77202e9ce262fa69f4734357480c292db73f4608ec4ec",
          "image": "registry.connect.redhat.com/redislabs/redis-enterprise-operator@sha256:afd2c3dd1e432c410af77202e9ce262fa69f4734357480c292db73f4608ec4ec",
          "name": "admission-image"
        },
        {
          "digest": "sha256:afd2c3dd1e432c410af77202e9ce262fa69f4734357480c292db73f4608ec4ec",
          "image": "registry.connect.redhat.com/redislabs/redis-enterprise-operator@sha256:afd2c3dd1e432c410af77202e9ce262fa69f4734357480c292db73f4608ec4ec",
          "name": "bootstrapper-image"
        },
        {
          "digest": "sha256:1c20d9f1dc847db43aea55832caef6e497c1971d2dbcec4e8421092e469907e9",
          "image": "registry.connect.redhat.com/redislabs/services-manager@sha256:1c20d9f1dc847db43aea55832caef6e497c1971d2dbcec4e8421092e469907e9",
          "name": "k8s-controller-image"
        },
        {
          "digest": "sha256:5f83e568da6b9ae9ca8dcd91b799efd52d4afe94eeb5b0471e82914b2a987d25",
          "image": "registry.connect.redhat.com/redislabs/redis-enterprise@sha256:5f83e568da6b9ae9ca8dcd91b799efd52d4afe94eeb5b0471e82914b2a987d25",
          "name": "rs-image"
        },
        {
          "digest": "sha256:afd2c3dd1e432c410af77202e9ce262fa69f4734357480c292db73f4608ec4ec",
          "image": "registry.connect.redhat.com/redislabs/redis-enterprise-operator@sha256:afd2c3dd1e432c410af77202e9ce262fa69f4734357480c292db73f4608ec4ec",
          "name": "redis-enterprise-operator-afd2c3dd1e432c410af77202e9ce262fa69f4734357480c292db73f4608ec4ec-annotation"
        },
        {
          "digest": "sha256:afd2c3dd1e432c410af77202e9ce262fa69f4734357480c292db73f4608ec4ec",
          "image": "registry.connect.redhat.com/redislabs/redis-enterprise-operator@sha256:afd2c3dd1e432c410af77202e9ce262fa69f4734357480c292db73f4608ec4ec",
          "name": "admission"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.9",
      "version": "6.2.18-3a",
      "version_original": "6.2.18-3a"
    },
    {
      "_id": "638db7e53df2cad99bdc86d6",
      "alm_examples": [
        {
          "api_version": "app.redislabs.com/v1",
          "kind": "RedisEnterpriseCluster",
          "metadata": {
            "name": "rec"
          },
          "spec": {
            "bootstrapperImageSpec": {
              "repository": "registry.connect.redhat.com/redislabs/redis-enterprise-operator"
            },
            "nodes": 3,
            "persistentSpec": {
              "enabled": true
            },
            "redisEnterpriseImageSpec": {
              "imagePullPolicy": "IfNotPresent",
              "repository": "registry.connect.redhat.com/redislabs/redis-enterprise"
            },
            "redisEnterpriseNodeResources": {
              "limits": {
                "cpu": "4000m",
                "memory": "4Gi"
              },
              "requests": {
                "cpu": "4000m",
                "memory": "4Gi"
              }
            },
            "redisEnterpriseServicesRiggerImageSpec": {
              "repository": "registry.connect.redhat.com/redislabs/services-manager"
            },
            "uiServiceType": "ClusterIP",
            "username": "demo@redislabs.com"
          }
        },
        {
          "api_version": "app.redislabs.com/v1alpha1",
          "kind": "RedisEnterpriseDatabase",
          "metadata": {
            "name": "redb"
          },
          "spec": {
            "redisEnterpriseCluster": {
              "name": "rec"
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [
          "disconnected"
        ],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/redislabs/redis-enterprise-operator-bundle@sha256:b9fe65e0fd84903b1b9256395a204990dcc0acf8a1759e6b6694e7c450377837",
      "bundle_path_digest": "sha256:b9fe65e0fd84903b1b9256395a204990dcc0acf8a1759e6b6694e7c450377837",
      "capabilities": [
        "Deep Insights"
      ],
      "channel_name": "production",
      "creation_date": "2022-12-05T09:20:37.156000+00:00",
      "csv_description": "Redis Enterprise Software is enterprise grade, distributed, in-memory NoSQL database server, fully compatible with open source Redis by Redis.\nRedis Enterprise Software extends open source Redis and delivers stable high performance, zero-downtime linear scaling and high availability, with significant operational savings.\nRedis Enterprise provides a flexible and cost-effective data platform so developers can focus on rapid, high-quality development for sophisticated use cases that meet the needs of a modern digital enterprise. With Redis Enterprise, you can:\n* Enjoy high performance and record-setting low latencies with minimal operational overhead\n* Develop highly scalable microservices-based and Kubernetes-orchestrated applications\n* Use versatile data structures for a variety of use cases, such as high-speed transactions, user session management, real-time analytics, caching and many more\n* Leverage enterprise-grade operational controls for high availability, security and seamless scale\n* Automate operational best practices for deploying and managing the Redis Enterprise data platform with built-in Kubernetes Operator support\n* Deploy, manage and move applications to and from any cloud for seamless portability\n\n### Core capabilities\n* **Speed application development and time to market** - Unburden developers from the complexity of infrastructure operations and empower them to manage data with maximum flexibility. This helps them build modern applications quickly, make changes rapidly and support a variety of different data structures, models, relationships and use cases.\n* **Ensure business continuity with always-on data service** - Maintain service uptime and ensure failsafe high availability, instant failover and automatic recovery to protect your applications against unplanned downtime, outages and data loss.\n* **Design for performance and unmatched user experience** - Deliver the highest level of responsiveness and interactive customer experiences with best-in-class exceptional linear scaling high performance through a shared-nothing architecture and in-memory processing.\n* **Simplify operations with automated lifecycle management and layered orchestration** - Automate database provisioning, management and maintenance, and abstract away the complexities of high availability, seamless scale and zero-downtime upgrades with built-in support for Kubernetes and Operators.\n* **Enjoy multi-level robust security** - Provide granular control to meet self-imposed and regulatory compliance regulations and standards with built-in finegrained security controls and data encryption.\n* **Gain platform independence with flexible deployment options** - Ensure portability with multiple deployment options across any cloud of choice for multicloud and hybrid deployments and on-premises.\n* **Ensure success** - Leverage 24x7 enterprise-grade support backed by expertise in managing and scaling hundreds of thousands of Redis databases for thousands of enterprise customers worldwide.\n* **Future proof investments** - Use the most popular database amongst developers, built on open source innovation and entrenched firmly in the application landscape with over 1B downloads on Docker. Redis preserves your investment for years with easy extensibility and versatility through its Modules and support for over 50 different programming languages\n\n#### Prerequisites\nA minimum of 3 nodes which support the following requirements:\n* RAM - At least 3GB with 4GB recommended\n* Persistent Storage - At least 10GB of free space with 20GB recommended.\n* A kubernetes version of 1.9 or higher\n\n#### Recommendations/known issues\n*  Changing the name of the Redis Enterprise Cluster in the spec from the\n  default requires running commands to grant permissions to the pods:\n\n  Operator service account:\n    > *oc adm policy add-scc-to-user redis-enterprise-scc system:serviceaccount:my-project:redis-enterprise-operator*<br>\n\n    Redis Enterprise Cluster service account:\n    > *oc adm policy add-scc-to-user redis-enterprise-scc system:serviceaccount:my-project:<name of Redis Enterprise Cluster>*\n\n    Replace my-project with the actual name of the project/namespace. Replace <name of Redis Enterprise Cluster> with the name of the Redis Enterprise Cluster.\n\n* When creating a Redis Enterprise Cluster without specifying in its spec a `persistentSpec` with explicit `storageClassName`,\nthe default Storage Class of the Kubernetes cluster will be used to dynamically provision storage for the Redis Enterprise Cluster's nodes.\nHowever, if the Kubernetes cluster doesn't have a default Storage Class configured\nand no specific Storage Class was specified in the Redis Enterprise Cluster Spec, the installation of the cluster could fail.\nTo avoid such scenario in cases where a specific Storage Class isn't specified, make sure the Kubernetes Cluster has a default Storage Cluster configured.\nThis can be done by running a `kubectl get storageclass` command and see if one of the Storage Classes' names contains a `(default)` mark.",
      "csv_display_name": "Redis Enterprise Operator",
      "csv_metadata_description": "",
      "csv_name": "redis-enterprise-operator.v6.2.18-3a",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-12-05T09:20:37.156000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.9",
      "organization": "certified-operators",
      "package": "redis-enterprise-operator-cert",
      "provided_apis": [
        {
          "group": "app.redislabs.com",
          "kind": "RedisEnterpriseCluster",
          "plural": "redisenterpriseclusters",
          "version": "v1"
        },
        {
          "group": "app.redislabs.com",
          "kind": "RedisEnterpriseCluster",
          "plural": "redisenterpriseclusters",
          "version": "v1alpha1"
        },
        {
          "group": "app.redislabs.com",
          "kind": "RedisEnterpriseDatabase",
          "plural": "redisenterprisedatabases",
          "version": "v1alpha1"
        }
      ],
      "provider": "Redis",
      "related_images": [
        {
          "digest": "sha256:afd2c3dd1e432c410af77202e9ce262fa69f4734357480c292db73f4608ec4ec",
          "image": "registry.connect.redhat.com/redislabs/redis-enterprise-operator@sha256:afd2c3dd1e432c410af77202e9ce262fa69f4734357480c292db73f4608ec4ec",
          "name": "redis-enterprise-operator"
        },
        {
          "digest": "sha256:afd2c3dd1e432c410af77202e9ce262fa69f4734357480c292db73f4608ec4ec",
          "image": "registry.connect.redhat.com/redislabs/redis-enterprise-operator@sha256:afd2c3dd1e432c410af77202e9ce262fa69f4734357480c292db73f4608ec4ec",
          "name": "admission-image"
        },
        {
          "digest": "sha256:afd2c3dd1e432c410af77202e9ce262fa69f4734357480c292db73f4608ec4ec",
          "image": "registry.connect.redhat.com/redislabs/redis-enterprise-operator@sha256:afd2c3dd1e432c410af77202e9ce262fa69f4734357480c292db73f4608ec4ec",
          "name": "bootstrapper-image"
        },
        {
          "digest": "sha256:1c20d9f1dc847db43aea55832caef6e497c1971d2dbcec4e8421092e469907e9",
          "image": "registry.connect.redhat.com/redislabs/services-manager@sha256:1c20d9f1dc847db43aea55832caef6e497c1971d2dbcec4e8421092e469907e9",
          "name": "k8s-controller-image"
        },
        {
          "digest": "sha256:5f83e568da6b9ae9ca8dcd91b799efd52d4afe94eeb5b0471e82914b2a987d25",
          "image": "registry.connect.redhat.com/redislabs/redis-enterprise@sha256:5f83e568da6b9ae9ca8dcd91b799efd52d4afe94eeb5b0471e82914b2a987d25",
          "name": "rs-image"
        },
        {
          "digest": "sha256:afd2c3dd1e432c410af77202e9ce262fa69f4734357480c292db73f4608ec4ec",
          "image": "registry.connect.redhat.com/redislabs/redis-enterprise-operator@sha256:afd2c3dd1e432c410af77202e9ce262fa69f4734357480c292db73f4608ec4ec",
          "name": "redis-enterprise-operator-afd2c3dd1e432c410af77202e9ce262fa69f4734357480c292db73f4608ec4ec-annotation"
        },
        {
          "digest": "sha256:afd2c3dd1e432c410af77202e9ce262fa69f4734357480c292db73f4608ec4ec",
          "image": "registry.connect.redhat.com/redislabs/redis-enterprise-operator@sha256:afd2c3dd1e432c410af77202e9ce262fa69f4734357480c292db73f4608ec4ec",
          "name": "admission"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.9",
      "version": "6.2.18-3a",
      "version_original": "6.2.18-3a"
    },
    {
      "_id": "638db91bb5dcf804fdd31bba",
      "alm_examples": [
        {
          "api_version": "app.redislabs.com/v1",
          "kind": "RedisEnterpriseCluster",
          "metadata": {
            "name": "rec"
          },
          "spec": {
            "bootstrapperImageSpec": {
              "repository": "registry.connect.redhat.com/redislabs/redis-enterprise-operator"
            },
            "nodes": 3,
            "persistentSpec": {
              "enabled": true
            },
            "redisEnterpriseImageSpec": {
              "imagePullPolicy": "IfNotPresent",
              "repository": "registry.connect.redhat.com/redislabs/redis-enterprise"
            },
            "redisEnterpriseNodeResources": {
              "limits": {
                "cpu": "4000m",
                "memory": "4Gi"
              },
              "requests": {
                "cpu": "4000m",
                "memory": "4Gi"
              }
            },
            "redisEnterpriseServicesRiggerImageSpec": {
              "repository": "registry.connect.redhat.com/redislabs/services-manager"
            },
            "uiServiceType": "ClusterIP",
            "username": "demo@redislabs.com"
          }
        },
        {
          "api_version": "app.redislabs.com/v1alpha1",
          "kind": "RedisEnterpriseDatabase",
          "metadata": {
            "name": "redb"
          },
          "spec": {
            "redisEnterpriseCluster": {
              "name": "rec"
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [
          "disconnected"
        ],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/redislabs/redis-enterprise-operator-bundle@sha256:b9fe65e0fd84903b1b9256395a204990dcc0acf8a1759e6b6694e7c450377837",
      "bundle_path_digest": "sha256:b9fe65e0fd84903b1b9256395a204990dcc0acf8a1759e6b6694e7c450377837",
      "capabilities": [
        "Deep Insights"
      ],
      "channel_name": "6.2.18",
      "creation_date": "2022-12-05T09:25:47.075000+00:00",
      "csv_description": "Redis Enterprise Software is enterprise grade, distributed, in-memory NoSQL database server, fully compatible with open source Redis by Redis.\nRedis Enterprise Software extends open source Redis and delivers stable high performance, zero-downtime linear scaling and high availability, with significant operational savings.\nRedis Enterprise provides a flexible and cost-effective data platform so developers can focus on rapid, high-quality development for sophisticated use cases that meet the needs of a modern digital enterprise. With Redis Enterprise, you can:\n* Enjoy high performance and record-setting low latencies with minimal operational overhead\n* Develop highly scalable microservices-based and Kubernetes-orchestrated applications\n* Use versatile data structures for a variety of use cases, such as high-speed transactions, user session management, real-time analytics, caching and many more\n* Leverage enterprise-grade operational controls for high availability, security and seamless scale\n* Automate operational best practices for deploying and managing the Redis Enterprise data platform with built-in Kubernetes Operator support\n* Deploy, manage and move applications to and from any cloud for seamless portability\n\n### Core capabilities\n* **Speed application development and time to market** - Unburden developers from the complexity of infrastructure operations and empower them to manage data with maximum flexibility. This helps them build modern applications quickly, make changes rapidly and support a variety of different data structures, models, relationships and use cases.\n* **Ensure business continuity with always-on data service** - Maintain service uptime and ensure failsafe high availability, instant failover and automatic recovery to protect your applications against unplanned downtime, outages and data loss.\n* **Design for performance and unmatched user experience** - Deliver the highest level of responsiveness and interactive customer experiences with best-in-class exceptional linear scaling high performance through a shared-nothing architecture and in-memory processing.\n* **Simplify operations with automated lifecycle management and layered orchestration** - Automate database provisioning, management and maintenance, and abstract away the complexities of high availability, seamless scale and zero-downtime upgrades with built-in support for Kubernetes and Operators.\n* **Enjoy multi-level robust security** - Provide granular control to meet self-imposed and regulatory compliance regulations and standards with built-in finegrained security controls and data encryption.\n* **Gain platform independence with flexible deployment options** - Ensure portability with multiple deployment options across any cloud of choice for multicloud and hybrid deployments and on-premises.\n* **Ensure success** - Leverage 24x7 enterprise-grade support backed by expertise in managing and scaling hundreds of thousands of Redis databases for thousands of enterprise customers worldwide.\n* **Future proof investments** - Use the most popular database amongst developers, built on open source innovation and entrenched firmly in the application landscape with over 1B downloads on Docker. Redis preserves your investment for years with easy extensibility and versatility through its Modules and support for over 50 different programming languages\n\n#### Prerequisites\nA minimum of 3 nodes which support the following requirements:\n* RAM - At least 3GB with 4GB recommended\n* Persistent Storage - At least 10GB of free space with 20GB recommended.\n* A kubernetes version of 1.9 or higher\n\n#### Recommendations/known issues\n*  Changing the name of the Redis Enterprise Cluster in the spec from the\n  default requires running commands to grant permissions to the pods:\n\n  Operator service account:\n    > *oc adm policy add-scc-to-user redis-enterprise-scc system:serviceaccount:my-project:redis-enterprise-operator*<br>\n\n    Redis Enterprise Cluster service account:\n    > *oc adm policy add-scc-to-user redis-enterprise-scc system:serviceaccount:my-project:<name of Redis Enterprise Cluster>*\n\n    Replace my-project with the actual name of the project/namespace. Replace <name of Redis Enterprise Cluster> with the name of the Redis Enterprise Cluster.\n\n* When creating a Redis Enterprise Cluster without specifying in its spec a `persistentSpec` with explicit `storageClassName`,\nthe default Storage Class of the Kubernetes cluster will be used to dynamically provision storage for the Redis Enterprise Cluster's nodes.\nHowever, if the Kubernetes cluster doesn't have a default Storage Class configured\nand no specific Storage Class was specified in the Redis Enterprise Cluster Spec, the installation of the cluster could fail.\nTo avoid such scenario in cases where a specific Storage Class isn't specified, make sure the Kubernetes Cluster has a default Storage Cluster configured.\nThis can be done by running a `kubectl get storageclass` command and see if one of the Storage Classes' names contains a `(default)` mark.",
      "csv_display_name": "Redis Enterprise Operator",
      "csv_metadata_description": "",
      "csv_name": "redis-enterprise-operator.v6.2.18-3a",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": false,
      "last_update_date": "2022-12-05T09:25:47.075000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.11",
      "organization": "certified-operators",
      "package": "redis-enterprise-operator-cert",
      "provided_apis": [
        {
          "group": "app.redislabs.com",
          "kind": "RedisEnterpriseCluster",
          "version": "v1"
        },
        {
          "group": "app.redislabs.com",
          "kind": "RedisEnterpriseCluster",
          "version": "v1alpha1"
        },
        {
          "group": "app.redislabs.com",
          "kind": "RedisEnterpriseDatabase",
          "version": "v1alpha1"
        }
      ],
      "provider": "Redis",
      "related_images": [
        {
          "digest": "sha256:afd2c3dd1e432c410af77202e9ce262fa69f4734357480c292db73f4608ec4ec",
          "image": "registry.connect.redhat.com/redislabs/redis-enterprise-operator@sha256:afd2c3dd1e432c410af77202e9ce262fa69f4734357480c292db73f4608ec4ec",
          "name": "redis-enterprise-operator"
        },
        {
          "digest": "sha256:afd2c3dd1e432c410af77202e9ce262fa69f4734357480c292db73f4608ec4ec",
          "image": "registry.connect.redhat.com/redislabs/redis-enterprise-operator@sha256:afd2c3dd1e432c410af77202e9ce262fa69f4734357480c292db73f4608ec4ec",
          "name": "admission-image"
        },
        {
          "digest": "sha256:afd2c3dd1e432c410af77202e9ce262fa69f4734357480c292db73f4608ec4ec",
          "image": "registry.connect.redhat.com/redislabs/redis-enterprise-operator@sha256:afd2c3dd1e432c410af77202e9ce262fa69f4734357480c292db73f4608ec4ec",
          "name": "bootstrapper-image"
        },
        {
          "digest": "sha256:1c20d9f1dc847db43aea55832caef6e497c1971d2dbcec4e8421092e469907e9",
          "image": "registry.connect.redhat.com/redislabs/services-manager@sha256:1c20d9f1dc847db43aea55832caef6e497c1971d2dbcec4e8421092e469907e9",
          "name": "k8s-controller-image"
        },
        {
          "digest": "sha256:5f83e568da6b9ae9ca8dcd91b799efd52d4afe94eeb5b0471e82914b2a987d25",
          "image": "registry.connect.redhat.com/redislabs/redis-enterprise@sha256:5f83e568da6b9ae9ca8dcd91b799efd52d4afe94eeb5b0471e82914b2a987d25",
          "name": "rs-image"
        },
        {
          "digest": "sha256:afd2c3dd1e432c410af77202e9ce262fa69f4734357480c292db73f4608ec4ec",
          "image": "registry.connect.redhat.com/redislabs/redis-enterprise-operator@sha256:afd2c3dd1e432c410af77202e9ce262fa69f4734357480c292db73f4608ec4ec",
          "name": "redis-enterprise-operator-afd2c3dd1e432c410af77202e9ce262fa69f4734357480c292db73f4608ec4ec-annotation"
        },
        {
          "digest": "sha256:afd2c3dd1e432c410af77202e9ce262fa69f4734357480c292db73f4608ec4ec",
          "image": "registry.connect.redhat.com/redislabs/redis-enterprise-operator@sha256:afd2c3dd1e432c410af77202e9ce262fa69f4734357480c292db73f4608ec4ec",
          "name": "admission"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.11",
      "version": "6.2.18-3a",
      "version_original": "6.2.18-3a"
    },
    {
      "_id": "638db91ed6793f9ccb5134cb",
      "alm_examples": [
        {
          "api_version": "app.redislabs.com/v1",
          "kind": "RedisEnterpriseCluster",
          "metadata": {
            "name": "rec"
          },
          "spec": {
            "bootstrapperImageSpec": {
              "repository": "registry.connect.redhat.com/redislabs/redis-enterprise-operator"
            },
            "nodes": 3,
            "persistentSpec": {
              "enabled": true
            },
            "redisEnterpriseImageSpec": {
              "imagePullPolicy": "IfNotPresent",
              "repository": "registry.connect.redhat.com/redislabs/redis-enterprise"
            },
            "redisEnterpriseNodeResources": {
              "limits": {
                "cpu": "4000m",
                "memory": "4Gi"
              },
              "requests": {
                "cpu": "4000m",
                "memory": "4Gi"
              }
            },
            "redisEnterpriseServicesRiggerImageSpec": {
              "repository": "registry.connect.redhat.com/redislabs/services-manager"
            },
            "uiServiceType": "ClusterIP",
            "username": "demo@redislabs.com"
          }
        },
        {
          "api_version": "app.redislabs.com/v1alpha1",
          "kind": "RedisEnterpriseDatabase",
          "metadata": {
            "name": "redb"
          },
          "spec": {
            "redisEnterpriseCluster": {
              "name": "rec"
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [
          "disconnected"
        ],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/redislabs/redis-enterprise-operator-bundle@sha256:b9fe65e0fd84903b1b9256395a204990dcc0acf8a1759e6b6694e7c450377837",
      "bundle_path_digest": "sha256:b9fe65e0fd84903b1b9256395a204990dcc0acf8a1759e6b6694e7c450377837",
      "capabilities": [
        "Deep Insights"
      ],
      "channel_name": "production",
      "creation_date": "2022-12-05T09:25:50.535000+00:00",
      "csv_description": "Redis Enterprise Software is enterprise grade, distributed, in-memory NoSQL database server, fully compatible with open source Redis by Redis.\nRedis Enterprise Software extends open source Redis and delivers stable high performance, zero-downtime linear scaling and high availability, with significant operational savings.\nRedis Enterprise provides a flexible and cost-effective data platform so developers can focus on rapid, high-quality development for sophisticated use cases that meet the needs of a modern digital enterprise. With Redis Enterprise, you can:\n* Enjoy high performance and record-setting low latencies with minimal operational overhead\n* Develop highly scalable microservices-based and Kubernetes-orchestrated applications\n* Use versatile data structures for a variety of use cases, such as high-speed transactions, user session management, real-time analytics, caching and many more\n* Leverage enterprise-grade operational controls for high availability, security and seamless scale\n* Automate operational best practices for deploying and managing the Redis Enterprise data platform with built-in Kubernetes Operator support\n* Deploy, manage and move applications to and from any cloud for seamless portability\n\n### Core capabilities\n* **Speed application development and time to market** - Unburden developers from the complexity of infrastructure operations and empower them to manage data with maximum flexibility. This helps them build modern applications quickly, make changes rapidly and support a variety of different data structures, models, relationships and use cases.\n* **Ensure business continuity with always-on data service** - Maintain service uptime and ensure failsafe high availability, instant failover and automatic recovery to protect your applications against unplanned downtime, outages and data loss.\n* **Design for performance and unmatched user experience** - Deliver the highest level of responsiveness and interactive customer experiences with best-in-class exceptional linear scaling high performance through a shared-nothing architecture and in-memory processing.\n* **Simplify operations with automated lifecycle management and layered orchestration** - Automate database provisioning, management and maintenance, and abstract away the complexities of high availability, seamless scale and zero-downtime upgrades with built-in support for Kubernetes and Operators.\n* **Enjoy multi-level robust security** - Provide granular control to meet self-imposed and regulatory compliance regulations and standards with built-in finegrained security controls and data encryption.\n* **Gain platform independence with flexible deployment options** - Ensure portability with multiple deployment options across any cloud of choice for multicloud and hybrid deployments and on-premises.\n* **Ensure success** - Leverage 24x7 enterprise-grade support backed by expertise in managing and scaling hundreds of thousands of Redis databases for thousands of enterprise customers worldwide.\n* **Future proof investments** - Use the most popular database amongst developers, built on open source innovation and entrenched firmly in the application landscape with over 1B downloads on Docker. Redis preserves your investment for years with easy extensibility and versatility through its Modules and support for over 50 different programming languages\n\n#### Prerequisites\nA minimum of 3 nodes which support the following requirements:\n* RAM - At least 3GB with 4GB recommended\n* Persistent Storage - At least 10GB of free space with 20GB recommended.\n* A kubernetes version of 1.9 or higher\n\n#### Recommendations/known issues\n*  Changing the name of the Redis Enterprise Cluster in the spec from the\n  default requires running commands to grant permissions to the pods:\n\n  Operator service account:\n    > *oc adm policy add-scc-to-user redis-enterprise-scc system:serviceaccount:my-project:redis-enterprise-operator*<br>\n\n    Redis Enterprise Cluster service account:\n    > *oc adm policy add-scc-to-user redis-enterprise-scc system:serviceaccount:my-project:<name of Redis Enterprise Cluster>*\n\n    Replace my-project with the actual name of the project/namespace. Replace <name of Redis Enterprise Cluster> with the name of the Redis Enterprise Cluster.\n\n* When creating a Redis Enterprise Cluster without specifying in its spec a `persistentSpec` with explicit `storageClassName`,\nthe default Storage Class of the Kubernetes cluster will be used to dynamically provision storage for the Redis Enterprise Cluster's nodes.\nHowever, if the Kubernetes cluster doesn't have a default Storage Class configured\nand no specific Storage Class was specified in the Redis Enterprise Cluster Spec, the installation of the cluster could fail.\nTo avoid such scenario in cases where a specific Storage Class isn't specified, make sure the Kubernetes Cluster has a default Storage Cluster configured.\nThis can be done by running a `kubectl get storageclass` command and see if one of the Storage Classes' names contains a `(default)` mark.",
      "csv_display_name": "Redis Enterprise Operator",
      "csv_metadata_description": "",
      "csv_name": "redis-enterprise-operator.v6.2.18-3a",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-12-05T09:25:50.535000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.11",
      "organization": "certified-operators",
      "package": "redis-enterprise-operator-cert",
      "provided_apis": [
        {
          "group": "app.redislabs.com",
          "kind": "RedisEnterpriseCluster",
          "version": "v1"
        },
        {
          "group": "app.redislabs.com",
          "kind": "RedisEnterpriseCluster",
          "version": "v1alpha1"
        },
        {
          "group": "app.redislabs.com",
          "kind": "RedisEnterpriseDatabase",
          "version": "v1alpha1"
        }
      ],
      "provider": "Redis",
      "related_images": [
        {
          "digest": "sha256:afd2c3dd1e432c410af77202e9ce262fa69f4734357480c292db73f4608ec4ec",
          "image": "registry.connect.redhat.com/redislabs/redis-enterprise-operator@sha256:afd2c3dd1e432c410af77202e9ce262fa69f4734357480c292db73f4608ec4ec",
          "name": "redis-enterprise-operator"
        },
        {
          "digest": "sha256:afd2c3dd1e432c410af77202e9ce262fa69f4734357480c292db73f4608ec4ec",
          "image": "registry.connect.redhat.com/redislabs/redis-enterprise-operator@sha256:afd2c3dd1e432c410af77202e9ce262fa69f4734357480c292db73f4608ec4ec",
          "name": "admission-image"
        },
        {
          "digest": "sha256:afd2c3dd1e432c410af77202e9ce262fa69f4734357480c292db73f4608ec4ec",
          "image": "registry.connect.redhat.com/redislabs/redis-enterprise-operator@sha256:afd2c3dd1e432c410af77202e9ce262fa69f4734357480c292db73f4608ec4ec",
          "name": "bootstrapper-image"
        },
        {
          "digest": "sha256:1c20d9f1dc847db43aea55832caef6e497c1971d2dbcec4e8421092e469907e9",
          "image": "registry.connect.redhat.com/redislabs/services-manager@sha256:1c20d9f1dc847db43aea55832caef6e497c1971d2dbcec4e8421092e469907e9",
          "name": "k8s-controller-image"
        },
        {
          "digest": "sha256:5f83e568da6b9ae9ca8dcd91b799efd52d4afe94eeb5b0471e82914b2a987d25",
          "image": "registry.connect.redhat.com/redislabs/redis-enterprise@sha256:5f83e568da6b9ae9ca8dcd91b799efd52d4afe94eeb5b0471e82914b2a987d25",
          "name": "rs-image"
        },
        {
          "digest": "sha256:afd2c3dd1e432c410af77202e9ce262fa69f4734357480c292db73f4608ec4ec",
          "image": "registry.connect.redhat.com/redislabs/redis-enterprise-operator@sha256:afd2c3dd1e432c410af77202e9ce262fa69f4734357480c292db73f4608ec4ec",
          "name": "redis-enterprise-operator-afd2c3dd1e432c410af77202e9ce262fa69f4734357480c292db73f4608ec4ec-annotation"
        },
        {
          "digest": "sha256:afd2c3dd1e432c410af77202e9ce262fa69f4734357480c292db73f4608ec4ec",
          "image": "registry.connect.redhat.com/redislabs/redis-enterprise-operator@sha256:afd2c3dd1e432c410af77202e9ce262fa69f4734357480c292db73f4608ec4ec",
          "name": "admission"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.11",
      "version": "6.2.18-3a",
      "version_original": "6.2.18-3a"
    },
    {
      "_id": "638e3b8424d0beb3100e7897",
      "alm_examples": [
        {
          "api_version": "infoscale.veritas.com/v1",
          "kind": "DNS",
          "metadata": {
            "name": "test-dns"
          },
          "spec": {
            "domain": "vcslnxdns.com",
            "resRecord": {
              "abc": "r7515-054-vm9",
              "r7515-054-vm10": "10.221.85.83",
              "r7515-054-vm8": "10.221.85.81",
              "r7515-054-vm9": "10.221.85.82",
              "www": "r7515-054-vm8",
              "xyz": "r7515-054-vm10"
            },
            "stealthMasters": [
              "1.2.3.4"
            ],
            "tsigKeyFile": "/Kvcslnxdns.com.+157+10641.private"
          }
        },
        {
          "api_version": "infoscale.veritas.com/v1",
          "kind": "DataReplication",
          "metadata": {
            "name": "test-datareplication"
          },
          "spec": {
            "cloudVendor": "Local",
            "currentPrimary": "Clus1",
            "force": false,
            "lbEnabled": false,
            "localHostAddress": "10.0.0.1",
            "localNIC": "eth0",
            "localNetMask": "255.255.240.0",
            "remoteClusterDetails": [
              {
                "bandwidthLimit": "N/A",
                "clusterName": "Clus2",
                "latencyProtection": "disable",
                "logProtection": "autodcm",
                "networkTransportProtocol": "TCP",
                "remoteCloudVendor": "Local",
                "remoteHostAddress": "10.0.0.2",
                "remoteLbEnabled": false,
                "remoteNIC": "eth0",
                "remoteNetMask": "255.255.240.0",
                "replicationState": "start",
                "replicationType": "async"
              }
            ],
            "selector": {
              "namespace": "mysql"
            }
          }
        },
        {
          "api_version": "infoscale.veritas.com/v1",
          "kind": "DisasterRecoveryPlan",
          "metadata": {
            "name": "test-disaster-recovery-plan"
          },
          "spec": {
            "clusterFailOverPolicy": "Manual",
            "dataReplicationPointer": "test-datareplication",
            "force": false,
            "preferredClusterList": [
              "Clus1",
              "Clus2"
            ],
            "primaryCluster": "Clus1",
            "selector": {
              "namespace": "mysql"
            }
          }
        },
        {
          "api_version": "infoscale.veritas.com/v1",
          "kind": "GlobalClusterMembership",
          "metadata": {
            "name": "global-cluster-membership"
          },
          "spec": {
            "backupClusterScopeCRD": true,
            "counterMissTolerance": 5,
            "datarepRefreshStatusFrequency": 10,
            "globalClusterOperation": "none",
            "globalMemberClusters": [
              {
                "clusterID": "Clus1",
                "drControllerAddress": "10.0.10.1",
                "drControllerPort": "8080"
              },
              {
                "clusterID": "Clus2",
                "drControllerAddress": "10.0.10.2",
                "drControllerPort": "9090"
              }
            ],
            "localClusterName": "Clus1",
            "maximumMetadataCopies": 5,
            "metadataBackupInterval": 15
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/veritas-technologies/infoscale-dr-manager-bundle@sha256:705aadff4e50585c7bc08e022ef79979f6616cebd9ea55889c3c45e0d8f3fe91",
      "bundle_path_digest": "sha256:705aadff4e50585c7bc08e022ef79979f6616cebd9ea55889c3c45e0d8f3fe91",
      "capabilities": [
        "Seamless Upgrades"
      ],
      "channel_name": "stable",
      "creation_date": "2022-12-05T18:42:12.136000+00:00",
      "csv_description": "## InfoScale\u2122 DR Manager\n\nInfoScale\u2122 DR Manager manages the Disaster Recovery operations of InfoScale\u2122 for container environment\n\n## Overview\n\n- Disaster Recovery(DR) is provided to applications hosted in container ecosystems. Native container HA capabilities provide high availability to application components within a cluster. However, DR functionality provides disaster recovery in the event of a entire cluster failure and application components can be restored on another peer cluster in membership.\n- You can form a logical notion called 'Global Cluster' comprising clusters that can be used to migrate DR-enabled objects. DR-enabled objects migrate to peer cluster in case of a disaster like entire cluster going down, loss of connectivity with a particular cluster, user-initiated planned migration across cluster.\n\n---\n\n## Pre-requisites\n\n**1.** InfoScale must be installed and InfoScale pods must be configured on the clusters. [[More Details](https://www.veritas.com/content/support/en_US/doc/155483965-155483981-0/v155481520-155483981)]\n\n**2.** Create storage class with name, which must always be **`infoscale-dr-csi-sc`.**\n\n```\napiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: infoscale-dr-csi-sc\nparameters:\n  fstype: vxfs\n  layout: mirror\nallowVolumeExpansion: true\nprovisioner: org.veritas.infoscale\nreclaimPolicy: Delete\n\n```\n\n**3.** Create persistent volume claim with name, which must always be **`infoscale-dr-meta-bkp-pvc`.**\n\n```\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: infoscale-dr-meta-bkp-pvc\nspec:\n  accessModes:\n  - ReadWriteOnce\n  resources:\n    requests:\n      storage: 4Gi\n  storageClassName: infoscale-dr-csi-sc\n  volumeMode: Filesystem\n\n```\n\n- **Note:** User must use InfoScale\u2122 CSI provisioned storage.\n\n- [For more details, please refer to 'Installing InfoScale DR Manager on openshift' section from official documentation](https://www.veritas.com/content/support/en_US/doc/155483965-155483981-0/v151984431-155483981)\n\n## InfoScale\u2122 DR Manager custom resources\n\n- **Note:** InfoScale\u2122 DR Manager custom resources must be created only in the following order\n\n**1. Global Cluster Membership:**\n\n```\napiVersion: infoscale.veritas.com/v1\nkind: GlobalClusterMembership\nmetadata:\n  name: global-cluster-membership\nspec:\n  # Local cluster name in the global membership\n  localClusterName: Clus1\n\n  globalMemberClusters:\n      # Cluster ID of member of global cluster membership\n    - clusterID: Clus1\n\n      # Address Used For Communicating With Peer Cluster's DR Controller\n      drControllerAddress: \"10.0.10.1\"\n\n      # Port used for DR controller\n      drControllerPort: \"8080\"\n\n      # Cluster ID of member of global cluster membership\n    - clusterID: Clus2\n\n      # Address Used For Communicating With Peer Cluster's DR Controller\n      drControllerAddress: \"10.0.10.2\"\n\n      # Port used for DR controller\n      drControllerPort: \"9090\"\n\n```\n\n**2. Data Replication:**\n\n```\napiVersion: infoscale.veritas.com/v1\nkind: DataReplication\nmetadata:\n  name: test-datareplication\nspec:\n  # Primary cluster details\n\n  # Virtual IP address to configure VVR\n  localHostAddress: 10.0.0.1\n\n  # Corresponding netmask to configure VVR\n  localNetMask: 255.255.240.0\n\n  # Corresponding network interface to configure VVR (If NIC name is identical for all nodes)\n  localNIC: eth0\n\n  # Namespace and optional labels for which you want to configure data replication\n  selector:\n    namespace: mysql\n    #labels:\n    #  app: db\n\n  # Current primary cluster name - Name of the cluster you want to back up\n  currentPrimary: Clus1\n\n  # Secondary cluster details\n  remoteClusterDetails:\n      # ID of the Cluster to be used for a backup\n    - clusterName: Clus2\n\n      # Virtual IP address for VVR configuration of this cluster\n      remoteHostAddress: 10.0.0.2\n\n      # Corresponding Netmask of this cluster\n      remoteNetMask: 255.255.240.0\n\n      # Corresponding Network interface of this cluster\n      remoteNIC: eth0\n\n      # (optional) replication type can be sync or async.\n      # default value will be async if not specified.\n      replicationType: async\n\n```\n**3. Disaster Recovery Plan:**\n\n```\napiVersion: infoscale.veritas.com/v1\nkind: DisasterRecoveryPlan\nmetadata:\n  name: test-disaster-recovery-plan\nspec:\n  # Name of cluster that should be treated as primary for this DR plan\n  primaryCluster: Clus1\n\n  # (optional) Set force to True if peer cluster(s) is not reachable and local cluster needs to perform takeover\n  force: false\n\n  # List of member cluster(s) where this DRPlan can failover.\n  # Sequence of MemberCluster specified in this list denotes relative preference of member cluster(s)\n  # Must be subset of Global Cluster Membership\n  preferredClusterList: [\"Clus1\", \"Clus2\"]\n\n  # Kind of corrective action in case of disaster\n  # default value will be \"Manual\" if not specified\n  clusterFailOverPolicy: Manual\n\n  # Specify namespace and optional labels to decide what all needs to be part of the disaster recovery plan\n  selector:\n    namespace: mysql\n    #labels:\n    #  app: db\n\n  # (optional) Pointer to manage data replication\n  dataReplicationPointer: test-datareplication\n\n```\n",
      "csv_display_name": "InfoScale\u2122 DR Manager",
      "csv_metadata_description": "InfoScale\u2122 DR Manager manages the Disaster Recovery operations of InfoScale\u2122 for container environment",
      "csv_name": "infoscale-dr-manager.v8.0.201",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": false,
          "type": "OwnNamespace"
        },
        {
          "supported": false,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-12-05T18:42:12.136000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.11",
      "organization": "certified-operators",
      "package": "infoscale-dr-manager",
      "provided_apis": [
        {
          "group": "infoscale.veritas.com",
          "kind": "DNS",
          "version": "v1"
        },
        {
          "group": "infoscale.veritas.com",
          "kind": "DataReplication",
          "version": "v1"
        },
        {
          "group": "infoscale.veritas.com",
          "kind": "DisasterRecoveryPlan",
          "version": "v1"
        },
        {
          "group": "infoscale.veritas.com",
          "kind": "GlobalClusterMembership",
          "version": "v1"
        }
      ],
      "provider": "Veritas Technologies LLC",
      "related_images": [
        {
          "digest": "sha256:309095679cde718063f9559fc473148044fd5292c5d70add0a4b5a6432fc249d",
          "image": "registry.connect.redhat.com/veritas-technologies/infoscale-dr-manager@sha256:309095679cde718063f9559fc473148044fd5292c5d70add0a4b5a6432fc249d",
          "name": "manager"
        },
        {
          "digest": "sha256:309095679cde718063f9559fc473148044fd5292c5d70add0a4b5a6432fc249d",
          "image": "registry.connect.redhat.com/veritas-technologies/infoscale-dr-manager@sha256:309095679cde718063f9559fc473148044fd5292c5d70add0a4b5a6432fc249d",
          "name": "pre-manager"
        },
        {
          "digest": "sha256:309095679cde718063f9559fc473148044fd5292c5d70add0a4b5a6432fc249d",
          "image": "registry.connect.redhat.com/veritas-technologies/infoscale-dr-manager@sha256:309095679cde718063f9559fc473148044fd5292c5d70add0a4b5a6432fc249d",
          "name": "infoscale-dr-manager-309095679cde718063f9559fc473148044fd5292c5d70add0a4b5a6432fc249d-annotation"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.11",
      "version": "8.0.201",
      "version_original": "8.0.201"
    },
    {
      "_id": "638e3d2ea9e0e29d92073d36",
      "alm_examples": [
        {
          "api_version": "infoscale.veritas.com/v1",
          "kind": "DNS",
          "metadata": {
            "name": "test-dns"
          },
          "spec": {
            "domain": "vcslnxdns.com",
            "resRecord": {
              "abc": "r7515-054-vm9",
              "r7515-054-vm10": "10.221.85.83",
              "r7515-054-vm8": "10.221.85.81",
              "r7515-054-vm9": "10.221.85.82",
              "www": "r7515-054-vm8",
              "xyz": "r7515-054-vm10"
            },
            "stealthMasters": [
              "1.2.3.4"
            ],
            "tsigKeyFile": "/Kvcslnxdns.com.+157+10641.private"
          }
        },
        {
          "api_version": "infoscale.veritas.com/v1",
          "kind": "DataReplication",
          "metadata": {
            "name": "test-datareplication"
          },
          "spec": {
            "cloudVendor": "Local",
            "currentPrimary": "Clus1",
            "force": false,
            "lbEnabled": false,
            "localHostAddress": "10.0.0.1",
            "localNIC": "eth0",
            "localNetMask": "255.255.240.0",
            "remoteClusterDetails": [
              {
                "bandwidthLimit": "N/A",
                "clusterName": "Clus2",
                "latencyProtection": "disable",
                "logProtection": "autodcm",
                "networkTransportProtocol": "TCP",
                "remoteCloudVendor": "Local",
                "remoteHostAddress": "10.0.0.2",
                "remoteLbEnabled": false,
                "remoteNIC": "eth0",
                "remoteNetMask": "255.255.240.0",
                "replicationState": "start",
                "replicationType": "async"
              }
            ],
            "selector": {
              "namespace": "mysql"
            }
          }
        },
        {
          "api_version": "infoscale.veritas.com/v1",
          "kind": "DisasterRecoveryPlan",
          "metadata": {
            "name": "test-disaster-recovery-plan"
          },
          "spec": {
            "clusterFailOverPolicy": "Manual",
            "dataReplicationPointer": "test-datareplication",
            "force": false,
            "preferredClusterList": [
              "Clus1",
              "Clus2"
            ],
            "primaryCluster": "Clus1",
            "selector": {
              "namespace": "mysql"
            }
          }
        },
        {
          "api_version": "infoscale.veritas.com/v1",
          "kind": "GlobalClusterMembership",
          "metadata": {
            "name": "global-cluster-membership"
          },
          "spec": {
            "backupClusterScopeCRD": true,
            "counterMissTolerance": 5,
            "datarepRefreshStatusFrequency": 10,
            "globalClusterOperation": "none",
            "globalMemberClusters": [
              {
                "clusterID": "Clus1",
                "drControllerAddress": "10.0.10.1",
                "drControllerPort": "8080"
              },
              {
                "clusterID": "Clus2",
                "drControllerAddress": "10.0.10.2",
                "drControllerPort": "9090"
              }
            ],
            "localClusterName": "Clus1",
            "maximumMetadataCopies": 5,
            "metadataBackupInterval": 15
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/veritas-technologies/infoscale-dr-manager-bundle@sha256:705aadff4e50585c7bc08e022ef79979f6616cebd9ea55889c3c45e0d8f3fe91",
      "bundle_path_digest": "sha256:705aadff4e50585c7bc08e022ef79979f6616cebd9ea55889c3c45e0d8f3fe91",
      "capabilities": [
        "Seamless Upgrades"
      ],
      "channel_name": "stable",
      "creation_date": "2022-12-05T18:49:18.512000+00:00",
      "csv_description": "## InfoScale\u2122 DR Manager\n\nInfoScale\u2122 DR Manager manages the Disaster Recovery operations of InfoScale\u2122 for container environment\n\n## Overview\n\n- Disaster Recovery(DR) is provided to applications hosted in container ecosystems. Native container HA capabilities provide high availability to application components within a cluster. However, DR functionality provides disaster recovery in the event of a entire cluster failure and application components can be restored on another peer cluster in membership.\n- You can form a logical notion called 'Global Cluster' comprising clusters that can be used to migrate DR-enabled objects. DR-enabled objects migrate to peer cluster in case of a disaster like entire cluster going down, loss of connectivity with a particular cluster, user-initiated planned migration across cluster.\n\n---\n\n## Pre-requisites\n\n**1.** InfoScale must be installed and InfoScale pods must be configured on the clusters. [[More Details](https://www.veritas.com/content/support/en_US/doc/155483965-155483981-0/v155481520-155483981)]\n\n**2.** Create storage class with name, which must always be **`infoscale-dr-csi-sc`.**\n\n```\napiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: infoscale-dr-csi-sc\nparameters:\n  fstype: vxfs\n  layout: mirror\nallowVolumeExpansion: true\nprovisioner: org.veritas.infoscale\nreclaimPolicy: Delete\n\n```\n\n**3.** Create persistent volume claim with name, which must always be **`infoscale-dr-meta-bkp-pvc`.**\n\n```\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: infoscale-dr-meta-bkp-pvc\nspec:\n  accessModes:\n  - ReadWriteOnce\n  resources:\n    requests:\n      storage: 4Gi\n  storageClassName: infoscale-dr-csi-sc\n  volumeMode: Filesystem\n\n```\n\n- **Note:** User must use InfoScale\u2122 CSI provisioned storage.\n\n- [For more details, please refer to 'Installing InfoScale DR Manager on openshift' section from official documentation](https://www.veritas.com/content/support/en_US/doc/155483965-155483981-0/v151984431-155483981)\n\n## InfoScale\u2122 DR Manager custom resources\n\n- **Note:** InfoScale\u2122 DR Manager custom resources must be created only in the following order\n\n**1. Global Cluster Membership:**\n\n```\napiVersion: infoscale.veritas.com/v1\nkind: GlobalClusterMembership\nmetadata:\n  name: global-cluster-membership\nspec:\n  # Local cluster name in the global membership\n  localClusterName: Clus1\n\n  globalMemberClusters:\n      # Cluster ID of member of global cluster membership\n    - clusterID: Clus1\n\n      # Address Used For Communicating With Peer Cluster's DR Controller\n      drControllerAddress: \"10.0.10.1\"\n\n      # Port used for DR controller\n      drControllerPort: \"8080\"\n\n      # Cluster ID of member of global cluster membership\n    - clusterID: Clus2\n\n      # Address Used For Communicating With Peer Cluster's DR Controller\n      drControllerAddress: \"10.0.10.2\"\n\n      # Port used for DR controller\n      drControllerPort: \"9090\"\n\n```\n\n**2. Data Replication:**\n\n```\napiVersion: infoscale.veritas.com/v1\nkind: DataReplication\nmetadata:\n  name: test-datareplication\nspec:\n  # Primary cluster details\n\n  # Virtual IP address to configure VVR\n  localHostAddress: 10.0.0.1\n\n  # Corresponding netmask to configure VVR\n  localNetMask: 255.255.240.0\n\n  # Corresponding network interface to configure VVR (If NIC name is identical for all nodes)\n  localNIC: eth0\n\n  # Namespace and optional labels for which you want to configure data replication\n  selector:\n    namespace: mysql\n    #labels:\n    #  app: db\n\n  # Current primary cluster name - Name of the cluster you want to back up\n  currentPrimary: Clus1\n\n  # Secondary cluster details\n  remoteClusterDetails:\n      # ID of the Cluster to be used for a backup\n    - clusterName: Clus2\n\n      # Virtual IP address for VVR configuration of this cluster\n      remoteHostAddress: 10.0.0.2\n\n      # Corresponding Netmask of this cluster\n      remoteNetMask: 255.255.240.0\n\n      # Corresponding Network interface of this cluster\n      remoteNIC: eth0\n\n      # (optional) replication type can be sync or async.\n      # default value will be async if not specified.\n      replicationType: async\n\n```\n**3. Disaster Recovery Plan:**\n\n```\napiVersion: infoscale.veritas.com/v1\nkind: DisasterRecoveryPlan\nmetadata:\n  name: test-disaster-recovery-plan\nspec:\n  # Name of cluster that should be treated as primary for this DR plan\n  primaryCluster: Clus1\n\n  # (optional) Set force to True if peer cluster(s) is not reachable and local cluster needs to perform takeover\n  force: false\n\n  # List of member cluster(s) where this DRPlan can failover.\n  # Sequence of MemberCluster specified in this list denotes relative preference of member cluster(s)\n  # Must be subset of Global Cluster Membership\n  preferredClusterList: [\"Clus1\", \"Clus2\"]\n\n  # Kind of corrective action in case of disaster\n  # default value will be \"Manual\" if not specified\n  clusterFailOverPolicy: Manual\n\n  # Specify namespace and optional labels to decide what all needs to be part of the disaster recovery plan\n  selector:\n    namespace: mysql\n    #labels:\n    #  app: db\n\n  # (optional) Pointer to manage data replication\n  dataReplicationPointer: test-datareplication\n\n```\n",
      "csv_display_name": "InfoScale\u2122 DR Manager",
      "csv_metadata_description": "InfoScale\u2122 DR Manager manages the Disaster Recovery operations of InfoScale\u2122 for container environment",
      "csv_name": "infoscale-dr-manager.v8.0.201",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": false,
          "type": "OwnNamespace"
        },
        {
          "supported": false,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-12-05T18:49:18.512000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.10",
      "organization": "certified-operators",
      "package": "infoscale-dr-manager",
      "provided_apis": [
        {
          "group": "infoscale.veritas.com",
          "kind": "DNS",
          "plural": "dns",
          "version": "v1"
        },
        {
          "group": "infoscale.veritas.com",
          "kind": "GlobalClusterMembership",
          "plural": "globalclustermemberships",
          "version": "v1"
        },
        {
          "group": "infoscale.veritas.com",
          "kind": "DataReplication",
          "plural": "datareplications",
          "version": "v1"
        },
        {
          "group": "infoscale.veritas.com",
          "kind": "DisasterRecoveryPlan",
          "plural": "disasterrecoveryplans",
          "version": "v1"
        }
      ],
      "provider": "Veritas Technologies LLC",
      "related_images": [
        {
          "digest": "sha256:309095679cde718063f9559fc473148044fd5292c5d70add0a4b5a6432fc249d",
          "image": "registry.connect.redhat.com/veritas-technologies/infoscale-dr-manager@sha256:309095679cde718063f9559fc473148044fd5292c5d70add0a4b5a6432fc249d",
          "name": "manager"
        },
        {
          "digest": "sha256:309095679cde718063f9559fc473148044fd5292c5d70add0a4b5a6432fc249d",
          "image": "registry.connect.redhat.com/veritas-technologies/infoscale-dr-manager@sha256:309095679cde718063f9559fc473148044fd5292c5d70add0a4b5a6432fc249d",
          "name": "pre-manager"
        },
        {
          "digest": "sha256:309095679cde718063f9559fc473148044fd5292c5d70add0a4b5a6432fc249d",
          "image": "registry.connect.redhat.com/veritas-technologies/infoscale-dr-manager@sha256:309095679cde718063f9559fc473148044fd5292c5d70add0a4b5a6432fc249d",
          "name": "infoscale-dr-manager-309095679cde718063f9559fc473148044fd5292c5d70add0a4b5a6432fc249d-annotation"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.10",
      "version": "8.0.201",
      "version_original": "8.0.201"
    },
    {
      "_id": "638e3d4bb5dcf804fdd48024",
      "alm_examples": [
        {
          "api_version": "infoscale.veritas.com/v1",
          "kind": "DNS",
          "metadata": {
            "name": "test-dns"
          },
          "spec": {
            "domain": "vcslnxdns.com",
            "resRecord": {
              "abc": "r7515-054-vm9",
              "r7515-054-vm10": "10.221.85.83",
              "r7515-054-vm8": "10.221.85.81",
              "r7515-054-vm9": "10.221.85.82",
              "www": "r7515-054-vm8",
              "xyz": "r7515-054-vm10"
            },
            "stealthMasters": [
              "1.2.3.4"
            ],
            "tsigKeyFile": "/Kvcslnxdns.com.+157+10641.private"
          }
        },
        {
          "api_version": "infoscale.veritas.com/v1",
          "kind": "DataReplication",
          "metadata": {
            "name": "test-datareplication"
          },
          "spec": {
            "cloudVendor": "Local",
            "currentPrimary": "Clus1",
            "force": false,
            "lbEnabled": false,
            "localHostAddress": "10.0.0.1",
            "localNIC": "eth0",
            "localNetMask": "255.255.240.0",
            "remoteClusterDetails": [
              {
                "bandwidthLimit": "N/A",
                "clusterName": "Clus2",
                "latencyProtection": "disable",
                "logProtection": "autodcm",
                "networkTransportProtocol": "TCP",
                "remoteCloudVendor": "Local",
                "remoteHostAddress": "10.0.0.2",
                "remoteLbEnabled": false,
                "remoteNIC": "eth0",
                "remoteNetMask": "255.255.240.0",
                "replicationState": "start",
                "replicationType": "async"
              }
            ],
            "selector": {
              "namespace": "mysql"
            }
          }
        },
        {
          "api_version": "infoscale.veritas.com/v1",
          "kind": "DisasterRecoveryPlan",
          "metadata": {
            "name": "test-disaster-recovery-plan"
          },
          "spec": {
            "clusterFailOverPolicy": "Manual",
            "dataReplicationPointer": "test-datareplication",
            "force": false,
            "preferredClusterList": [
              "Clus1",
              "Clus2"
            ],
            "primaryCluster": "Clus1",
            "selector": {
              "namespace": "mysql"
            }
          }
        },
        {
          "api_version": "infoscale.veritas.com/v1",
          "kind": "GlobalClusterMembership",
          "metadata": {
            "name": "global-cluster-membership"
          },
          "spec": {
            "backupClusterScopeCRD": true,
            "counterMissTolerance": 5,
            "datarepRefreshStatusFrequency": 10,
            "globalClusterOperation": "none",
            "globalMemberClusters": [
              {
                "clusterID": "Clus1",
                "drControllerAddress": "10.0.10.1",
                "drControllerPort": "8080"
              },
              {
                "clusterID": "Clus2",
                "drControllerAddress": "10.0.10.2",
                "drControllerPort": "9090"
              }
            ],
            "localClusterName": "Clus1",
            "maximumMetadataCopies": 5,
            "metadataBackupInterval": 15
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/veritas-technologies/infoscale-dr-manager-bundle@sha256:705aadff4e50585c7bc08e022ef79979f6616cebd9ea55889c3c45e0d8f3fe91",
      "bundle_path_digest": "sha256:705aadff4e50585c7bc08e022ef79979f6616cebd9ea55889c3c45e0d8f3fe91",
      "capabilities": [
        "Seamless Upgrades"
      ],
      "channel_name": "stable",
      "creation_date": "2022-12-05T18:49:47.446000+00:00",
      "csv_description": "## InfoScale\u2122 DR Manager\n\nInfoScale\u2122 DR Manager manages the Disaster Recovery operations of InfoScale\u2122 for container environment\n\n## Overview\n\n- Disaster Recovery(DR) is provided to applications hosted in container ecosystems. Native container HA capabilities provide high availability to application components within a cluster. However, DR functionality provides disaster recovery in the event of a entire cluster failure and application components can be restored on another peer cluster in membership.\n- You can form a logical notion called 'Global Cluster' comprising clusters that can be used to migrate DR-enabled objects. DR-enabled objects migrate to peer cluster in case of a disaster like entire cluster going down, loss of connectivity with a particular cluster, user-initiated planned migration across cluster.\n\n---\n\n## Pre-requisites\n\n**1.** InfoScale must be installed and InfoScale pods must be configured on the clusters. [[More Details](https://www.veritas.com/content/support/en_US/doc/155483965-155483981-0/v155481520-155483981)]\n\n**2.** Create storage class with name, which must always be **`infoscale-dr-csi-sc`.**\n\n```\napiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: infoscale-dr-csi-sc\nparameters:\n  fstype: vxfs\n  layout: mirror\nallowVolumeExpansion: true\nprovisioner: org.veritas.infoscale\nreclaimPolicy: Delete\n\n```\n\n**3.** Create persistent volume claim with name, which must always be **`infoscale-dr-meta-bkp-pvc`.**\n\n```\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: infoscale-dr-meta-bkp-pvc\nspec:\n  accessModes:\n  - ReadWriteOnce\n  resources:\n    requests:\n      storage: 4Gi\n  storageClassName: infoscale-dr-csi-sc\n  volumeMode: Filesystem\n\n```\n\n- **Note:** User must use InfoScale\u2122 CSI provisioned storage.\n\n- [For more details, please refer to 'Installing InfoScale DR Manager on openshift' section from official documentation](https://www.veritas.com/content/support/en_US/doc/155483965-155483981-0/v151984431-155483981)\n\n## InfoScale\u2122 DR Manager custom resources\n\n- **Note:** InfoScale\u2122 DR Manager custom resources must be created only in the following order\n\n**1. Global Cluster Membership:**\n\n```\napiVersion: infoscale.veritas.com/v1\nkind: GlobalClusterMembership\nmetadata:\n  name: global-cluster-membership\nspec:\n  # Local cluster name in the global membership\n  localClusterName: Clus1\n\n  globalMemberClusters:\n      # Cluster ID of member of global cluster membership\n    - clusterID: Clus1\n\n      # Address Used For Communicating With Peer Cluster's DR Controller\n      drControllerAddress: \"10.0.10.1\"\n\n      # Port used for DR controller\n      drControllerPort: \"8080\"\n\n      # Cluster ID of member of global cluster membership\n    - clusterID: Clus2\n\n      # Address Used For Communicating With Peer Cluster's DR Controller\n      drControllerAddress: \"10.0.10.2\"\n\n      # Port used for DR controller\n      drControllerPort: \"9090\"\n\n```\n\n**2. Data Replication:**\n\n```\napiVersion: infoscale.veritas.com/v1\nkind: DataReplication\nmetadata:\n  name: test-datareplication\nspec:\n  # Primary cluster details\n\n  # Virtual IP address to configure VVR\n  localHostAddress: 10.0.0.1\n\n  # Corresponding netmask to configure VVR\n  localNetMask: 255.255.240.0\n\n  # Corresponding network interface to configure VVR (If NIC name is identical for all nodes)\n  localNIC: eth0\n\n  # Namespace and optional labels for which you want to configure data replication\n  selector:\n    namespace: mysql\n    #labels:\n    #  app: db\n\n  # Current primary cluster name - Name of the cluster you want to back up\n  currentPrimary: Clus1\n\n  # Secondary cluster details\n  remoteClusterDetails:\n      # ID of the Cluster to be used for a backup\n    - clusterName: Clus2\n\n      # Virtual IP address for VVR configuration of this cluster\n      remoteHostAddress: 10.0.0.2\n\n      # Corresponding Netmask of this cluster\n      remoteNetMask: 255.255.240.0\n\n      # Corresponding Network interface of this cluster\n      remoteNIC: eth0\n\n      # (optional) replication type can be sync or async.\n      # default value will be async if not specified.\n      replicationType: async\n\n```\n**3. Disaster Recovery Plan:**\n\n```\napiVersion: infoscale.veritas.com/v1\nkind: DisasterRecoveryPlan\nmetadata:\n  name: test-disaster-recovery-plan\nspec:\n  # Name of cluster that should be treated as primary for this DR plan\n  primaryCluster: Clus1\n\n  # (optional) Set force to True if peer cluster(s) is not reachable and local cluster needs to perform takeover\n  force: false\n\n  # List of member cluster(s) where this DRPlan can failover.\n  # Sequence of MemberCluster specified in this list denotes relative preference of member cluster(s)\n  # Must be subset of Global Cluster Membership\n  preferredClusterList: [\"Clus1\", \"Clus2\"]\n\n  # Kind of corrective action in case of disaster\n  # default value will be \"Manual\" if not specified\n  clusterFailOverPolicy: Manual\n\n  # Specify namespace and optional labels to decide what all needs to be part of the disaster recovery plan\n  selector:\n    namespace: mysql\n    #labels:\n    #  app: db\n\n  # (optional) Pointer to manage data replication\n  dataReplicationPointer: test-datareplication\n\n```\n",
      "csv_display_name": "InfoScale\u2122 DR Manager",
      "csv_metadata_description": "InfoScale\u2122 DR Manager manages the Disaster Recovery operations of InfoScale\u2122 for container environment",
      "csv_name": "infoscale-dr-manager.v8.0.201",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": false,
          "type": "OwnNamespace"
        },
        {
          "supported": false,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-12-05T18:49:47.446000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.9",
      "organization": "certified-operators",
      "package": "infoscale-dr-manager",
      "provided_apis": [
        {
          "group": "infoscale.veritas.com",
          "kind": "DataReplication",
          "plural": "datareplications",
          "version": "v1"
        },
        {
          "group": "infoscale.veritas.com",
          "kind": "DisasterRecoveryPlan",
          "plural": "disasterrecoveryplans",
          "version": "v1"
        },
        {
          "group": "infoscale.veritas.com",
          "kind": "DNS",
          "plural": "dns",
          "version": "v1"
        },
        {
          "group": "infoscale.veritas.com",
          "kind": "GlobalClusterMembership",
          "plural": "globalclustermemberships",
          "version": "v1"
        }
      ],
      "provider": "Veritas Technologies LLC",
      "related_images": [
        {
          "digest": "sha256:309095679cde718063f9559fc473148044fd5292c5d70add0a4b5a6432fc249d",
          "image": "registry.connect.redhat.com/veritas-technologies/infoscale-dr-manager@sha256:309095679cde718063f9559fc473148044fd5292c5d70add0a4b5a6432fc249d",
          "name": "manager"
        },
        {
          "digest": "sha256:309095679cde718063f9559fc473148044fd5292c5d70add0a4b5a6432fc249d",
          "image": "registry.connect.redhat.com/veritas-technologies/infoscale-dr-manager@sha256:309095679cde718063f9559fc473148044fd5292c5d70add0a4b5a6432fc249d",
          "name": "pre-manager"
        },
        {
          "digest": "sha256:309095679cde718063f9559fc473148044fd5292c5d70add0a4b5a6432fc249d",
          "image": "registry.connect.redhat.com/veritas-technologies/infoscale-dr-manager@sha256:309095679cde718063f9559fc473148044fd5292c5d70add0a4b5a6432fc249d",
          "name": "infoscale-dr-manager-309095679cde718063f9559fc473148044fd5292c5d70add0a4b5a6432fc249d-annotation"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.9",
      "version": "8.0.201",
      "version_original": "8.0.201"
    }
  ]
}
